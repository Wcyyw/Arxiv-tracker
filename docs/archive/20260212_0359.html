<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 03:59</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0359</div>
    <div class="row"><div class="card">
<div class="title">ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation</div>
<div class="meta-line">Authors: Mingyang Wu, Ashirbad Mishra, Soumik Dey, Shuo Xing, Naveen Ravipati, Hansi Wu, Binbin Li, Zhengzhong Tu</div>
<div class="meta-line">First: 2026-02-10T18:59:51+00:00 · Latest: 2026-02-10T18:59:51+00:00</div>
<div class="meta-line">Comments: Project page: https://myangwu.github.io/ConsID-Gen</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10113v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://myangwu.github.io/ConsID-Gen">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConsID-Gen：视图一致性与身份保持的图像到视频生成</div>
<div class="mono" style="margin-top:8px">图像到视频生成（I2V）将静态图像动画化为遵循文本指令的时间一致视频序列，但在变化视角下保持细粒度对象身份仍然是一个持续的挑战。与文本到视频模型不同，现有的I2V管道往往受到外观漂移和几何失真的影响，这些伪影归因于单视图2D观察的稀疏性和跨模态对齐的弱性。我们从数据和模型的角度解决这个问题。首先，我们策划了ConsIDVid，这是一个大型对象中心数据集，采用可扩展管道构建高质量、时间对齐的视频，并建立了ConsIDVid-Bench，在这里我们提出了一种新颖的基准和评估框架，用于使用对细微几何和外观偏差敏感的指标进行多视图一致性评估。我们进一步提出了ConsID-Gen，这是一种视图辅助的I2V生成框架，通过未摆姿的辅助视图增强第一帧，并通过双流视觉-几何编码器以及文本-视觉连接器融合语义和结构线索，为扩散变换器主干提供统一的条件。ConsIDVid-Bench上的实验表明，ConsID-Gen在多个指标上始终表现优异，最佳整体性能超越了领先的视频生成模型，如Wan2.1和HunyuanVideo，在具有挑战性的现实场景中提供了卓越的身份保真度和时间一致性。我们将在https://myangwu.github.io/ConsID-Gen发布我们的模型和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image-to-video generation (I2V) by addressing the challenges of maintaining object identity and coherence in videos generated from static images. The authors developed a large-scale dataset called ConsIDVid and a benchmarking framework, ConsIDVid-Bench, to evaluate multi-view consistency. They proposed a novel I2V generation framework, ConsID-Gen, which utilizes auxiliary views and a dual-stream encoder to enhance the quality of generated videos. Experimental results show that ConsID-Gen outperforms existing models in terms of identity fidelity and temporal coherence, particularly in complex real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在不同视角下保持物体身份一致性的问题，即将静态图像动画化为连贯的视频序列。作者开发了一个名为ConsIDVid的大规模数据集，其中包含高质量、时间对齐的视频，并引入了ConsIDVid-Bench，一个用于评估多视角一致性的基准框架。他们提出了ConsID-Gen，一个视图辅助的图像到视频生成框架，通过辅助视图增强初始帧，并通过双流编码器整合语义和结构信息，从而提高身份保真度和时间一致性，实验结果表明ConsID-Gen在性能上优于现有模型如Wan2.1和HunyuanVideo。</div>
</details>
</div>
<div class="card">
<div class="title">Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders</div>
<div class="meta-line">Authors: Amandeep Kumar, Vishal M. Patel</div>
<div class="meta-line">First: 2026-02-10T18:58:04+00:00 · Latest: 2026-02-10T18:58:04+00:00</div>
<div class="meta-line">Comments: Technical Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10099v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10099v1">PDF</a> · <a href="https://github.com/amandpkr/RJF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流形上的学习：通过表示编码器解锁标准扩散变换器</div>
<div class="mono" style="margin-top:8px">利用表示编码器进行生成建模为高效、高保真合成提供了一条路径。然而，标准扩散变换器无法直接在这些表示上收敛。尽管最近的研究将此归因于容量瓶颈，提出了计算成本高昂的扩散变换器宽度扩展，但我们证明这一失败在根本上是几何性的。我们识别出几何干扰是根本原因：标准的欧几里得流匹配强迫概率路径穿过表示编码器的超球面特征空间的低密度内部，而不是沿着流形表面。为了解决这个问题，我们提出了带有雅可比正则化的黎曼流匹配（RJF）。通过将生成过程约束在流形测地线并纠正由曲率引起的误差传播，RJF使标准扩散变换器架构能够在不扩展宽度的情况下收敛。我们的方法RJF使标准DiT-B架构（131M参数）有效收敛，达到FID 3.37，而之前的方法未能收敛。代码：https://github.com/amandpkr/RJF</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the convergence of standard diffusion transformers when utilizing representation encoders for generative modeling, as previous methods struggled due to geometric issues rather than capacity limitations. The authors propose a novel method called Riemannian Flow Matching with Jacobi Regularization (RJF), which constrains the generative process to follow the manifold geodesics and corrects for curvature-induced errors. Experimental results demonstrate that RJF allows the standard DiT-B architecture, with 131 million parameters, to converge effectively, achieving a Fréchet Inception Distance (FID) of 3.37, where previous approaches failed to converge.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用表示编码器高效合成高保真生成模型的挑战，因为标准扩散变换器在这些表示上难以收敛。作者确定了几何干扰是一个根本问题，传统的欧几里得流匹配不当地将概率路径引导通过超球面特征空间的低密度区域。为了解决这个问题，他们提出了带有雅可比正则化的黎曼流匹配（RJF），该方法将生成过程约束为沿流形测地线进行，并纠正了由曲率引起的误差传播。该方法使标准扩散变换器架构能够有效收敛，达到3.37的Fréchet Inception Distance（FID），显著优于之前未能收敛的方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到行动：将视觉-语言-行动模型基于空间基础先验</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-行动（VLA）模型在3D现实世界中运作，但通常基于2D编码器构建，留下了限制泛化和适应性的空间推理差距。最近的VLA 3D集成技术要么需要专用传感器且在不同模态间转移效果差，要么注入缺乏几何信息的弱线索，导致视觉-语言对齐下降。在本研究中，我们引入了FALCON（从空间到行动），一种新颖的范式，将丰富的3D空间标记注入行动头。FALCON利用空间基础模型仅通过RGB提供强大的几何先验，并包括一个具身空间模型，可以在可用时选择性地融合深度或姿态，以提高保真度，而无需重新训练或架构更改。为了保留语言推理，空间标记被空间增强行动头消耗，而不是与视觉-语言主干连接。这些设计使FALCON能够解决空间表示、模态可转移性和对齐的局限性。在三个仿真基准和十一项现实任务的全面评估中，我们提出的FALCON实现了最先进的性能，始终超越竞争基线，并在杂乱、空间提示条件和物体尺度与高度变化下保持稳健。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing vision-language-action (VLA) models that rely on 2D encoders, which hinder spatial reasoning and adaptability in 3D environments. The authors introduce FALCON, a novel paradigm that incorporates rich 3D spatial tokens into the action head, utilizing spatial foundation models to provide strong geometric priors from RGB images. Experimental results demonstrate that FALCON achieves state-of-the-art performance across three simulation benchmarks and eleven real-world tasks, consistently outperforming competitive baselines while maintaining robustness in various challenging conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有的视觉-语言-动作（VLA）模型依赖于2D编码器所带来的局限性，这限制了其在真实3D环境中的空间推理和适应能力。作者提出了FALCON这一新范式，将丰富的3D空间标记融入动作头中，利用空间基础模型从RGB输入中提供强大的几何先验。实验结果表明，FALCON在三个仿真基准和十一项真实世界任务中实现了最先进的性能，始终超越竞争基线，并在杂乱和物体尺度变化等各种条件下保持稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Disentangled Representations for Controllable Music Generation</div>
<div class="meta-line">Authors: Laura Ibáñez-Martínez, Chukwuemeka Nkama, Andrea Poltronieri, Xavier Serra, Martín Rocamora</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-10T18:25:04+00:00 · Latest: 2026-02-10T18:25:04+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10058v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10058v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估可控音乐生成的解耦表示</div>
<div class="mono" style="margin-top:8px">最近的音乐生成方法依赖于解耦表示，通常标记为结构和音色或局部和全局，以实现可控合成。然而，这些嵌入的基本属性仍然未被充分探索。在本研究中，我们在一组音乐音频模型中评估这种解耦表示，以可控生成为目标，采用超越标准下游任务的探测框架。所选模型反映了多种无监督解耦策略，包括归纳偏差、数据增强、对抗目标和分阶段训练程序。我们进一步隔离特定策略以分析其影响。我们的分析涵盖四个关键轴心：信息性、等变性、不变性和解耦性，这些在数据集、任务和受控变换中进行评估。我们的发现揭示了嵌入的预期语义与实际语义之间的不一致，表明当前策略未能产生真正的解耦表示，并促使我们重新审视音乐生成中的可控性方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the properties of disentangled representations in music generation, which are crucial for enabling controllable synthesis. The authors employed a probing-based framework to evaluate various music audio models that utilize different unsupervised disentanglement strategies, such as inductive biases and adversarial objectives. The key findings indicate inconsistencies between the intended and actual semantics of the embeddings, revealing that current methods do not achieve truly disentangled representations, thereby necessitating a reconsideration of controllability in music generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨音乐生成中解耦表示的特性，这对实现可控合成至关重要。作者采用基于探测的框架评估了多种使用不同无监督解耦策略的音乐音频模型，包括归纳偏差和对抗目标。主要发现表明，嵌入的预期语义与实际语义之间存在不一致，强调当前方法未能实现真正的解耦表示，并建议重新思考音乐生成中的可控性。</div>
</details>
</div>
<div class="card">
<div class="title">CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration</div>
<div class="meta-line">Authors: Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, Shayan Baghayi Nejad, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</div>
<div class="meta-line">First: 2025-09-22T07:51:28+00:00 · Latest: 2026-02-10T15:47:08+00:00</div>
<div class="meta-line">Comments: Accepted at TMLR (2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17458v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models, such as Stable Diffusion, can produce high-quality and diverse images but often fail to achieve compositional alignment, particularly when prompts describe complex object relationships, attributes, or spatial arrangements. Recent inference-time approaches address this by optimizing or exploring the initial noise under the guidance of reward functions that score text-image alignment without requiring model fine-tuning. While promising, each strategy has intrinsic limitations when used alone: optimization can stall due to poor initialization or unfavorable search trajectories, whereas exploration may require a prohibitively large number of samples to locate a satisfactory output. Our analysis further shows that neither single reward metrics nor ad-hoc combinations reliably capture all aspects of compositionality, leading to weak or inconsistent guidance. To overcome these challenges, we present Category-Aware Reward-based Initial Noise Optimization and Exploration (CARINOX), a unified framework that combines noise optimization and exploration with a principled reward selection procedure grounded in correlation with human judgments. Evaluations on two complementary benchmarks covering diverse compositional challenges show that CARINOX raises average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS benchmark, consistently outperforming state-of-the-art optimization and exploration-based methods across all major categories, while preserving image quality and diversity. The project page is available at https://amirkasaei.com/carinox/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARINOX：基于类别感知奖励的初始噪声优化与探索的推理时间扩展</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型，如稳定扩散，能够生成高质量和多样化的图像，但在实现组合对齐方面常常失败，尤其是当提示描述复杂的物体关系、属性或空间排列时。最近的推理时间方法通过在奖励函数的指导下优化或探索初始噪声来解决这个问题，这些奖励函数对文本-图像对齐进行评分，而无需模型微调。尽管前景看好，但每种策略在单独使用时都有固有的局限性：优化可能因初始化不良或搜索轨迹不利而停滞，而探索可能需要大量样本才能找到令人满意的输出。我们的分析进一步表明，单一奖励指标或临时组合无法可靠地捕捉组合性的所有方面，导致指导弱或不一致。为克服这些挑战，我们提出了基于类别感知奖励的初始噪声优化与探索（CARINOX），这是一个统一框架，将噪声优化和探索与基于与人类判断相关性的原则性奖励选择程序相结合。在涵盖多样化组合挑战的两个互补基准上的评估表明，CARINOX在T2I-CompBench++上将平均对齐分数提高了16%，在HRS基准上提高了11%，在所有主要类别中始终优于最先进的优化和基于探索的方法，同时保持图像质量和多样性。项目页面可访问 https://amirkasaei.com/carinox/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the compositional alignment of text-to-image diffusion models, which often struggle with complex object relationships and spatial arrangements. The authors propose CARINOX, a unified framework that integrates category-aware reward-based initial noise optimization and exploration, addressing the limitations of existing methods that either optimize or explore noise independently. Experimental results demonstrate that CARINOX significantly enhances alignment scores by 16% on T2I-CompBench++ and 11% on the HRS benchmark, outperforming state-of-the-art methods while maintaining image quality and diversity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善文本到图像扩散模型的组合对齐能力，这些模型在处理复杂的物体关系和空间排列时常常表现不佳。作者提出了一种名为CARINOX的新框架，该框架结合了基于类别的奖励初始噪声优化和探索，以增强生成过程，而无需对模型进行微调。实验结果表明，CARINOX在T2I-CompBench++上提高了+16%的对齐分数，在HRS基准上提高了+11%，在保持图像质量和多样性的同时，超越了现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Regulated Reading with AI Support: An Eight-Week Study with Students</div>
<div class="meta-line">Authors: Yue Fu, Joel Wester, Niels Van Berkel, Alexis Hiniker</div>
<div class="meta-line">First: 2026-02-10T15:41:15+00:00 · Latest: 2026-02-10T15:41:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09907v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09907v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">College students increasingly use AI chatbots to support academic reading, yet we lack granular understanding of how these interactions shape their reading experience and cognitive engagement. We conducted an eight-week longitudinal study with 15 undergraduates who used AI to support assigned readings in a course. We collected 838 prompts across 239 reading sessions and developed a coding schema categorizing prompts into four cognitive themes: Decoding, Comprehension, Reasoning, and Metacognition. Comprehension prompts dominated (59.6%), with Reasoning (29.8%), Metacognition (8.5%), and Decoding (2.1%) less frequent. Most sessions (72%) contained exactly three prompts, the required minimum of the reading assignment. Within sessions, students showed natural cognitive progression from comprehension toward reasoning, but this progression was truncated. Across eight weeks, students&#x27; engagement patterns remained stable, with substantial individual differences persisting throughout. Qualitative analysis revealed an intention-behavior gap: students recognized that effective prompting required effort but rarely applied this knowledge, with efficiency emerging as the primary driver. Students also strategically triaged their engagement based on interest and academic pressures, exhibiting a novel pattern of reading through AI rather than with it: using AI-generated summaries as primary material to filter which sections merited deeper attention. We discuss design implications for AI reading systems that scaffold sustained cognitive engagement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI支持的自我调节阅读：与学生的八周研究</div>
<div class="mono" style="margin-top:8px">大学生越来越多地使用AI聊天机器人来支持学术阅读，但我们对这些互动如何塑造他们的阅读体验和认知参与缺乏细致的理解。我们对15名本科生进行了为期八周的纵向研究，他们在一门课程中使用AI支持指定阅读。我们收集了239个阅读会话中的838个提示，并开发了一个编码方案，将提示分类为四个认知主题：解码、理解、推理和元认知。理解提示占主导地位（59.6%），推理（29.8%）、元认知（8.5%）和解码（2.1%）则较少。大多数会话（72%）恰好包含三个提示，这是阅读作业的最低要求。在会话中，学生表现出从理解到推理的自然认知进展，但这种进展被截断。在八周内，学生的参与模式保持稳定，个体差异显著。定性分析揭示了意图与行为之间的差距：学生意识到有效提示需要努力，但很少应用这一知识，效率成为主要驱动因素。学生还根据兴趣和学业压力战略性地调整他们的参与，表现出一种通过AI而非与AI共同阅读的新模式：使用AI生成的摘要作为主要材料，以筛选哪些部分值得更深入的关注。我们讨论了AI阅读系统的设计启示，以支撑持续的认知参与。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how AI chatbots influence college students&#x27; academic reading experiences and cognitive engagement, motivated by the increasing use of such technologies. An eight-week longitudinal study was conducted with 15 undergraduates, during which 838 prompts were collected across 239 reading sessions, categorized into four cognitive themes: Decoding, Comprehension, Reasoning, and Metacognition. The findings revealed that comprehension prompts were most prevalent, while students exhibited a natural cognitive progression from comprehension to reasoning that was often truncated. Engagement patterns remained stable over the study period, with significant individual differences, and students tended to use AI-generated summaries to prioritize their reading, indicating a gap between their understanding of effective prompting and actual behavior, driven primarily by efficiency. The results suggest important design considerations for AI reading systems to enhance cognitive engagement.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨大学生如何利用人工智能聊天机器人来增强他们的学术阅读体验和认知参与。研究人员进行了为期八周的纵向研究，涉及15名本科生，他们在指定阅读中与人工智能互动，收集了838个提示，跨239个会话，并将其分类为四个认知主题：解码、理解、推理和元认知。研究结果显示，理解提示最为普遍，学生在理解到推理的自然认知进程常常被截断。尽管在研究期间参与模式保持稳定，但显著的个体差异被注意到，定性分析表明学生对有效提示的理解与实际应用之间存在差距，效率成为他们参与策略的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">SNAP: Towards Segmenting Anything in Any Point Cloud</div>
<div class="meta-line">Authors: Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang</div>
<div class="meta-line">First: 2025-10-13T16:07:00+00:00 · Latest: 2026-02-10T15:12:42+00:00</div>
<div class="meta-line">Comments: Project Page, https://neu-vi.github.io/SNAP/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11565v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11565v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://neu-vi.github.io/SNAP/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present SNAP (Segment aNything in Any Point cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SNAP：在任意点云中进行任意分割</div>
<div class="mono" style="margin-top:8px">交互式3D点云分割通过用户引导的提示实现复杂3D场景的高效标注。然而，当前的方法通常仅限于单一领域（室内或室外）和单一形式的用户交互（空间点击或文本提示）。此外，在多个数据集上训练往往会导致负迁移，导致缺乏通用性的领域特定工具。为了解决这些限制，我们提出了SNAP（在任意点云中分割任意对象），这是一个统一的交互式3D分割模型，支持跨多个领域的基于点和基于文本的提示。我们的方法通过在涵盖室内、室外和空中环境的7个数据集上训练，实现了跨领域的通用性，同时采用领域自适应归一化以防止负迁移。对于文本提示的分割，我们自动生成掩码提议，无需人工干预，并将其与文本查询的CLIP嵌入进行匹配，从而实现全景和开放词汇分割。大量实验表明，SNAP始终提供高质量的分割结果。在9个空间提示分割的零样本基准中，我们在8个上实现了最先进的性能，并在所有5个文本提示基准上展示了竞争力的结果。这些结果表明，统一模型可以匹配或超越专业的领域特定方法，为可扩展的3D标注提供了实用工具。项目页面在：https://neu-vi.github.io/SNAP/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve interactive 3D point cloud segmentation, which is often limited by domain specificity and user interaction methods. The authors introduce SNAP, a unified model that allows for both point-based and text-based prompts across various environments by training on seven diverse datasets and utilizing domain-adaptive normalization to mitigate negative transfer. Experimental results indicate that SNAP achieves state-of-the-art performance on eight out of nine zero-shot benchmarks for spatial prompts and competitive results on all five benchmarks for text prompts, demonstrating its effectiveness as a generalizable tool for 3D annotation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善交互式3D点云分割，当前方法通常受到领域特异性和用户交互方式的限制。作者提出了SNAP，一个统一模型，支持跨多个领域的点基和文本基提示，训练于七个数据集以增强跨领域的泛化能力，同时采用领域自适应归一化以减轻负迁移。实验结果表明，SNAP在九个零样本基准中有八个达到了最先进的性能，并在所有五个文本提示基准中表现出竞争力，证明其作为3D标注的多功能工具的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning</div>
<div class="meta-line">Authors: Minh Le, Bao-Ngoc Dao, Huy Nguyen, Quyen Tran, Anh Nguyen, Nhat Ho</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-29T08:54:58+00:00 · Latest: 2026-02-10T14:18:22+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24483v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24483v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt-based methods have recently gained prominence in Continual Learning (CL) due to their strong performance and memory efficiency. A prevalent strategy in this paradigm assigns a dedicated subset of prompts to each task, which, while effective, incurs substantial computational overhead and causes memory requirements to scale linearly with the number of tasks. Conversely, approaches employing a single shared prompt across tasks offer greater efficiency but often suffer from degraded performance due to knowledge interference. To reconcile this trade-off, we propose SMoPE, a novel framework that integrates the benefits of both task-specific and shared prompt strategies. Inspired by recent findings on the relationship between Prefix Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into multiple &quot;prompt experts&quot; within a sparse MoE architecture. For each input, only a select subset of relevant experts is activated, effectively mitigating interference. To facilitate expert selection, we introduce a prompt-attention score aggregation mechanism that computes a unified proxy score for each expert, enabling dynamic and sparse activation. Additionally, we propose an adaptive noise mechanism to encourage balanced expert utilization while preserving knowledge from prior tasks. To further enhance expert specialization, we design a prototype-based loss function that leverages prefix keys as implicit memory representations. Extensive experiments across multiple CL benchmarks demonstrate that SMoPE consistently outperforms task-specific prompt methods and achieves performance competitive with state-of-the-art approaches, all while significantly reducing parameter counts and computational costs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一提示反击：稀疏专家混合模型用于基于提示的持续学习</div>
<div class="mono" style="margin-top:8px">基于提示的方法因其强大的性能和内存效率，近年来在持续学习（CL）中获得了显著关注。在这一范式中，一种普遍策略是为每个任务分配一个专用的提示子集，这虽然有效，但会产生大量的计算开销，并导致内存需求随着任务数量线性增长。相反，采用跨任务共享单一提示的方法提供了更高的效率，但通常由于知识干扰而导致性能下降。为了解决这一权衡，我们提出了SMoPE，一个新颖的框架，结合了任务特定和共享提示策略的优点。受最近关于前缀调优与专家混合（MoE）之间关系的研究启发，SMoPE将共享提示组织为稀疏MoE架构中的多个“提示专家”。对于每个输入，仅激活相关专家的选定子集，有效减轻干扰。为了促进专家选择，我们引入了一种提示注意力得分聚合机制，为每个专家计算统一的代理得分，从而实现动态和稀疏激活。此外，我们提出了一种自适应噪声机制，以鼓励平衡的专家利用，同时保留先前任务的知识。为了进一步增强专家的专业化，我们设计了一种基于原型的损失函数，利用前缀键作为隐式记忆表示。在多个CL基准上的广泛实验表明，SMoPE始终优于任务特定的提示方法，并在性能上与最先进的方法具有竞争力，同时显著减少了参数数量和计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational overhead and memory requirements associated with prompt-based methods in Continual Learning (CL), particularly when using task-specific prompts. The authors propose a novel framework called SMoPE, which integrates task-specific and shared prompt strategies by organizing a shared prompt into multiple &#x27;prompt experts&#x27; within a sparse Mixture of Experts architecture. Experimental results show that SMoPE outperforms task-specific prompt methods and achieves competitive performance with state-of-the-art approaches while significantly reducing parameter counts and computational costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于提示的方法在持续学习（CL）中使用特定任务提示时所带来的计算开销和内存低效问题。作者提出了一种名为SMoPE的新框架，该框架通过在稀疏专家混合（MoE）架构中将共享提示组织为多个“提示专家”，结合了特定任务和共享提示策略的优点。各种CL基准测试的实验结果表明，SMoPE在性能上超越了传统的特定任务提示方法，并在参数数量和计算成本显著降低的同时，达到了与最先进方法竞争的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing</div>
<div class="meta-line">Authors: Tong Zhang, Honglin Lin, Zhou Liu, Chong Chen, Wentao Zhang</div>
<div class="meta-line">First: 2026-02-10T14:15:35+00:00 · Latest: 2026-02-10T14:15:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SciFlow-Bench：通过逆解析评估结构感知科学图表生成</div>
<div class="mono" style="margin-top:8px">科学图表传达明确的结构信息，但现代文本到图像模型往往生成视觉上合理但结构上不正确的结果。现有基准要么依赖于以图像为中心或对结构不敏感的主观指标，要么评估中间符号表示而非最终渲染图像，从而使基于像素的图表生成未得到充分探索。我们引入了SciFlow-Bench，这是一个优先考虑结构的基准，用于直接评估从像素级输出生成的科学图表。SciFlow-Bench基于真实的科学PDF构建，将每个源框架图与标准的真实图配对，并在闭环、往返协议下将生成的图表图像逆解析回结构图进行比较，从而将模型作为黑箱图像生成器进行评估。该设计通过结构可恢复性而非仅仅视觉相似性来强制评估，并由一个协调规划、感知和结构推理的分层多代理系统实现。实验表明，保持结构正确性仍然是一个基本挑战，特别是对于具有复杂拓扑的图表，强调了结构感知评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current text-to-image models in generating scientifically accurate diagrams, as they often produce visually appealing but structurally flawed outputs. The authors introduce SciFlow-Bench, a benchmark that evaluates scientific diagram generation by focusing on pixel-level outputs and employing a closed-loop, round-trip protocol that inverse-parses generated images into structured graphs for comparison. Experimental results reveal that maintaining structural correctness is a significant challenge, especially for diagrams with complex topologies, highlighting the necessity for evaluations that prioritize structural integrity over mere visual similarity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前文本到图像模型在生成科学准确图表方面的局限性，这些模型往往产生视觉上吸引但结构上不正确的输出。作者提出了SciFlow-Bench，这是一个新颖的基准，通过直接比较像素级输出与结构图来评估科学图表生成，采用闭环协议，涉及对生成图像的逆解析。实验结果表明，保持结构正确性仍然是一个重大挑战，尤其是对于具有复杂拓扑的图表，这突显了优先考虑结构保真度而非单纯视觉相似性的评估必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-IPO: Dual-Iterative Preference Optimization for Text-to-Video Generation</div>
<div class="meta-line">Authors: Xiaomeng Yang, Mengping Yang, Jia Gong, Luozheng Qin, Zhiyu Tan, Hao Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-02-04T08:14:34+00:00 · Latest: 2026-02-10T14:12:49+00:00</div>
<div class="meta-line">Comments: To appear in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02088v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.02088v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have enabled thrilling experiences in producing realistic videos driven by scalable diffusion transformers. However, they usually fail to produce satisfactory outputs that are aligned to users&#x27; authentic demands and preferences. In this work, we introduce Dual-Iterative Optimization (Dual-IPO), an iterative paradigm that sequentially optimizes both the reward model and the video generation model for improved synthesis quality and human preference alignment. For the reward model, our framework ensures reliable and robust reward signals via CoT-guided reasoning, voting-based self-consistency, and preference certainty estimation. Given this, we optimize video foundation models with guidance of signals from reward model&#x27;s feedback, thus improving the synthesis quality in subject consistency, motion smoothness and aesthetic quality, etc. The reward model and video generation model complement each other and are progressively improved in the multi-round iteration, without requiring tediously manual preference annotations. Comprehensive experiments demonstrate that the proposed Dual-IPO can effectively and consistently improve the video generation quality of base model with various architectures and sizes, even help a model with only 2B parameters surpass a 5B one. Moreover, our analysis experiments and ablation studies identify the rational of our systematic design and the efficacy of each component.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双重IPO：用于文本到视频生成的双重迭代偏好优化</div>
<div class="mono" style="margin-top:8px">最近在视频生成方面的进展使得通过可扩展的扩散变换器制作逼真视频的体验变得令人兴奋。然而，它们通常无法产生与用户真实需求和偏好一致的令人满意的输出。在这项工作中，我们引入了双重迭代优化（Dual-IPO），这是一种迭代范式，依次优化奖励模型和视频生成模型，以提高合成质量和人类偏好的一致性。对于奖励模型，我们的框架通过CoT引导推理、基于投票的自一致性和偏好确定性估计，确保可靠和稳健的奖励信号。在此基础上，我们在奖励模型反馈信号的指导下优化视频基础模型，从而提高主题一致性、运动平滑性和美学质量等方面的合成质量。奖励模型和视频生成模型相辅相成，在多轮迭代中逐步改进，无需繁琐的手动偏好注释。全面的实验表明，所提出的Dual-IPO能够有效且持续地提高各种架构和规模的基础模型的视频生成质量，甚至帮助一个只有2B参数的模型超越一个5B参数的模型。此外，我们的分析实验和消融研究确定了我们系统设计的合理性和每个组件的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current video generation models in producing outputs that align with user preferences. The authors propose a method called Dual-Iterative Optimization (Dual-IPO), which iteratively optimizes both a reward model and a video generation model to enhance synthesis quality and better meet human demands. Experimental results show that Dual-IPO significantly improves video generation quality across various model architectures and sizes, with a notable finding that a model with only 2 billion parameters can outperform a 5 billion parameter model, demonstrating the effectiveness of the proposed approach and its components through comprehensive analysis and ablation studies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视频生成输出的质量，以更好地符合用户偏好，因为现有方法通常无法满足这一需求。作者提出了一种新方法，称为双重迭代优化（Dual-IPO），该方法通过迭代优化奖励模型和视频生成模型来改善合成质量。实验结果表明，Dual-IPO显著提高了各种模型架构和规模的视频生成质量，甚至证明了一个具有20亿参数的模型可以超越一个具有50亿参数的模型，同时通过分析和消融研究提供了对每个组件有效性的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path</div>
<div class="meta-line">Authors: Andres Saurez, Neha Sengar, Dongsoo Har</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2026-02-10T13:43:59+00:00 · Latest: 2026-02-10T13:43:59+00:00</div>
<div class="meta-line">Comments: Submitted to ICML 2026. 15 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09784v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09784v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods. The same directions that identify circuit components also enable controlled steering -- achieving 69.8\% emotion classification accuracy versus 53.1\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>电路指纹：答案令牌如何编码其几何路径</div>
<div class="mono" style="margin-top:8px">电路发现和变压器中的激活引导已发展为独立的研究方向，但两者都在同一表示空间中操作。它们是同一基础结构的两种视角吗？我们展示它们遵循一个单一的几何原理：孤立处理的答案令牌编码了产生它们的方向。这个电路指纹假设使得在没有梯度或因果干预的情况下进行电路发现成为可能——仅通过几何对齐恢复与基于梯度的方法相当的结构。我们在四个模型系列的标准基准（IOI、SVA、MCQA）上验证了这一点，取得了与基于梯度的方法相当的电路发现性能。识别电路组件的相同方向也使得控制引导成为可能——在保持事实准确性的同时，实现了69.8%的情感分类准确率，而指令提示的准确率为53.1%。超越方法开发，这种读写二元性揭示了变压器电路本质上是几何结构：可解释性和可控性是同一对象的两个方面。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the relationship between circuit discovery and activation steering in transformers, motivated by the question of whether they represent two perspectives of the same underlying structure. The authors propose the Circuit Fingerprint hypothesis, which suggests that answer tokens encode the geometric directions necessary for their production, allowing for circuit discovery without gradients or causal interventions. Experimental results demonstrate that this method achieves circuit discovery performance comparable to traditional gradient-based approaches across standard benchmarks, and it also enables controlled steering with a significant improvement in emotion classification accuracy compared to instruction prompting, while maintaining factual accuracy.</div>
<div class="mono" style="margin-top:8px">本研究探讨了变压器中电路发现与激活引导之间的关系，旨在理解其基础几何结构。作者提出了电路指纹假设，认为答案令牌编码了生成它们所需的方向，从而使得在没有梯度或因果干预的情况下进行电路发现成为可能。实验结果表明，该方法在标准基准测试中实现了与传统基于梯度的方法相当的电路发现性能，同时在情感分类准确性方面显著优于指令提示，突显了变压器电路的几何特性及其可解释性和可控性作为相互关联的方面。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Personalized Reward Model for Vision Generation</div>
<div class="meta-line">Authors: Yibin Wang, Yuhang Zang, Feng Han, Jiazi Bu, Yujie Zhou, Cheng Jin, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-02T17:44:21+00:00 · Latest: 2026-02-10T12:52:22+00:00</div>
<div class="meta-line">Comments: Website: https://codegoat24.github.io/UnifiedReward/flex</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02380v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02380v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codegoat24.github.io/UnifiedReward/flex">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一个性化奖励模型用于视觉生成</div>
<div class="mono" style="margin-top:8px">最近多模态奖励模型（RMs）的进展显著推动了视觉生成的发展。现有框架通常采用Bradley-Terry风格的偏好建模或利用生成性VLM作为评判者，随后通过强化学习优化视觉生成模型。然而，当前的RMs存在固有的局限性：它们通常遵循一刀切的范式，假设单一的偏好分布或依赖固定的评估标准。因此，它们对特定内容的视觉线索不敏感，导致与主观和上下文依赖的人类偏好系统性不一致。为此，受到人类评估的启发，我们提出了UnifiedReward-Flex，一个统一的个性化奖励模型，用于视觉生成，将奖励建模与灵活和上下文自适应推理相结合。具体而言，给定一个提示和生成的视觉内容，它首先解释语义意图并基于视觉证据进行定位，然后通过在预定义和自生成的高层维度下实例化细粒度标准，动态构建分层评估。我们的训练流程遵循两个阶段的过程：（1）我们首先从先进的闭源VLM中提取结构化、高质量的推理轨迹，以启动SFT，使模型具备灵活和上下文自适应的推理行为；（2）然后我们在精心策划的偏好对上执行直接偏好优化（DPO），进一步增强推理的真实性和区分性对齐。为了验证有效性，我们将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛的结果证明了其优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing multimodal reward models in visual generation, which often adopt a one-size-fits-all approach and fail to account for context-specific human preferences. The authors propose UnifiedReward-Flex, a personalized reward model that combines reward modeling with flexible reasoning to better align with human assessments. Their method involves a two-stage training process that distills reasoning from advanced models and optimizes preferences through direct preference optimization. Experimental results show that integrating UnifiedReward-Flex into the GRPO framework significantly enhances image and video synthesis performance compared to existing models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态奖励模型在视觉生成中的局限性，这些模型通常采用一刀切的方法，未能考虑特定上下文的人类偏好。作者提出了UnifiedReward-Flex，这是一种个性化奖励模型，结合了奖励建模和灵活推理，能够根据语义意图和视觉证据对生成的视觉内容进行更细致的评估。实验结果表明，将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成显著提高了性能，相较于传统方法具有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">UGround: Towards Unified Visual Grounding with Unrolled Transformers</div>
<div class="meta-line">Authors: Rui Qian, Xin Yin, Chuanhang Deng, Zhiyuan Peng, Jian Xiong, Wei Zhai, Dejing Dou</div>
<div class="meta-line">First: 2025-10-04T15:56:52+00:00 · Latest: 2026-02-10T12:37:46+00:00</div>
<div class="meta-line">Comments: https://github.com/rui-qian/UGround</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03853v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03853v2">PDF</a> · <a href="https://github.com/rui-qian/UGround">Code1</a> · <a href="https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm that dynamically selects intermediate layers across \textbf{U}nrolled transformers as ``mask as prompt&#x27;&#x27;, diverging from the prevailing pipeline that leverages the fixed last hidden layer as ``\texttt{&lt;SEG&gt;} as prompt&#x27;&#x27;. UGround addresses two primary challenges posed by the prevailing paradigm: (1) its reliance on the fixed last hidden layer, which sequentially amplifies cumulative errors arising from layer-by-layer propagation without intermediate correction, and (2) its use of \texttt{&lt;SEG&gt;} as a prompt, which implicitly projects textual embeddings into visual space without explicit spatial cues (\eg, coordinates). Central to UGround is Policy-Prompted Masking, which comprises two key components: Stochastic Skip Connection (SSC) and Mask as Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic sampling, allows each \texttt{&lt;SEG&gt;} token to slide across unrolled transformer layers, enabling dynamic layer selection at which it connects to the vision model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer, MasP uses the similarity map derived from the \texttt{&lt;SEG&gt;} token and image tokens as a soft logit mask to prompt SAM for mask generation, offering explicit spatial cues through its activation regions. To validate the effectiveness of UGround, we, for the first time, have unified visual grounding within a single framework from an attribute perspective, spanning from traditional refer expression segmentation to newly proposed reasoning segmentation, single-target to multi-target, positive query to false premise (empty target). All codes and models are publicly available at \href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UGround：朝着统一视觉定位的展开变换器</div>
<div class="mono" style="margin-top:8px">我们提出了UGround，一个\textbf{U}nified视觉\textbf{G}rounding范式，动态选择展开变换器中的中间层作为“掩码作为提示”，与利用固定最后隐藏层作为“\texttt{&lt;SEG&gt;}作为提示”的主流流程相悖。UGround解决了主流范式提出的两个主要挑战：（1）依赖固定最后隐藏层，逐层传播的累积错误未得到中间修正而不断放大；（2）使用\texttt{&lt;SEG&gt;}作为提示，隐式地将文本嵌入投影到视觉空间而没有明确的空间线索（例如，坐标）。UGround的核心是策略提示掩码，包含两个关键组件：随机跳跃连接（SSC）和掩码作为提示（MasP）。SSC是一种强化学习策略，通过随机采样，允许每个\texttt{&lt;SEG&gt;}标记在展开的变换器层之间滑动，以跳跃连接的方式动态选择与视觉模型（例如，SAM）连接的层。给定所选的隐藏层，MasP使用从\texttt{&lt;SEG&gt;}标记和图像标记派生的相似性图作为软逻辑掩码，提示SAM生成掩码，通过其激活区域提供明确的空间线索。为了验证UGround的有效性，我们首次从属性的角度统一了视觉定位，涵盖了从传统的参考表达分割到新提出的推理分割，从单目标到多目标，从正查询到虚假前提（空目标）。所有代码和模型均可在\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind UGround is to improve visual grounding by addressing the limitations of existing methods that rely on a fixed last hidden layer and the use of &lt;SEG&gt; as a prompt, which can lead to cumulative errors and lack explicit spatial cues. The proposed method employs a unified visual grounding paradigm that utilizes Policy-Prompted Masking, incorporating Stochastic Skip Connection (SSC) and Mask as Prompt (MasP) to dynamically select intermediate layers in unrolled transformers. Experimental results demonstrate that UGround effectively unifies various visual grounding tasks, including traditional referential expression segmentation and reasoning segmentation, achieving improved performance across different scenarios such as single-target and multi-target queries.</div>
<div class="mono" style="margin-top:8px">UGround的动机在于通过解决现有方法中依赖固定最后隐藏层和使用没有空间线索的隐式提示的局限性来改善视觉定位。作者提出了一种统一的视觉定位范式，采用政策提示掩蔽，其中包括用于动态层选择的随机跳过连接（SSC）和用于生成明确空间线索的掩蔽作为提示（MasP）。实验结果表明，UGround有效地统一了各种视觉定位任务，从传统的指称表达分割到推理分割，并适应单目标和多目标场景，显示出相较于以前方法的显著改进。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions</div>
<div class="meta-line">Authors: Ada Gorgun, Fawaz Sammani, Nikos Deligiannis, Bernt Schiele, Jonas Fischer</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-09T11:05:08+00:00 · Latest: 2026-02-10T12:37:43+00:00</div>
<div class="meta-line">Comments: Accepted at the International Conference on Learning Representations 2026 (ICLR 2026). Code is available at: https://adagorgun.github.io/PCI-Project/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08486v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.08486v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adagorgun.github.io/PCI-Project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://adagorgun.github.io/PCI-Project/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过提示条件干预研究扩散模型中的时间概念动态</div>
<div class="mono" style="margin-top:8px">扩散模型通常通过其最终输出进行评估，逐渐将随机噪声去噪成有意义的图像。然而，生成过程沿着一条轨迹展开，分析这一动态过程对于理解这些模型在成功/失败模式下的可控性、可靠性和可预测性至关重要。在本研究中，我们提出了一个问题：噪声何时转变为特定概念（例如，年龄）并锁定去噪轨迹？我们提出了PCI（提示条件干预）来研究这个问题。PCI是一个无训练和模型无关的框架，通过扩散时间分析概念动态。其核心思想是分析概念插入成功率（CIS），定义为在给定时间步插入的概念被保留并反映在最终图像中的概率，提供了一种表征概念形成时间动态的方法。应用于多个最先进的文本到图像扩散模型和广泛的概念分类，PCI揭示了扩散模型中多样的时间行为，其中轨迹的某些阶段对特定概念更为有利，即使在同一概念类型内。这些发现还为基于文本的图像编辑提供了可操作的见解，突出了何时干预最有效，而无需访问模型内部或训练，并产生在语义准确性和内容保留之间取得平衡的定量更强的编辑效果，优于强基线。代码可在以下网址获取：https://adagorgun.github.io/PCI-Project/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the dynamics of concept formation in diffusion models, motivated by the need to understand how these models transition from noise to specific concepts during image generation. The authors introduce a training-free and model-agnostic framework called Prompt-Conditioned Intervention (PCI) to analyze the temporal dynamics of concept insertion through the metric of Concept Insertion Success (CIS). The experiments demonstrate that different diffusion models exhibit varied temporal behaviors, with certain phases of the denoising trajectory being more conducive to the preservation of specific concepts, leading to improved text-driven image editing outcomes that balance semantic accuracy and content preservation compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解扩散模型在图像生成过程中如何将噪声转化为特定概念的动态过程，这对于评估其可控性和可靠性至关重要。作者提出了一种名为提示条件干预（PCI）的框架，该框架无需训练且与模型无关，旨在通过概念插入成功率（CIS）指标分析概念形成的时间动态。主要发现表明，不同的扩散模型表现出不同的时间行为，生成轨迹的某些阶段更有利于特定概念的保留，从而在文本驱动的图像编辑中实现了比现有方法更好的语义准确性和内容保留。</div>
</details>
</div>
<div class="card">
<div class="title">Shifting the Breaking Point of Flow Matching for Multi-Instance Editing</div>
<div class="meta-line">Authors: Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta Tintoré Gazulla, Rita Cucchiara, Alessio Tonioni, Silvia Cascianelli</div>
<div class="meta-line">First: 2026-02-09T14:52:45+00:00 · Latest: 2026-02-10T12:18:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08749v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08749v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多实例编辑中的流匹配破裂点转移</div>
<div class="mono" style="margin-top:8px">流匹配模型最近作为扩散的高效替代方案出现，特别是在文本引导的图像生成和编辑中，通过连续时间动态提供更快的推理。然而，现有的基于流的编辑器主要支持全局或单指令编辑，在多实例场景中表现不佳，在这些场景中，必须独立编辑参考输入的多个部分而不产生语义干扰。我们将这一限制视为全局条件速度场和联合注意机制的结果，这使得并发编辑相互纠缠。为了解决这个问题，我们引入了实例解耦注意机制，该机制将联合注意操作进行分区，在速度场估计过程中强制实例特定文本指令与空间区域之间的绑定。我们在自然图像编辑和一个新引入的具有区域级编辑指令的文本密集信息图基准上评估了我们的方法。实验结果表明，我们的方法促进了编辑解耦和局部性，同时保持了全局输出的一致性，实现了单次通过的实例级编辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing flow-based editors in handling multi-instance scenarios during image generation and editing, where multiple parts of an image need to be edited independently. To overcome this challenge, the authors propose a novel mechanism called Instance-Disentangled Attention, which separates joint attention operations to ensure that instance-specific textual instructions are accurately linked to their corresponding spatial regions during the estimation of velocity fields. Experimental results show that this method enhances edit disentanglement and locality while maintaining global coherence in the output, facilitating efficient single-pass, instance-level editing.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善流匹配模型在图像生成中的多实例编辑能力，因为现有方法由于纠缠的注意机制而难以实现独立编辑。作者提出了一种新颖的实例解耦注意机制，该机制分离了联合注意操作，使得特定实例的文本指令能够与速度场估计中的特定空间区域相连接。实验结果表明，该方法提高了编辑的解耦性和局部性，同时保持了整体输出的一致性，从而在自然图像和一个新的文本密集信息图基准中实现了高效的单次实例级编辑。</div>
</details>
</div>
<div class="card">
<div class="title">Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models</div>
<div class="meta-line">Authors: Ruisi Zhao, Haoren Zheng, Zongxin Yang, Hehe Fan, Yi Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T12:17:00+00:00 · Latest: 2026-02-10T12:17:00+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09713v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton&#x27;s graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE&#x27;s decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Stroke3D：通过潜在扩散模型将2D笔画提升为带骨架的3D模型</div>
<div class="mono" style="margin-top:8px">带骨架的3D资产是3D变形和动画的基础。然而，现有的3D生成方法在生成可动画几何体方面面临挑战，而绑定技术在骨架创建上缺乏细粒度的结构控制。为了解决这些限制，我们提出了Stroke3D，一个新颖的框架，直接从用户输入（2D绘制的笔画和描述性文本提示）生成带骨架的网格。我们的方法开创了一个两阶段的流程，将生成分为：1）可控骨架生成，我们采用骨架图VAE（Sk-VAE）将骨架的图结构编码到潜在空间中，骨架图DiT（Sk-DiT）生成骨架嵌入。生成过程同时依赖于文本语义和2D笔画的显式结构控制，VAE的解码器重建最终的高质量3D骨架；2）通过TextuRig和SKA-DPO增强网格合成，我们在生成的骨架的基础上合成一个纹理网格。在这一阶段，我们首先通过用TextuRig（一个带有说明的纹理和带骨架网格的数据集，来自Objaverse-XL）增强现有的骨架到网格模型的训练数据。除此之外，我们采用了一种偏好优化策略SKA-DPO，基于骨架-网格对齐分数，进一步提高几何保真度。总的来说，我们的框架使创建可动画的3D内容的工作流程更加直观。据我们所知，我们的工作是首个基于用户绘制的2D笔画生成带骨架的3D网格的研究。大量实验表明，Stroke3D生成了可信的骨架和高质量的网格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of animatable 3D assets, addressing the limitations of existing methods in creating rigged geometry and fine-grained control over skeleton creation. The authors propose Stroke3D, a two-stage framework that first generates a controllable skeleton using a Skeletal Graph VAE to encode the skeleton&#x27;s structure, conditioned on user-provided 2D strokes and text prompts, and then synthesizes a textured mesh using enhanced training data and a preference optimization strategy. Experimental results indicate that Stroke3D successfully produces plausible skeletons and high-quality 3D meshes, marking a significant advancement in the intuitive creation of ready-to-animate 3D content.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善可动画3D资产的生成，解决现有方法在创建绑定几何体和对骨架创建进行细粒度控制方面的局限性。作者提出了Stroke3D，一个两阶段框架，首先使用骨架图变分自编码器生成可控骨架，然后基于生成的骨架合成纹理网格，利用增强的训练数据和偏好优化策略。实验结果表明，Stroke3D能够成功从用户绘制的2D笔画生成合理的骨架和高质量的3D网格，标志着3D内容创作工作流程的显著进步。</div>
</details>
</div>
<div class="card">
<div class="title">GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation</div>
<div class="meta-line">Authors: Sandesh Hegde, Jaison Saji Chacko, Debarshi Banerjee, Uma Mahesh</div>
<div class="meta-line">First: 2026-02-10T11:59:14+00:00 · Latest: 2026-02-10T11:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09701v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenSeg-R1：基于RL的视觉-语言基础细粒度指称分割</div>
<div class="mono" style="margin-top:8px">我们通过解耦的推理-再分割流程研究细粒度指称图像分割。视觉-语言模型（VLM）接收图像和自然语言查询，推理场景，并发出结构化空间提示：每个被指称实例的边界框加两个内部关键点。一个冻结的可提示分割器（SAM 2）将这些提示转换为高质量的掩码。在我们的GenSeg-R1框架内，我们使用群体相对策略优化（GRPO）微调Qwen3-VL模型（4B和8B参数），无需监督推理链注释。在RefCOCOg验证中，我们的最佳模型（GenSeg-R1-8B）实现了0.7127 cIoU和0.7382 mIoU，显著超越相应的Qwen3-VL指令基线（分别提高15.3和21.9分），并在相同评估下超越Seg-Zero-7B [3] 3.3 cIoU。我们进一步介绍GenSeg-R1-G，一个在GRefCOCO [9]上训练的变体，具有在环奖励的SAM 2，直接优化掩码质量。在GRefCOCO验证中，GenSeg-R1-G实现了76.69%的目标mIoU，负向（无目标）提示的准确率为82.40%，显著超越缺乏无目标检测能力的Seg-R1-7B和Seg-Zero-7B。在ReasonSeg测试中，GenSeg-R1-4B达到68.40%的mIoU，分别超越Seg-Zero-7B 7.0分和Seg-R1-7B 10.7分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of fine-grained referring image segmentation using a decoupled reason-then-segment approach. The method employs a vision-language model that processes an image and a natural-language query to generate structured spatial prompts, which are then converted into high-quality masks by a frozen promptable segmenter. The experimental results demonstrate that the best model, GenSeg-R1-8B, achieves significant improvements in cIoU and mIoU metrics on the RefCOCOg validation set, outperforming baseline models and showing enhanced performance on the GRefCOCO validation set with a focus on mask quality optimization.</div>
<div class="mono" style="margin-top:8px">本研究针对细粒度指称图像分割的挑战，提出了一种解耦的推理-分割管道，利用视觉-语言模型（VLM）处理图像和自然语言查询，生成结构化空间提示以进行分割。该方法通过使用组相对策略优化（GRPO）对Qwen3-VL模型进行微调，而无需监督推理链注释。实验结果表明，最佳模型GenSeg-R1-8B在RefCOCOg验证集上在cIoU和mIoU指标上显著优于基线模型，而GenSeg-R1-G变体进一步提高了GRefCOCO验证集上的掩码质量，在目标和无目标检测能力方面超越了其他模型。</div>
</details>
</div>
<div class="card">
<div class="title">MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images</div>
<div class="meta-line">Authors: Qinyue Tong, Ziqian Lu, Jun Liu, Rui Zuo, Zheming Lu</div>
<div class="meta-line">First: 2025-11-15T08:59:21+00:00 · Latest: 2026-02-10T10:36:26+00:00</div>
<div class="meta-line">Comments: 16pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12110v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12110v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment &amp; Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MediRound：医学图像中的多轮实体级推理分割</div>
<div class="mono" style="margin-top:8px">尽管医学图像分割取得了进展，但大多数现有方法仍然是特定任务的，缺乏交互性。尽管最近基于文本提示的分割方法增强了用户驱动和基于推理的分割，但仍然局限于单轮对话，无法进行多轮推理。在本研究中，我们引入了多轮实体级医学推理分割（MEMR-Seg），这是一项新任务，要求通过多轮查询和实体级推理生成分割掩码。为支持此任务，我们构建了MR-MedSeg，这是一个包含177K多轮医学分割对话的大规模数据集，涵盖了跨轮的基于实体的推理。此外，我们提出了MediRound，这是一个为多轮医学推理分割设计的有效基线模型。为了减轻多轮分割链式管道中固有的错误传播，我们在模型推理过程中引入了一种轻量且有效的判断与修正机制。实验结果表明，我们的方法有效解决了MEMR-Seg任务，并优于传统医学参考分割方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing medical image segmentation methods, which are often task-specific and lack interactivity, particularly in multi-round reasoning scenarios. The authors introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg) as a new task that involves generating segmentation masks through multi-round queries, supported by a newly constructed dataset, MR-MedSeg, containing 177K dialogues. They propose MediRound, a baseline model that incorporates a Judgment &amp; Correction Mechanism to reduce error propagation during inference, and experimental results show that this approach effectively tackles the MEMR-Seg task, outperforming traditional medical referring segmentation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法的局限性来改善医学图像分割，这些方法通常是特定任务的且缺乏交互性。作者引入了一项新任务，称为多轮实体级医学推理分割（MEMR-Seg），该任务涉及通过需要实体级推理的多轮查询生成分割掩码。为了支持这一任务，他们创建了一个大规模数据集MR-MedSeg，包含177K个多轮医学分割对话。作者还提出了MediRound，一个基线模型，采用判断与纠正机制以减少推理过程中的错误传播。实验结果表明，MediRound有效应对MEMR-Seg任务，并超越了传统的医学参考分割方法。</div>
</details>
</div>
<div class="card">
<div class="title">Delving into Spectral Clustering with Vision-Language Representations</div>
<div class="meta-line">Authors: Bo Peng, Yuanwei Hu, Bo Liu, Ling Chen, Jie Lu, Zhen Fang</div>
<div class="meta-line">First: 2026-02-10T09:36:24+00:00 · Latest: 2026-02-10T09:36:24+00:00</div>
<div class="meta-line">Comments: ICLR26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用视觉-语言表示深入研究谱聚类</div>
<div class="mono" style="margin-top:8px">谱聚类被认为是无监督数据分析中的一种强大技术。绝大多数谱聚类方法都是由单一模态驱动的，未能利用多模态表示中的丰富信息。受到最近视觉-语言预训练成功的启发，本文将谱聚类的范畴从单模态扩展到多模态。特别地，我们提出了神经切线核谱聚类，利用预训练视觉-语言模型中的跨模态对齐。通过将神经切线核与积极名词（即与感兴趣图像语义上接近的名词）锚定，我们得出将图像之间的亲和力表述为其视觉接近性和语义重叠的结合。我们展示了这种表述在增强聚类内连接的同时抑制聚类间的虚假连接，从而鼓励块对角结构。此外，我们提出了一种正则化的亲和力扩散机制，能够自适应地集成由不同提示引发的亲和力矩阵。在包括经典、大规模、细粒度和领域转移数据集在内的\textbf{16}个基准上的广泛实验表明，我们的方法始终以较大幅度超越了最先进的技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance spectral clustering techniques by incorporating multi-modal representations, as traditional methods primarily rely on single modalities. The authors propose a novel approach called Neural Tangent Kernel Spectral Clustering, which utilizes cross-modal alignment from pre-trained vision-language models to improve the clustering process. The key experimental findings demonstrate that this method significantly strengthens within-cluster connections while reducing spurious connections between clusters, leading to improved clustering performance across 16 diverse benchmarks, outperforming existing state-of-the-art techniques by a considerable margin.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统谱聚类方法通常依赖单一模态数据的局限性，这种方法未能充分利用多模态表示的潜力。作者提出了一种新方法，称为神经切线核谱聚类，该方法结合了来自预训练视觉-语言模型的跨模态对齐，以增强聚类性能。对16个不同基准的实验结果表明，该方法通过增强聚类内连接并减少虚假连接，显著提高了聚类准确性，超越了现有的最先进技术。</div>
</details>
</div>
<div class="card">
<div class="title">MOVA: Towards Scalable and Synchronized Video-Audio Generation</div>
<div class="meta-line">Authors: SII-OpenMOSS Team, :, Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo, Qianyi Wu, Qinyuan Cheng, Ruixiao Li, Tianyi Liang, Wenbo Zhang, Wenming Tu, Xiangyu Peng, Yang Gao, Yanru Huo, Ying Zhu, Yinze Luo, Yiyang Zhang, Yuerong Song, Zhe Xu, Zhiyu Zhang, Chenchen Yang, Cheng Chang, Chushu Zhou, Hanfu Chen, Hongnan Ma, Jiaxi Li, Jingqi Tong, Junxi Liu, Ke Chen, Shimin Li, Shiqi Jiang, Songlin Wang, Wei Jiang, Zhaoye Fei, Zhiyuan Ning, Chunguo Li, Chenhui Li, Ziwei He, Zengfeng Huang, Xie Chen, Xipeng Qiu</div>
<div class="meta-line">First: 2026-02-09T15:31:54+00:00 · Latest: 2026-02-10T09:29:18+00:00</div>
<div class="meta-line">Comments: Technical report for MOVA (open-source video-audio generation model). 38 pages, 10 figures, 22 tables. Project page: https://mosi.cn/models/mova Code: https://github.com/OpenMOSS/MOVA Models: https://huggingface.co/collections/OpenMOSS-Team/mova. Qinyuan Cheng and Tianyi Liang are project leader. Xie Chen and Xipeng Qiu are corresponding authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08794v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08794v2">PDF</a> · <a href="https://github.com/OpenMOSS/MOVA">Code1</a> · <a href="https://huggingface.co/collections/OpenMOSS-Team/mova">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MOVA：迈向可扩展和同步的视频音频生成</div>
<div class="mono" style="margin-top:8px">音频是现实世界视频不可或缺的部分，但生成模型在很大程度上忽视了音频组件。目前生成音视频内容的方法通常依赖于级联管道，这增加了成本，累积了错误，并降低了整体质量。虽然Veo 3和Sora 2等系统强调同时生成的价值，但联合多模态建模在架构、数据和训练方面引入了独特的挑战。此外，现有系统的闭源特性限制了该领域的进展。在本研究中，我们介绍了MOVA（MOSS视频和音频），这是一个开源模型，能够生成高质量、同步的音视频内容，包括逼真的口型同步语音、环境感知音效和内容对齐音乐。MOVA采用混合专家（MoE）架构，总共有320亿个参数，其中在推理过程中有180亿个是活跃的。它支持IT2VA（图像-文本到视频-音频）生成任务。通过发布模型权重和代码，我们旨在推动研究并培养一个充满活力的创作者社区。发布的代码库全面支持高效推理、LoRA微调和提示增强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing audio-visual generation models, which often neglect audio components and rely on inefficient cascaded pipelines that compromise quality. The authors introduce MOVA, an open-source model that utilizes a Mixture-of-Experts architecture with 32 billion parameters to generate synchronized audio-visual content, including lip-synced speech and environment-aware sound effects. Key experimental findings demonstrate that MOVA effectively supports the IT2VA generation task while providing high-quality outputs, thus contributing to the advancement of the field and encouraging community engagement through the release of model weights and code.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有音视频生成模型的局限性，这些模型往往忽视音频组件，并依赖低效的级联管道，导致质量下降。作者介绍了MOVA，一个开源模型，利用混合专家架构，拥有320亿个参数，能够生成高质量、同步的音视频内容，包括逼真的口型同步语音和环境感知音效。主要实验结果表明，MOVA有效支持IT2VA生成任务，同时提供了高效推理和微调的强大框架，从而推动该领域的发展并促进创作者社区的形成。</div>
</details>
</div>
<div class="card">
<div class="title">Controllable Dance Generation with Style-Guided Motion Diffusion</div>
<div class="meta-line">Authors: Hongsong Wang, Ying Zhu, Xin Geng, Liang Wang</div>
<div class="meta-line">First: 2024-06-12T04:55:14+00:00 · Latest: 2026-02-10T09:28:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.07871v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.07871v3">PDF</a> · <a href="https://github.com/mucunzhuzhu/DGSDP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dance plays an important role as an artistic form and expression in human culture, yet automatically generating dance sequences is a significant yet challenging endeavor. Existing approaches often neglect the critical aspect of controllability in dance generation. Additionally, they inadequately model the nuanced impact of music styles, resulting in dances that lack alignment with the expressive characteristics inherent in the conditioned music. To address this gap, we propose Style-Guided Motion Diffusion (SGMD), which integrates the Transformer-based architecture with a Style Modulation module. By incorporating music features with user-provided style prompts, the SGMD ensures that the generated dances not only match the musical content but also reflect the desired stylistic characteristics. To enable flexible control over the generated dances, we introduce a spatial-temporal masking mechanism. As controllable dance generation has not been fully studied, we construct corresponding experimental setups and benchmarks for tasks such as trajectory-based dance generation, dance in-betweening, and dance inpainting. Extensive experiments demonstrate that our approach can generate realistic and stylistically consistent dances, while also empowering users to create dances tailored to diverse artistic and practical needs. Code is available on Github: https://github.com/mucunzhuzhu/DGSDP</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于风格引导的运动扩散可控舞蹈生成</div>
<div class="mono" style="margin-top:8px">舞蹈作为一种艺术形式和人类文化的表达方式，扮演着重要角色，但自动生成舞蹈序列是一项重要且具有挑战性的任务。现有方法往往忽视了舞蹈生成中的可控性这一关键方面。此外，它们对音乐风格的细微影响建模不足，导致生成的舞蹈与条件音乐固有的表现特征不一致。为了解决这一问题，我们提出了风格引导的运动扩散（SGMD），它将基于Transformer的架构与风格调制模块相结合。通过结合音乐特征和用户提供的风格提示，SGMD确保生成的舞蹈不仅与音乐内容匹配，还反映所需的风格特征。为了实现对生成舞蹈的灵活控制，我们引入了一种时空掩蔽机制。由于可控舞蹈生成尚未得到充分研究，我们构建了相应的实验设置和基准，用于轨迹基础的舞蹈生成、舞蹈插值和舞蹈修复等任务。大量实验表明，我们的方法能够生成真实且风格一致的舞蹈，同时使用户能够创建符合多样艺术和实际需求的舞蹈。代码可在Github上获取：https://github.com/mucunzhuzhu/DGSDP</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the controllability and stylistic alignment of automatically generated dance sequences, which is often overlooked in existing methods. The authors propose a novel approach called Style-Guided Motion Diffusion (SGMD), which combines a Transformer-based architecture with a Style Modulation module to incorporate music features and user-defined style prompts. Experimental results indicate that SGMD can generate realistic and stylistically coherent dances that align with the musical content, while also allowing users to customize dances for various artistic and practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强舞蹈序列的自动生成，解决现有方法中常常忽视的可控性和与音乐风格的对齐问题。作者提出了一种新方法，称为风格引导运动扩散（SGMD），该方法结合了基于Transformer的架构和风格调制模块，以融入音乐特征和用户定义的风格提示。实验结果表明，SGMD成功生成了真实且风格一致的舞蹈序列，使用户能够创作满足各种艺术和实际需求的舞蹈。</div>
</details>
</div>
<div class="card">
<div class="title">Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</div>
<div class="meta-line">Authors: Rohan Asthana, Vasileios Belagiannis</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T14:29:42+00:00 · Latest: 2026-02-10T08:24:56+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20642v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20642v2">PDF</a> · <a href="https://github.com/rohanasthana/memorization-anisotropy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric. The code is available at https://github.com/rohanasthana/memorization-anisotropy .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过对数概率的各向异性检测和缓解扩散模型中的记忆化</div>
<div class="mono" style="margin-top:8px">基于扩散的图像生成模型通过迭代去噪生成高保真图像，但仍然容易受到记忆化的影响，即无意中重现训练图像的精确副本或部分。最近的记忆化检测方法主要基于得分差异的范数作为记忆化的指标。我们证明，这种基于范数的度量在各向同性对数概率分布的假设下主要有效，这通常在高或中等噪声水平下成立。相反，分析各向异性状态揭示，记忆样本在低噪声环境中表现出指导向量与无条件得分之间的强角度对齐。通过这些见解，我们开发了一种通过整合各向同性范数和各向异性对齐的记忆化检测度量。我们的检测度量可以通过两个条件和无条件的前向传递直接在纯噪声输入上计算，消除了昂贵的去噪步骤的需要。在Stable Diffusion v1.4和v2上的检测实验表明，我们的度量在性能上优于现有的无去噪检测方法，同时速度至少比之前的最佳方法快5倍。最后，我们通过利用一种基于我们开发的度量适应记忆提示的缓解策略，展示了我们方法的有效性。代码可在https://github.com/rohanasthana/memorization-anisotropy获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of memorization in diffusion-based image generative models, which can lead to the unintentional reproduction of training images. The authors propose a new detection metric that combines isotropic norm and anisotropic alignment, revealing that memorized samples show strong angular alignment in low-noise settings. Their experimental results demonstrate that this metric outperforms existing methods and is approximately five times faster than the best previous approach, while also enabling a mitigation strategy that adapts memorized prompts based on the detection metric.</div>
<div class="mono" style="margin-top:8px">本研究解决了扩散式图像生成模型中的记忆化问题，该问题可能导致训练图像的再现。作者提出了一种新的检测指标，结合了各向同性范数和各向异性对齐，揭示了在低噪声环境下，记忆样本表现出强烈的角对齐。实验结果表明，该指标优于现有方法，速度约为最佳先前方法的五倍，同时还能够基于检测指标调整记忆提示。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Depth Super-Resolution via Adaptive Diffusion Sampling</div>
<div class="meta-line">Authors: Kun Wang, Yun Zhu, Pan Zhou, Na Zhao</div>
<div class="meta-line">First: 2026-02-10T08:10:02+00:00 · Latest: 2026-02-10T08:10:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09510v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09510v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS&#x27;s superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应扩散采样实现鲁棒深度超分辨率</div>
<div class="mono" style="margin-top:8px">我们提出了AdaDS，这是一个可推广的深度超分辨率框架，能够从任意降级的低分辨率输入中稳健地恢复高分辨率深度图。与直接回归深度值并在严重或未知降级下常常出现伪影的传统方法不同，AdaDS利用高斯平滑的收缩特性：随着噪声在前向过程中积累，降级输入与其原始高质量对应物之间的分布差异减小，最终收敛到各向同性高斯先验。基于此，AdaDS根据估计的精细化不确定性自适应选择反向扩散轨迹中的起始时间步，并随后注入定制噪声，以将中间样本定位在目标后验分布的高概率区域内。这一策略确保了固有的鲁棒性，使得预训练扩散模型的生成先验在上游估计不完美时仍能主导恢复。对真实世界和合成基准的广泛实验表明，与最先进的方法相比，AdaDS在零样本泛化和对多样降级模式的韧性方面表现优越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve depth super-resolution by effectively recovering high-resolution depth maps from low-resolution inputs that may be severely degraded. The proposed method, AdaDS, utilizes the contraction property of Gaussian smoothing to address the challenges posed by noise and degradation, allowing for adaptive selection of starting timesteps in the reverse diffusion process based on refinement uncertainty. Experimental results show that AdaDS outperforms existing state-of-the-art methods in terms of zero-shot generalization and resilience to various degradation patterns, demonstrating its robustness in recovering depth information even under imperfect conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效恢复低分辨率输入的高分辨率深度图来改善深度超分辨率，尤其是在输入可能严重退化的情况下。提出的方法AdaDS利用高斯平滑的收缩特性，最小化退化深度图与高质量深度图之间的分布差异，基于精细化不确定性自适应选择起始时间步，并注入定制噪声以增强恢复效果。实验结果表明，AdaDS在零-shot 泛化方面优于最先进的方法，并对各种退化模式表现出韧性，确认了其在实际应用中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">OSI: One-step Inversion Excels in Extracting Diffusion Watermarks</div>
<div class="meta-line">Authors: Yuwei Chen, Zhenliang He, Jia Tang, Meina Kan, Shiguang Shan</div>
<div class="meta-line">First: 2026-02-10T07:43:16+00:00 · Latest: 2026-02-10T07:43:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSI：一步反演在提取扩散水印方面表现卓越</div>
<div class="mono" style="margin-top:8px">水印是扩散生成图像的来源和版权保护的重要机制。无训练方法，如高斯阴影，将水印嵌入扩散模型的初始噪声中，对生成图像的质量影响微乎其微。然而，提取这种类型的水印通常需要多步扩散反演以获得精确的初始噪声，这在计算上成本高且耗时。为了解决这个问题，我们提出了一种显著更快且更准确的提取高斯阴影风格水印的方法——一步反演（OSI）。OSI将水印提取重新表述为可学习的符号分类问题，消除了对初始噪声精确回归的需求。然后，我们从扩散主干初始化OSI模型，并在合成噪声-图像对上进行微调，目标是符号分类。通过这种方式，OSI模型能够在一步内高效完成水印提取。我们的OSI在多步扩散反演方法上表现显著优越：速度快20倍，提取准确率更高，水印负载能力翻倍。针对不同调度器、扩散主干和加密方案的广泛实验一致显示出改进，证明了我们OSI框架的通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and accuracy of watermark extraction from diffusion-generated images, which is crucial for copyright protection. The authors propose a novel method called One-step Inversion (OSI), which reformulates the watermark extraction process as a learnable sign classification problem, eliminating the need for computationally intensive multi-step diffusion inversion. Experimental results indicate that OSI is 20 times faster than traditional methods, achieves higher extraction accuracy, and doubles the watermark payload capacity, demonstrating its effectiveness across various schedulers, diffusion backbones, and cryptographic schemes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高从扩散生成图像中提取水印的效率和准确性，这对版权保护至关重要。作者提出了一种新方法，称为单步反演（OSI），将水印提取重新表述为可学习的符号分类问题，从而消除了多步扩散反演的需求。实验结果表明，OSI的速度比传统方法快20倍，提取准确性更高，并且水印负载能力翻倍，在各种调度器、扩散骨干网和密码方案中均观察到一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive recurrent flow map operator learning for reaction diffusion dynamics</div>
<div class="meta-line">Authors: Huseyin Tunc</div>
<div class="meta-line">First: 2026-02-10T07:33:13+00:00 · Latest: 2026-02-10T07:33:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09487v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09487v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reaction-diffusion (RD) equations underpin pattern formation across chemistry, biology, and physics, yet learning stable operators that forecast their long-term dynamics from data remains challenging. Neural-operator surrogates provide resolution-robust prediction, but autoregressive rollouts can drift due to the accumulation of error, and out-of-distribution (OOD) initial conditions often degrade accuracy. Physics-based numerical residual objectives can regularize operator learning, although they introduce additional assumptions, sensitivity to discretization and loss design, and higher training cost. Here we develop a purely data-driven operator learner with adaptive recurrent training (DDOL-ART) using a robust recurrent strategy with lightweight validation milestones that early-exit unproductive rollout segments and redirect optimization. Trained only on a single in-distribution toroidal Gaussian family over short horizons, DDOL-ART learns one-step operators that remain stable under long rollouts and generalize zero-shot to strong morphology shifts across FitzHugh-Nagumo (FN), Gray-Scott (GS), and Lambda-Omega (LO) systems. Across these benchmarks, DDOL-ART delivers a strong accuracy and cost trade-off. It is several-fold faster than a physics-based numerical-loss operator learner (NLOL) under matched settings, and it remains competitive on both in-distribution stability and OOD robustness. Training-dynamics diagnostics show that adaptivity strengthens the correlation between validation error and OOD test error performance, acting as a feedback controller that limits optimization drift. Our results indicate that feedback-controlled recurrent training of DDOL-ART generates robust flow-map surrogates without PDE residuals, while simultaneously maintaining competitiveness with NLOL at significantly reduced training costs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反应扩散动力学的自适应递归流图算子学习</div>
<div class="mono" style="margin-top:8px">反应扩散（RD）方程是化学、生物学和物理学中模式形成的基础，但从数据中学习能够预测其长期动态的稳定算子仍然具有挑战性。神经算子代理提供了分辨率稳健的预测，但自回归展开可能因误差积累而漂移，且分布外（OOD）初始条件常常降低准确性。基于物理的数值残差目标可以对算子学习进行正则化，尽管它们引入了额外的假设、对离散化和损失设计的敏感性以及更高的训练成本。在此，我们开发了一种纯数据驱动的算子学习器，采用自适应递归训练（DDOL-ART），使用一种稳健的递归策略和轻量级验证里程碑，提前退出无效的展开段并重新定向优化。DDOL-ART仅在短时间内对单一的分布内环形高斯族进行训练，学习到的单步算子在长时间展开下保持稳定，并在FitzHugh-Nagumo（FN）、Gray-Scott（GS）和Lambda-Omega（LO）系统中实现零样本的强形态迁移泛化。在这些基准测试中，DDOL-ART提供了强大的准确性和成本权衡。在匹配设置下，它比基于物理的数值损失算子学习器（NLOL）快数倍，并且在分布内稳定性和OOD鲁棒性方面保持竞争力。训练动态诊断表明，自适应性增强了验证误差与OOD测试误差性能之间的相关性，充当限制优化漂移的反馈控制器。我们的结果表明，DDOL-ART的反馈控制递归训练生成了稳健的流图代理，而无需PDE残差，同时在显著降低训练成本的情况下与NLOL保持竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of learning stable operators for predicting long-term dynamics in reaction-diffusion equations, which are crucial in various scientific fields. The authors propose a data-driven operator learner called DDOL-ART that utilizes adaptive recurrent training and lightweight validation milestones to optimize the learning process. Experimental results demonstrate that DDOL-ART achieves strong accuracy and cost efficiency, significantly outperforming a physics-based operator learner in terms of speed while maintaining stability and robustness against out-of-distribution conditions across multiple systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善反应扩散方程中稳定算子的学习，以便更好地预测长期动态，这在多个科学领域中至关重要。作者提出了一种名为DDOL-ART的数据驱动算子学习器，利用自适应递归训练来提高预测准确性，同时降低训练成本。实验结果表明，DDOL-ART在准确性和效率方面表现出色，显著优于基于物理的数值损失算子学习器，尤其是在长时间预测中的稳定性和对不同系统形态的泛化能力方面，无需额外假设或残差。</div>
</details>
</div>
<div class="card">
<div class="title">Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution</div>
<div class="meta-line">Authors: Xun Zhang, Kaicheng Yang, Hongliang Lu, Haotong Qin, Yong Guo, Yulun Zhang</div>
<div class="meta-line">First: 2026-02-01T15:07:59+00:00 · Latest: 2026-02-10T07:16:36+00:00</div>
<div class="meta-line">Comments: Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01273v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01273v2">PDF</a> · <a href="https://github.com/xunzhang1128/Q-DiT4SR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Q-DiT4SR：细节保留扩散变换器量化在真实世界图像超分辨率中的探索</div>
<div class="mono" style="margin-top:8px">最近，扩散变换器（DiTs）在真实世界图像超分辨率（Real-ISR）中出现，以生成高质量纹理，但其沉重的推理负担阻碍了实际部署。虽然后训练量化（PTQ）是加速的有希望的解决方案，但现有的超分辨率方法主要集中在U-Net架构上，而通用的DiT量化通常是为文本到图像任务设计的。直接将这些方法应用于基于DiT的超分辨率模型会导致局部纹理严重退化。因此，我们提出了Q-DiT4SR，这是第一个专门为基于DiT的Real-ISR量身定制的PTQ框架。我们提出了H-SVD，一种层次化的SVD，它在匹配的参数预算下，将全局低秩分支与局部块状秩-1分支集成在一起。我们进一步提出了方差感知时空混合精度：VaSMP根据率失真理论以无数据的方式分配跨层权重位宽，而VaTMP通过动态规划（DP）在扩散时间步中调度层内激活精度，且校准最小。对多个真实世界数据集的实验表明，我们的Q-DiT4SR在W4A6和W4A4设置下均实现了SOTA性能。值得注意的是，W4A4量化配置将模型大小减少了5.8$\times$，计算操作减少了60$\times$以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the heavy inference burden of Diffusion Transformers (DiTs) in Real-World Image Super-Resolution (Real-ISR), which limits their practical deployment despite their ability to generate high-quality textures. The authors propose Q-DiT4SR, a Post-Training Quantization (PTQ) framework specifically designed for DiT-based Real-ISR, incorporating a hierarchical SVD method and a variance-aware mixed precision approach to optimize model performance. Experimental results on various real-world datasets show that Q-DiT4SR achieves state-of-the-art performance while significantly reducing model size by 5.8 times and computational operations by over 60 times under the W4A4 quantization setting.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决扩散变换器（DiTs）在真实世界图像超分辨率（Real-ISR）中的高推理负担，这限制了其实际应用，尽管它们能够生成高质量的纹理。作者提出了Q-DiT4SR，这是一个专门为基于DiT的Real-ISR设计的新型后训练量化（PTQ）框架，结合了分层奇异值分解方法和基于方差的混合精度方法，以优化模型性能并降低计算成本。对多个真实世界数据集的实验结果表明，Q-DiT4SR在W4A6和W4A4量化设置下均实现了最先进的性能，其中W4A4配置使模型大小减少了5.8倍，计算操作减少了60倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning</div>
<div class="meta-line">Authors: Xu Ma, Yitian Zhang, Qihua Dong, Yun Fu</div>
<div class="meta-line">First: 2026-02-10T06:06:54+00:00 · Latest: 2026-02-10T06:06:54+00:00</div>
<div class="meta-line">Comments: Dataset: https://huggingface.co/datasets/ma-xu/fine-t2i</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09439v1">PDF</a> · <a href="https://huggingface.co/datasets/ma-xu/fine-t2i">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fine-T2I：一个开放的大规模多样化高质量T2I微调数据集</div>
<div class="mono" style="margin-top:8px">高质量和开放的数据集仍然是文本到图像（T2I）微调的主要瓶颈。尽管模型架构和训练流程迅速进步，但大多数公开可用的微调数据集存在低分辨率、文本与图像对齐差差或多样性有限的问题，导致开放研究模型与企业级模型之间存在明显的性能差距。在本研究中，我们提出了Fine-T2I，这是一个大规模、高质量且完全开放的T2I微调数据集。Fine-T2I涵盖10个任务组合、32个提示类别、11种视觉风格和5个提示模板，结合了强大现代模型生成的合成图像和专业摄影师精心策划的真实图像。所有样本都经过严格筛选，以确保文本与图像的对齐、视觉保真度和提示质量，初始候选者中超过95%被剔除。最终数据集包含超过600万个文本-图像对，磁盘占用约2 TB，接近预训练数据集的规模，同时保持微调级别的质量。在多种预训练扩散和自回归模型上，Fine-T2I的微调始终提高了生成质量和指令遵循性，经过人工评估、视觉比较和自动指标验证。我们以开放许可证发布Fine-T2I，以帮助缩小开放社区中T2I微调的数据差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing text-to-image (T2I) fine-tuning datasets, which often suffer from low resolution, poor text-image alignment, and limited diversity. The authors introduce Fine-T2I, a large-scale and high-quality dataset that includes over 6 million text-image pairs, combining synthetic images generated by advanced models with curated real images from professional photographers. Experimental results demonstrate that fine-tuning on Fine-T2I significantly enhances generation quality and instruction adherence across various pretrained models, as confirmed by human evaluations and automatic metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有文本到图像（T2I）微调数据集的局限性，这些数据集通常存在低分辨率、文本与图像对齐差差和多样性有限的问题。作者介绍了Fine-T2I，这是一个大规模且高质量的数据集，包含超过600万个经过严格筛选的文本-图像对，结合了来自先进模型的合成图像和精心策划的真实图像。实验结果表明，在Fine-T2I上进行微调显著提高了各种预训练模型的生成质量和指令遵循性，这一结果得到了人工评估和自动指标的验证。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime</div>
<div class="meta-line">Authors: Herman Errico</div>
<div class="meta-line">First: 2026-02-10T05:57:30+00:00 · Latest: 2026-02-10T05:57:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09433v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09433v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主行动运行时管理（AARM）：确保运行时AI驱动行动的系统规范</div>
<div class="mono" style="margin-top:8px">随着人工智能系统从被动助手演变为能够执行重要行动的自主代理，安全边界从模型输出转移到工具执行。传统的安全范式——日志聚合、周界防御和事后取证——无法保护那些AI驱动的行动不可逆、以机器速度执行且源自可能被攻陷的编排层的系统。本文介绍了自主行动运行时管理（AARM），这是一个确保运行时AI驱动行动的开放规范。AARM定义了一个运行时安全系统，在执行之前拦截行动，积累会话上下文，评估政策和意图的一致性，执行授权决策，并记录防篡改的收据以便进行取证重建。我们形式化了一个威胁模型，解决提示注入、混淆代理攻击、数据外泄和意图漂移等问题。我们引入了一个行动分类框架，区分禁止、上下文依赖的拒绝和上下文依赖的允许行动。我们提出了四种实施架构——协议网关、SDK仪器、内核eBPF和供应商集成——具有不同的信任属性，并指定了AARM合规系统的最低符合性要求。AARM是模型无关、框架无关和供应商中立的，将行动执行视为稳定的安全边界。该规范旨在在专有碎片化导致互操作性受限之前，建立行业范围的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for enhanced security measures as AI systems transition from passive tools to autonomous agents capable of executing irreversible actions. The authors propose the Autonomous Action Runtime Management (AARM) framework, which provides a runtime security system that intercepts AI-driven actions before execution, evaluates them against established policies, and records evidence for forensic purposes. Key experimental findings include the formalization of a threat model addressing various attack vectors and the introduction of an action classification framework, alongside four implementation architectures that ensure compliance with AARM specifications while maintaining interoperability across different systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于应对人工智能系统从被动助手转变为能够执行不可逆行动的自主代理所带来的安全挑战。作者提出了一种名为自主行动运行管理（AARM）的系统，旨在通过在执行前拦截行动、根据既定政策评估行动并执行授权决策来保护AI驱动的行动。主要实验结果包括对各种攻击向量的威胁模型的形式化，以及引入行动分类框架和四种实施架构，以确保不同系统之间的合规性和互操作性。</div>
</details>
</div>
<div class="card">
<div class="title">DiffBreak: Is Diffusion-Based Purification Robust?</div>
<div class="meta-line">Authors: Andre Kassis, Urs Hengartner, Yaoliang Yu</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2024-11-25T17:30:32+00:00 · Latest: 2026-02-10T05:30:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.16598v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.16598v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP&#x27;s outputs to align with adversarial distributions. This prompts a reassessment of DBP&#x27;s robustness, attributing it to two critical flaws: incorrect gradients and inappropriate evaluation protocols that test only a single random purification of the AE. We show that with proper accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient flaws that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP&#x27;s viability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffBreak：基于扩散的净化是否稳健？</div>
<div class="mono" style="margin-top:8px">基于扩散的净化（DBP）已成为对抗样本（AEs）的基石防御，因其使用扩散模型（DMs）将AEs投影到自然数据流形上而被视为稳健。我们驳斥了这一核心主张，理论上证明基于梯度的攻击有效地针对DM而非分类器，导致DBP的输出与对抗分布一致。这促使我们重新评估DBP的稳健性，归因于两个关键缺陷：不正确的梯度和仅测试单个随机净化的评估协议。我们表明，在适当考虑随机性和重新提交风险的情况下，DBP会崩溃。为支持这一点，我们引入了DiffBreak，这是第一个可靠的通过DBP进行微分的工具包，消除了之前进一步夸大稳健性估计的梯度缺陷。我们还分析了当前用于DBP的防御方案，其中分类依赖于单个净化，指出其固有的无效性。我们提供了一种统计基础的多数投票（MV）替代方案，聚合多个净化副本的预测，显示出部分但有意义的稳健性提升。然后，我们提出了一种针对深伪水印的优化方法的新适应，制造系统性扰动，即使在MV下也能击败DBP，挑战DBP的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to critically evaluate the robustness of diffusion-based purification (DBP) as a defense against adversarial examples (AEs), which is widely considered effective due to its reliance on diffusion models. The authors employ theoretical analysis to demonstrate that gradient-based attacks can exploit the diffusion model rather than the classifier, leading to outputs that conform to adversarial distributions. Their key findings reveal that DBP&#x27;s perceived robustness is undermined by incorrect gradients and flawed evaluation protocols, and they introduce DiffBreak, a toolkit that corrects these gradient issues, while also proposing a majority-vote method that improves robustness by aggregating predictions from multiple purifications, ultimately challenging the effectiveness of DBP against sophisticated attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是批判性地评估基于扩散的净化（DBP）对抗敌对样本的鲁棒性，这种方法被广泛认为是一种强有力的防御机制。作者采用理论分析并引入DiffBreak工具包，旨在解决DBP中的梯度缺陷，同时提出了一种多数投票方法，通过聚合多个净化的预测来提高分类准确性。主要发现表明，DBP的有效性受到不正确梯度和评估协议缺陷的影响，当考虑随机性和重新提交风险时，其鲁棒性会崩溃，并且他们展示了即使采用多数投票方法，DBP仍然可以被精心设计的扰动击败。</div>
</details>
</div>
<div class="card">
<div class="title">Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models</div>
<div class="meta-line">Authors: Alireza F. Pour, Farnam Mansouri, Shai Ben-David</div>
<div class="meta-line">First: 2026-02-10T04:17:02+00:00 · Latest: 2026-02-10T04:17:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09402v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09402v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models. For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多正确答案的学习——不同反馈模型下的遗憾界限三分法</div>
<div class="mono" style="margin-top:8px">我们研究一个具有多个正确答案的在线学习问题，其中每个实例都有一组有效标签，在每一轮中，学习者必须为查询示例输出一个有效标签。这个设置受到语言生成任务的启发，其中一个提示可能有许多可接受的完成，但并不是每个完成都是可接受的。我们在三种反馈模型下研究这个问题。对于每个模型，我们使用适当的组合维度来表征可实现设置下的最优错误界限。然后，我们在不可知设置下建立三种模型之间的遗憾界限三分法。我们的结果还暗示了批量设置的样本复杂度界限，这取决于各自的组合维度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of online learning with multiple correct answers, particularly relevant to language generation tasks where prompts can have several acceptable completions. The authors investigate this problem using three distinct feedback models and characterize the optimal mistake bounds in a realizable setting through combinatorial dimensions. They establish a trichotomy of regret bounds in an agnostic setting and derive sample complexity bounds for batch setups based on the identified combinatorial dimensions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了从多个正确答案中选择的在线学习挑战，特别是与语言生成任务相关的情况，其中提示可以有多个可接受的完成。作者在三种不同的反馈模型下研究这个问题，通过组合维度表征可实现设置中的最佳错误界限。他们在这些模型的不可知设置中建立了遗憾界限的三分法，这也导致了基于各自组合维度的批量设置的样本复杂度界限。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260211_0404.html">20260211_0404</a>
<a href="archive/20260210_0406.html">20260210_0406</a>
<a href="archive/20260209_0325.html">20260209_0325</a>
<a href="archive/20260208_0323.html">20260208_0323</a>
<a href="archive/20260207_0339.html">20260207_0339</a>
<a href="archive/20260206_0339.html">20260206_0339</a>
<a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
