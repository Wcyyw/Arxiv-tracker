<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-28 19:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251028_1955</div>
    <div class="row"><div class="card">
<div class="title">Alita-G: Self-Evolving Generative Agent for Agent Generation</div>
<div class="meta-line">Authors: Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2025-10-27T17:59:14+00:00 · Latest: 2025-10-27T17:59:14+00:00</div>
<div class="meta-line">Comments: 15 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23601v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool&#x27;s descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity&#x27;s Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alita-G: 自我进化生成代理用于代理生成</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在具备记忆、工具和反馈的代理框架下表现更佳。自我进化代理的出现超越了这一点，但目前的工作主要将适应性限制在提示重写或失败重试。因此，我们提出了ALITA-G，一个自我进化框架，通过系统地生成、抽象和策划模型上下文协议（MCP）工具，将通用代理转变为领域专家。在该框架中，通用代理执行一套策划的目标领域任务，并从成功轨迹中合成候选MCP。这些候选MCP随后被抽象为参数化原语，并整合到MCP盒中。在推理时，ALITA-G借助每个工具的描述和用例执行检索增强的MCP选择，然后执行配备MCP执行器的代理。在多个基准GAIA、PathVQA和人类最后考试中，ALITA-G取得了显著的提升，同时降低了计算成本。在GAIA验证中，它达到了83.03%的pass@1和89.09%的pass@3，建立了新的最先进结果，同时相对于强基线代理减少了每个示例的平均标记约15%。因此，ALITA-G提供了一条从通用能力到可重用领域特定能力的原则性路径，提高了复杂推理任务的准确性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of large language models (LLMs) by developing self-evolving agents that can adapt beyond simple prompt rewriting. The authors introduce ALITA-G, a framework that enables a general-purpose agent to evolve into a domain expert by generating and curating Model Context Protocol (MCP) tools through systematic execution of target-domain tasks. Experimental results demonstrate that ALITA-G achieves significant improvements across benchmarks such as GAIA, where it reaches a pass rate of 83.03% at pass@1 and 89.09% at pass@3, while also reducing computation costs by approximately 15% compared to a strong baseline agent, thus establishing a new state-of-the-art performance in complex reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过将大型语言模型（LLM）发展为能够自我适应的代理，从而提高其在特定领域的表现。作者提出了ALITA-G，一个自我进化框架，使通用代理能够通过执行目标领域任务和合成成功轨迹系统地生成和策划模型上下文协议（MCP）工具。实验结果表明，ALITA-G在GAIA、PathVQA和人类最后考试等基准测试中取得了显著的改进，在GAIA验证中实现了83.03%的pass@1和89.09%的pass@3的最新性能，同时与强基线代理相比，计算成本降低了约15%。</div>
</details>
</div>
<div class="card">
<div class="title">ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</div>
<div class="meta-line">Authors: Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-11T20:13:59+00:00 · Latest: 2025-10-27T17:51:21+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight Paper at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15963v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.15963v2">PDF</a> · <a href="https://github.com/video-fm/LASER">Code1</a> · <a href="https://github.com/video-fm/ESCA">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, existing MLLMs do not reliably
capture fine-grained links between low-level visual features and high-level
textual semantics, leading to weak grounding and inaccurate perception. To
overcome this challenge, we propose ESCA, a framework that contextualizes
embodied agents by grounding their perception in spatial-temporal scene graphs.
At its core is SGCLIP, a novel, open-domain, promptable foundation model for
generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+
open-domain videos using a neurosymbolic pipeline that aligns automatically
generated captions with scene graphs produced by the model itself, eliminating
the need for human-labeled annotations. We demonstrate that SGCLIP excels in
both prompt-based inference and task-specific fine-tuning, achieving
state-of-the-art results on scene graph generation and action localization
benchmarks. ESCA with SGCLIP improves perception for embodied agents based on
both open-source and commercial MLLMs, achieving state of-the-art performance
across two embodied environments. Notably, ESCA significantly reduces agent
perception errors and enables open-source models to surpass proprietary
baselines. We release the source code for SGCLIP model training at
https://github.com/video-fm/LASER and for the embodied agent at
https://github.com/video-fm/ESCA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESCA：通过场景图生成对具身代理进行情境化</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）正在快速发展，朝着通用具身代理的方向迈进。然而，现有的MLLMs并不能可靠地捕捉低级视觉特征与高级文本语义之间的细粒度联系，导致基础不稳和感知不准确。为了解决这一挑战，我们提出了ESCA，一个通过将具身代理的感知与时空场景图相结合来进行情境化的框架。其核心是SGCLIP，一个新颖的开放域、可提示的基础模型，用于生成基于CLIP的场景图。SGCLIP在87K+开放域视频上进行训练，使用神经符号管道自动对齐生成的字幕与模型自身生成的场景图，消除了对人工标注的需求。我们证明SGCLIP在基于提示的推理和任务特定的微调方面表现出色，在场景图生成和动作定位基准测试中取得了最先进的结果。基于开源和商业MLLM的ESCA与SGCLIP结合，提高了具身代理的感知能力，在两个具身环境中实现了最先进的性能。值得注意的是，ESCA显著减少了代理感知错误，并使开源模型超越专有基线。我们在https://github.com/video-fm/LASER发布了SGCLIP模型训练的源代码，并在https://github.com/video-fm/ESCA发布了具身代理的源代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the grounding of embodied agents by improving their perception of visual features in relation to textual semantics, which is often weak in existing multi-modal large language models (MLLMs). The authors propose a framework called ESCA, which utilizes a novel foundation model named SGCLIP for generating scene graphs from open-domain videos, trained through a neurosymbolic pipeline that aligns automatically generated captions with the scene graphs, thus eliminating the need for human annotations. Experimental results show that SGCLIP achieves state-of-the-art performance in scene graph generation and action localization, significantly improving the perception of embodied agents in various environments and reducing perception errors, allowing open-source models to outperform proprietary ones.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态大型语言模型（MLLMs）在准确链接低级视觉特征与高级文本语义方面的局限性，这导致了具身代理的基础薄弱和感知问题。作者提出了一个名为ESCA的框架，利用一种基于CLIP的生成场景图的新模型SGCLIP，该模型通过神经符号管道在超过87,000个开放域视频上训练，自动生成的字幕与场景图对齐。实验结果表明，SGCLIP在场景图生成和动作定位方面取得了最先进的性能，显著改善了具身代理的感知并减少了感知错误，使开源模型在两个不同的具身环境中超越专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language   Models via Selective Layer-Wise Model Merging</div>
<div class="meta-line">Authors: Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Holger Boche</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-03-21T15:44:09+00:00 · Latest: 2025-10-27T17:40:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.17239v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.17239v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning large language models (LLMs) is a common practice to adapt
generalist models to specialized domains. However, recent studies show that
fine-tuning can erode safety alignment, causing LLMs to respond to harmful or
unethical prompts. Many methods to realign safety have been proposed, but often
introduce custom algorithms that are difficult to implement or compromise task
utility. In this work, we propose SafeMERGE, a lightweight, post-fine-tuning
framework that preserves safety while maintaining downstream performance.
SafeMERGE selectively merges fine-tuned with safety-aligned model layers only
when they deviate from safe behavior, measured by a cosine similarity
criterion. Across three LLMs and two tasks, SafeMERGE consistently reduces
harmful outputs compared to other defenses, with negligible or even positive
impact on utility. Our results demonstrate that selective layer-wise merging
offers an effective safeguard against the inadvertent loss of safety during
fine-tuning, establishing SafeMERGE as a simple post-fine-tuning defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeMERGE：通过选择性层级模型合并保持微调大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）是将通用模型适应于专业领域的常见做法。然而，最近的研究表明，微调可能会侵蚀安全对齐，导致LLMs对有害或不道德的提示做出反应。虽然提出了许多重新对齐安全的方法，但通常会引入难以实施的自定义算法或妨碍任务效用。在本研究中，我们提出了SafeMERGE，这是一种轻量级的后微调框架，能够在保持下游性能的同时保护安全。SafeMERGE仅在微调与安全对齐的模型层偏离安全行为时，选择性地合并它们，合并依据余弦相似度标准进行。在三个LLM和两个任务中，SafeMERGE始终能有效减少有害输出，相较于其他防御方法，其对效用的影响微乎其微，甚至是积极的。我们的结果表明，选择性层级合并为微调过程中意外丧失安全提供了有效的保护，确立了SafeMERGE作为一种简单的后微调防御。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of safety alignment erosion in fine-tuned large language models (LLMs), which can lead to harmful or unethical responses. The authors introduce SafeMERGE, a lightweight framework that selectively merges layers from fine-tuned models with safety-aligned layers based on a cosine similarity criterion, aiming to preserve safety without compromising performance. Experimental results across three LLMs and two tasks show that SafeMERGE effectively reduces harmful outputs compared to other methods, while maintaining or even enhancing task utility.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在微调大型语言模型（LLMs）过程中安全对齐的侵蚀问题，这可能导致有害或不道德的响应。作者提出了一种名为SafeMERGE的方法，这是一种轻量级框架，根据余弦相似度标准选择性地将微调模型的层与安全对齐的层合并。三种LLM和两项任务的实验结果表明，SafeMERGE在有效减少有害输出方面优于其他方法，同时保持甚至改善任务效用。</div>
</details>
</div>
<div class="card">
<div class="title">Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image   Synthesis</div>
<div class="meta-line">Authors: Boming Miao, Chunxiao Li, Xiaoxiao Wang, Andi Zhang, Rui Sun, Zizhe Wang, Yao Zhu</div>
<div class="meta-line">First: 2024-11-25T15:40:47+00:00 · Latest: 2025-10-27T17:31:22+00:00</div>
<div class="meta-line">Comments: Updated author formatting; no substantive changes</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2411.16503v2">Abs</a> · <a href="http://arxiv.org/pdf/2411.16503v2">PDF</a> · <a href="https://github.com/Bomingmiao/NoiseDiffusion">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have achieved impressive success in generating
photorealistic images, but challenges remain in ensuring precise semantic
alignment with input prompts. Optimizing the initial noisy latent offers a more
efficient alternative to modifying model architectures or prompt engineering
for improving semantic alignment. A latest approach, InitNo, refines the
initial noisy latent by leveraging attention maps; however, these maps capture
only limited information, and the effectiveness of InitNo is highly dependent
on the initial starting point, as it tends to converge on a local optimum near
this point. To this end, this paper proposes leveraging the language
comprehension capabilities of large vision-language models (LVLMs) to guide the
optimization of the initial noisy latent, and introduces the Noise Diffusion
process, which updates the noisy latent to generate semantically faithful
images while preserving distribution consistency. Furthermore, we provide a
theoretical analysis of the condition under which the update improves semantic
faithfulness. Experimental results demonstrate the effectiveness and
adaptability of our framework, consistently enhancing semantic alignment across
various diffusion models. The code is available at
https://github.com/Bomingmiao/NoiseDiffusion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声扩散在文本到图像合成中增强语义忠实度</div>
<div class="mono" style="margin-top:8px">扩散模型在生成逼真图像方面取得了显著成功，但在确保与输入提示的精确语义对齐方面仍然面临挑战。优化初始噪声潜变量提供了一种比修改模型架构或提示工程更有效的替代方案，以改善语义对齐。最新的方法InitNo通过利用注意力图来细化初始噪声潜变量；然而，这些图仅捕捉有限的信息，InitNo的有效性高度依赖于初始起点，因为它往往会收敛到该点附近的局部最优解。为此，本文提出利用大型视觉-语言模型（LVLM）的语言理解能力来指导初始噪声潜变量的优化，并引入噪声扩散过程，该过程更新噪声潜变量以生成语义忠实的图像，同时保持分布一致性。此外，我们提供了更新改善语义忠实度的条件的理论分析。实验结果证明了我们框架的有效性和适应性，在各种扩散模型中持续增强语义对齐。代码可在https://github.com/Bomingmiao/NoiseDiffusion获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in achieving precise semantic alignment in text-to-image synthesis using diffusion models. The authors propose a novel method called Noise Diffusion, which utilizes the language comprehension capabilities of large vision-language models to optimize the initial noisy latent, rather than relying solely on attention maps as in previous approaches. Experimental results indicate that this method consistently enhances semantic alignment across various diffusion models, demonstrating its effectiveness and adaptability in generating semantically faithful images while maintaining distribution consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像合成中使用扩散模型时确保精确语义对齐的挑战，这些模型在生成逼真的图像方面取得了成功。作者提出了一种名为噪声扩散的新方法，该方法利用大型视觉语言模型（LVLMs）来优化初始噪声潜变量，改进了现有的InitNo方法，该方法依赖于有限的注意力图。实验结果表明，噪声扩散过程有效地增强了语义对齐，同时在各种扩散模型中保持了分布一致性，证明了该框架在生成语义忠实图像方面的适应性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time</div>
<div class="meta-line">Authors: Yaoli Liu, Yao-Xiang Ding, Kun Zhou</div>
<div class="meta-line">First: 2025-10-27T16:54:08+00:00 · Latest: 2025-10-27T16:54:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23515v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://future-item.github.io/FreeFuse/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes FreeFuse, a novel training-free approach for
multi-subject text-to-image generation through automatic fusion of multiple
subject LoRAs. In contrast to existing methods that either focus on
pre-inference LoRA weight merging or rely on segmentation models and complex
techniques like noise blending to isolate LoRA outputs, our key insight is that
context-aware dynamic subject masks can be automatically derived from
cross-attention layer weights. Mathematical analysis shows that directly
applying these masks to LoRA outputs during inference well approximates the
case where the subject LoRA is integrated into the diffusion model and used
individually for the masked region. FreeFuse demonstrates superior practicality
and efficiency as it requires no additional training, no modification to LoRAs,
no auxiliary models, and no user-defined prompt templates or region
specifications. Alternatively, it only requires users to provide the LoRA
activation words for seamless integration into standard workflows. Extensive
experiments validate that FreeFuse outperforms existing approaches in both
generation quality and usability under the multi-subject generation tasks. The
project page is at https://future-item.github.io/FreeFuse/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeFuse：通过测试时自动掩码实现多主题LoRA融合</div>
<div class="mono" style="margin-top:8px">本文提出了FreeFuse，一种新颖的无训练方法，通过自动融合多个主题LoRA实现多主题文本到图像生成。与现有方法（这些方法要么专注于推理前的LoRA权重合并，要么依赖于分割模型和复杂技术如噪声混合来隔离LoRA输出）不同，我们的关键见解是可以从交叉注意力层权重中自动推导出上下文感知的动态主题掩码。数学分析表明，在推理过程中将这些掩码直接应用于LoRA输出，可以很好地近似将主题LoRA集成到扩散模型中并单独用于掩码区域的情况。FreeFuse展示了卓越的实用性和效率，因为它不需要额外的训练，不需要修改LoRA，不需要辅助模型，也不需要用户定义的提示模板或区域规范。相反，它只需要用户提供LoRA激活词，以便无缝集成到标准工作流程中。大量实验验证了FreeFuse在多主题生成任务中在生成质量和可用性方面优于现有方法。项目页面为 https://future-item.github.io/FreeFuse/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve multi-subject text-to-image generation without the need for additional training or complex techniques. The authors propose FreeFuse, a training-free method that utilizes context-aware dynamic subject masks derived from cross-attention layer weights to automatically fuse multiple subject LoRAs during inference. Experimental results show that FreeFuse significantly enhances generation quality and usability compared to existing methods, demonstrating its practicality by requiring only LoRA activation words for integration into standard workflows.</div>
<div class="mono" style="margin-top:8px">本研究针对多主体文本到图像生成的挑战，提出了一种名为FreeFuse的无训练方法，该方法通过从交叉注意力层权重中自动生成上下文感知的动态主体掩码来融合多个主体LoRA。该方法避免了现有技术需要预推理权重合并或复杂分割方法的局限性。实验结果表明，FreeFuse在生成质量和可用性方面显著优于传统方法，成为在不需要额外训练或对现有模型进行修改的情况下集成多个主体的实用解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Large Language Models for Stance Detection on Financial   Targets from SEC Filing Reports and Earnings Call Transcripts</div>
<div class="meta-line">Authors: Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea</div>
<div class="meta-line">First: 2025-10-27T16:03:20+00:00 · Latest: 2025-10-27T16:03:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23464v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23464v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs&#x27; performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型在金融目标的立场检测中的应用：基于SEC文件报告和财报电话会议记录</div>
<div class="mono" style="margin-top:8px">来自美国证券交易委员会（SEC）文件报告和季度财报电话会议记录（ECTs）的金融叙述对投资者、审计师和监管机构非常重要。然而，它们的长度、金融术语和细微的语言使得细粒度分析变得困难。以往在金融领域的情感分析需要一个大型、昂贵的标注数据集，使得对特定金融目标的句子级立场分析具有挑战性。在本研究中，我们引入了一个专注于三个核心金融指标（债务、每股收益（EPS）和销售额）的句子级语料库进行立场检测。这些句子来自Form 10-K年度报告和ECTs，并使用先进的ChatGPT-o3-pro模型在严格的人为验证下进行立场标注（积极、消极、中立）。利用该语料库，我们对现代大型语言模型（LLMs）进行了系统评估，采用零-shot、少量-shot和思维链（CoT）提示策略。我们的结果表明，少量-shot与CoT提示的表现优于监督基线，LLMs在SEC和ECT数据集上的表现各异。我们的研究结果突显了在金融领域利用LLMs进行目标特定立场分析的实际可行性，而无需大量标注数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenges in analyzing financial narratives from SEC filing reports and earnings call transcripts due to their complexity and length. The authors developed a sentence-level corpus for stance detection, focusing on key financial metrics such as debt, earnings per share, and sales, by extracting and labeling sentences from Form 10-K reports and earnings call transcripts using the ChatGPT-o3-pro model with human validation. The systematic evaluation of various large language models revealed that few-shot prompting with Chain-of-Thought strategies outperformed supervised baselines, indicating that LLMs can effectively determine target-specific stance in financial texts without the need for extensive labeled datasets.</div>
<div class="mono" style="margin-top:8px">本研究解决了在SEC文件报告和财报电话会议记录中进行立场检测的挑战，这些文本通常较长且充满复杂的术语。作者引入了一个新的句子级语料库，重点关注三项财务指标：债务、每股收益和销售，句子通过ChatGPT-o3-pro模型进行立场标注，并经过人工验证。对各种大型语言模型进行零样本、少样本和思维链提示策略的评估表明，少样本与思维链提示的组合优于监督基线，表明在金融领域进行立场检测时，LLM的潜力无需大量标注数据。</div>
</details>
</div>
<div class="card">
<div class="title">Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal   Queries</div>
<div class="meta-line">Authors: Pengfei Cai, Yan Song, Qing Gu, Nan Jiang, Haoyu Song, Ian McLoughlin</div>
<div class="meta-line">Venue: MM 2025</div>
<div class="meta-line">First: 2025-07-22T08:24:01+00:00 · Latest: 2025-10-27T15:55:48+00:00</div>
<div class="meta-line">Comments: Accepted by MM 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.16343v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.16343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cai525.github.io/Transformer4SED/demo_page/DASM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most existing sound event detection~(SED) algorithms operate under a
closed-set assumption, restricting their detection capabilities to predefined
classes. While recent efforts have explored language-driven zero-shot SED by
exploiting audio-language models, their performance is still far from
satisfactory due to the lack of fine-grained alignment and cross-modal feature
fusion. In this work, we propose the Detect Any Sound Model (DASM), a
query-based framework for open-vocabulary SED guided by multi-modal queries.
DASM formulates SED as a frame-level retrieval task, where audio features are
matched against query vectors derived from text or audio prompts. To support
this formulation, DASM introduces a dual-stream decoder that explicitly
decouples event recognition and temporal localization: a cross-modality event
decoder performs query-feature fusion and determines the presence of sound
events at the clip-level, while a context network models temporal dependencies
for frame-level localization. Additionally, an inference-time attention masking
strategy is proposed to leverage semantic relations between base and novel
classes, substantially enhancing generalization to novel classes. Experiments
on the AudioSet Strong dataset demonstrate that DASM effectively balances
localization accuracy with generalization to novel classes, outperforming
CLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in
the closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot
evaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the
supervised CRNN baseline. The project page is available at
https://cai525.github.io/Transformer4SED/demo_page/DASM/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检测任何声音：基于多模态查询的开放词汇声音事件检测</div>
<div class="mono" style="margin-top:8px">大多数现有的声音事件检测（SED）算法在封闭集假设下运行，限制了它们对预定义类别的检测能力。尽管最近的努力通过利用音频-语言模型探索了基于语言的零-shot SED，但由于缺乏细粒度对齐和跨模态特征融合，其性能仍然远未令人满意。在这项工作中，我们提出了检测任何声音模型（DASM），这是一个基于查询的开放词汇SED框架，由多模态查询引导。DASM将SED公式化为帧级检索任务，其中音频特征与来自文本或音频提示的查询向量进行匹配。为了支持这一公式，DASM引入了一个双流解码器，明确解耦事件识别和时间定位：跨模态事件解码器执行查询-特征融合并确定声音事件在片段级别的存在，而上下文网络则建模帧级定位的时间依赖性。此外，提出了一种推理时注意力掩蔽策略，以利用基础类和新类之间的语义关系，显著增强对新类的泛化能力。在AudioSet Strong数据集上的实验表明，DASM有效平衡了定位准确性与对新类的泛化能力，在开放词汇设置中超越了基于CLAP的方法（+7.8 PSDS）和封闭集设置中的基线（+6.9 PSDS）。此外，在DESED的跨数据集零-shot评估中，DASM获得了42.2的PSDS1分数，甚至超过了监督的CRNN基线。项目页面可访问https://cai525.github.io/Transformer4SED/demo_page/DASM/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve sound event detection (SED) capabilities beyond predefined classes, addressing limitations in existing algorithms that operate under a closed-set assumption. The authors propose the Detect Any Sound Model (DASM), a query-based framework that treats SED as a frame-level retrieval task, utilizing a dual-stream decoder to separate event recognition from temporal localization. Experimental results on the AudioSet Strong dataset show that DASM significantly enhances both localization accuracy and generalization to novel classes, outperforming CLAP-based methods by 7.8 PSDS and a closed-set baseline by 6.9 PSDS, while also achieving a PSDS1 score of 42.2 in cross-dataset zero-shot evaluation on DESED, surpassing the supervised CRNN baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善声音事件检测（SED）的能力，使其超越预定义类别，解决现有算法在封闭集假设下的局限性。作者提出了检测任何声音模型（DASM），这是一种基于查询的框架，将SED形式化为帧级检索任务，利用双流解码器将事件识别与时间定位分开。对AudioSet Strong数据集的实验结果表明，DASM显著提高了定位准确性和对新类别的泛化能力，超越了基于CLAP的方法7.8 PSDS和封闭集基线6.9 PSDS，同时在DESED的跨数据集零样本评估中获得了42.2的PSDS1分数，超过了监督CRNN基线。</div>
</details>
</div>
<div class="card">
<div class="title">Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</div>
<div class="meta-line">Authors: Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-05-26T16:53:47+00:00 · Latest: 2025-10-27T14:42:01+00:00</div>
<div class="meta-line">Comments: Published as a main conference paper at EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23799v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.23799v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are prone to hallucinations and sensitiveto
prompt perturbations, often resulting in inconsistent or unreliablegenerated
text. Different methods have been proposed to mitigate suchhallucinations and
fragility, one of which is to measure theconsistency of LLM responses -- the
model&#x27;s confidence in the responseor likelihood of generating a similar
response when resampled. Inprevious work, measuring LLM response consistency
often relied oncalculating the probability of a response appearing within a
pool of resampledresponses, analyzing internal states, or evaluating logits of
resopnses.However, it was not clear how well theseapproaches approximated
users&#x27; perceptions of consistency of LLMresponses. To find out, we performed a
user study ($n=2,976$)demonstrating that current methods for measuring LLM
responseconsistency typically do not align well with humans&#x27; perceptions of
LLMconsistency. We propose a logit-based ensemble method for estimatingLLM
consistency and show that our method matches the performance of
thebest-performing existing metric in estimating human ratings of
LLMconsistency. Our results suggest that methods for estimating LLMconsistency
without human evaluation are sufficiently imperfect towarrant broader use of
evaluation with human input; this would avoidmisjudging the adequacy of models
because of the imperfections ofautomated consistency metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>估计大型语言模型一致性：用户基线与替代指标</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）容易出现幻觉，并对提示扰动敏感，常常导致生成文本不一致或不可靠。为减轻这种幻觉和脆弱性，提出了不同的方法，其中之一是测量LLM响应的一致性——模型对响应的信心或在重新采样时生成相似响应的可能性。在之前的研究中，测量LLM响应一致性通常依赖于计算响应在重新采样响应池中出现的概率、分析内部状态或评估响应的logits。然而，这些方法与用户对LLM响应一致性的感知之间的匹配程度并不明确。为此，我们进行了用户研究（$n=2,976$），证明当前测量LLM响应一致性的方法通常与人类对LLM一致性的感知不太一致。我们提出了一种基于logit的集成方法来估计LLM一致性，并显示我们的方法在估计人类对LLM一致性的评分时与表现最佳的现有指标的性能相匹配。我们的结果表明，未经过人类评估的LLM一致性估计方法存在足够的缺陷，值得更广泛地使用人类输入的评估；这将避免因自动一致性指标的缺陷而错误判断模型的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inconsistency and unreliability of large language models (LLMs), which often produce hallucinations and are sensitive to prompt changes. The authors conducted a user study with 2,976 participants to evaluate how well existing methods for measuring LLM response consistency align with human perceptions. The findings reveal that these methods generally do not reflect users&#x27; views on consistency, leading to the proposal of a logit-based ensemble method that performs comparably to the best existing metrics in estimating human ratings of LLM consistency, highlighting the need for human evaluation in assessing model performance accurately.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）的一致性和可靠性问题，这些模型往往容易产生幻觉并对提示变化敏感。作者进行了一个包含2976名参与者的用户研究，以比较现有的LLM响应一致性测量方法与人类对一致性的感知。研究结果表明，传统指标与用户感知不太一致，作者提出了一种基于logit的集成方法，其性能与现有最佳指标相当，强调了在评估LLM一致性时需要进行人类评估，以避免基于有缺陷的自动化指标而产生的误判。</div>
</details>
</div>
<div class="card">
<div class="title">An Efficient Remote Sensing Super Resolution Method Exploring Diffusion   Priors and Multi-Modal Constraints for Crop Type Mapping</div>
<div class="meta-line">Authors: Songxi Yang, Tang Sui, Qunying Huang</div>
<div class="meta-line">First: 2025-10-27T14:34:52+00:00 · Latest: 2025-10-27T14:34:52+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23382v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super resolution offers a way to harness medium even lowresolution but
historically valuable remote sensing image archives. Generative models,
especially diffusion models, have recently been applied to remote sensing super
resolution (RSSR), yet several challenges exist. First, diffusion models are
effective but require expensive training from scratch resources and have slow
inference speeds. Second, current methods have limited utilization of auxiliary
information as real-world constraints to reconstruct scientifically realistic
images. Finally, most current methods lack evaluation on downstream tasks. In
this study, we present a efficient LSSR framework for RSSR, supported by a new
multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built
on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention
with auxiliary knowledge (Digital Elevation Model, land cover, month) and
Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier
NDVI loss to balance spatial details and spectral fidelity. Extensive
experiments demonstrate that LSSR significantly improves crop boundary
delineation and recovery, achieving state-of-the-art performance with Peak
Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)
and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while
maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers
effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,
yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:
0.85). These results highlight the potential of RSSR to advance precision
agriculture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的遥感超分辨率方法：探索扩散先验和多模态约束用于作物类型映射</div>
<div class="mono" style="margin-top:8px">超分辨率提供了一种利用中等甚至低分辨率但历史上有价值的遥感图像档案的方法。生成模型，特别是扩散模型，最近被应用于遥感超分辨率（RSSR），但仍存在若干挑战。首先，扩散模型有效但需要昂贵的从零开始的训练资源，并且推理速度较慢。其次，当前方法对辅助信息的利用有限，无法作为现实世界约束来重建科学上真实的图像。最后，大多数当前方法缺乏对下游任务的评估。在本研究中，我们提出了一种高效的LSSR框架用于RSSR，支持一个新的多模态数据集，包含配对的30米Landsat 8和10米Sentinel 2影像。基于冻结的预训练稳定扩散，LSSR将跨模态注意力与辅助知识（数字高程模型、土地覆盖、月份）和合成孔径雷达指导相结合，通过适配器和定制的傅里叶NDVI损失来平衡空间细节和光谱保真度。大量实验表明，LSSR显著改善了作物边界的描绘和恢复，达到最先进的性能，峰值信噪比/结构相似性指数分别为32.63/0.84（RGB）和23.99/0.78（IR），NDVI均方误差最低（0.042），同时保持高效推理（0.39秒/图像）。此外，LSSR有效转移到NASA协调的Landsat和Sentinel（HLS）超分辨率，产生比Sentinel-2更可靠的作物分类（F1: 0.86，Sentinel-2: F1: 0.85）。这些结果突显了RSSR在推动精准农业方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the utilization of low-resolution remote sensing images for crop type mapping, addressing challenges such as expensive training and slow inference speeds associated with diffusion models. The authors propose an efficient LSSR framework that leverages a new multimodal dataset of paired Landsat 8 and Sentinel 2 imagery, integrating cross-modal attention with auxiliary knowledge and a tailored Fourier NDVI loss. Experimental results show that LSSR significantly enhances crop boundary delineation and recovery, achieving state-of-the-art performance metrics and efficient inference times, while also demonstrating effective transferability to NASA&#x27;s HLS super resolution for improved crop classification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过超分辨率技术增强低分辨率遥感图像在作物类型映射中的实用性。作者提出了一种高效的LSSR框架，该框架利用了一组配对的Landsat 8和Sentinel 2影像的新多模态数据集，采用了预训练的稳定扩散模型，并结合了跨模态注意力和辅助知识，如数字高程模型和土地覆盖信息。实验结果表明，LSSR框架显著改善了作物边界的划分和恢复，达到了最先进的性能指标，包括峰值信噪比32.63和均方误差0.042，同时在NASA的HLS超分辨率中也表现出有效的可转移性，能够实现可靠的作物分类。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning as an Adaptive Defense for Safety</div>
<div class="meta-line">Authors: Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar</div>
<div class="meta-line">First: 2025-07-01T17:20:04+00:00 · Latest: 2025-10-27T14:28:11+00:00</div>
<div class="meta-line">Comments: 44 pages, 10 Figures, 7 Tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.00971v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.00971v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a ``lightweight&#x27;&#x27; warmstart SFT stage, (2) a mix of
harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as
too many refusals, and (3) a reward function to prevent degeneration of
reasoning capabilities during training. Models trained with TARS exhibit
adaptive behaviors by spending more compute on ambiguous queries, leading to
better safety-refusal trade-offs. They also internally learn to better
distinguish between safe and unsafe prompts and attain greater robustness to
both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our
work provides an effective, open recipe for training LLMs against jailbreaks
and harmful requests by reasoning per prompt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>作为安全适应性防御的推理</div>
<div class="mono" style="margin-top:8px">适应性分配测试时计算的推理方法提高了大型语言模型在易于验证的领域（如数学和代码）上的表现。在这项工作中，我们研究如何利用这种方法训练在安全漏洞方面表现出一定鲁棒性的模型，并展示这样做可以带来好处。我们构建了一种名为$\textit{TARS}$（安全训练适应性推理器）的方案，这是一种强化学习（RL）方法，训练模型通过思维链迹和一个平衡安全与任务完成的奖励信号来推理安全性。为了构建TARS，我们确定了三个关键设计选择：（1）一个“轻量级”热启动SFT阶段，（2）混合有害、无害和模糊提示以防止过多拒绝等捷径行为，以及（3）一个奖励函数以防止训练过程中推理能力的退化。使用TARS训练的模型通过在模糊查询上花费更多计算展现出适应性行为，从而实现更好的安全与拒绝权衡。它们还在内部学习更好地区分安全和不安全的提示，并在白盒（例如GCG）和黑盒攻击（例如PAIR）方面获得更大的鲁棒性。总体而言，我们的工作提供了一种有效的、开放的方案，用于通过逐个提示推理来训练大型语言模型以抵御越狱和有害请求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the robustness of large language models (LLMs) against safety vulnerabilities by employing adaptive reasoning methods. The authors propose a reinforcement learning approach called TARS (Training Adaptive Reasoners for Safety), which trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. Experimental results demonstrate that models trained with TARS exhibit improved adaptive behaviors, allocating more computational resources to ambiguous queries, which leads to better safety-refusal trade-offs and enhanced robustness against both white-box and black-box attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用自适应推理方法增强大型语言模型（LLM）对安全漏洞的鲁棒性，这些方法在简单领域中已提高了性能。作者提出了一种名为TARS（安全自适应推理训练）的强化学习方法，该方法通过思维链迹和平衡安全与任务完成的奖励信号来训练模型进行安全推理。实验结果表明，使用TARS训练的模型表现出自适应行为，在模糊查询上分配更多计算资源，从而改善了安全拒绝权衡，并在白盒和黑盒攻击中表现出更大的鲁棒性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
