<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-25 03:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260125_0317</div>
    <div class="row"><div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：文档的基础视觉-语言前端</div>
<div class="mono" style="margin-top:8px">GutenOCR是一系列通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的基础OCR前端。生成的单检查点视觉-语言模型通过统一的基于提示的接口展示阅读、检测和定位。模型在商业文档、科学文章和合成定位数据上进行训练，支持全页和局部阅读，具有行级和段落级边界框以及条件“x在哪里？”查询。我们引入了一种基础OCR评估协议，并显示GutenOCR-7B在10.5K保留的商业和科学页面上将其Qwen2.5-VL-7B主干的复合基础OCR分数提高了两倍以上（从0.40提高到0.82）。在Fox和OmniDocBench v1.5上，我们的方法显著改善了区域和行级OCR以及文本检测召回，但在页面级线性化、颜色引导OCR和公式密集布局方面显示出权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GutenOCR is to enhance the capabilities of optical character recognition (OCR) systems for reading and understanding documents through a unified vision-language model. The method involves fine-tuning two versions of the Qwen2.5-VL model to create a grounded OCR front-end that can handle full-page and localized reading tasks, utilizing a prompt-based interface. Key experimental findings indicate that GutenOCR-7B significantly improves the grounded OCR score from 0.40 to 0.82 on a dataset of 10.5K business and scientific pages, while also enhancing region- and line-level OCR performance, although it presents challenges in page-level linearization and handling complex layouts.</div>
<div class="mono" style="margin-top:8px">GutenOCR的研究动机在于通过整合基础视觉语言模型来增强光学字符识别（OCR）的能力，以改善文档的阅读和理解。该方法涉及对两种版本的Qwen2.5-VL模型进行微调，使用包括商业文档、科学文章和合成基础数据在内的多样化数据集，使模型能够执行全页和局部阅读，并提供精确的边界框。主要实验结果表明，GutenOCR-7B在10.5K商业和科学页面的数据集上将复合基础OCR得分从0.40显著提高到0.82，同时改善了区域和行级OCR性能，但在页面级线性化和处理复杂布局方面存在挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</div>
<div class="meta-line">Authors: Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</div>
<div class="meta-line">First: 2026-01-22T18:58:16+00:00 · Latest: 2026-01-22T18:58:16+00:00</div>
<div class="meta-line">Comments: website: https://rae-dit.github.io/scale-rae/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rae-dit.github.io/scale-rae/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用表示自编码器扩展文本到图像扩散变换器</div>
<div class="mono" style="margin-top:8px">表示自编码器（RAEs）在ImageNet上的扩散建模中显示出明显优势，通过在高维语义潜在空间中进行训练。在这项工作中，我们研究了该框架是否可以扩展到大规模、自由形式的文本到图像（T2I）生成。我们首先在冻结的表示编码器（SigLIP-2）上扩展RAE解码器，超越ImageNet，训练于网络、合成和文本渲染数据，发现尽管规模提高了整体保真度，但特定领域（如文本）的目标数据组合至关重要。然后，我们严格测试了最初为ImageNet提出的RAE设计选择。我们的分析表明，扩展简化了框架：尽管维度相关的噪声调度仍然至关重要，但诸如宽扩散头和噪声增强解码等架构复杂性在规模上提供的好处微乎其微。在这个简化的框架基础上，我们对RAE与最先进的FLUX VAE在0.5B到9.8B参数的扩散变换器规模上进行了受控比较。RAEs在所有模型规模的预训练中始终优于VAEs。此外，在高质量数据集上进行微调时，基于VAE的模型在64个周期后灾难性过拟合，而RAE模型在256个周期内保持稳定并实现持续更好的性能。在所有实验中，基于RAE的扩散模型显示出更快的收敛速度和更好的生成质量，确立了RAEs作为大规模T2I生成的比VAEs更简单、更强大的基础。此外，由于视觉理解和生成都可以在共享表示空间中操作，多模态模型可以直接对生成的潜在变量进行推理，为统一模型开辟了新的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to explore the scalability of Representation Autoencoders (RAEs) for large-scale text-to-image (T2I) generation, building on their advantages in diffusion modeling observed in ImageNet. The authors scaled RAE decoders using a frozen representation encoder and trained on diverse datasets, finding that while increased scale enhances general fidelity, specific data composition is crucial for text-related tasks. The study reveals that scaling simplifies the RAE framework, with dimension-dependent noise scheduling being vital, while complex architectural features provide minimal benefits. In controlled comparisons against state-of-the-art FLUX VAEs, RAEs consistently outperform VAEs in pretraining across various model sizes and maintain stability during finetuning, achieving superior performance and faster convergence, thus establishing RAEs as a more effective foundation for large-scale T2I generation.</div>
<div class="mono" style="margin-top:8px">本研究探讨了表示自编码器（RAE）在大规模文本到图像（T2I）生成中的可扩展性，动机源于其在ImageNet上的扩散建模优势。作者使用冻结的表示编码器扩展RAE解码器，并在多样化的数据集上进行训练，发现虽然更大的规模提高了整体保真度，但特定数据组成对文本领域至关重要。实验结果表明，扩展简化了RAE框架，维度依赖的噪声调度至关重要，而复杂的架构特征提供的好处微乎其微。在与最先进的FLUX VAE的对比实验中，RAE在预训练中始终优于VAE，并在微调过程中保持稳定，取得更好的性能和更快的收敛速度，从而确立了RAE作为大规模T2I生成的更有效基础。</div>
</details>
</div>
<div class="card">
<div class="title">360Anything: Geometry-Free Lifting of Images and Videos to 360°</div>
<div class="meta-line">Authors: Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker, Saurabh Saxena</div>
<div class="meta-line">First: 2026-01-22T18:45:59+00:00 · Latest: 2026-01-22T18:45:59+00:00</div>
<div class="meta-line">Comments: Project page: https://360anything.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://360anything.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything&#x27;s deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>360Anything：无几何图形的图像和视频提升至360°</div>
<div class="mono" style="margin-top:8px">将透视图像和视频提升为360°全景图可以实现沉浸式3D世界生成。现有方法通常依赖于透视图与等矩形投影（ERP）空间之间的显式几何对齐。然而，这需要已知的相机元数据，这使得在缺乏或噪声较大的野外数据中应用变得困难。我们提出了360Anything，一个基于预训练扩散变换器的无几何框架。通过将透视输入和全景目标简单地视为令牌序列，360Anything以纯数据驱动的方式学习透视到等矩形的映射，消除了对相机信息的需求。我们的方法在图像和视频透视到360°生成上实现了最先进的性能，超越了使用真实相机信息的先前工作。我们还追踪到ERP边界处接缝伪影的根本原因是VAE编码器中的零填充，并引入了循环潜在编码以促进无缝生成。最后，我们在零-shot相机视场和方向估计基准测试中展示了竞争性结果，证明了360Anything的深层几何理解和在计算机视觉任务中的更广泛应用。更多结果可在https://360anything.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enable the lifting of perspective images and videos to 360° panoramas without relying on explicit geometric alignment, which is often hindered by the lack of accurate camera metadata in real-world data. The authors propose a geometry-free framework called 360Anything that utilizes pre-trained diffusion transformers to learn the mapping from perspective inputs to equirectangular outputs purely through data-driven methods. Experimental results demonstrate that 360Anything achieves state-of-the-art performance in generating 360° panoramas from both images and videos, surpassing previous methods that depend on ground-truth camera information, while also addressing seam artifacts through a novel Circular Latent Encoding technique and showing competitive results in zero-shot camera field of view and orientation estimation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过将透视图像和视频提升为360°全景，来实现沉浸式3D世界的生成，而无需依赖于明确的几何对齐，这在现实数据中常常受到缺乏准确相机元数据的限制。作者提出了一种名为360Anything的无几何框架，该框架利用预训练的扩散变换器以数据驱动的方式学习透视输入与等矩形输出之间的映射。实验结果表明，360Anything在生成图像和视频的360°全景方面达到了最先进的性能，超越了依赖真实相机信息的先前方法，同时通过一种新颖的循环潜在编码技术解决了接缝伪影问题，并在零-shot相机视场和方向估计任务中显示出竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">From Text to Image: Exploring GPT-4Vision&#x27;s Potential in Advanced Radiological Analysis across Subspecialties</div>
<div class="meta-line">Authors: Felix Busch, Tianyu Han, Marcus Makowski, Daniel Truhn, Keno Bressem, Lisa Adams</div>
<div class="meta-line">Venue: J Med Internet Res 2024;26:e54948</div>
<div class="meta-line">First: 2023-11-24T15:39:29+00:00 · Latest: 2026-01-22T18:06:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.14777v2">Abs</a> · <a href="https://arxiv.org/pdf/2311.14777v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The study evaluates and compares GPT-4 and GPT-4Vision for radiological tasks, suggesting GPT-4Vision may recognize radiological features from images, thereby enhancing its diagnostic potential over text-based descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从文本到图像：探索GPT-4Vision在各亚专业高级放射学分析中的潜力</div>
<div class="mono" style="margin-top:8px">本研究评估并比较了GPT-4和GPT-4Vision在放射学任务中的表现，建议GPT-4Vision可能能够从图像中识别放射学特征，从而增强其相较于基于文本描述的诊断潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the capabilities of GPT-4 and GPT-4Vision in performing advanced radiological analysis, motivated by the need for improved diagnostic tools in radiology. The researchers conducted evaluations and comparisons of both models to assess their performance on radiological tasks. The findings indicate that GPT-4Vision demonstrates a greater ability to recognize radiological features from images, suggesting its enhanced diagnostic potential compared to traditional text-based descriptions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了GPT-4Vision在高级放射学分析中的能力，动机是为了满足放射学中对改进诊断工具的需求。研究人员比较了GPT-4和GPT-4Vision在识别图像中的放射学特征与基于文本的描述方面的表现。结果表明，GPT-4Vision通过有效识别图像中的特征，展现出更强的诊断潜力，表明其优于传统的基于文本的方法。</div>
</details>
</div>
<div class="card">
<div class="title">ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</div>
<div class="meta-line">Authors: Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier</div>
<div class="meta-line">First: 2026-01-22T17:41:13+00:00 · Latest: 2026-01-22T17:41:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16148v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes &quot;in action&quot; in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed &quot;temporal 3D diffusion&quot;. Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActionMesh：基于时间3D扩散的动画3D网格生成</div>
<div class="mono" style="margin-top:8px">生成动画3D对象是许多应用的核心，但大多数先进的作品通常因其有限的设置、长时间的运行或有限的质量而难以实际应用。我们介绍了ActionMesh，这是一种生成模型，以前馈方式预测“动态”生产就绪的3D网格。我们的关键见解是修改现有的3D扩散模型以包含时间轴，从而形成我们称之为“时间3D扩散”的框架。具体而言，我们首先调整3D扩散阶段，以生成一系列同步的潜变量，表示时间变化和独立的3D形状。其次，我们设计了一个时间3D自编码器，将一系列独立形状转换为预定义参考形状的相应变形，从而构建动画。结合这两个组件，ActionMesh能够从单目视频、文本描述或甚至带有描述其动画的文本提示的3D网格等不同输入生成动画3D网格。此外，与之前的方法相比，我们的方法快速且生成的结果无骨架且拓扑一致，从而实现快速迭代和无缝应用，如纹理处理和重定向。我们在标准视频到4D基准（Consistent4D，Objaverse）上评估了我们的模型，并报告了在几何精度和时间一致性方面的最新性能，证明我们的模型能够以空前的速度和质量交付动画3D网格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of animated 3D objects, which often face challenges related to setup complexity, runtime, and quality. The authors introduce ActionMesh, a generative model that employs a novel approach called temporal 3D diffusion, modifying existing 3D diffusion models to incorporate a temporal axis. The key experimental findings demonstrate that ActionMesh can generate animated 3D meshes from various inputs, such as monocular videos and text prompts, achieving state-of-the-art performance in geometric accuracy and temporal consistency while being faster and producing rig-free, topology-consistent results.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善动画3D物体的生成，而现有方法常常受到实际限制的困扰。作者提出了ActionMesh，这是一种生成模型，利用修改后的3D扩散方法，结合时间轴以前馈方式生成3D网格。主要发现表明，ActionMesh能够从多种输入生成高质量的动画3D网格，在标准基准测试中在几何精度和时间一致性方面实现了最先进的性能，同时比以前的方法更快且更具多样性。</div>
</details>
</div>
<div class="card">
<div class="title">ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation</div>
<div class="meta-line">Authors: Yuan Lin, Murong Xu, Marc Hölle, Chinmay Prabhakar, Andreas Maier, Vasileios Belagiannis, Bjoern Menze, Suprosanna Shit</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2026-01-22T15:56:21+00:00 · Latest: 2026-01-22T15:56:21+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures. It has been accepted by IEEE ISBI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16060v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProGiDiff：基于提示引导的扩散医学图像分割</div>
<div class="mono" style="margin-top:8px">广泛采用的医学图像分割方法虽然高效，但主要是确定性的，且对自然语言提示的适应性较差。因此，它们缺乏估计多个提案、人机交互和跨模态适应的能力。最近，文本到图像的扩散模型显示出弥合这一差距的潜力。然而，从头开始训练它们需要大量数据集，这对医学图像分割构成了限制。此外，它们通常仅限于二元分割，无法基于自然语言提示进行条件化。为此，我们提出了一种新框架ProGiDiff，利用现有的图像生成模型进行医学图像分割。具体而言，我们提出了一种ControlNet风格的条件机制，配备定制编码器，适合图像条件化，以引导预训练的扩散模型输出分割掩膜。通过提示目标器官，它自然扩展到多类设置。我们在CT图像的器官分割实验中展示了与之前方法相比的强大性能，并且在专家参与的设置中可以大大受益于多个提案。重要的是，我们证明了学习到的条件机制可以通过低秩、少量样本适应轻松转移到MR图像的分割。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing medical image segmentation methods, which are primarily deterministic and do not effectively utilize natural language prompts. The authors propose a novel framework called ProGiDiff that employs a ControlNet-style conditioning mechanism with a custom encoder to adapt a pre-trained diffusion model for medical image segmentation. Experimental results show that ProGiDiff achieves strong performance in organ segmentation from CT images, outperforming previous methods, and demonstrates the ability to transfer the learned conditioning mechanism for segmenting MR images through low-rank, few-shot adaptation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善医学图像分割方法，这些方法通常是确定性的，并且不适合自然语言提示，从而限制了它们的适应性和提案生成能力。作者提出了ProGiDiff框架，利用现有的图像生成模型和一种ControlNet风格的条件机制，结合自定义编码器，引导预训练的扩散模型生成分割掩膜。在CT图像的器官分割实验中，ProGiDiff的表现优于以往的方法，并且能够通过低秩、少量样本适应有效地适应MR图像分割，突显了其在专家辅助应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Embracing Ambiguity: Bayesian Nonparametrics and Stakeholder Participation for Ambiguity-Aware Safety Evaluation</div>
<div class="meta-line">Authors: Yanan Long</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-04-21T16:31:15+00:00 · Latest: 2026-01-22T15:49:05+00:00</div>
<div class="meta-line">Comments: AAAI 2026 workshop MURE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15211v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.15211v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluations of generative AI models often collapse nuanced behaviour into a single number computed for a single decoding configuration. Such point estimates obscure tail risks, demographic disparities, and the existence of multiple near-optimal operating points. We propose a unified framework that embraces multiplicity by modelling the distribution of harmful behaviour across the entire space of decoding knobs and prompts, quantifying risk through tail-focused metrics, and integrating stakeholder preferences. Our technical contributions are threefold: (i) we formalise decoding Rashomon sets, regions of knob space whose risk is near-optimal under given criteria and measure their size and disagreement; (ii) we develop a dependent Dirichlet process (DDP) mixture with stakeholder-conditioned stick-breaking weights to learn multi-modal harm surfaces; and (iii) we introduce an active sampling pipeline that uses Bayesian deep learning surrogates to explore knob space efficiently. Our approach bridges multiplicity theory, Bayesian nonparametrics, and stakeholder-aligned sensitivity analysis, paving the way for trustworthy deployment of generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拥抱模糊性：贝叶斯非参数方法与利益相关者参与的模糊安全评估</div>
<div class="mono" style="margin-top:8px">生成性人工智能模型的评估通常将细微的行为简化为单一数字，该数字是针对单一解码配置计算的。这种点估计掩盖了尾部风险、人口差异以及多个近似最优操作点的存在。我们提出了一个统一框架，通过对整个解码旋钮和提示空间中有害行为的分布建模，量化风险并整合利益相关者偏好，从而拥抱多样性。我们的技术贡献有三方面：（i）我们形式化了解码Rashomon集，即在给定标准下风险接近最优的旋钮空间区域，并测量其大小和分歧；（ii）我们开发了一种依赖于Dirichlet过程（DDP）混合模型，使用利益相关者条件的粘性权重来学习多模态危害表面；（iii）我们引入了一种主动采样管道，利用贝叶斯深度学习代理高效探索旋钮空间。我们的方法桥接了多样性理论、贝叶斯非参数方法和利益相关者对齐的敏感性分析，为生成模型的可信部署铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of traditional evaluations of generative AI models, which often reduce complex behaviors to single metrics, thereby masking important risks and disparities. The authors propose a unified framework that models the distribution of harmful behaviors across various decoding configurations, utilizing tail-focused metrics and incorporating stakeholder preferences. Key findings include the formalization of decoding Rashomon sets to measure risk and disagreement, the development of a dependent Dirichlet process mixture for learning multi-modal harm surfaces, and the introduction of an active sampling pipeline that efficiently explores the knob space using Bayesian deep learning surrogates.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统生成AI模型评估的局限性，这些评估通常将复杂行为简化为单一的点估计，从而掩盖了重要的风险和差异。作者提出了一个统一框架，建模有害行为在各种解码配置中的分布，利用以尾部为重点的指标并结合利益相关者的偏好。主要实验结果包括对解码Rashomon集合的形式化，以测量风险和分歧，开发了依赖于Dirichlet过程的混合模型以学习多模态伤害表面，以及引入了一个主动采样管道，使用贝叶斯深度学习代理有效探索解码空间。</div>
</details>
</div>
<div class="card">
<div class="title">HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models</div>
<div class="meta-line">Authors: Xin Xie, Jiaxian Guo, Dong Gong</div>
<div class="meta-line">First: 2026-01-22T13:49:47+00:00 · Latest: 2026-01-22T13:49:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15968v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15968v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model&#x27;s generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperAlign：高效测试时对齐扩散模型的超网络</div>
<div class="mono" style="margin-top:8px">扩散模型实现了最先进的性能，但往往无法生成与人类偏好和意图一致的输出，导致图像的美学质量差和语义不一致。现有的对齐方法存在困难的权衡：微调方法在奖励过度优化时会失去多样性，而测试时缩放方法则引入了显著的计算开销，并倾向于欠优化。为了解决这些局限性，我们提出了HyperAlign，一个训练超网络以实现高效和有效测试时对齐的新框架。HyperAlign动态生成低秩适应权重来调节扩散模型的生成算子，而不是修改潜在状态。这使得去噪轨迹能够根据输入潜在、时间步和奖励条件对齐的提示进行自适应调整。我们引入了多种HyperAlign变体，频率不同，平衡性能和效率。此外，我们使用带有偏好数据的奖励分数目标来优化超网络，以减少奖励黑客行为。我们在多个扩展生成范式上评估HyperAlign，包括Stable Diffusion和FLUX。它在增强语义一致性和视觉吸引力方面显著优于现有的微调和测试时缩放基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of diffusion models with human preferences and intentions, addressing issues of poor aesthetic quality and semantic inconsistencies in generated images. The authors propose HyperAlign, a framework that employs a hypernetwork to generate low-rank adaptation weights for modulating the diffusion model&#x27;s generation operators without altering latent states. Experimental results demonstrate that HyperAlign significantly enhances semantic consistency and visual appeal compared to existing fine-tuning and test-time scaling methods across various generative paradigms, including Stable Diffusion and FLUX.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善扩散模型与人类偏好和意图的一致性，解决生成输出中美学质量差和语义不一致的问题。作者提出了HyperAlign框架，利用超网络生成低秩适应权重，以调节扩散模型的生成操作，而不改变潜在状态。实验结果表明，HyperAlign在多个生成范式（包括Stable Diffusion和FLUX）中显著提高了语义一致性和视觉吸引力，优于现有的微调和测试时间缩放方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion</div>
<div class="meta-line">Authors: Yonghao Xu, Pedram Ghamisi, Qihao Weng</div>
<div class="meta-line">First: 2026-01-22T10:30:32+00:00 · Latest: 2026-01-22T10:30:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15829v1">PDF</a> · <a href="https://github.com/YonghaoXu/DPD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于判别原型引导扩散的真实遥感数据集蒸馏研究</div>
<div class="mono" style="margin-top:8px">近年来，深度学习在遥感图像解读方面取得了显著成功，这得益于大规模基准数据集的可用性。然而，这种对大量训练数据的依赖也带来了两个主要挑战：（1）高存储和计算成本，以及（2）数据泄露的风险，尤其是在涉及敏感类别时。为了解决这些挑战，本研究首次将数据集蒸馏的概念引入遥感图像解读领域。具体而言，我们训练了一个文本到图像的扩散模型，将大规模遥感数据集浓缩为一个紧凑且具有代表性的蒸馏数据集。为了提高合成样本的判别质量，我们提出了一种分类器驱动的引导方法，通过将预训练模型的分类一致性损失注入扩散训练过程中。此外，考虑到遥感图像的丰富语义复杂性，我们进一步对训练样本进行潜在空间聚类，以选择具有代表性和多样性的原型作为视觉风格引导，同时使用视觉语言模型提供聚合文本描述。在三个高分辨率遥感场景分类基准上的实验表明，所提出的方法能够为下游模型训练蒸馏出真实且多样的样本。代码和预训练模型在线提供（https://github.com/YonghaoXu/DPD）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of high storage and computational costs, as well as the risk of data leakage in remote sensing image interpretation, by introducing dataset distillation. The authors employ a text-to-image diffusion model to create a compact and representative distilled dataset from a large-scale remote sensing dataset, enhancing the quality of synthesized samples through classifier-driven guidance and classification consistency loss. Experimental results on three high-resolution remote sensing scene classification benchmarks demonstrate that the proposed method successfully distills realistic and diverse samples suitable for downstream model training.</div>
<div class="mono" style="margin-top:8px">本研究针对遥感图像解读中高存储和计算成本以及数据泄露风险的挑战，引入了数据集蒸馏的概念。作者提出了一种文本到图像的扩散模型，将大规模遥感数据集浓缩为紧凑的蒸馏数据集，通过分类器驱动的引导和分类一致性损失来提高合成样本的质量。对三个高分辨率遥感场景分类基准的实验结果表明，该方法成功蒸馏出适合下游模型训练的真实且多样的样本。</div>
</details>
</div>
<div class="card">
<div class="title">Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</div>
<div class="meta-line">Authors: Libo Wang</div>
<div class="meta-line">First: 2025-11-30T08:37:01+00:00 · Latest: 2026-01-22T10:28:40+00:00</div>
<div class="meta-line">Comments: The Sigma model has been open-sourced on Hugging Face. Weights, dataset, some scripts, and logs are all available. The link is: https://huggingface.co/Veltraxor/Sigma</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00783v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.00783v3">PDF</a> · <a href="https://huggingface.co/Veltraxor/Sigma">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address a fundamental limitation in cognitive systems, namely the absence of a time-updatable mediating thought space between semantics and continuous control, this work constructs and trains a vision-language-action model termed Sigma, deployed on a single RTX 4090. The model is built upon the open-source pi0.5_base backbone, with the svla_so101_pickplace dataset preprocessed into a structured training corpus. An independently designed VLA architecture is introduced to integrate deep semantic understanding with associative reasoning, enabling telepathic-style alignment between perception and action. Training proceeds through iterative optimization of data preprocessing, LoRA-based fine-tuning, and inference-stage adapter design. Evaluation is conducted using offline closed-loop replay, comparing Sigma against the untuned pi0.5_base under identical data conditions. Experimental results indicate a consistent reduction in control MSE across vector-, fragment-, and trajectory-level scales, while preserving the stability of the telepathy norm and semantic-text alignment quality. These findings demonstrate that mind-responsive alignment control can be quantitatively achieved through semantic and associative architectural integration without retraining the base model, providing a reproducible pathway for semantic alignment and intention-driven behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sigma：通向心灵感应对齐的视觉-语言-动作模型的关键</div>
<div class="mono" style="margin-top:8px">为了解决认知系统中的一个基本限制，即缺乏语义与连续控制之间的可时间更新的中介思维空间，本研究构建并训练了一个名为Sigma的视觉-语言-动作模型，部署在单个RTX 4090上。该模型基于开源的pi0.5_base骨干网络，使用经过预处理的svla_so101_pickplace数据集构建成结构化训练语料库。引入了独立设计的VLA架构，以将深层语义理解与联想推理相结合，实现感知与行动之间的心灵感应式对齐。训练通过数据预处理的迭代优化、基于LoRA的微调和推理阶段适配器设计进行。评估使用离线闭环重放进行，将Sigma与在相同数据条件下未调优的pi0.5_base进行比较。实验结果表明，在向量、片段和轨迹级别上，控制均方误差（MSE）持续降低，同时保持心灵感应规范和语义-文本对齐质量的稳定性。这些发现表明，通过语义和联想架构的整合，可以在不重新训练基础模型的情况下，定量实现心灵响应对齐控制，为语义对齐和意图驱动行为提供了可重复的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses a critical limitation in cognitive systems related to the lack of a dynamic mediating thought space between semantics and continuous control by developing a vision-language-action model called Sigma. The model, based on the open-source pi0.5_base backbone and trained on a structured dataset derived from svla_so101_pickplace, employs a novel VLA architecture to facilitate telepathic alignment between perception and action. Experimental evaluations reveal that Sigma consistently reduces control mean squared error across various scales while maintaining the stability of telepathy norms and semantic-text alignment quality, demonstrating that effective mind-responsive alignment control can be achieved without retraining the base model.</div>
<div class="mono" style="margin-top:8px">本研究解决了认知系统缺乏语义与连续控制之间可随时间更新的中介思维空间的局限性，开发了一种称为Sigma的视觉-语言-行动模型。该模型利用开源的pi0.5_base骨干，并在从svla_so101_pickplace数据集中派生的结构化语料库上进行训练，结合了一种新颖的VLA架构以实现深层语义理解和联想推理。实验结果表明，Sigma在各个尺度上持续降低控制均方误差，同时保持了心灵感应规范和语义-文本对齐的稳定性，表明有效的心灵响应对齐控制可以在不重新训练基础模型的情况下实现。</div>
</details>
</div>
<div class="card">
<div class="title">Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition</div>
<div class="meta-line">Authors: Yujian Ma, Xikun Lu, Jinqiu Sang, Xianquan Jiang, Ruizhe Li</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-10T09:54:27+00:00 · Latest: 2026-01-22T09:28:02+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.08454v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.08454v3">PDF</a> · <a href="https://github.com/harryporry77/Behind-the-Scenes">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large pre-trained speech models such as Whisper offer strong generalization but pose significant challenges for resource-efficient adaptation. Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its underlying mechanisms in speech tasks remain poorly understood. In this work, we conduct the first systematic mechanistic interpretability study of LoRA within the Whisper encoder for speech emotion recognition (SER). Using a suite of analytical tools, including layer contribution probing, logit-lens inspection, and representational similarity via singular value decomposition (SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a delayed specialization process that preserves general features in early layers before consolidating task-specific information, and a forward alignment, backward differentiation dynamic between LoRA&#x27;s matrices. Our findings clarify how LoRA reshapes encoder hierarchies, providing both empirical insights and a deeper mechanistic understanding for designing efficient and interpretable adaptation strategies in large speech models. Our code is available at https://github.com/harryporry77/Behind-the-Scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>幕后揭秘：LoRA适配Whisper在语音情感识别中的机制可解释性</div>
<div class="mono" style="margin-top:8px">大型预训练语音模型如Whisper提供了强大的泛化能力，但在资源高效适配方面面临重大挑战。低秩适配（LoRA）已成为一种流行的参数高效微调方法，但其在语音任务中的基本机制仍然不够清楚。在本研究中，我们首次系统地研究了LoRA在Whisper编码器中用于语音情感识别（SER）的机制可解释性。通过一系列分析工具，包括层贡献探测、logit透镜检查，以及通过奇异值分解（SVD）和中心核对齐（CKA）的表征相似性，我们揭示了两个关键机制：一个延迟专业化过程，在早期层中保留一般特征，然后整合任务特定信息，以及LoRA矩阵之间的前向对齐和后向区分动态。我们的发现阐明了LoRA如何重塑编码器层次结构，为设计高效且可解释的适配策略提供了经验见解和更深层次的机制理解。我们的代码可在https://github.com/harryporry77/Behind-the-Scenes获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of understanding the mechanisms behind Low-Rank Adaptation (LoRA) in the context of adapting large pre-trained speech models like Whisper for speech emotion recognition (SER). The authors employ a systematic mechanistic interpretability approach using various analytical tools such as layer contribution probing and singular value decomposition to investigate LoRA&#x27;s effects on the Whisper encoder. The key findings reveal a delayed specialization process that maintains general features in the early layers before focusing on task-specific information, along with a dynamic interaction between LoRA&#x27;s matrices that enhances the model&#x27;s performance in SER tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解低秩适应（LoRA）在将大型预训练语音模型如Whisper应用于语音情感识别（SER）时的机制。作者使用层贡献探测、逻辑透镜检查以及通过奇异值分解（SVD）和中心核对齐（CKA）进行的表征相似性分析等分析工具，进行了系统的机制可解释性研究。主要发现揭示了一种延迟专业化过程，该过程在早期层中保持一般特征，然后集中于特定任务信息，以及LoRA矩阵中的前向对齐和后向微分动态，从而增强了对LoRA如何修改编码器层次结构以实现更高效适应策略的理解。</div>
</details>
</div>
<div class="card">
<div class="title">Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)</div>
<div class="meta-line">Authors: Qi Zeng, Weide Liu, Bo Li, Ryne Didier, P. Ellen Grant, Davood Karimi</div>
<div class="meta-line">First: 2026-01-22T08:49:33+00:00 · Latest: 2026-01-22T08:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15759v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15759v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM&#x27;s segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM&#x27;s robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM&#x27;s potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图谱的胎儿脑MRI分割模型（FeTal-SAM）</div>
<div class="mono" style="margin-top:8px">本文提出了FeTal-SAM，一种针对胎儿脑MRI分割的Segment Anything Model（SAM）的新型适应。传统深度学习方法通常需要大量带注释的数据集，并且对于固定标签集不够灵活，尤其在临床或研究需求变化时。通过整合基于图谱的提示和基础模型原则，FeTal-SAM解决了胎儿脑MRI分割中的两个关键限制：（1）需要为不同标签定义重新训练模型，以及（2）缺乏对分割是否由真实图像对比度或学习的空间先验驱动的洞察。我们利用多图谱配准生成空间对齐的标签模板，作为密集提示，配合边界框提示，用于SAM的分割解码器。这一策略使得可以在每个结构基础上进行二元分割，随后融合以重建完整的3D分割体积。在两个数据集（dHCP数据集和内部数据集）上的评估表明，FeTal-SAM在不同妊娠年龄下表现出强大的性能。值得注意的是，它在与为每个数据集和标签定义训练的最先进基线相当的Dice分数下，能够灵活分割任何用户指定的解剖结构。尽管对于细微、低对比度结构（如海马体、杏仁核）观察到略低的准确性，但我们的结果突显了FeTal-SAM作为通用分割模型的潜力，无需进行大量重新训练。因此，该方法构成了朝着临床可适应的胎儿脑MRI分析工具迈出的有希望的一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve fetal brain MRI segmentation, which traditionally relies on large annotated datasets and is inflexible to changes in clinical needs. The authors present FeTal-SAM, an adaptation of the Segment Anything Model that integrates atlas-based prompts and foundation-model principles to overcome limitations in existing methods. Experimental results on the dHCP dataset and an in-house dataset show that FeTal-SAM achieves Dice scores comparable to state-of-the-art models for well-contrasted structures while allowing for flexible segmentation of user-specified anatomy, although it shows slightly lower accuracy for subtle structures like the hippocampus and amygdala.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善胎儿脑部MRI的分割，传统方法依赖于大量标注数据集和固定标签，限制了临床应用的灵活性。作者提出了FeTal-SAM，这是一种结合了基于图谱的提示和基础模型原则的Segment Anything Model的适应版本，旨在克服不同标签定义下模型重训练的需求，并增强分割的可解释性。在dHCP数据集和一个内部数据集上的实验结果表明，FeTal-SAM在对比度良好的结构上取得了与最先进方法相当的Dice分数，同时也允许对任何用户指定的解剖结构进行分割，尽管对于海马体和杏仁体等细微结构的准确性略低，这表明其在临床环境中作为多功能分割工具的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting Generative Image Modeling via Joint Image-Feature Synthesis</div>
<div class="meta-line">Authors: Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-04-22T17:41:42+00:00 · Latest: 2026-01-22T08:23:58+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 (Spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16064v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.16064v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://representationdiffusion.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling. Project page and code: https://representationdiffusion.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过联合图像-特征合成提升生成图像建模</div>
<div class="mono" style="margin-top:8px">潜在扩散模型（LDMs）在高质量图像生成中占主导地位，但将表示学习与生成建模结合仍然是一个挑战。我们提出了一种新颖的生成图像建模框架，通过利用扩散模型无缝地桥接这一差距，联合建模低级图像潜变量（来自变分自编码器）和高级语义特征（来自预训练的自监督编码器，如DINO）。我们的潜在-语义扩散方法学习从纯噪声生成一致的图像-特征对，显著提高了生成质量和训练效率，同时仅需对标准扩散变换器架构进行最小修改。通过消除对复杂蒸馏目标的需求，我们的统一设计简化了训练，并解锁了一种强大的新推理策略：表示引导，利用学习到的语义来引导和优化图像生成。在条件和无条件设置下评估，我们的方法在图像质量和训练收敛速度上都提供了显著改善，为表示感知生成建模建立了新的方向。项目页面和代码：https://representationdiffusion.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of integrating representation learning with generative modeling in high-quality image generation using latent diffusion models (LDMs). The authors propose a novel framework that utilizes a diffusion model to jointly model low-level image latents from a variational autoencoder and high-level semantic features from a pretrained self-supervised encoder. The key experimental findings indicate that this latent-semantic diffusion approach significantly enhances generative quality and training efficiency while simplifying the training process and introducing a new inference strategy called Representation Guidance, which improves image quality and convergence speed in both conditional and unconditional settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在高质量图像生成中将表征学习与生成建模相结合的挑战，特别是在潜在扩散模型（LDMs）中。作者提出了一种新颖的框架，利用扩散模型共同建模来自变分自编码器的低级图像潜变量和来自预训练自监督编码器的高级语义特征。关键实验结果表明，这种潜在-语义扩散方法显著提高了生成质量和训练效率，同时简化了训练过程，并引入了一种新的推理策略，称为表征引导，改善了条件和无条件设置下的图像质量和收敛速度。</div>
</details>
</div>
<div class="card">
<div class="title">Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation</div>
<div class="meta-line">Authors: Shadi Alijani, Fereshteh Aghaee Meibodi, Homayoun Najjaran</div>
<div class="meta-line">First: 2026-01-22T08:03:17+00:00 · Latest: 2026-01-22T08:03:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15734v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>子区域感知模态融合与自适应提示在多模态脑肿瘤分割中的应用</div>
<div class="mono" style="margin-top:8px">基础模型在多模态医学影像中的成功适应是一个关键但尚未解决的挑战。现有模型往往难以有效融合来自多个来源的信息，并适应病理组织的异质性。为此，我们提出了一种新颖的框架，用于将基础模型适应于多模态医学影像，具有两个关键技术创新：子区域感知模态注意力和自适应提示工程。注意力机制使模型能够学习每个肿瘤子区域的最佳模态组合，而自适应提示策略利用基础模型的固有能力来提高分割精度。我们在BraTS 2020脑肿瘤分割数据集上验证了我们的框架，结果表明我们的方法显著优于基线方法，特别是在具有挑战性的坏死核心子区域。我们的工作为多模态融合和提示提供了一种有原则且有效的方法，为基于基础模型的医学影像解决方案的更准确和更强健的应用铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adaptation of foundation models for multi-modal medical imaging, particularly in the context of brain tumor segmentation, where existing models struggle with information fusion and the heterogeneous nature of tissues. The authors propose a novel framework that incorporates sub-region-aware modality attention and adaptive prompt engineering to enhance segmentation accuracy. Experimental results on the BraTS 2020 dataset show that this approach significantly outperforms baseline methods, especially in accurately segmenting the challenging necrotic core sub-region of tumors.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高基础模型在多模态医学影像中的适应性，特别是在脑肿瘤分割的背景下，现有模型在信息融合和病理组织的多样性方面存在困难。作者提出了一种新颖的框架，结合了子区域感知模态注意力和自适应提示工程，以提高分割精度。在BraTS 2020数据集上的实验结果表明，该方法显著优于基线方法，尤其是在准确分割肿瘤的挑战性坏死核心子区域方面。</div>
</details>
</div>
<div class="card">
<div class="title">DF-LLaVA: Unlocking MLLM&#x27;s potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</div>
<div class="meta-line">Authors: Zhuokang Shen, Kaisen Zhang, Bohan Jia, Heming Jia, Yuan Fang, Zhou Yu, Shaohui Lin</div>
<div class="meta-line">First: 2025-09-18T13:43:42+00:00 · Latest: 2026-01-22T07:44:40+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14957v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14957v2">PDF</a> · <a href="https://github.com/Eliot-Shen/DF-LLaVA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increasing prevalence of synthetic images, evaluating image authenticity and locating forgeries accurately while maintaining human interpretability remains a challenging task. Existing detection models primarily focus on simple authenticity classification, ultimately providing only a forgery probability or binary judgment, which offers limited explanatory insights into image authenticity. Moreover, while MLLM-based detection methods can provide more interpretable results, they still lag behind expert models in terms of pure authenticity classification accuracy. To address this, we propose DF-LLaVA, a simple yet effective framework that unlocks the intrinsic discrimination potential of MLLMs. Our approach first extracts latent knowledge from MLLMs and then injects it into training via prompts. This framework allows LLaVA to achieve outstanding detection accuracy exceeding expert models while still maintaining the interpretability offered by MLLMs. Extensive experiments confirm the superiority of our DF-LLaVA, achieving both high accuracy and explainability in synthetic image detection. Code is available online at: https://github.com/Eliot-Shen/DF-LLaVA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DF-LLaVA：通过提示引导知识注入释放MLLM在合成图像检测中的潜力</div>
<div class="mono" style="margin-top:8px">随着合成图像的日益普及，评估图像真实性和准确定位伪造品，同时保持人类可解释性，仍然是一项具有挑战性的任务。现有的检测模型主要集中在简单的真实性分类上，最终仅提供伪造概率或二元判断，这对图像真实性的解释性见解有限。此外，尽管基于MLLM的检测方法可以提供更可解释的结果，但在纯真实性分类准确性方面仍落后于专家模型。为了解决这个问题，我们提出了DF-LLaVA，一个简单而有效的框架，释放MLLM的内在区分潜力。我们的方法首先从MLLM中提取潜在知识，然后通过提示将其注入训练。该框架使LLaVA在检测准确性上超越专家模型，同时保持MLLM提供的可解释性。大量实验确认了我们DF-LLaVA的优越性，在合成图像检测中实现了高准确性和可解释性。代码可在线获取： https://github.com/Eliot-Shen/DF-LLaVA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of synthetic images has created a need for effective methods to evaluate image authenticity while ensuring human interpretability. To address the limitations of existing detection models, which primarily focus on binary classification and lack explanatory insights, the authors propose DF-LLaVA, a framework that leverages prompt-guided knowledge injection from MLLMs. Experimental results demonstrate that DF-LLaVA achieves superior detection accuracy compared to expert models while maintaining the interpretability advantages of MLLMs, confirming its effectiveness in synthetic image detection.</div>
<div class="mono" style="margin-top:8px">随着合成图像的日益普及，评估图像真实性和准确定位伪造品的需求变得愈加迫切，同时还需确保人类可解释性。该研究提出了DF-LLaVA框架，通过提取潜在知识并通过提示将其注入训练过程，增强了多模态大型语言模型（MLLMs）的检测能力。实验结果表明，DF-LLaVA在检测准确性上优于专家模型，同时保持了MLLMs的可解释性优势，从而解决了现有检测方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans</div>
<div class="meta-line">Authors: Shubhankar Borse, Seokeon Choi, Sunghyun Park, Jeongho Kim, Shreya Kadambi, Risheek Garrepalli, Sungrack Yun, Munawar Hayat, Fatih Porikli</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-25T23:00:57+00:00 · Latest: 2026-01-22T06:59:37+00:00</div>
<div class="meta-line">Comments: Accepted at the NeurIPS 2025 D&amp;B Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20879v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.20879v4">PDF</a> · <a href="https://github.com/Qualcomm-AI-research/MultiHuman-Testbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1,800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation. The dataset and evaluation codes will be available at https://github.com/Qualcomm-AI-research/MultiHuman-Testbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MultiHuman-Testbench：多人的图像生成基准测试</div>
<div class="mono" style="margin-top:8px">生成包含多个人类、执行复杂动作的图像，同时保持其面部身份，是一个重大挑战。造成这一问题的主要因素是缺乏专门的基准。为了解决这个问题，我们引入了MultiHuman-Testbench，这是一个用于严格评估多人人物生成的生成模型的新基准。该基准包含1800个样本，包括精心策划的文本提示，描述从简单到复杂的人类动作。这些提示与总共5550个独特的人脸图像相匹配，均匀抽样以确保在年龄、种族背景和性别上的多样性。除了标题外，我们还提供了人类选择的姿势条件图像，这些图像与提示准确匹配。我们提出了一个多方面的评估套件，采用四个关键指标来量化面部数量、身份相似性、提示对齐和动作检测。我们对一组多样化的模型进行了全面评估，包括零样本方法和基于训练的方法，带有和不带区域先验。我们还提出了新技术，通过人类分割和匈牙利匹配来结合图像和区域隔离，显著提高身份相似性。我们提出的基准和关键发现为推动多人人物图像生成的研究提供了有价值的见解和标准化工具。数据集和评估代码将可在https://github.com/Qualcomm-AI-research/MultiHuman-Testbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in generating images with multiple humans while maintaining their facial identities, which is hindered by the absence of a dedicated benchmark. The authors introduce MultiHuman-Testbench, a benchmark consisting of 1,800 samples with text prompts for various human actions and 5,550 unique human face images to ensure diversity. They employ a comprehensive evaluation suite with four metrics to assess face count, ID similarity, prompt alignment, and action detection, and demonstrate that their novel techniques for image and region isolation enhance ID similarity, providing a standardized tool for advancing multi-human image generation research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成多个人类在执行复杂动作时保持面部身份的图像所面临的挑战，而这一挑战主要源于缺乏专门的基准。作者提出了MultiHuman-Testbench，一个包含1800个样本的基准，配有文本提示和5550个独特的人脸图像，确保在年龄、种族和性别上的多样性。评估采用四个指标来评估面部数量、身份相似性、提示对齐和动作检测，结果表明，结合人类分割和匈牙利匹配技术显著提高了身份相似性，从而为推动多人人物图像生成研究提供了标准化工具。</div>
</details>
</div>
<div class="card">
<div class="title">PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</div>
<div class="meta-line">Authors: Po-Han Huang, Jeng-Lin Li, Po-Hsuan Huang, Ming-Ching Chang, Wei-Chao Chen</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-09-30T06:52:08+00:00 · Latest: 2026-01-22T06:50:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures. WACV 2026 (Accepted)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25856v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25856v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatchEAD：统一工业视觉提示框架以进行补丁专属异常检测</div>
<div class="mono" style="margin-top:8px">工业异常检测越来越依赖基础模型，旨在实现强大的分布外泛化和在现实世界部署中的快速适应。值得注意的是，过去的研究主要集中在文本提示调优上，导致内在的视觉对应部分碎片化为特定于每个基础模型的处理步骤。我们旨在通过提出一个统一的补丁聚焦框架Patch-Exclusive Anomaly Detection（PatchEAD）来解决这一限制，实现与多种基础模型兼容的无训练异常检测。该框架构建了视觉提示技术，包括对齐模块和前景遮罩。我们的实验显示，与之前的工作相比，尽管缺乏文本特征，但在少量样本和批量零样本性能上表现优越。我们的研究进一步考察了主干结构和预训练特性如何影响补丁相似性鲁棒性，为选择和配置基础模型以进行现实世界视觉检查提供了可操作的指导。这些结果确认，一个良好统一的仅补丁框架可以实现快速、低校准的部署，而无需精心设计的文本提示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve industrial anomaly detection by addressing the limitations of existing methods that focus primarily on textual prompt tuning, which leaves visual processing fragmented. The authors propose a unified framework called Patch-Exclusive Anomaly Detection (PatchEAD) that facilitates training-free anomaly detection compatible with various foundation models through visual prompting techniques, including an alignment module and foreground masking. Experimental results demonstrate that PatchEAD achieves superior few-shot and batch zero-shot performance compared to previous approaches, while also providing insights into how the backbone structure and pretrained characteristics influence patch-similarity robustness, thereby offering guidance for selecting foundation models for practical visual inspection applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法主要集中于文本提示调优的局限性，来改善工业异常检测，这使得视觉处理变得支离破碎。作者提出了一种名为Patch-Exclusive Anomaly Detection（PatchEAD）的统一框架，通过视觉提示技术（包括对齐模块和前景掩蔽）实现与各种基础模型兼容的无训练异常检测。实验结果表明，PatchEAD在少量样本和批量零样本的表现上优于以往方法，突显了该框架在无需依赖文本特征的实际视觉检测中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Multi-Contrast Super-Resolution: Self-Supervised Implicit Re-Representation for Unpaired Cross-Modal Synthesis</div>
<div class="meta-line">Authors: Yinzhe Wu, Hongyu Rui, Fanwen Wang, Jiahao Huang, Zhenxuan Zhang, Haosen Zhang, Zi Wang, Guang Yang</div>
<div class="meta-line">First: 2025-05-09T07:48:52+00:00 · Latest: 2026-01-22T06:39:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05855v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.05855v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited. They typically require large, paired low- and high-resolution (LR/HR) training datasets, which are scarce, and are trained for fixed upsampling scales. While recent self-supervised methods remove the paired data requirement, they fail to leverage valuable population-level priors. In this work, we propose a novel, decoupled MCSR framework that resolves both limitations. We reformulate MCSR into two stages: (1) an unpaired cross-modal synthesis (uCMS) module, trained once on unpaired population data to learn a robust anatomical prior; and (2) a lightweight, patient-specific implicit re-representation (IrR) module. This IrR module is optimized in a self-supervised manner to fuse the population prior with the subject&#x27;s own LR target data. This design uniquely fuses population-level knowledge with patient-specific fidelity without requiring any paired LR/HR or paired cross-modal training data. By building the IrR module on an implicit neural representation, our framework is also inherently scale-agnostic. Our method demonstrates superior quantitative performance on different datasets, with exceptional robustness at extreme scales (16x, 32x), a regime where competing methods fail. Our work presents a data-efficient, flexible, and computationally lightweight paradigm for MCSR, enabling high-fidelity, arbitrary-scale</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦多对比超分辨率：用于无配对跨模态合成的自监督隐式重表征</div>
<div class="mono" style="margin-top:8px">多对比超分辨率（MCSR）对增强MRI至关重要，但当前的深度学习方法受到限制。它们通常需要大量配对的低分辨率和高分辨率（LR/HR）训练数据集，而这些数据集稀缺，并且是针对固定的上采样比例进行训练的。尽管最近的自监督方法消除了配对数据的需求，但它们未能利用有价值的人群级先验。在本研究中，我们提出了一种新颖的解耦MCSR框架，解决了这两个限制。我们将MCSR重新构建为两个阶段：（1）一个无配对跨模态合成（uCMS）模块，基于无配对人群数据进行一次训练，以学习稳健的解剖先验；（2）一个轻量级、患者特定的隐式重表征（IrR）模块。该IrR模块以自监督的方式进行优化，将人群先验与受试者自己的LR目标数据融合。该设计独特地将人群级知识与患者特定的保真度融合，而无需任何配对的LR/HR或配对的跨模态训练数据。通过在隐式神经表征上构建IrR模块，我们的框架也本质上是尺度无关的。我们的方法在不同数据集上展示了优越的定量性能，在极端尺度（16x，32x）下表现出卓越的鲁棒性，而在这一范围内竞争方法失败。我们的工作提出了一种数据高效、灵活且计算轻量的MCSR范式，实现高保真度和任意尺度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current deep learning methods in multi-contrast super-resolution (MCSR) for MRI, which typically require large paired datasets that are often unavailable. The authors propose a novel decoupled MCSR framework that consists of two main components: an unpaired cross-modal synthesis (uCMS) module that learns a robust anatomical prior from unpaired population data, and a patient-specific implicit re-representation (IrR) module optimized in a self-supervised manner to integrate this prior with individual low-resolution data. Experimental results show that this approach achieves superior quantitative performance across various datasets, particularly excelling in extreme upsampling scenarios (16x, 32x), where traditional methods struggle, thus providing a data-efficient and flexible solution for high-fidelity MRI super-resolution.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前深度学习方法在MRI多对比度超分辨率（MCSR）中的局限性，这些方法通常需要大量配对的数据集，而这些数据集并不容易获得。作者提出了一种新颖的解耦MCSR框架，主要由两个部分组成：一个从未配对的群体数据中学习解剖先验的未配对跨模态合成（uCMS）模块，以及一个以自监督方式优化的患者特定隐式重表示（IrR）模块，用于将这些先验与个体的低分辨率数据结合。实验结果表明，该方法在不同数据集上表现出优越的定量性能，在16倍和32倍的极端上采样比例下表现出卓越的鲁棒性，而其他方法通常在这一范围内表现不佳。</div>
</details>
</div>
<div class="card">
<div class="title">Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model</div>
<div class="meta-line">Authors: Hongyang Wei, Baixin Xu, Hongbo Liu, Size Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, Wei Li, Ying He, Yang Liu, Xuchen Song, Yangguang Li, Yahui Zhou</div>
<div class="meta-line">First: 2025-09-04T17:00:17+00:00 · Latest: 2026-01-22T06:10:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04548v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04548v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skywork UniPic 2.0：基于在线强化学习构建统一多模态模型的上下文模型</div>
<div class="mono" style="margin-top:8px">最近多模态模型的进展展示了在统一图像生成和编辑方面的卓越能力。然而，许多知名的开源模型优先考虑模型参数的扩展，而非优化训练策略，限制了它们的效率和性能。在本研究中，我们提出了UniPic2-SD3.5M-Kontext，这是一个基于SD3.5-Medium的2B参数DiT模型，能够实现最先进的图像生成和编辑，同时无缝扩展到统一的多模态框架。我们的方法从对SD3.5-Medium的架构修改和在高质量数据上的大规模预训练开始，使得文本到图像的生成和编辑能力得以联合实现。为了增强指令跟随和编辑一致性，我们提出了一种新颖的渐进式双任务强化策略（PDTR），有效地以分阶段的方式增强这两个任务。我们通过实验证实，不同任务的强化阶段是互利的，并且不会引发负干扰。在预训练和强化策略之后，UniPic2-SD3.5M-Kontext展示了比生成参数显著更大的模型（包括BAGEL（7B）和Flux-Kontext（12B））更强的图像生成和编辑能力。此外，遵循MetaQuery，我们通过连接器将UniPic2-SD3.5M-Kontext和Qwen2.5-VL-7B连接起来，并进行联合训练，以推出统一的多模态模型UniPic2-Metaquery。UniPic2-Metaquery整合了理解、生成和编辑，在多样化任务中实现了顶级性能，采用简单且可扩展的训练范式。这一切持续验证了我们提出的训练范式的有效性和普适性，我们将其形式化为Skywork UniPic 2.0。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and performance of multimodal models, which often focus on scaling parameters rather than optimizing training strategies. The authors introduce UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model that incorporates architectural modifications and large-scale pre-training on high-quality data to enable effective text-to-image generation and editing. Key experimental findings reveal that their proposed Progressive Dual-Task Reinforcement strategy enhances instruction following and editing consistency, leading to superior image generation and editing capabilities compared to larger models like BAGEL and Flux-Kontext, and demonstrating the effectiveness of their training paradigm in creating a unified multimodal model, UniPic2-Metaquery, which excels across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高多模态模型的效率和性能，这些模型通常侧重于参数扩展而非优化训练策略。作者提出了UniPic2-SD3.5M-Kontext，这是一种2B参数的DiT模型，结合了架构修改和在高质量数据上的大规模预训练，以实现有效的文本到图像生成和编辑。关键实验结果表明，所提出的渐进式双任务强化策略增强了指令遵循和编辑一致性，使得图像生成和编辑能力优于更大模型，并且统一的多模态模型UniPic2-Metaquery在各种任务中以可扩展的训练方法实现了顶级性能。</div>
</details>
</div>
<div class="card">
<div class="title">PromptHelper: A Prompt Recommender System for Encouraging Creativity in AI Chatbot Interactions</div>
<div class="meta-line">Authors: Jason Kim, Maria Teleki, James Caverlee</div>
<div class="meta-line">First: 2026-01-22T01:44:51+00:00 · Latest: 2026-01-22T01:44:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15575v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15575v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompting is central to interaction with AI systems, yet many users struggle to explore alternative directions, articulate creative intent, or understand how variations in prompts shape model outputs. We introduce prompt recommender systems (PRS) as an interaction approach that supports exploration, suggesting contextually relevant follow-up prompts. We present PromptHelper, a PRS prototype integrated into an AI chatbot that surfaces semantically diverse prompt suggestions while users work on real writing tasks. We evaluate PromptHelper in a 2x2 fully within-subjects study (N=32) across creative and academic writing tasks. Results show that PromptHelper significantly increases users&#x27; perceived exploration and expressiveness without increasing cognitive workload. Qualitative findings illustrate how prompt recommendations help users branch into new directions, overcome uncertainty about what to ask next, and better articulate their intent. We discuss implications for designing AI interfaces that scaffold exploratory interaction while preserving user agency, and release open-source resources to support research on prompt recommendation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PromptHelper：一个促进AI聊天机器人互动创意的提示推荐系统</div>
<div class="mono" style="margin-top:8px">提示在与AI系统的互动中至关重要，但许多用户在探索替代方向、表达创意意图或理解提示变化如何影响模型输出方面存在困难。我们引入提示推荐系统（PRS）作为一种支持探索的互动方法，建议上下文相关的后续提示。我们展示了PromptHelper，一个集成到AI聊天机器人中的PRS原型，在用户进行实际写作任务时提供语义多样的提示建议。我们在一个2x2的完全内被试研究中评估PromptHelper（N=32），涵盖创意和学术写作任务。结果表明，PromptHelper显著提高了用户的感知探索性和表现力，而没有增加认知负担。定性研究结果说明提示推荐如何帮助用户拓展新方向，克服对下一步提问的不确定性，并更好地表达他们的意图。我们讨论了设计AI界面的影响，以支持探索性互动，同时保持用户的自主性，并发布开源资源以支持提示推荐的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance user interaction with AI systems, particularly in prompting, where users often face challenges in exploring creative directions and articulating their intent. The authors developed PromptHelper, a prompt recommender system integrated into an AI chatbot, which provides contextually relevant follow-up prompts to aid users during writing tasks. Experimental results from a study involving 32 participants indicate that PromptHelper significantly improves users&#x27; perceived exploration and expressiveness while maintaining a low cognitive workload, with qualitative feedback highlighting its effectiveness in guiding users to new ideas and clarifying their requests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强用户与人工智能系统的互动，特别是在创意和学术写作任务中，用户常常难以探索替代方案和表达他们的意图。作者开发了PromptHelper，这是一种集成到人工智能聊天机器人的提示推荐系统，旨在为用户在写作过程中提供上下文相关的后续提示。32名参与者的实验结果表明，PromptHelper显著提高了用户的探索感和表达能力，同时保持了较低的认知负担，定性反馈显示推荐帮助用户克服不确定性，更好地定义他们的创意目标。</div>
</details>
</div>
<div class="card">
<div class="title">Controllable Layered Image Generation for Real-World Editing</div>
<div class="meta-line">Authors: Jinrui Yang, Qing Liu, Yijun Li, Mengwei Ren, Letian Zhang, Zhe Lin, Cihang Xie, Yuyin Zhou</div>
<div class="meta-line">First: 2026-01-21T22:29:33+00:00 · Latest: 2026-01-21T22:29:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15507v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15507v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rayjryang.github.io/LASAGNA-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可控分层图像生成用于现实世界编辑</div>
<div class="mono" style="margin-top:8px">最近的图像生成模型取得了显著进展，但在用户尝试编辑现有图像中的特定元素时，往往难以产生可控且一致的结果。分层表示使灵活的用户驱动内容创作成为可能，但现有方法通常未能生成具有连贯合成关系的层，其对象层通常缺乏真实的视觉效果，如阴影和反射。为克服这些限制，我们提出了LASAGNA，一个新颖的统一框架，能够与其组成层共同生成图像——一个逼真的背景和一个具有引人注目的视觉效果的高质量透明前景。与之前的工作不同，LASAGNA有效地从广泛的条件输入中学习正确的图像合成——文本提示、前景、背景和位置掩码——为现实世界应用提供更大的可控性。为此，我们引入了LASAGNA-48K，一个由干净背景和具有物理基础视觉效果的RGBA前景组成的新数据集。我们还提出了LASAGNABENCH，这是第一个用于层编辑的基准。我们展示了LASAGNA在同时生成多个图像层时能够产生高度一致和连贯的结果，支持多样的后期编辑应用，准确保留身份和视觉效果。LASAGNA-48K和LASAGNABENCH将公开发布，以促进社区的开放研究。项目页面是https://rayjryang.github.io/LASAGNA-Page/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing image generation models in producing controllable and consistent results for specific element editing within images. The authors propose LASAGNA, a unified framework that generates images alongside their compositional layers, including a photorealistic background and a high-quality transparent foreground with realistic visual effects. Experimental results demonstrate that LASAGNA achieves high consistency and coherence across multiple image layers, significantly enhancing the controllability for real-world applications and facilitating diverse post-editing tasks while preserving identity and visual effects.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有图像生成模型在对图像中特定元素进行编辑时，难以产生可控和一致结果的局限性。作者提出了LASAGNA，一个统一框架，可以生成图像及其组成层，包括逼真的背景和高质量的透明前景，具有真实的视觉效果。实验结果表明，LASAGNA能够有效地从多种条件输入中学习图像组合，并在多个层之间生成一致的结果，从而增强了多样化后期编辑应用的潜力，同时保持了身份和视觉效果。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization</div>
<div class="meta-line">Authors: Saptarshi Roy, Alessandro Rinaldo, Purnamrita Sarkar</div>
<div class="meta-line">First: 2026-01-21T22:09:27+00:00 · Latest: 2026-01-21T22:09:27+00:00</div>
<div class="meta-line">Comments: 32 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15500v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15500v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to log factors), where $\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of
  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>整流流的低维适应：通过扩散和随机定位的视角</div>
<div class="mono" style="margin-top:8px">近年来，整流流（RF）因其生成效率和先进性能而受到广泛关注。本文研究了RF在多大程度上自动适应目标分布支持的内在低维性以加速采样。我们表明，通过精心设计的时间离散化方案和足够准确的漂移估计，RF采样器的迭代复杂度为$O(k/\varepsilon)$（最多对数因子），其中$\varepsilon$是总变差距离的精度，$k$是目标分布的内在维度。此外，我们通过建立这些过程与随机定位之间的新联系，表明去噪扩散概率模型（DDPM）过程等价于RF的随机版本。在此基础上，我们进一步设计了一种随机RF采样器，该采样器在对漂移估计的准确性要求较低的情况下，也能适应目标分布的低维性，并且具有特定的时间安排。我们通过对合成数据和文本到图像数据实验的模拟，展示了实施新设计的时间离散化方案的采样器的改进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of Rectified Flow (RF) samplers by leveraging the low-dimensional structure of target distributions. The authors employ a novel time-discretization scheme and accurate drift estimates to demonstrate that the RF sampler achieves an iteration complexity of order O(k/ε), where ε represents precision and k denotes the intrinsic dimension. Key experimental findings reveal that the proposed stochastic RF sampler, which adapts to low-dimensionality with less stringent drift accuracy requirements, outperforms existing methods in simulations involving synthetic data and text-to-image tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用目标分布的低维特性来提高修正流（RF）采样器的效率。作者提出了一种方法，涉及精心设计的时间离散方案和准确的漂移估计，从而实现了O(k/ε)的迭代复杂度，其中ε表示总变距的精度，k表示目标分布的内在维度。实验结果表明，新设计的随机RF采样器在合成数据和文本到图像数据场景中优于传统方法，特别是在适应目标分布的低维性时，对漂移精度要求较低。</div>
</details>
</div>
<div class="card">
<div class="title">DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection</div>
<div class="meta-line">Authors: Morteza Poudineh, Marc Lalonde</div>
<div class="meta-line">First: 2026-01-21T20:35:51+00:00 · Latest: 2026-01-21T20:35:51+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DevPrompt：基于偏差的提示学习用于单正常样本图像异常检测</div>
<div class="mono" style="margin-top:8px">少量正常样本异常检测（FNSAD）旨在仅使用少量正常训练样本检测图像中的异常区域，由于监督有限和潜在缺陷的多样性，这一任务极具挑战性。最近的方法利用视觉-语言模型，如CLIP，通过基于提示的学习来对齐图像和文本特征。然而，现有方法通常在正常和异常提示之间表现出较弱的可区分性，并且缺乏针对补丁级异常的原则性评分机制。我们提出了一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差的评分的统计可靠性相结合。具体而言，我们用可学习的上下文向量替换固定的提示前缀，这些向量在正常和异常提示之间共享，而特定于异常的后缀标记则实现类感知对齐。为了增强可分离性，我们引入了带有Top-K多实例学习（MIL）的偏差损失，将补丁级特征建模为来自正态分布的高斯偏差。这使得网络能够为具有统计显著偏差的补丁分配更高的异常分数，从而改善定位和可解释性。在MVTecAD和VISA基准上的实验表明，与PromptAD和其他基线相比，像素级检测性能优越。消融研究进一步验证了可学习提示、基于偏差的评分和Top-K MIL策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of few-normal shot anomaly detection (FNSAD), which requires identifying abnormal regions in images with limited normal training samples. The authors propose a deviation-guided prompt learning framework that enhances the discriminability of prompts by using learnable context vectors and anomaly-specific suffix tokens, combined with a deviation loss and Top-K Multiple Instance Learning (MIL) to model patch-level features. Experimental results on the MVTecAD and VISA benchmarks show that this approach significantly improves pixel-level detection performance compared to existing methods like PromptAD, with ablation studies confirming the contributions of the proposed components to the overall effectiveness of the framework.</div>
<div class="mono" style="margin-top:8px">本研究解决了少量正常样本异常检测（FNSAD）的挑战，该任务涉及在有限的正常训练样本下识别图像中的异常区域。作者提出了一种基于偏差的提示学习框架，通过用可学习的上下文向量替换固定前缀，并结合特定异常的后缀标记以改善类别对齐，从而增强提示的可区分性。在MVTecAD和VISA基准上的实验结果表明，该方法在像素级检测上优于现有方法如PromptAD，消融研究进一步确认了所提可学习提示、基于偏差的评分和Top-K多实例学习策略的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reflexis: Supporting Reflexivity and Rigor in Collaborative Qualitative Analysis through Design for Deliberation</div>
<div class="meta-line">Authors: Runlong Ye, Oliver Huang, Patrick Yung Kang Lee, Michael Liut, Carolina Nobre, Ha-Kyung Kong</div>
<div class="meta-line">First: 2026-01-21T20:24:39+00:00 · Latest: 2026-01-21T20:24:39+00:00</div>
<div class="meta-line">Comments: Accepted at CHI 26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15445v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reflexive Thematic Analysis (RTA) is a critical method for generating deep interpretive insights. Yet its core tenets, including researcher reflexivity, tangible analytical evolution, and productive disagreement, are often poorly supported by software tools that prioritize speed and consensus over interpretive depth. To address this gap, we introduce Reflexis, a collaborative workspace that centers these practices. It supports reflexivity by integrating in-situ reflection prompts, makes code evolution transparent and tangible, and scaffolds collaborative interpretation by turning differences into productive, positionality-aware dialogue. Results from our paired-analyst study (N=12) indicate that Reflexis encouraged participants toward more granular reflection and reframed disagreements as productive conversations. The evaluation also surfaced key design tensions, including a desire for higher-level, networked memos and more user control over the timing of proactive alerts. Reflexis contributes a design framework for tools that prioritize rigor and transparency to support deep, collaborative interpretation in an age of automation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Reflexis：通过设计促进深思熟虑的协作定性分析中的反思性和严谨性</div>
<div class="mono" style="margin-top:8px">反思性主题分析（RTA）是一种生成深度解释性洞察的重要方法。然而，其核心原则，包括研究者反思性、可触及的分析演变和富有成效的分歧，常常得不到软件工具的良好支持，这些工具更注重速度和共识而非解释深度。为了解决这一问题，我们推出了Reflexis，一个以这些实践为中心的协作工作空间。它通过整合现场反思提示来支持反思性，使代码演变透明且可触及，并通过将差异转化为富有成效的、关注立场的对话来支撑协作解释。我们配对分析师研究的结果（N=12）表明，Reflexis鼓励参与者进行更细致的反思，并将分歧重新框定为富有成效的对话。评估还揭示了关键的设计张力，包括对更高层次的网络备忘录和对主动提醒时机的更多用户控制的渴望。Reflexis为优先考虑严谨性和透明度的工具提供了设计框架，以支持在自动化时代进行深度协作解释。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Reflexive Thematic Analysis (RTA) by addressing the limitations of existing software tools that often prioritize speed over interpretive depth. The authors developed Reflexis, a collaborative workspace designed to support reflexivity, transparency in code evolution, and productive dialogue among analysts. Experimental results from a paired-analyst study with 12 participants showed that Reflexis facilitated more detailed reflections and transformed disagreements into constructive discussions, while also revealing design tensions related to user control and memo management.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有软件工具在速度和共识上优先于解释深度的局限性，来增强反思主题分析（RTA）。作者开发了Reflexis，一个旨在通过现场反思提示、透明的代码演变和围绕分歧的建设性对话来支持反思性的协作工作空间。来自12名参与者的配对分析师研究的实验结果表明，Reflexis促进了更细致的反思，并将分歧转变为建设性的对话，同时也揭示了关于用户控制和备忘录管理的设计张力。</div>
</details>
</div>
<div class="card">
<div class="title">Ambient Dataloops: Generative Models for Dataset Refinement</div>
<div class="meta-line">Authors: Adrián Rodríguez-Muñoz, William Daspit, Adam Klivans, Antonio Torralba, Constantinos Daskalakis, Giannis Daras</div>
<div class="meta-line">First: 2026-01-21T19:29:04+00:00 · Latest: 2026-01-21T19:29:04+00:00</div>
<div class="meta-line">Comments: 27 pages, 9 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15417v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15417v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>环境数据循环：用于数据集精炼的生成模型</div>
<div class="mono" style="margin-top:8px">我们提出了环境数据循环，这是一个迭代框架，用于精炼数据集，使扩散模型更容易学习潜在的数据分布。现代数据集包含质量差异极大的样本，直接在这种异质数据上训练往往会产生次优模型。我们提出了一种数据集-模型共同演化过程；在我们方法的每次迭代中，数据集的质量逐渐提高，模型也相应改进。为了避免破坏性的自我消耗循环，在每次生成中，我们将合成改进的样本视为噪声，但噪声水平略低于前一次迭代，并使用环境扩散技术在损坏下进行学习。实证结果表明，环境数据循环在无条件和文本条件的图像生成以及新蛋白质设计中实现了最先进的性能。我们进一步提供了对所提框架的理论证明，捕捉数据循环过程的好处。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to refine datasets that contain samples of varying quality, which can lead to suboptimal model performance when training directly on such data. The authors propose Ambient Dataloops, an iterative framework that employs a dataset-model co-evolution process, where the dataset quality improves progressively with each iteration, and the model adapts accordingly. Experimental results demonstrate that Ambient Dataloops achieves state-of-the-art performance in both unconditional and text-conditional image generation, as well as in de novo protein design, supported by a theoretical justification for the benefits of the data looping procedure.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现代数据集中样本质量差异带来的挑战，这可能导致直接在这些数据上训练的模型性能不佳。作者提出了Ambient Dataloops，这是一个通过数据集与模型共同演化的迭代框架，在每次迭代中提高数据集质量，从而使模型能够更有效地学习。实验结果表明，Ambient Dataloops在无条件和文本条件的图像生成以及新蛋白质设计方面均达到了最先进的性能，并提供了数据循环过程益处的理论依据。</div>
</details>
</div>
<div class="card">
<div class="title">A Checklist for Trustworthy, Safe, and User-Friendly Mental Health Chatbots</div>
<div class="meta-line">Authors: Shreya Haran, Samiha Thatikonda, Dong Whi Yoo, Koustuv Saha</div>
<div class="meta-line">Venue: In 28th International Conference on Human-Computer Interaction, Springer LNCS, 2026</div>
<div class="meta-line">First: 2026-01-21T19:24:27+00:00 · Latest: 2026-01-21T19:24:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15412v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15412v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mental health concerns are rising globally, prompting increased reliance on technology to address the demand-supply gap in mental health services. In particular, mental health chatbots are emerging as a promising solution, but these remain largely untested, raising concerns about safety and potential harms. In this paper, we dive into the literature to identify critical gaps in the design and implementation of mental health chatbots. We contribute an operational checklist to help guide the development and design of more trustworthy, safe, and user-friendly chatbots. The checklist serves as both a developmental framework and an auditing tool to ensure ethical and effective chatbot design. We discuss how this checklist is a step towards supporting more responsible design practices and supporting new standards for sociotechnically sound digital mental health tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>值得信赖、安全且用户友好的心理健康聊天机器人检查清单</div>
<div class="mono" style="margin-top:8px">全球心理健康问题日益严重，促使人们越来越依赖技术来解决心理健康服务的供需缺口。特别是，心理健康聊天机器人作为一种有前景的解决方案正在出现，但这些机器人仍然在很大程度上未经测试，引发了关于安全性和潜在危害的担忧。本文深入文献，识别心理健康聊天机器人设计和实施中的关键缺口。我们贡献了一个操作检查清单，以帮助指导更值得信赖、安全和用户友好的聊天机器人的开发和设计。该检查清单既是一个开发框架，也是一个审计工具，以确保伦理和有效的聊天机器人设计。我们讨论了这个检查清单如何成为支持更负责任的设计实践和支持社会技术上合理的数字心理健康工具的新标准的一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rising global mental health concerns have led to an increased reliance on technology, particularly mental health chatbots, which are still largely untested and raise safety concerns. This paper reviews existing literature to identify critical gaps in the design and implementation of these chatbots and proposes an operational checklist aimed at guiding the development of more trustworthy, safe, and user-friendly mental health chatbots. The checklist serves as both a framework for development and an auditing tool to promote ethical and effective design practices in digital mental health tools.</div>
<div class="mono" style="margin-top:8px">全球心理健康问题的上升导致对技术的依赖增加，尤其是心理健康聊天机器人被视为潜在解决方案，尽管其未经过充分测试而引发安全担忧。本文回顾现有文献，识别这些聊天机器人设计和实施中的关键缺口，并提出一个操作性清单，旨在指导开发更值得信赖、安全和用户友好的心理健康聊天机器人。该清单既作为开发框架，又作为审计工具，促进伦理设计实践并建立有效数字心理健康工具的新标准。</div>
</details>
</div>
<div class="card">
<div class="title">FLEx: Language Modeling with Few-shot Language Explanations</div>
<div class="meta-line">Authors: Adar Avsian, Christopher Richardson, Anirudh Sundar, Larry Heck</div>
<div class="meta-line">First: 2026-01-07T18:12:05+00:00 · Latest: 2026-01-21T19:04:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04157v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04157v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\% of CoT&#x27;s remaining errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLEx：使用少量语言解释进行语言建模</div>
<div class="mono" style="margin-top:8px">语言模型在从数学问题解决到开放领域问答等广泛任务中变得有效。然而，它们仍然会犯错误，这些错误在相关查询中往往会重复。自然语言解释可以帮助纠正这些错误，但在需要专家注释者的领域，规模化收集这些解释可能不可行。为了解决这个问题，我们引入了FLEx（$\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations），一种使用少量解释性示例改善模型行为的方法。FLEx使用基于嵌入的聚类选择代表性模型错误，验证相关解释是否纠正了这些错误，并将其总结为在推理时前置的提示前缀。这个总结指导模型在新输入上避免类似错误，而不修改模型权重。我们在CounterBench、GSM8K和ReasonIF上评估FLEx。我们发现FLEx在所有三个数据集上始终优于链式思维（CoT）提示，并减少了高达83\%的CoT剩余错误。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of language models, which, despite their effectiveness in various tasks, still exhibit recurring errors. The authors propose FLEx, a method that utilizes a few-shot approach to improve model behavior by selecting representative errors through embedding-based clustering and summarizing corrective explanations into a prompt prefix for inference. Experimental results demonstrate that FLEx significantly outperforms chain-of-thought prompting across multiple datasets, including CounterBench, GSM8K, and ReasonIF, achieving a reduction of up to 83% in remaining errors compared to CoT prompting.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高语言模型的性能，尽管它们在各种任务中表现出色，但仍然存在重复错误。作者提出了FLEx，这是一种利用少量示例来改善模型行为的方法，通过基于嵌入的聚类选择代表性错误，并将纠正性解释总结为推理时的提示前缀。实验结果表明，FLEx在多个数据集上显著优于链式思维提示，减少了多达83%的剩余错误。</div>
</details>
</div>
<div class="card">
<div class="title">FedUMM: A General Framework for Federated Learning with Unified Multimodal Models</div>
<div class="meta-line">Authors: Zhaolong Su, Leheng Zhao, Xiaoying Wu, Ziyue Xu, Jindong Wang</div>
<div class="meta-line">First: 2026-01-21T19:02:52+00:00 · Latest: 2026-01-21T19:02:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15390v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15390v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FedUMM：统一多模态模型的联邦学习通用框架</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）作为强大的基础模型，能够在单一架构中同时进行生成和理解任务。然而，它们通常在集中式环境中训练，所有训练和下游数据集都集中在一个中央服务器上，这限制了在隐私敏感和地理分布场景中的部署。本文提出了FedUMM，一个针对非独立同分布多模态数据的低通信成本的UMMs的通用联邦学习框架。FedUMM基于NVIDIA FLARE，通过参数高效的微调为BLIP3o骨干网实例化联邦：客户端训练轻量级LoRA适配器，同时冻结基础模型，服务器仅聚合适配器更新。我们在VQA v2和GenEval组合生成基准上进行评估，控制Dirichlet异质性，最多支持16个客户端。结果显示，随着客户端数量和异质性的增加，性能略有下降，但仍与集中式训练具有竞争力。我们进一步分析计算-通信权衡，证明仅适配器的联邦相比于完全微调减少了每轮通信量超过一个数量级，从而实现了实用的联邦UMM训练。本研究为未来关于隐私保护的联邦统一多模态模型的研究提供了经验基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of centralized training for unified multimodal models (UMMs) in privacy-sensitive and geographically distributed scenarios. The authors propose FedUMM, a federated learning framework designed for UMMs that operates on non-IID multimodal data while minimizing communication costs. Experimental results indicate that while there is a slight performance degradation as the number of clients and data heterogeneity increases, FedUMM remains competitive with centralized training, and the use of adapter-only federation significantly reduces communication requirements, facilitating practical federated training of UMMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是使统一多模态模型（UMMs）能够在隐私敏感和地理分布的场景中部署，因为传统的训练方法依赖于集中数据收集。作者提出了FedUMM，这是一个为UMMs设计的联邦学习框架，能够处理非独立同分布的多模态数据，同时最小化通信成本。实验结果表明，尽管随着客户端数量和数据异质性的增加，性能略有下降，但FedUMM仍然与集中训练具有竞争力，并且仅使用适配器的联邦方法显著减少了通信需求，从而促进了UMMs的实际联邦训练。</div>
</details>
</div>
<div class="card">
<div class="title">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</div>
<div class="meta-line">Authors: Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Ziwei Liu, Chenyang Si</div>
<div class="meta-line">First: 2026-01-21T18:59:02+00:00 · Latest: 2026-01-21T18:59:02+00:00</div>
<div class="meta-line">Comments: 17 pages, 21 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StableWorld：朝着稳定和一致的长时交互视频生成</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中被忽视的稳定性和时间一致性挑战，该生成通过相机移动和文本提示等交互行为合成动态和可控的视频世界。尽管世界建模取得了显著进展，但当前方法仍然面临严重的不稳定性和时间退化，常常导致长时间交互中的空间漂移和场景崩溃。为了更好地理解这个问题，我们最初调查了不稳定性的根本原因，并确定主要的误差积累来源于同一场景，其中生成的帧逐渐偏离初始干净状态，并将误差传播到后续帧。基于这一观察，我们提出了一种简单而有效的方法，\textbf{StableWorld}，动态帧驱逐机制。通过持续过滤掉退化帧，同时保留几何一致的帧，StableWorld有效地防止了源头的累积漂移，从而提高了交互生成的稳定性和时间一致性。在多个交互视频模型上（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）取得的良好结果表明，StableWorld是模型无关的，可以应用于不同的交互视频生成框架，以显著提高稳定性、时间一致性和在多样化交互场景中的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of stability and temporal consistency in interactive video generation, which is crucial for synthesizing dynamic video worlds through user interactions. The authors investigate the causes of instability, identifying that error accumulation primarily arises from the same scene, where generated frames deviate from their initial state. To mitigate this issue, they propose StableWorld, a Dynamic Frame Eviction Mechanism that filters out degraded frames while retaining geometrically consistent ones. Experimental results across various interactive video models, including Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld significantly enhances stability and temporal consistency, proving its applicability across different frameworks.</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中稳定性和时间一致性的问题，这对于通过用户交互合成动态视频世界至关重要。作者研究了不稳定性的原因，发现错误累积主要发生在同一场景中，导致空间漂移和场景崩溃。为了解决这个问题，他们提出了StableWorld，一种动态帧驱逐机制，可以过滤掉退化帧，同时保留几何一致的帧。对多个交互视频模型（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）的实验结果表明，StableWorld显著提高了稳定性和时间一致性，证明了其在不同框架中的模型无关性适用性。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion</div>
<div class="meta-line">Authors: Zichen Xi, Hao-Xiang Chen, Nan Xue, Hongyu Yan, Qi-Yuan Feng, Levent Burak Kara, Joaquim Jorge, Qun-Ce Xu</div>
<div class="meta-line">First: 2026-01-21T18:32:27+00:00 · Latest: 2026-01-21T18:32:27+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSSC：通过一步潜在扩散实现通用生成单目语义场景补全</div>
<div class="mono" style="margin-top:8px">从单目RGB图像进行语义场景补全（SSC）是一项基本但具有挑战性的任务，因为从单一视角推断被遮挡的3D几何形状固有地存在模糊性。尽管前馈方法取得了一定进展，但它们在生成被遮挡区域的可信细节和保持物体的基本空间关系方面常常面临困难。对于整个3D空间的准确生成推理能力在实际应用中至关重要。本文提出了FlowSSC，这是第一个直接应用于单目语义场景补全的生成框架。FlowSSC将SSC任务视为条件生成问题，并可以与现有的前馈SSC方法无缝集成，以显著提升其性能。为了在不影响质量的情况下实现实时推理，我们引入了在紧凑的三平面潜在空间中操作的Shortcut Flow-matching。与需要数百步的标准扩散模型不同，我们的方法利用快捷机制在一步中实现高保真生成，使其能够在自主系统中实际部署。在SemanticKITTI上的大量实验表明，FlowSSC实现了最先进的性能，显著超越了现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of Semantic Scene Completion (SSC) from monocular RGB images, particularly the difficulty in accurately inferring occluded 3D geometry. The authors propose FlowSSC, a novel generative framework that treats SSC as a conditional generation problem and integrates with existing feed-forward methods to enhance their performance. Experimental results on the SemanticKITTI dataset show that FlowSSC achieves state-of-the-art performance, significantly surpassing existing baselines while enabling real-time inference through a unique Shortcut Flow-matching mechanism that allows high-fidelity generation in a single step.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从单目RGB图像进行语义场景补全（SSC）时面临的挑战，特别是在准确推断被遮挡的3D几何形状方面的困难。作者提出了FlowSSC，这是一种新颖的生成框架，将SSC视为条件生成问题，并与现有的前馈方法集成，以提高其性能。在SemanticKITTI数据集上的实验结果表明，FlowSSC实现了最先进的性能，显著超越了现有基准，同时通过独特的Shortcut Flow-matching机制实现了实时推理，能够在一步中生成高保真结果。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
