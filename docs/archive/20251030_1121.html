<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-30 11:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251030_1121</div>
    <div class="row"><div class="card">
<div class="title">Improving Temporal Consistency and Fidelity at Inference-time in   Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</div>
<div class="meta-line">Authors: Nasrin Rahimi, A. Murat Tekalp</div>
<div class="meta-line">First: 2025-10-29T11:40:06+00:00 · Latest: 2025-10-29T11:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25420v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as powerful priors for single-image
restoration, but their application to zero-shot video restoration suffers from
temporal inconsistencies due to the stochastic nature of sampling and
complexity of incorporating explicit temporal modeling. In this work, we
address the challenge of improving temporal coherence in video restoration
using zero-shot image-based diffusion models without retraining or modifying
their architecture. We propose two complementary inference-time strategies: (1)
Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired
perceptual straightening hypothesis, which steers the diffusion denoising
process towards smoother temporal evolution by incorporating a curvature
penalty in a perceptual space to improve temporal perceptual scores, such as
Fr\&#x27;echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path
Ensemble Sampling (MPES), which aims at reducing stochastic variation by
ensembling multiple diffusion trajectories to improve fidelity (distortion)
scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these
training-free techniques provide a practical path toward temporally stable
high-fidelity perceptual video restoration using large pretrained diffusion
models. We performed extensive experiments over multiple datasets and
degradation types, systematically evaluating each strategy to understand their
strengths and limitations. Our results show that while PSG enhances temporal
naturalness, particularly in case of temporal blur, MPES consistently improves
fidelity and spatio-temporal perception--distortion trade-off across all tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过零-shot基于图像的扩散模型在推理时改善感知视频恢复的时间一致性和保真度</div>
<div class="mono" style="margin-top:8px">扩散模型作为单图像恢复的强大先验已逐渐崭露头角，但其在零-shot视频恢复中的应用由于采样的随机性和显式时间建模的复杂性而面临时间不一致性的问题。在本研究中，我们解决了在不重新训练或修改架构的情况下，使用零-shot基于图像的扩散模型改善视频恢复中的时间一致性这一挑战。我们提出了两种互补的推理时策略：(1) 基于神经科学启发的感知拉直假设的感知拉直引导（PSG），通过在感知空间中引入曲率惩罚，引导扩散去噪过程朝向更平滑的时间演变，以提高时间感知评分，如Fréchet视频距离（FVD）和感知直线性；(2) 多路径集成采样（MPES），旨在通过集成多个扩散轨迹来减少随机变化，以提高保真度（失真）评分，如PSNR和SSIM，而不牺牲清晰度。这些无训练的技术共同为使用大型预训练扩散模型实现时间稳定的高保真感知视频恢复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估每种策略以理解其优缺点。我们的结果表明，尽管PSG在时间自然性方面有所增强，特别是在时间模糊的情况下，MPES在所有任务中始终改善了保真度和时空感知-失真权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the temporal inconsistencies in zero-shot video restoration using diffusion models, which arise from their stochastic sampling nature. The authors propose two inference-time strategies: Perceptual Straightening Guidance (PSG), which incorporates a curvature penalty to enhance temporal coherence, and Multi-Path Ensemble Sampling (MPES), which reduces stochastic variation by combining multiple diffusion paths. Experimental results demonstrate that PSG improves temporal naturalness, especially in cases of temporal blur, while MPES consistently enhances fidelity and the trade-off between perception and distortion across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在使用扩散模型进行零-shot视频恢复时出现的时间不一致性问题，这些模型在单图像恢复中表现出色，但由于其随机采样特性，在视频恢复中面临挑战。作者提出了两种推理时策略：感知直线引导（PSG），通过在感知空间中应用曲率惩罚来增强时间一致性，以及多路径集成采样（MPES），通过结合多个扩散轨迹来减少随机变化。实验结果表明，PSG在时间模糊的情况下改善了时间自然性，而MPES在各种任务中始终提高了保真度以及感知与失真之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired   Monocentric Design</div>
<div class="meta-line">Authors: Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang</div>
<div class="meta-line">First: 2025-10-29T09:27:38+00:00 · Latest: 2025-10-29T09:27:38+00:00</div>
<div class="meta-line">Comments: The source code will be publicly available at
  https://github.com/ZongxiYu-ZJU/BMI</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25314v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25314v1">PDF</a> · <a href="https://github.com/ZongxiYu-ZJU/BMI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清晰而深刻的观察：一种基于生物启发的单心设计的RGBD成像方法</div>
<div class="mono" style="margin-top:8px">实现高保真、紧凑的RGBD成像面临双重挑战：传统紧凑光学在整个景深范围内难以保持RGB清晰度，而仅依赖软件的单目深度估计（MDE）是一个不适定问题，依赖于不可靠的语义先验。虽然具有衍射光学元件（DOE）的深度光学可以编码深度，但它们在制造复杂性和色差方面引入了权衡，妨碍了简单性。为了解决这个问题，我们首先介绍了一种新颖的生物启发的全球面单心透镜，围绕它构建了仿生单心成像（BMI）框架，这是一个整体协同设计。该光学设计自然地将深度编码到其深度变化的点扩散函数（PSF）中，而无需复杂的衍射或自由形状元件。我们建立了一个严格的基于物理的前向模型，通过精确模拟光学退化过程生成合成数据集。该模拟管道与一个双头多尺度重建网络共同设计，该网络采用共享编码器共同恢复高保真的全聚焦（AiF）图像和精确的深度图。大量实验验证了所提框架的最先进性能。在深度估计中，该方法达到了0.026的绝对相对误差（Abs Rel）和0.130的均方根误差（RMSE），显著优于领先的软件方法和其他深度光学系统。在图像恢复方面，该系统达到了0.960的结构相似性指数（SSIM）和0.082的感知LPIPS分数，从而确认了图像保真度和深度准确性之间的优越平衡。本研究表明，生物启发的全球面光学与联合重建算法的结合构成了解决高性能紧凑RGBD成像内在挑战的有效策略。源代码将公开发布在https://github.com/ZongxiYu-ZJU/BMI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to overcome the challenges of achieving high-fidelity, compact RGBD imaging, which is hindered by conventional optics and unreliable software-based depth estimation methods. The authors introduce a novel bio-inspired all-spherical monocentric lens and develop the Bionic Monocentric Imaging (BMI) framework, which integrates optical design with a dual-head, multi-scale reconstruction network. Experimental results demonstrate that the method achieves an absolute relative error of 0.026 and a root mean square error of 0.130 in depth estimation, significantly outperforming existing software-only methods and deep optics systems, while also achieving a structural similarity index of 0.960 and a perceptual LPIPS score of 0.082 in image restoration, indicating a strong balance between image fidelity and depth accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服实现高保真、紧凑的RGBD成像所面临的挑战，这些挑战主要源于传统光学和不可靠的软件深度估计方法。作者提出了一种新颖的生物启发式全球面单光心透镜，并开发了生物单光心成像（BMI）框架，将光学设计与双头多尺度重建网络相结合。实验结果表明，该方法显著优于现有方法，在深度估计中实现了0.026的绝对相对误差和0.130的均方根误差，同时在图像恢复中获得了0.960的结构相似性指数（SSIM）和0.082的感知LPIPS分数，表明在图像保真度和深度准确性之间取得了成功的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt   State Space Model</div>
<div class="meta-line">Authors: Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao</div>
<div class="meta-line">First: 2025-04-24T16:46:32+00:00 · Latest: 2025-10-29T07:04:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.17732v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.17732v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-One image restoration aims to address multiple image degradation
problems using a single model, offering a more practical and versatile solution
compared to designing dedicated models for each degradation type. Existing
approaches typically rely on Degradation-specific models or coarse-grained
degradation prompts to guide image restoration. However, they lack fine-grained
modeling of degradation information and face limitations in balancing
multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a
novel All-in-One image restoration framework that introduces a fine-grained
degradation extractor and a Degradation-Aware Prompt State Space Model
(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured
by the extractor as dynamic prompts, which are then incorporated into the state
space modeling process. This enhances the model&#x27;s adaptability to diverse
degradation types, while a complementary High-Frequency Enhancement Block (HEB)
recovers local high-frequency details. Extensive experiments on a mixed dataset
containing seven degradation types show that DPMambaIR achieves the best
performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These
results highlight the potential and superiority of DPMambaIR as a unified
solution for All-in-One image restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPMambaIR：基于降级感知提示的全能图像修复状态空间模型</div>
<div class="mono" style="margin-top:8px">全能图像修复旨在通过单一模型解决多种图像降级问题，提供比为每种降级类型设计专用模型更实用和多功能的解决方案。现有方法通常依赖于特定降级模型或粗粒度降级提示来指导图像修复。然而，它们缺乏对降级信息的细粒度建模，并在平衡多任务冲突方面面临限制。为克服这些限制，我们提出了DPMambaIR，这是一种新颖的全能图像修复框架，引入了细粒度降级提取器和降级感知提示状态空间模型（DP-SSM）。DP-SSM利用提取器捕获的细粒度降级特征作为动态提示，然后将其纳入状态空间建模过程中。这增强了模型对多种降级类型的适应性，同时一个补充的高频增强模块（HEB）恢复局部高频细节。在包含七种降级类型的混合数据集上的广泛实验表明，DPMambaIR在PSNR和SSIM中分别达到了27.69dB和0.893的最佳性能。这些结果突显了DPMambaIR作为全能图像修复统一解决方案的潜力和优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a versatile solution for all-in-one image restoration that can effectively handle multiple types of image degradation without the need for separate models. The authors propose DPMambaIR, which incorporates a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM) to enhance the adaptability of the restoration process. Experimental results on a mixed dataset with seven degradation types demonstrate that DPMambaIR outperforms existing methods, achieving a PSNR of 27.69dB and an SSIM of 0.893, indicating its effectiveness as a unified restoration framework.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一种多功能的图像恢复解决方案，能够有效处理多种图像退化问题，而无需单独的模型。作者提出了DPMambaIR，该模型结合了细粒度退化提取器和退化感知提示状态空间模型（DP-SSM），以动态适应各种退化类型。实验结果表明，DPMambaIR在包含七种退化类型的混合数据集上表现优于现有方法，PSNR达到27.69dB，SSIM为0.893，表明其作为统一恢复框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Aware Super-Resolution via Distance-Adaptive Variational   Formulation</div>
<div class="meta-line">Authors: Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu</div>
<div class="meta-line">First: 2025-09-06T15:35:37+00:00 · Latest: 2025-10-29T04:32:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05746v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.05746v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于距离自适应变分公式的深度感知超分辨率</div>
<div class="mono" style="margin-top:8px">单幅图像超分辨率传统上假设空间不变的退化模型，但现实世界成像系统表现出复杂的距离依赖效应，包括大气散射、景深变化和透视失真。这一基本限制要求采用空间自适应重建策略，明确结合几何场景理解以实现最佳性能。我们提出了一个严格的变分框架，将超分辨率表征为空间变化的逆问题，将退化算子表述为具有距离依赖谱特性的伪微分算子，从而能够对不同深度范围的重建极限进行理论分析。我们的神经架构通过级联残差块和深度条件卷积核实现离散梯度流动态，确保收敛到理论能量泛函的驻点，同时结合学习的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的谱约束防止了远场区域的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。对五个基准数据集的全面评估表明，性能达到最先进水平，在KITTI户外场景中，在2倍和4倍缩放下分别实现36.89/0.9516和30.54/0.8721的PSNR/SSIM，分别比现有方法提高0.44dB和0.36dB。该工作建立了第一个理论基础的距离自适应超分辨率框架，并在深度变化场景中展示了显著改进，同时在传统基准上保持竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional single image super-resolution methods that rely on spatially-invariant degradation models, which do not account for real-world distance-dependent effects. The authors propose a variational framework that treats super-resolution as a spatially-varying inverse problem, utilizing a pseudodifferential operator with distance-dependent spectral characteristics to analyze reconstruction limits. Experimental results show that their method achieves state-of-the-art performance on five benchmark datasets, with notable improvements in PSNR and SSIM metrics, particularly in depth-variant scenarios, surpassing existing methods by 0.44dB and 0.36dB at different scales on KITTI outdoor scenes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统单幅图像超分辨率方法的局限性，这些方法依赖于空间不变的降解模型，而未考虑大气散射和深度变化等现实世界的复杂性。作者提出了一种变分框架，将超分辨率视为空间变化的逆问题，利用伪微分算子分析不同深度的重建极限。他们的方法结合了深度条件卷积核和自适应正则化项，在基准数据集上实现了显著的性能提升，超越了现有方法，获得了最先进的PSNR和SSIM分数，超出幅度显著。</div>
</details>
</div>
<div class="card">
<div class="title">WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and   Mamba-based Channel Modeling with Texture Enhancement</div>
<div class="meta-line">Authors: Shengyu Zhu, Congyi Fan, Fuxuan Zhang</div>
<div class="meta-line">First: 2025-10-19T09:11:58+00:00 · Latest: 2025-10-29T02:07:16+00:00</div>
<div class="meta-line">Comments: Chinese Conference on Pattern Recognition and Computer Vision (PRCV),
  Oral</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16765v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.16765v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaMaIR：通过多尺度小波卷积和基于Mamba的通道建模进行图像恢复与纹理增强</div>
<div class="mono" style="margin-top:8px">图像恢复是计算机视觉中的一项基础而具有挑战性的任务，基于CNN的框架展示了显著的计算效率。然而，之前的基于CNN的方法在充分恢复细腻纹理细节方面常常面临挑战，这受到CNN结构小感受野和缺乏通道特征建模的限制。本文提出了WaMaIR，这是一种具有大感受野的新框架，用于图像感知，并改善恢复图像中的纹理细节重建。具体而言，我们引入了全球多尺度小波变换卷积（GMWTConvs），以扩展感受野以提取图像特征，保留和丰富模型输入中的纹理特征。同时，我们提出了基于Mamba的通道感知模块（MCAM），专门设计用于捕捉特征通道中的长程依赖性，从而增强模型对颜色、边缘和纹理信息的敏感性。此外，我们提出了多尺度纹理增强损失（MTELoss）用于图像恢复，以有效指导模型保留详细的纹理结构。大量实验确认WaMaIR优于最先进的方法，实现了更好的图像恢复和模型的高效计算性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of CNN-based frameworks in image restoration, particularly their inability to restore fine texture details due to small receptive fields and inadequate channel feature modeling. The authors propose WaMaIR, a novel framework that utilizes Global Multiscale Wavelet Transform Convolutions to expand the receptive field and enhance texture feature extraction, along with a Mamba-Based Channel-Aware Module to capture long-range dependencies in feature channels. Experimental results demonstrate that WaMaIR significantly outperforms existing state-of-the-art methods in image restoration while maintaining efficient computational performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于CNN的图像恢复框架在恢复细微纹理细节方面的局限性，尤其是由于小感受野和不充分的通道特征建模。作者提出了WaMaIR，这是一种新颖的框架，利用全局多尺度小波变换卷积来扩展感受野并增强纹理特征提取，同时设计了基于Mamba的通道感知模块，以捕捉特征通道中的长程依赖关系。实验结果表明，WaMaIR在图像恢复方面显著优于现有的最先进方法，实现了更高的质量和计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">Frequency-Aware Vision Transformers for High-Fidelity Super-Resolution   of Earth System Models</div>
<div class="meta-line">Authors: Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić</div>
<div class="meta-line">First: 2025-02-18T01:52:41+00:00 · Latest: 2025-10-28T16:06:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.12427v4">Abs</a> · <a href="http://arxiv.org/pdf/2502.12427v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth
System Model (ESM) outputs, allowing fine-scale structures vital to climate
science to be recovered from coarse simulations. However, traditional deep
super-resolution methods, including convolutional and transformer-based models,
tend to exhibit spectral bias, reconstructing low-frequency content more
readily than valuable high-frequency details. In this work, we introduce two
frequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit
Representation (ViSIR), combining Vision Transformers and sinusoidal
activations to mitigate spectral bias, and the Vision Transformer Fourier
Representation Network (ViFOR), which integrates explicit Fourier-based
filtering for independent low- and high-frequency learning. Evaluated on the
E3SM-HR Earth system dataset across surface temperature, shortwave, and
longwave fluxes, these models outperform leading CNN, GAN, and vanilla
transformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in
PSNR and significantly higher SSIM. Detailed ablation and scaling studies
highlight the benefit of full-field training, the impact of frequency
hyperparameters, and the potential for generalization. The results establish
ViFOR as a state-of-the-art, scalable solution for climate data downscaling.
Future extensions will address temporal super-resolution, multimodal climate
variables, automated parameter selection, and integration of physical
conservation constraints to broaden scientific applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>频率感知视觉变换器用于地球系统模型的高保真超分辨率</div>
<div class="mono" style="margin-top:8px">超分辨率（SR）对于提高地球系统模型（ESM）输出的空间保真度至关重要，使得气候科学所需的细尺度结构能够从粗糙的模拟中恢复。然而，传统的深度超分辨率方法，包括卷积和基于变换器的模型，往往表现出光谱偏差，更容易重建低频内容而非有价值的高频细节。在本研究中，我们引入了两个频率感知框架：视觉变换器调谐的正弦隐式表示（ViSIR），结合视觉变换器和正弦激活以减轻光谱偏差，以及视觉变换器傅里叶表示网络（ViFOR），它集成了显式傅里叶滤波以实现独立的低频和高频学习。在E3SM-HR地球系统数据集上评估，涵盖表面温度、短波和长波通量，这些模型的表现超越了领先的CNN、GAN和普通变换器基线，ViFOR在PSNR上显示出高达2.6~dB的改善，并且SSIM显著更高。详细的消融和缩放研究突出了全场训练的好处、频率超参数的影响以及泛化的潜力。结果确立了ViFOR作为气候数据下采样的最先进、可扩展的解决方案。未来的扩展将解决时间超分辨率、多模态气候变量、自动参数选择以及物理守恒约束的整合，以扩大科学适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the spatial fidelity of Earth System Model outputs through super-resolution techniques, which are essential for recovering fine-scale structures important for climate science. The authors propose two frequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit Representation (ViSIR) and the Vision Transformer Fourier Representation Network (ViFOR), which address the spectral bias commonly found in traditional super-resolution methods by utilizing sinusoidal activations and Fourier-based filtering, respectively. Experimental results on the E3SM-HR dataset show that these models outperform existing CNN, GAN, and vanilla transformer approaches, with ViFOR achieving up to 2.6 dB improvements in PSNR and significantly higher SSIM, indicating their effectiveness in enhancing climate data downscaling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过超分辨率技术提高地球系统模型输出的空间保真度，这对于恢复气候科学中重要的细尺度结构至关重要。作者提出了两种频率感知框架：视觉变换器调谐的正弦隐式表示（ViSIR）和视觉变换器傅里叶表示网络（ViFOR），这两者通过分别利用正弦激活和傅里叶滤波来解决传统超分辨率方法中常见的频谱偏差。对E3SM-HR数据集的实验结果表明，这些模型在性能上优于现有的CNN、GAN和普通变换器方法，其中ViFOR在PSNR上提高了最多2.6 dB，并显著提高了SSIM，表明它们在气候数据下采样中的有效性，并具有在时间超分辨率和多模态变量方面未来增强的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Delving into Cascaded Instability: A Lipschitz Continuity View on Image   Restoration and Object Detection Synergy</div>
<div class="meta-line">Authors: Qing Zhao, Weijian Deng, Pengxu Wei, ZiYi Dong, Hannan Lu, Xiangyang Ji, Liang Lin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-28T09:41:42+00:00 · Latest: 2025-10-28T09:41:42+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24232v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector&#x27;s feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入探讨级联不稳定性：基于Lipschitz连续性的图像恢复与目标检测协同视角</div>
<div class="mono" style="margin-top:8px">为了提高在恶劣条件下（例如雾霾和低光照）的检测鲁棒性，图像恢复通常作为预处理步骤应用，以增强检测器的图像质量。然而，恢复网络与检测网络之间的功能不匹配可能引入不稳定性，并阻碍有效整合——这是一个尚未深入探讨的问题。我们通过Lipschitz连续性的视角重新审视这一局限，分析恢复网络与检测网络在输入空间和参数空间中的功能差异。我们的分析表明，恢复网络执行平滑、连续的变换，而目标检测器则在不连续的决策边界上操作，使其对微小扰动高度敏感。这种不匹配在传统级联框架中引入不稳定性，恢复过程中即使是微不可察的噪声在检测时也会被放大，干扰梯度流并阻碍优化。为了解决这个问题，我们提出了Lipschitz正则化目标检测（LROD），这是一个简单而有效的框架，将图像恢复直接整合到检测器的特征学习中，在训练过程中协调两个任务的Lipschitz连续性。我们将该框架实现为Lipschitz正则化YOLO（LR-YOLO），无缝扩展到现有的YOLO检测器。在雾霾和低光照基准上的大量实验表明，LR-YOLO始终提高了检测稳定性、优化平滑性和整体准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of improving detection robustness in adverse conditions, where traditional image restoration methods can introduce instability due to functional mismatches with detection networks. The authors analyze this issue through Lipschitz continuity, revealing that restoration networks provide smooth transformations while detection networks have discontinuous decision boundaries, leading to sensitivity to perturbations. To mitigate this instability, they propose a Lipschitz-regularized object detection framework (LROD), implemented as Lipschitz-regularized YOLO (LR-YOLO), which integrates image restoration into the detector&#x27;s feature learning. Experimental results on haze and low-light benchmarks show that LR-YOLO enhances detection stability, optimization smoothness, and overall accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高在雾霾和低光等恶劣条件下的检测鲁棒性，而传统的图像恢复方法由于与检测网络的功能不匹配，可能引入不稳定性。作者通过利普希茨连续性分析这一问题，揭示恢复网络提供平滑的变换，而检测网络则具有不连续的决策边界，导致对微小扰动的敏感性。为了解决这种不稳定性，他们提出了一种利普希茨正则化的目标检测框架（LROD），并实现为利普希茨正则化YOLO（LR-YOLO），将图像恢复直接集成到检测器的特征学习中。对雾霾和低光基准的实验结果表明，LR-YOLO显著提高了检测的稳定性、优化的平滑性和整体准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution   with Fourier Constraints</div>
<div class="meta-line">Authors: Kazutoshi Akita, Norimichi Ukita</div>
<div class="meta-line">First: 2025-10-28T01:19:54+00:00 · Latest: 2025-10-28T01:19:54+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23978v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有傅里叶约束的高效成本与质量可控任意尺度超分辨率</div>
<div class="mono" style="margin-top:8px">在任意尺度超分辨率中，成本与质量（CQ）可控性至关重要。现有方法使用递归神经网络逐个预测傅里叶分量。然而，这种方法由于独立预测导致性能下降和效率低下。本文提出联合预测多个分量，以提高质量和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution, addressing the inefficiencies and performance issues of existing methods that predict Fourier components independently using recurrent neural networks. The authors propose a novel method that predicts multiple Fourier components jointly, which aims to improve both the quality of the super-resolution output and the efficiency of the process. Experimental results demonstrate that this joint prediction approach significantly enhances performance compared to traditional independent prediction methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高任意尺度超分辨率中的成本和质量可控性，解决现有方法独立预测傅里叶分量所导致的性能下降和低效率的问题。作者提出了一种新方法，通过联合预测多个傅里叶分量，而不是按顺序进行预测，从而提高超分辨率输出的质量和过程的效率。实验结果表明，这种联合预测方法相比传统的独立预测方法显著提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution   of Rare-Earth Features</div>
<div class="meta-line">Authors: Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</div>
<div class="meta-line">First: 2025-10-27T19:56:43+00:00 · Latest: 2025-10-27T19:56:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23816v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow&#x27;s core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model&#x27;s
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RareFlow：面向物理的跨传感器稀土特征超分辨率流匹配</div>
<div class="mono" style="margin-top:8px">遥感图像的超分辨率（SR）在分布外（OOD）条件下常常失败，例如由不同传感器捕获的稀有地貌特征，产生视觉上合理但物理上不准确的结果。我们提出了RareFlow，一个旨在提高OOD鲁棒性的面向物理的SR框架。RareFlow的核心是一个双条件架构。门控控制网络从低分辨率输入中保留细粒度的几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上合理的输出，我们引入了一个多方面的损失函数，强制执行与传感器特性的一致性，包括光谱和辐射一致性。此外，该框架通过采用随机前向传播方法量化自身的预测不确定性；结果输出方差直接识别不熟悉的输入，减轻特征幻觉。我们在一个新的、精心策划的多传感器卫星图像基准上验证了RareFlow。在盲评中，地球物理专家将我们模型的输出评为接近真实图像的保真度，显著优于最先进的基线。这种定性优越性通过感知指标的定量提升得到了证实，包括FID减少近40%。RareFlow为数据稀缺的科学领域提供了一个高保真合成的强大框架，并为在严重领域转移下的受控生成提供了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of super-resolution (SR) in remote sensing imagery, particularly under out-of-distribution conditions where rare geomorphic features are captured by various sensors. The authors developed RareFlow, a physics-aware SR framework that employs a dual-conditioning architecture, combining a Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Experimental results demonstrate that RareFlow significantly outperforms state-of-the-art methods, with geophysical experts rating its outputs as nearly equivalent to ground truth imagery, and achieving a nearly 40% reduction in Fréchet Inception Distance (FID), indicating improved perceptual quality and robustness in data-scarce environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决遥感图像超分辨率（SR）在分布外条件下的挑战，特别是当各种传感器捕获稀有地貌特征时，导致结果不准确。作者提出了RareFlow，这是一种物理感知的SR框架，采用双重条件架构，结合了用于几何保真度的门控控制网络和用于语义指导的文本提示。实验结果表明，RareFlow显著优于最先进的方法，地球物理专家将其输出评估为几乎等同于真实图像，并且在Fréchet Inception Distance（FID）上实现了近40%的减少，表明其在数据稀缺环境中生成高保真图像的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">An Efficient Remote Sensing Super Resolution Method Exploring Diffusion   Priors and Multi-Modal Constraints for Crop Type Mapping</div>
<div class="meta-line">Authors: Songxi Yang, Tang Sui, Qunying Huang</div>
<div class="meta-line">First: 2025-10-27T14:34:52+00:00 · Latest: 2025-10-27T14:34:52+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23382v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super resolution offers a way to harness medium even lowresolution but
historically valuable remote sensing image archives. Generative models,
especially diffusion models, have recently been applied to remote sensing super
resolution (RSSR), yet several challenges exist. First, diffusion models are
effective but require expensive training from scratch resources and have slow
inference speeds. Second, current methods have limited utilization of auxiliary
information as real-world constraints to reconstruct scientifically realistic
images. Finally, most current methods lack evaluation on downstream tasks. In
this study, we present a efficient LSSR framework for RSSR, supported by a new
multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built
on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention
with auxiliary knowledge (Digital Elevation Model, land cover, month) and
Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier
NDVI loss to balance spatial details and spectral fidelity. Extensive
experiments demonstrate that LSSR significantly improves crop boundary
delineation and recovery, achieving state-of-the-art performance with Peak
Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)
and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while
maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers
effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,
yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:
0.85). These results highlight the potential of RSSR to advance precision
agriculture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的遥感超分辨率方法：探索扩散先验和多模态约束用于作物类型映射</div>
<div class="mono" style="margin-top:8px">超分辨率提供了一种利用中等甚至低分辨率但历史上有价值的遥感图像档案的方法。生成模型，尤其是扩散模型，最近被应用于遥感超分辨率（RSSR），但仍存在若干挑战。首先，扩散模型有效但需要昂贵的从零开始的训练资源，并且推理速度较慢。其次，当前方法对辅助信息的利用有限，无法作为现实世界约束来重建科学上真实的图像。最后，大多数当前方法缺乏对下游任务的评估。在本研究中，我们提出了一种高效的LSSR框架用于RSSR，支持一个新的多模态数据集，包含配对的30米Landsat 8和10米Sentinel 2影像。基于冻结的预训练稳定扩散，LSSR将跨模态注意力与辅助知识（数字高程模型、土地覆盖、月份）和合成孔径雷达指导相结合，通过适配器和定制的傅里叶NDVI损失来平衡空间细节和光谱保真度。大量实验表明，LSSR显著改善了作物边界的描绘和恢复，达到最先进的性能，峰值信噪比/结构相似性指数分别为32.63/0.84（RGB）和23.99/0.78（IR），NDVI均方误差最低（0.042），同时保持高效推理（0.39秒/图像）。此外，LSSR有效转移到NASA协调的Landsat和Sentinel（HLS）超分辨率，产生比Sentinel-2更可靠的作物分类（F1: 0.86，Sentinel-2: F1: 0.85）。这些结果突显了RSSR在推进精准农业方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to utilize low-resolution remote sensing images for effective crop type mapping, addressing challenges in existing super resolution methods. The authors propose an efficient LSSR framework that leverages a multimodal dataset of paired Landsat 8 and Sentinel 2 imagery, integrating crossmodal attention with auxiliary knowledge and a tailored Fourier NDVI loss. Experimental results show that LSSR significantly enhances crop boundary delineation and recovery, achieving state-of-the-art performance metrics and efficient inference times, while also demonstrating effective transferability to NASA&#x27;s HLS super resolution for improved crop classification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效利用低分辨率遥感图像进行作物类型映射，解决现有扩散模型的高训练成本和慢推理速度等挑战。作者提出了一种高效的LSSR框架，利用新的配对Landsat 8和Sentinel 2影像的多模态数据集，结合跨模态注意力和辅助知识，如数字高程模型和土地覆盖信息。实验结果表明，LSSR框架显著增强了作物边界的描绘和恢复，达到了最先进的性能指标，并展示了其在NASA HLS超分辨率中的有效迁移性，从而改善了作物分类。</div>
</details>
</div>
<div class="card">
<div class="title">Residual Diffusion Bridge Model for Image Restoration</div>
<div class="meta-line">Authors: Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</div>
<div class="meta-line">First: 2025-10-27T08:35:49+00:00 · Latest: 2025-10-27T08:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23116v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23116v1">PDF</a> · <a href="https://github.com/MiliLab/RDBM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion bridge models establish probabilistic paths between arbitrary
paired distributions and exhibit great potential for universal image
restoration. Most existing methods merely treat them as simple variants of
stochastic interpolants, lacking a unified analytical perspective. Besides,
they indiscriminately reconstruct images through global noise injection and
removal, inevitably distorting undegraded regions due to imperfect
reconstruction. To address these challenges, we propose the Residual Diffusion
Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic
differential equations of generalized diffusion bridge and derive the
analytical formulas of its forward and reverse processes. Crucially, we
leverage the residuals from given distributions to modulate the noise injection
and removal, enabling adaptive restoration of degraded regions while preserving
intact others. Moreover, we unravel the fundamental mathematical essence of
existing bridge models, all of which are special cases of RDBM and empirically
demonstrate the optimality of our proposed models. Extensive experiments are
conducted to demonstrate the state-of-the-art performance of our method both
qualitatively and quantitatively across diverse image restoration tasks. Code
is publicly available at https://github.com/MiliLab/RDBM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于图像恢复的残差扩散桥模型</div>
<div class="mono" style="margin-top:8px">扩散桥模型在任意配对分布之间建立概率路径，展现出在通用图像恢复中的巨大潜力。现有大多数方法仅将其视为随机插值的简单变体，缺乏统一的分析视角。此外，它们通过全局噪声注入和去除不加区分地重建图像，因不完美的重建不可避免地扭曲未退化区域。为了解决这些挑战，我们提出了残差扩散桥模型（RDBM）。具体而言，我们从理论上重新表述广义扩散桥的随机微分方程，并推导出其正向和反向过程的解析公式。关键是，我们利用给定分布的残差来调节噪声的注入和去除，实现对退化区域的自适应恢复，同时保留完整的其他区域。此外，我们揭示了现有桥模型的基本数学本质，所有这些模型都是RDBM的特例，并实证证明了我们提出模型的最优性。我们进行了广泛的实验，以定性和定量的方式展示我们方法在多种图像恢复任务中的最先进性能。代码可在 https://github.com/MiliLab/RDBM 上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing diffusion bridge models in image restoration, which often distort undegraded regions due to indiscriminate noise handling. The authors propose the Residual Diffusion Bridge Model (RDBM), which reformulates the stochastic differential equations of generalized diffusion bridges and derives analytical formulas for its processes. Experimental results show that RDBM achieves state-of-the-art performance in various image restoration tasks by adaptively restoring degraded areas while preserving intact regions, demonstrating the model&#x27;s optimality compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有扩散桥模型的局限性来改善图像恢复技术，这些模型在重建过程中常常扭曲未退化区域。作者提出了残差扩散桥模型（RDBM），该模型重新公式化了随机微分方程，并推导出前向和反向过程的解析公式。关键实验结果表明，RDBM有效地利用给定分布的残差来调节噪声注入和去除，实现了自适应恢复，既增强了退化区域，又保留了完整区域，且在各种图像恢复任务中表现优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
