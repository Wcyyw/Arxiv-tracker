<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-09 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260109_0321</div>
    <div class="row"><div class="card">
<div class="title">FLEx: Language Modeling with Few-shot Language Explanations</div>
<div class="meta-line">Authors: Adar Avsian, Christopher Richardson, Anirudh Sundar, Larry Heck</div>
<div class="meta-line">First: 2026-01-07T18:12:05+00:00 · Latest: 2026-01-07T18:12:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\% of CoT&#x27;s remaining errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLEx：使用少量语言解释进行语言建模</div>
<div class="mono" style="margin-top:8px">语言模型在从数学问题解决到开放领域问答等广泛任务中变得有效。然而，它们仍然会犯错，这些错误往往在相关查询中重复出现。自然语言解释可以帮助纠正这些错误，但在需要专家注释的领域，规模化收集这些解释可能不可行。为了解决这个问题，我们引入了FLEx（$\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations），一种使用少量解释性示例改善模型行为的方法。FLEx使用基于嵌入的聚类选择代表性模型错误，验证相关解释是否纠正了这些错误，并将其总结为在推理时前置的提示前缀。这个总结指导模型在新输入上避免类似错误，而不修改模型权重。我们在CounterBench、GSM8K和ReasonIF上评估FLEx。我们发现FLEx在所有三个数据集上始终优于链式思维（CoT）提示，并减少了高达83\%的CoT剩余错误。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of language models, which often make repeated mistakes across similar queries, by utilizing natural language explanations. The authors propose FLEx, a method that employs embedding-based clustering to identify representative model errors, verifies the effectiveness of associated explanations, and summarizes them into a prompt prefix for inference-time guidance. Experimental results demonstrate that FLEx significantly outperforms chain-of-thought prompting on the CounterBench, GSM8K, and ReasonIF datasets, achieving up to an 83% reduction in remaining errors compared to CoT prompting.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高语言模型的性能，尽管它们在各种任务中表现出色，但仍然在相似查询中出现重复错误。作者提出了FLEx，这是一种利用少量示例来改善模型行为的方法，通过基于嵌入的聚类选择代表性错误，验证自然语言解释提供的修正，并将这些总结为推理时的提示前缀。实验结果表明，FLEx在CounterBench、GSM8K和ReasonIF数据集上显著优于链式思维提示，剩余错误减少了多达83%。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</div>
<div class="meta-line">Authors: Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag</div>
<div class="meta-line">First: 2026-01-07T18:05:08+00:00 · Latest: 2026-01-07T18:05:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Diffusion-DRF：用于视频扩散微调的可微分奖励流</div>
<div class="mono" style="margin-top:8px">直接偏好优化（DPO）最近通过增强视觉保真度和文本对齐改善了文本到视频（T2V）生成。然而，当前方法依赖于来自人类注释或学习奖励模型的非可微分偏好信号。这种依赖使得训练标签密集、易受偏见影响且容易被操控，常常导致奖励黑客行为和不稳定的训练。我们提出了Diffusion-DRF，一种可微分奖励流，用于使用冻结的现成视觉-语言模型（VLM）作为无训练批评者来微调视频扩散模型。Diffusion-DRF通过扩散去噪链直接反向传播VLM反馈，将logit级响应转换为令牌感知梯度以进行优化。我们提出了一种自动化的、基于方面结构的提示管道，以获取可靠的多维VLM反馈，同时梯度检查点使得通过最终去噪步骤进行高效更新成为可能。Diffusion-DRF在改善视频质量和语义对齐的同时，减轻了奖励黑客行为和崩溃——无需额外的奖励模型或偏好数据集。它是模型无关的，并且可以很好地推广到其他基于扩散的生成任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current Text-to-Video generation methods, which depend on non-differentiable preference signals that can lead to training inefficiencies and biases. The authors introduce Diffusion-DRF, a novel approach that utilizes a frozen Vision-Language Model as a training-free critic to provide differentiable feedback during the fine-tuning of video diffusion models. Experimental results demonstrate that Diffusion-DRF enhances video quality and semantic alignment while reducing issues such as reward hacking and training instability, all without the need for additional reward models or preference datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前文本到视频生成方法的局限性，这些方法依赖于非可微的偏好信号，可能导致偏见和不稳定的训练。作者提出了Diffusion-DRF，一种利用冻结的视觉-语言模型（VLM）作为无训练的评论者，为视频扩散模型的微调提供可微的奖励流的方法。实验结果表明，Diffusion-DRF提高了视频质量和语义对齐，同时减少了奖励黑客和模型崩溃等问题，实现了这些改进而无需额外的奖励模型或偏好数据集，并在各种基于扩散的生成任务中表现出通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Low Resource Reconstruction Attacks Through Benign Prompts</div>
<div class="meta-line">Authors: Sol Yarkoni, Mahmood Sharif, Roi Livni</div>
<div class="meta-line">First: 2025-07-10T17:32:26+00:00 · Latest: 2026-01-07T17:17:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.07947v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.07947v3">PDF</a> · <a href="https://github.com/TheSolY/lr-tmi">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative models, such as diffusion models, have raised concerns related to privacy, copyright infringement, and data stewardship. To better understand and control these risks, prior work has introduced techniques and attacks that reconstruct images, or parts of images, from training data. While these results demonstrate that training data can be recovered, existing methods often rely on high computational resources, partial access to the training set, or carefully engineered prompts.
  In this work, we present a new attack that requires low resources, assumes little to no access to the training data, and identifies seemingly benign prompts that can lead to potentially risky image reconstruction. We further show that such reconstructions may occur unintentionally, even for users without specialized knowledge. For example, we observe that for one existing model, the prompt ``blue Unisex T-Shirt&#x27;&#x27; generates the face of a real individual. Moreover, by combining the identified vulnerabilities with real-world prompt data, we discover prompts that reproduce memorized visual elements.
  Our approach builds on insights from prior work and leverages domain knowledge to expose a fundamental vulnerability arising from the use of scraped e-commerce data, where templated layouts and images are closely tied to pattern-like textual prompts.
  The code for our attack is publicly available at https://github.com/TheSolY/lr-tmi.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过良性提示进行低资源重建攻击</div>
<div class="mono" style="margin-top:8px">最近生成模型（如扩散模型）的进展引发了与隐私、版权侵犯和数据管理相关的担忧。为了更好地理解和控制这些风险，之前的研究引入了从训练数据中重建图像或部分图像的技术和攻击。尽管这些结果表明训练数据可以被恢复，但现有方法通常依赖于高计算资源、对训练集的部分访问或精心设计的提示。
在本研究中，我们提出了一种新的攻击，要求低资源，假设对训练数据几乎没有访问，并识别出看似良性的提示，这些提示可能导致潜在的风险图像重建。我们进一步表明，这种重建可能是无意中发生的，即使对于没有专业知识的用户。例如，我们观察到对于一个现有模型，提示“蓝色男女通用T恤”生成了一个真实个体的面孔。此外，通过将识别出的漏洞与现实世界的提示数据结合，我们发现了重现记忆视觉元素的提示。
我们的方法基于先前工作的见解，并利用领域知识揭示了由于使用抓取的电子商务数据而产生的基本漏洞，其中模板化的布局和图像与模式化的文本提示紧密相关。
我们的攻击代码已公开可用，网址为 https://github.com/TheSolY/lr-tmi。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing concerns about privacy and data security associated with generative models, particularly in the context of image reconstruction from training data. The authors propose a novel attack method that requires minimal computational resources and limited access to training data, utilizing benign prompts to trigger unintended image reconstructions. Key findings reveal that seemingly innocuous prompts can lead to the generation of identifiable images, such as the face of a real person from the prompt &#x27;blue Unisex T-Shirt&#x27;, highlighting significant vulnerabilities in models trained on e-commerce data.</div>
<div class="mono" style="margin-top:8px">本研究关注与生成模型相关的隐私和数据安全问题，特别是在从训练数据重建图像的背景下。作者提出了一种新颖的攻击方法，该方法在计算资源要求较低且几乎不需要访问训练数据集的情况下，利用看似无害的提示，可能会导致敏感图像的重建。主要发现表明，某些提示（如“蓝色男女通用T恤”）可以生成可识别的面孔，通过分析真实世界的提示数据，研究还识别出其他可以重现训练数据中特定视觉元素的提示。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</div>
<div class="meta-line">Authors: Zhihao Zhu, Jiafeng Liang, Shixin Jiang, Jinlan Fu, Ming Liu, Guanglu Sun, See-Kiong Ng, Bing Qin</div>
<div class="meta-line">First: 2026-01-07T16:39:34+00:00 · Latest: 2026-01-07T16:39:34+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04073v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在跨模态冲突下分析大型多模态模型的推理一致性</div>
<div class="mono" style="margin-top:8px">大型多模态模型（LMMs）在通过思维链（CoT）进行视频推理方面表现出令人印象深刻的能力。然而，它们的推理链的稳健性仍然值得怀疑。本文中，我们识别出一种称为文本惯性的关键失效模式，即一旦在思维过程中发生文本幻觉，模型往往会盲目坚持错误文本，而忽视冲突的视觉证据。为了系统地研究这一点，我们提出了逻辑图扰动协议，该协议在多种LMM的推理链中结构性地注入扰动，涵盖本地推理架构和基于提示的范式，以评估它们的自我反思能力。结果表明，模型在不到10%的情况下成功自我修正，主要屈服于盲目的文本错误传播。为此，我们引入了主动视觉上下文精炼，这是一种无训练推理范式，协调主动视觉再定位机制，以强制进行细粒度验证，并结合自适应上下文精炼策略来总结和去噪推理历史。实验表明，我们的方法显著抑制了幻觉传播，并增强了推理的稳健性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the robustness issues in Large Multimodal Models (LMMs) during video reasoning, particularly focusing on a failure mode called textual inertia, where models adhere to erroneous textual information despite conflicting visual evidence. The authors propose the LogicGraph Perturbation Protocol to systematically inject perturbations into reasoning chains of various LMMs, assessing their ability to self-correct. Experimental results indicate that these models only self-correct in less than 10% of cases and often propagate textual errors, leading to the introduction of Active Visual-Context Refinement, which improves reasoning robustness by implementing an active visual re-grounding mechanism and adaptive context refinement to reduce hallucination propagation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型多模态模型（LMMs）在视频推理中的鲁棒性问题，特别关注一种称为文本惯性的失效模式，即模型在面对冲突的视觉证据时仍然坚持错误的文本信息。作者提出了逻辑图扰动协议，系统性地向各种LMM的推理链中注入扰动，以评估它们自我纠正的能力。实验结果表明，这些模型仅在不到10%的情况下能够自我纠正，并且容易传播文本错误，因此提出了主动视觉上下文精炼方法，通过实施主动的视觉重新定位机制和自适应上下文精炼来增强推理的鲁棒性，从而减少幻觉传播。</div>
</details>
</div>
<div class="card">
<div class="title">Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models</div>
<div class="meta-line">Authors: Zitong Huang, Kaidong Zhang, Yukang Ding, Chao Gao, Rui Ding, Ying Chen, Wangmeng Zuo</div>
<div class="meta-line">First: 2026-01-07T16:32:17+00:00 · Latest: 2026-01-07T16:32:17+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04068v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关注生成细节：视频扩散模型的直接局部细节偏好优化</div>
<div class="mono" style="margin-top:8px">将文本到视频的扩散模型与人类偏好对齐对于生成高质量视频至关重要。现有的直接偏好优化（DPO）方法依赖于多样本排名和特定任务的评判模型，这效率低下且常常产生模糊的全局监督。为了解决这些局限性，我们提出了LocalDPO，这是一种新颖的后训练框架，从真实视频中构建局部偏好对，并在时空区域级别上优化对齐。我们设计了一个自动化管道，以高效收集偏好对数据，通过每个提示进行单次推理生成偏好对，消除了对外部评判模型或手动注释的需求。具体而言，我们将高质量的真实视频视为正样本，并通过随机时空掩码局部破坏它们生成相应的负样本，仅使用冻结的基础模型恢复被掩盖的区域。在训练过程中，我们引入了一种区域感知的DPO损失，将偏好学习限制在破坏区域，以实现快速收敛。在Wan2.1和CogVideoX上的实验表明，LocalDPO在视频保真度、时间一致性和人类偏好评分方面始终优于其他后训练方法，建立了一个更高效和细致的视频生成器对齐范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the alignment of text-to-video diffusion models with human preferences to produce higher quality videos. The authors introduce LocalDPO, a post-training framework that optimizes localized preference pairs derived from real videos, addressing inefficiencies in existing Direct Preference Optimization methods. Experimental results on Wan2.1 and CogVideoX show that LocalDPO significantly improves video fidelity, temporal coherence, and human preference scores compared to other post-training techniques, establishing a more efficient approach for video generator alignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高文本到视频扩散模型与人类偏好的对齐，以生成高质量的视频。作者提出了LocalDPO，这是一种新颖的后训练框架，通过真实视频构建局部偏好对，并在时空层面优化对齐，从而解决现有直接偏好优化方法的低效性。Wan2.1和CogVideoX上的实验结果表明，LocalDPO显著提高了视频的保真度、时间一致性和人类偏好评分，相较于其他后训练方法，展示了其在微调视频生成器对齐方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Padé Neurons for Efficient Neural Models</div>
<div class="meta-line">Authors: Onur Keleş, A. Murat Tekalp</div>
<div class="meta-line">First: 2026-01-07T15:15:30+00:00 · Latest: 2026-01-07T15:15:30+00:00</div>
<div class="meta-line">Comments: Accepted for Publication in IEEE TRANSACTIONS ON IMAGE PROCESSING; 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04005v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04005v1">PDF</a> · <a href="https://github.com/onur-keles/Paon">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效神经模型的Padé神经元</div>
<div class="mono" style="margin-top:8px">神经网络通常采用McCulloch-Pitts神经元模型，该模型是一个线性模型，后接逐点非线性激活。各种研究者已经提出了固有非线性的神经元模型，如二次神经元、广义运算神经元、生成神经元和超神经元，这些模型相比逐点激活函数提供了更强的非线性。在本文中，我们介绍了一种新颖且更好的非线性神经元模型，称为Padé神经元（Paons），其灵感来自Padé逼近。Paons具有多种优势，例如非线性的多样性，因为每个Paon学习其输入的不同非线性函数，以及层效率，因为与分段线性逼近相比，Paons在更少的层中提供更强的非线性。此外，Paons将所有先前提出的神经元模型作为特例，因此任何网络中的神经元模型都可以被Paons替代。我们注意到，已经提出将Padé逼近作为广义逐点激活函数，这与我们的模型根本不同。为了验证Paons的有效性，在我们的实验中，我们用Paons替换了一些基于ResNet架构的知名神经图像超分辨率、压缩和分类模型中的经典神经元。我们的综合实验结果和分析表明，基于Paons构建的神经模型在层数更少的情况下提供了比经典模型更好或相等的性能。Paon的PyTorch实现代码已开源，网址为https://github.com/onur-keles/Paon。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the non-linearity of neural network models beyond the limitations of the traditional McCulloch-Pitts neuron model. The authors introduce Padé neurons (Paons), a novel non-linear neuron model inspired by Padé approximants, which allows for a diverse range of non-linear functions and greater layer efficiency. Experimental results show that neural models utilizing Paons in place of classic neurons in ResNet-based architectures achieve equal or superior performance in image super-resolution, compression, and classification tasks while requiring fewer layers.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于超越传统麦卡洛克-皮茨神经元模型的局限性，提高神经网络中的非线性。作者提出了一种新的神经元模型，称为帕德神经元（Paons），灵感来自帕德逼近，能够实现多样化的非线性函数并提高层效率。实验结果表明，使用Paons的神经模型在图像超分辨率、压缩和分类任务中实现了与经典模型相等或更好的性能，同时所需层数更少。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings</div>
<div class="meta-line">Authors: Liang Hou, Cong Liu, Mingwu Zheng, Xin Tao, Pengfei Wan, Di Zhang, Kun Gai</div>
<div class="meta-line">First: 2025-03-24T14:30:38+00:00 · Latest: 2026-01-07T14:18:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18719v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.18719v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolution generalization in image generation tasks enables the production of higher-resolution images with lower training resolution overhead. However, a key obstacle for diffusion transformers in addressing this problem is the mismatch between positional encodings seen at inference and those used during training. Existing strategies such as positional encodings interpolation, extrapolation, or hybrids, do not fully resolve this mismatch. In this paper, we propose a novel two-dimensional randomized positional encodings, namely RPE-2D, that prioritizes the order of image patches rather than their absolute distances, enabling seamless high- and low-resolution generation without training on multiple resolutions. Concretely, RPE-2D independently samples positions along the horizontal and vertical axes over an expanded range during training, ensuring that the encodings used at inference lie within the training distribution and thereby improving resolution generalization. We further introduce a simple random resize-and-crop augmentation to strengthen order modeling and add micro-conditioning to indicate the applied cropping pattern. On the ImageNet dataset, RPE-2D achieves state-of-the-art resolution generalization performance, outperforming competitive methods when trained at $256^2$ and evaluated at $384^2$ and $512^2$, and when trained at $512^2$ and evaluated at $768^2$ and $1024^2$. RPE-2D also exhibits outstanding capabilities in low-resolution image generation, multi-stage training acceleration, and multi-resolution inheritance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过随机位置编码提升扩散变换器的分辨率泛化</div>
<div class="mono" style="margin-top:8px">图像生成任务中的分辨率泛化使得以较低的训练分辨率开销生成更高分辨率的图像成为可能。然而，扩散变换器在解决此问题时面临的一个关键障碍是推理时看到的位置编码与训练期间使用的位置编码之间的不匹配。现有的策略，如位置编码插值、外推或混合，未能完全解决这一不匹配。在本文中，我们提出了一种新颖的二维随机位置编码，称为RPE-2D，优先考虑图像块的顺序而非其绝对距离，从而实现无须在多个分辨率上训练的无缝高低分辨率生成。具体而言，RPE-2D在训练期间独立地在水平和垂直轴上沿扩展范围采样位置，确保推理时使用的编码位于训练分布内，从而改善分辨率泛化。我们进一步引入了一种简单的随机缩放和裁剪增强，以加强顺序建模，并添加微条件以指示应用的裁剪模式。在ImageNet数据集上，RPE-2D实现了最先进的分辨率泛化性能，在$256^2$训练并在$384^2$和$512^2$评估时超越了竞争方法，以及在$512^2$训练并在$768^2$和$1024^2$评估时。RPE-2D在低分辨率图像生成、多阶段训练加速和多分辨率继承方面也表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance resolution generalization in image generation tasks, addressing the challenge posed by the mismatch in positional encodings during training and inference for diffusion transformers. The authors propose a novel method called randomized positional encodings in two dimensions (RPE-2D), which focuses on the order of image patches rather than their absolute distances, allowing for effective generation of images at varying resolutions without the need for training on multiple resolutions. Experimental results on the ImageNet dataset demonstrate that RPE-2D achieves state-of-the-art performance in resolution generalization, outperforming existing methods when trained at 256^2 and evaluated at 384^2 and 512^2, as well as when trained at 512^2 and evaluated at 768^2 and 1024^2, while also showing strong performance in low-resolution image generation and multi-stage training acceleration.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像生成任务中的分辨率泛化能力，尤其是针对扩散变换器，由于训练和推理期间位置编码的不匹配而面临挑战。作者提出了一种新方法，称为二维随机位置编码（RPE-2D），该方法关注图像块的顺序而非其绝对距离，从而实现有效的高低分辨率图像生成，而无需在多个分辨率上进行训练。对ImageNet数据集的实验结果表明，RPE-2D在分辨率泛化方面达到了最先进的性能，超越了现有方法，在256^2训练并在384^2和512^2评估时表现优异，同时在512^2训练并在768^2和1024^2评估时也表现出色，并且在低分辨率生成和多阶段训练加速方面也显示出强大的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Big Reasoning with Small Models: Instruction Retrieval at Inference Time</div>
<div class="meta-line">Authors: Kenan Alkiek, David Jurgens, Vinod Vydiswaran</div>
<div class="meta-line">First: 2025-10-15T15:51:13+00:00 · Latest: 2026-01-07T14:13:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13935v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13935v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Small language models (SLMs) enable low-cost, private, on-device inference, but they often fail on problems that require specialized domain knowledge or multi-step reasoning. Existing approaches for improving reasoning either rely on scale (e.g., chain-of-thought prompting), require task-specific training that limits reuse and generality (e.g., distillation), or retrieve unstructured information that still leaves the SLM to determine an appropriate reasoning strategy. We propose instruction retrieval, an inference-time intervention that augments an SLM with structured, reusable reasoning procedures rather than raw passages. We construct an Instruction Corpus by clustering similar training questions and using a teacher model to generate generalizable guides that pair domain background with explicit step-by-step procedures. At inference, the SLM retrieves the instructions most relevant to a given query and executes the associated procedures without any additional fine-tuning. Across three challenging domains: medicine, law, and mathematics, instruction retrieval yields consistent gains for models with at least 3B parameters, improving accuracy by 9.4%, 7.9%, and 5.1%, respectively, with the strongest 14B model surpassing GPT-4o&#x27;s zero-shot performance on knowledge-intensive tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小模型的大推理：推理时的指令检索</div>
<div class="mono" style="margin-top:8px">小型语言模型（SLMs）实现了低成本、私密的设备端推理，但在需要专业领域知识或多步推理的问题上往往表现不佳。现有的推理改进方法要么依赖于规模（例如，思维链提示），要么需要特定任务的训练，限制了重用性和通用性（例如，蒸馏），或者检索非结构化信息，仍然需要SLM确定合适的推理策略。我们提出了指令检索，这是一种推理时的干预，使用结构化、可重用的推理程序来增强SLM，而不是原始段落。我们通过聚类相似的训练问题并使用教师模型生成可推广的指南，构建了一个指令语料库，将领域背景与明确的逐步程序配对。在推理时，SLM检索与给定查询最相关的指令，并执行相关程序，而无需任何额外的微调。在医学、法律和数学三个具有挑战性的领域中，指令检索为至少3B参数的模型带来了持续的提升，准确率分别提高了9.4%、7.9%和5.1%，其中最强的14B模型在知识密集型任务上超越了GPT-4o的零-shot表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of small language models (SLMs) in handling complex reasoning tasks that require specialized knowledge. The authors introduce a method called instruction retrieval, which enhances SLMs at inference time by providing structured reasoning procedures instead of unstructured information. Experimental results demonstrate that this approach significantly improves accuracy in three domains—medicine, law, and mathematics—yielding increases of 9.4%, 7.9%, and 5.1% respectively for models with at least 3 billion parameters, with the 14 billion parameter model outperforming GPT-4o on knowledge-intensive tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高小型语言模型（SLMs）在需要专业知识和多步骤推理的任务中的表现，这些任务通常是它们的弱项。作者提出了一种名为指令检索的方法，通过将SLMs与从指令语料库中生成的结构化推理程序相结合来增强其能力，该语料库是通过聚类相似的训练问题并使用教师模型生成可推广的指南而构建的。实验结果表明，指令检索显著提高了至少30亿参数的SLMs在医学、法律和数学三个领域的准确性，分别提高了9.4%、7.9%和5.1%，其中140亿参数的模型在知识密集型任务的零-shot表现上超过了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis</div>
<div class="meta-line">Authors: Julie van Logtestijn, Petru Manescu</div>
<div class="meta-line">First: 2026-01-07T13:31:33+00:00 · Latest: 2026-01-07T13:31:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03915v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HemBLIP：用于可解释白血病细胞形态分析的视觉语言模型</div>
<div class="mono" style="margin-top:8px">白血球形态的显微评估是白血病诊断的核心，但当前的深度学习模型往往作为黑箱运作，限制了临床信任和采用。我们介绍了HemBLIP，这是一种旨在生成可解释的、形态感知的外周血细胞描述的视觉语言模型。通过使用一组新构建的14,000个健康和白血病细胞的数据集，并配以专家生成的属性说明，我们通过全量微调和基于LoRA的参数高效训练来调整通用VLM，并与生物医学基础模型MedGEMMA进行基准测试。HemBLIP在说明质量和形态准确性方面表现更高，而LoRA适应提供了进一步的收益，同时显著降低了计算成本。这些结果突显了视觉语言模型在透明和可扩展的血液学诊断中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the interpretability of deep learning models used in leukemia diagnosis, as current models often lack transparency, hindering clinical trust. The authors introduce HemBLIP, a vision-language model that generates interpretable descriptions of peripheral blood cell morphology by utilizing a newly constructed dataset of 14,000 healthy and leukemic cells with expert-derived captions. The experimental results demonstrate that HemBLIP outperforms the biomedical foundation model MedGEMMA in terms of caption quality and morphological accuracy, with the LoRA adaptation method achieving these improvements while significantly reducing computational costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高用于白血病诊断的深度学习模型的可解释性，因为当前模型往往缺乏透明度，阻碍了临床信任。作者提出了HemBLIP，这是一种生成外周血细胞形态可解释描述的视觉语言模型，利用了一个包含14,000个健康和白血病细胞的专家标注数据集。实验结果表明，HemBLIP在描述质量和形态准确性方面优于生物医学基础模型MedGEMMA，而LoRA适应方法在显著降低计算成本的同时还带来了额外的改进。</div>
</details>
</div>
<div class="card">
<div class="title">U-REPA: Aligning Diffusion U-Nets to ViTs</div>
<div class="meta-line">Authors: Yuchuan Tian, Hanting Chen, Mengyu Zheng, Yuchen Liang, Chao Xu, Yunhe Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-24T07:46:00+00:00 · Latest: 2026-01-07T13:23:00+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18414v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.18414v3">PDF</a> · <a href="https://github.com/YuchuanTian/U-REPA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representation Alignment (REPA) that aligns Diffusion Transformer (DiT) hidden-states with ViT visual encoders has proven highly effective in DiT training, demonstrating superior convergence properties, but it has not been validated on the canonical diffusion U-Net architecture that shows faster convergence compared to DiTs. However, adapting REPA to U-Net architectures presents unique challenges: (1) different block functionalities necessitate revised alignment strategies; (2) spatial-dimension inconsistencies emerge from U-Net&#x27;s spatial downsampling operations; (3) space gaps between U-Net and ViT hinder the effectiveness of tokenwise alignment. To encounter these challenges, we propose \textbf{U-REPA}, a representation alignment paradigm that bridges U-Net hidden states and ViT features as follows: Firstly, we propose via observation that due to skip connection, the middle stage of U-Net is the best alignment option. Secondly, we propose upsampling of U-Net features after passing them through MLPs. Thirdly, we observe difficulty when performing tokenwise similarity alignment, and further introduces a manifold loss that regularizes the relative similarity between samples. Experiments indicate that the resulting U-REPA could achieve excellent generation quality and greatly accelerates the convergence speed. With CFG guidance interval, U-REPA could reach $FID&lt;1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\times$ 256, and needs only half the total epochs to perform better than REPA under sd-vae-ft-ema. Codes: https://github.com/YuchuanTian/U-REPA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U-REPA：将扩散U-Net与ViT对齐</div>
<div class="mono" style="margin-top:8px">表示对齐（REPA）将扩散变换器（DiT）隐藏状态与ViT视觉编码器对齐，在DiT训练中证明了其高效性，展现出优越的收敛特性，但尚未在显示出比DiT更快收敛的经典扩散U-Net架构上进行验证。然而，将REPA适配到U-Net架构面临独特挑战：（1）不同的模块功能需要修订对齐策略；（2）U-Net的空间下采样操作导致空间维度不一致；（3）U-Net与ViT之间的空间间隙阻碍了逐标记对齐的有效性。为应对这些挑战，我们提出了\textbf{U-REPA}，一种将U-Net隐藏状态与ViT特征连接的表示对齐范式：首先，我们通过观察提出，由于跳跃连接，U-Net的中间阶段是最佳对齐选项。其次，我们建议在通过MLP处理后对U-Net特征进行上采样。第三，我们观察到在进行逐标记相似性对齐时的困难，并进一步引入了一种流形损失，以规范样本之间的相对相似性。实验表明，生成的U-REPA能够实现优异的生成质量，并大大加快收敛速度。在CFG指导间隔下，U-REPA能够在ImageNet 256 $\times$ 256上以200个周期或100万次迭代达到$FID&lt;1.5$，并且只需一半的总周期即可在sd-vae-ft-ema下表现优于REPA。代码：https://github.com/YuchuanTian/U-REPA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the training efficiency of diffusion U-Net architectures by adapting the Representation Alignment (REPA) method, which has shown effectiveness in Diffusion Transformers (DiTs). The authors propose U-REPA, which aligns U-Net hidden states with ViT features by focusing on the middle stage of U-Net for alignment, upsampling U-Net features after MLP processing, and introducing a manifold loss to address tokenwise similarity alignment challenges. Experimental results demonstrate that U-REPA significantly improves generation quality and accelerates convergence, achieving an FID score of less than 1.5 in 200 epochs on ImageNet 256 × 256, outperforming traditional REPA with only half the required epochs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过调整表示对齐（REPA）方法来提高扩散U-Net架构的训练效率，该方法在扩散变换器（DiT）训练中表现出有效性。作者提出了U-REPA，这是一种新的对齐范式，解决了不同模块功能、空间维度不一致和逐标记对齐困难等挑战。关键实验结果表明，U-REPA显著提高了生成质量并加速了收敛速度，在ImageNet 256 × 256上以200个周期达到小于1.5的FID分数，超越了传统REPA，同时只需一半的总周期。</div>
</details>
</div>
<div class="card">
<div class="title">FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images</div>
<div class="meta-line">Authors: Sanidhya Ghosal, Anurag Sharma, Sushil Ghildiyal, Mukesh Saini</div>
<div class="meta-line">First: 2026-01-07T12:51:28+00:00 · Latest: 2026-01-07T12:51:28+00:00</div>
<div class="meta-line">Comments: Accepted for oral presentation at the 10th International Conference on Computer Vision and Image Processing (CVIP 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03884v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical &quot;Full Damage&quot; F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLNet：基于卫星图像超分辨率的洪水引发农业损害评估</div>
<div class="mono" style="margin-top:8px">洪水后分配政府救助工作具有挑战性。在印度，作物受到洪水的广泛影响，因此快速准确的作物损害评估对于有效的灾后农业管理至关重要。传统的人工调查速度慢且存在偏差，而当前基于卫星的方法面临云层覆盖和低空间分辨率等挑战。因此，为了填补这一空白，本文介绍了FLNet，一种新颖的基于深度学习的架构，利用超分辨率将Sentinel-2卫星图像的10米空间分辨率提升至3米，然后进行损害分类。我们在比哈尔洪水影响农田数据集（BFCD-22）上测试了我们的模型，结果显示“完全损害”F1分数从0.83提高到0.89，几乎与商业高分辨率影像的0.89分数相匹配。这项工作提供了一种具有成本效益和可扩展的解决方案，为全国范围内从人工到自动化、高保真度的损害评估转变铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and accuracy of crop damage assessment following floods in India, where traditional methods are slow and biased. The authors developed FLNet, a deep learning architecture that utilizes super-resolution techniques to enhance the spatial resolution of Sentinel-2 satellite images from 10 m to 3 m before classifying the extent of damage. Experimental results on the Bihar Flood Impacted Croplands Dataset demonstrated a significant improvement in the &#x27;Full Damage&#x27; F1-score from 0.83 to 0.89, closely aligning with the performance of high-resolution commercial imagery, thus offering a scalable solution for automated damage assessment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高印度洪灾后作物损失评估的准确性和速度，因为传统方法既缓慢又存在偏差。作者开发了FLNet，这是一种深度学习架构，利用超分辨率技术将Sentinel-2卫星图像的空间分辨率从10米提升到3米，然后进行损失分类。在比哈尔洪灾受影响农田数据集上的实验结果显示，&#x27;完全损坏&#x27;的F1分数从0.83显著提高到0.89，接近商业高分辨率图像的表现，从而为自动化损失评估提供了一种可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in Open-Set Domain Generalization</div>
<div class="meta-line">Authors: Kunyu Peng, Di Wen, M. Saquib Sarfraz, Yufan Chen, Junwei Zheng, David Schneider, Kailun Yang, Jiamin Wu, Alina Roitberg, Rainer Stiefelhagen</div>
<div class="meta-line">First: 2024-12-24T11:00:23+00:00 · Latest: 2026-01-07T11:06:48+00:00</div>
<div class="meta-line">Comments: Accepted to International Journal of Computer Vision (IJCV). The source code of this work is released at https://github.com/KPeng9510/HyProMeta</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.18342v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.18342v2">PDF</a> · <a href="https://github.com/KPeng9510/HyProMeta">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Set Domain Generalization (OSDG) is a challenging task requiring models to accurately predict familiar categories while minimizing confidence for unknown categories to effectively reject them in unseen domains. While the OSDG field has seen considerable advancements, the impact of label noise--a common issue in real-world datasets--has been largely overlooked. Label noise can mislead model optimization, thereby exacerbating the challenges of open-set recognition in novel domains. In this study, we take the first step towards addressing Open-Set Domain Generalization under Noisy Labels (OSDG-NL) by constructing dedicated benchmarks derived from widely used OSDG datasets, including PACS and DigitsDG. We evaluate baseline approaches by integrating techniques from both label denoising and OSDG methodologies, highlighting the limitations of existing strategies in handling label noise effectively. To address these limitations, we propose HyProMeta, a novel framework that integrates hyperbolic category prototypes for label noise-aware meta-learning alongside a learnable new-category agnostic prompt designed to enhance generalization to unseen classes. Our extensive experiments demonstrate the superior performance of HyProMeta compared to state-of-the-art methods across the newly established benchmarks. The source code of this work is released at https://github.com/KPeng9510/HyProMeta.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在开放集领域泛化中使用基于提示的双曲元学习减轻标签噪声</div>
<div class="mono" style="margin-top:8px">开放集领域泛化（OSDG）是一项具有挑战性的任务，要求模型准确预测已知类别，同时在未知类别上降低置信度，以有效拒绝在未见领域中的未知类别。尽管OSDG领域取得了相当大的进展，但标签噪声这一现实世界数据集中常见的问题却被大大忽视。标签噪声可能会误导模型优化，从而加剧在新领域中开放集识别的挑战。在本研究中，我们迈出了在噪声标签下解决开放集领域泛化（OSDG-NL）的第一步，通过构建源自广泛使用的OSDG数据集（包括PACS和DigitsDG）的专用基准来实现。我们通过整合标签去噪和OSDG方法论中的技术来评估基线方法，突显现有策略在有效处理标签噪声方面的局限性。为了解决这些局限性，我们提出了HyProMeta，这是一种新颖的框架，集成了用于标签噪声感知元学习的双曲类别原型，以及旨在增强对未见类别泛化的可学习的新类别无关提示。我们的广泛实验表明，HyProMeta在新建立的基准上相比于最先进的方法表现优越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by label noise in Open-Set Domain Generalization (OSDG), which can mislead model optimization and hinder performance in novel domains. The authors constructed dedicated benchmarks from existing OSDG datasets and evaluated baseline approaches that combine label denoising and OSDG techniques, revealing their limitations in effectively managing label noise. To overcome these challenges, they proposed HyProMeta, a novel framework that employs hyperbolic category prototypes for label noise-aware meta-learning and a learnable prompt for generalization to unseen classes, demonstrating superior performance compared to state-of-the-art methods in extensive experiments across the new benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了开放集领域泛化（OSDG）在标签噪声存在下的挑战，尽管标签噪声在现实数据集中普遍存在，但这一问题在研究中被忽视。作者从现有的OSDG数据集中构建了新的基准，并评估了结合标签去噪和OSDG技术的基线方法，揭示了它们在处理标签噪声方面的局限性。为了克服这些挑战，他们提出了HyProMeta，这是一种新颖的框架，利用超曲率类别原型进行元学习，并设计了一个可学习的提示以提高对未见类别的泛化能力，在新基准上的广泛实验中表现出优于现有方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST</div>
<div class="meta-line">Authors: Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan Weng, Bin Li, Yujie Liu</div>
<div class="meta-line">First: 2025-09-24T03:44:16+00:00 · Latest: 2026-01-07T10:51:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19742v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19742v3">PDF</a> · <a href="https://github.com/carsonz/HiCoLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiCoLoRA：通过层次协作LoRA解决上下文提示不匹配以实现零-shot对话状态跟踪</div>
<div class="mono" style="margin-top:8px">零-shot对话状态跟踪（zs-DST）对于使任务导向对话系统（TODs）能够在没有昂贵数据标注的情况下推广到新领域至关重要。一个主要挑战在于动态对话上下文与静态提示之间的语义不匹配，导致跨层协调不灵活、领域干扰和灾难性遗忘。为了解决这个问题，我们提出了层次协作低秩适应（HiCoLoRA），这是一个通过强健的提示对齐增强零-shot槽推断的框架。它具有层次LoRA架构，用于动态层特定处理（结合低层启发式分组和高层全交互），集成谱联合领域-槽聚类以识别可转移关联（供给自适应线性融合机制），并采用语义增强SVD初始化（SemSVD-Init）以保留预训练知识。在多领域数据集MultiWOZ和SGD上的实验表明，HiCoLoRA优于基线，达到了zs-DST的SOTA。代码可在https://github.com/carsonz/HiCoLoRA获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve zero-shot dialog state tracking (zs-DST) for task-oriented dialog systems, addressing the challenges of semantic misalignment between dynamic dialog contexts and static prompts. The authors propose a novel framework called Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), which utilizes a hierarchical architecture for layer-specific processing, integrates domain-slot clustering for transferable associations, and employs a method to preserve pre-trained knowledge. Experimental results on multi-domain datasets, MultiWOZ and SGD, demonstrate that HiCoLoRA significantly outperforms existing baselines, achieving state-of-the-art performance in zs-DST.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善任务导向对话系统的零-shot对话状态跟踪（zs-DST），解决动态对话上下文与静态提示之间的语义不对齐所带来的挑战。作者提出了一种名为层次协作低秩适应（HiCoLoRA）的新框架，该框架利用层次结构进行处理，并结合谱联合领域-槽聚类和语义增强SVD初始化等技术，以增强提示对齐并保留预训练知识。在MultiWOZ和SGD等多领域数据集上的实验结果表明，HiCoLoRA显著优于现有基线，在zs-DST中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs</div>
<div class="meta-line">Authors: Sethupathy Parameswaran, Suresh Sundaram, Yuan Fang</div>
<div class="meta-line">Venue: WSDM 2026</div>
<div class="meta-line">First: 2026-01-07T10:50:18+00:00 · Latest: 2026-01-07T10:50:18+00:00</div>
<div class="meta-line">Comments: Accepted by WSDM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03793v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03793v1">PDF</a> · <a href="https://github.com/Sethup123/ZPT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无标签样本的提示调优用于文本属性图中的零-shot节点分类</div>
<div class="mono" style="margin-top:8px">节点分类是信息检索中的一个基本问题，具有许多现实世界的应用，如社交网络中的社区检测、在线发布文章的分组和电子商务中的产品分类。在文本属性图（TAGs）中进行零-shot节点分类面临重大挑战，特别是由于缺乏标记数据。本文提出了一种新颖的零-shot提示调优（ZPT）框架，通过利用通用双模态条件生成器（UBCG）来解决这一问题。我们的方法首先预训练一个图-语言模型，以捕捉图结构和每个节点的相关文本描述。随后，训练一个条件生成模型，以学习图和文本模态中节点的联合分布，从而仅基于类名生成每个类的合成样本。这些合成节点和文本嵌入随后用于进行连续提示调优，从而在零-shot环境中实现有效的节点分类。此外，我们在多个基准数据集上进行了广泛的实验，证明我们的框架优于现有的最先进基线。我们还提供了消融研究，以验证双模态生成器的贡献。代码可在：https://github.com/Sethup123/ZPT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenge of zero-shot node classification in text-attributed graphs, particularly in scenarios where labeled data is unavailable. The authors propose a Zero-shot Prompt Tuning (ZPT) framework that utilizes a Universal Bimodal Conditional Generator (UBCG) to pre-train a graph-language model, capturing both graph structures and textual descriptions. Experimental results on multiple benchmark datasets show that the proposed framework outperforms existing state-of-the-art methods, and ablation studies confirm the effectiveness of the bimodal generator in enhancing classification performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本属性图中零样本节点分类的挑战，特别是在缺乏标记数据的情况下。作者提出了一种零样本提示调优（ZPT）框架，利用通用双模态条件生成器（UBCG）预训练图语言模型，以捕捉图结构和节点的文本描述。多个基准数据集上的实验结果表明，所提出的框架优于现有的最先进方法，消融研究验证了双模生成器在提升分类性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning</div>
<div class="meta-line">Authors: Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Handong Cui, Chaoqun Du, Li Gong, Feng Gu, Xuefeng Hao, Wei He, Jiabang He, Yi Hu, Bin Huang, Shanshan Li, Qizhen Li, Jing Luo, Zide Liu, Xiaobo Liu, Ning Mao, Lifu Mu, Xuhao Pan, Zhiheng Qu, Chang Ren, Xudong Rao, Haoyi Sun, Qian Wang, Shuai Wang, Zhichao Wang, Wei Wang, Lian Wen, Jiqing Zhan, Hongfu Yang, Sheng Yang, Jiajun Yang, Pengfei Yu, Hongyuan Zhang, Bin Zhang, Chunpeng Zhou, Zheng Zhou, Shucheng Zhou, Shuo Xie, Yun Zhu, Hao Ma, Tao Wei, Pan Zhou, Wei Chen</div>
<div class="meta-line">First: 2025-12-29T12:16:12+00:00 · Latest: 2026-01-07T10:27:34+00:00</div>
<div class="meta-line">Comments: Technique Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23412v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23412v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MindWatcher：迈向更智能的多模态工具集成推理</div>
<div class="mono" style="margin-top:8px">传统的基于工作流的智能体在处理需要工具调用的现实世界问题时表现出有限的智能。能够自主推理和工具调用的工具集成推理（TIR）智能体迅速成为处理涉及与外部环境多步交互的复杂决策任务的强大方法。在本研究中，我们介绍了MindWatcher，一个集成交错思维和多模态思维链（CoT）推理的TIR智能体。MindWatcher能够自主决定是否以及如何调用多种工具并协调其使用，而无需依赖人类提示或工作流。交错思维范式使模型能够在任何中间阶段在思考和工具调用之间切换，而其多模态CoT能力允许在推理过程中操控图像，以产生更精确的搜索结果。我们实现了自动化的数据审计和评估管道，并辅以手动策划的高质量数据集进行训练，同时构建了一个基准，称为MindWatcher-Evaluate Bench（MWE-Bench），以评估其性能。MindWatcher配备了一整套辅助推理工具，使其能够解决广域多模态问题。一个大规模、高质量的本地图像检索数据库，涵盖包括汽车、动物和植物在内的八个类别，使模型具备强大的物体识别能力，尽管其规模较小。最后，我们为MindWatcher设计了更高效的训练基础设施，提高了训练速度和硬件利用率。实验不仅表明MindWatcher通过优越的工具调用匹配或超越了更大或更新模型的性能，还揭示了智能体训练中的关键见解，例如智能体强化学习中的遗传继承现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the intelligence of workflow-based agents in real-world problem-solving through tool-integrated reasoning (TIR). The authors introduce MindWatcher, a TIR agent that employs interleaved thinking and multimodal chain-of-thought reasoning, allowing it to autonomously decide on tool invocation and coordination without human input. Experimental results show that MindWatcher performs on par with or surpasses larger models in tool invocation efficiency, while also providing insights into agent training dynamics, including the genetic inheritance phenomenon in reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过开发能够自主推理和工具调用的工具集成推理（TIR）代理，来增强工作流代理在现实问题解决中的智能性。作者介绍了MindWatcher，这是一种采用交错思维和多模态链式思维推理的TIR代理，使其能够在没有人工干预的情况下自主决定工具的使用。实验结果表明，MindWatcher不仅在工具调用方面与更大模型的性能相匹配或超越，而且还提供了关于代理训练的宝贵见解，包括代理强化学习中的遗传继承现象。</div>
</details>
</div>
<div class="card">
<div class="title">Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation</div>
<div class="meta-line">Authors: Yushe Cao, Dianxi Shi, Xing Fu, Xuechao Zou, Haikuo Peng, Xueqi Li, Chun Yu, Junliang Xing</div>
<div class="meta-line">First: 2025-11-16T14:52:54+00:00 · Latest: 2026-01-07T10:24:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12631v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12631v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有解耦注意力的多变量扩散变换器用于高保真掩码-文本协作面部生成</div>
<div class="mono" style="margin-top:8px">尽管在使用语义掩码和文本描述进行多模态面部生成方面取得了显著进展，但传统的特征融合方法往往无法有效实现跨模态交互，从而导致生成结果不理想。为了解决这一挑战，我们引入了MDiTFace——一个定制的扩散变换器框架，采用统一的标记化策略处理语义掩码和文本输入，消除异构模态表示之间的差异。该框架通过堆叠的新设计的多变量变换器块实现全面的多模态特征交互，所有条件同步处理。此外，我们设计了一种新颖的解耦注意力机制，通过解耦掩码标记和时间嵌入之间的隐式依赖关系，将内部计算分为动态和静态路径，使得在初始计算后可以缓存和重用静态路径中计算的特征，从而在保持性能的同时将掩码条件引入的额外计算开销减少了94%以上。大量实验表明，MDiTFace在面部保真度和条件一致性方面显著优于其他竞争方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve multimodal facial generation, which has been hindered by ineffective cross-modal interactions in conventional feature fusion methods. The authors propose the MDiTFace, a diffusion transformer framework that utilizes a unified tokenization strategy to harmonize semantic mask and text inputs, allowing for better multimodal feature interaction through newly designed multivariate transformer blocks. Experimental results indicate that MDiTFace achieves over 94% reduction in computational overhead while significantly enhancing facial fidelity and conditional consistency compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善多模态面部生成，传统的特征融合方法常常因跨模态交互不有效而导致生成效果不佳。作者提出了MDiTFace，这是一种定制的扩散变换器框架，利用统一的标记化策略处理语义掩码和文本输入，从而实现不同模态之间更好的交互。关键实验结果表明，MDiTFace在保持性能的同时，计算开销减少超过94%，并且在面部逼真度和条件一致性方面显著优于其他竞争方法。</div>
</details>
</div>
<div class="card">
<div class="title">Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</div>
<div class="meta-line">Authors: Zabir Al Nazi, GM Shahariar, Md. Abrar Hossain, Wei Peng</div>
<div class="meta-line">First: 2025-12-19T09:47:38+00:00 · Latest: 2026-01-07T09:58:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17394v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17394v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Theory of Mind (ToM) - the ability to attribute beliefs and intents to others - is fundamental for social intelligence, yet Vision-Language Model (VLM) evaluations remain largely Western-centric. In this work, we introduce CulturalToM-VQA, a benchmark of 5,095 visually situated ToM probes across diverse cultural contexts, rituals, and social norms. Constructed through a frontier proprietary MLLM, human-verified pipeline, the dataset spans a taxonomy of six ToM tasks and four complexity levels. We benchmark 10 VLMs (2023-2025) and observe a significant performance leap: while earlier models struggle, frontier models achieve high accuracy (&gt;93%). However, significant limitations persist: models struggle with false belief reasoning (19-83% accuracy) and show high regional variance (20-30% gaps). Crucially, we find that SOTA models exhibit social desirability bias - systematically favoring semantically positive answer choices over negative ones. Ablation experiments reveal that some frontier models rely heavily on parametric social priors, frequently defaulting to safety-aligned predictions. Furthermore, while Chain-of-Thought prompting aids older models, it yields minimal gains for newer ones. Overall, our work provides a testbed for cross-cultural social reasoning, underscoring that despite architectural gains, achieving robust, visually grounded understanding remains an open challenge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型是跨文化心智理论推理者吗？</div>
<div class="mono" style="margin-top:8px">心智理论（ToM）——将信念和意图归因于他人的能力——是社会智能的基础，但视觉语言模型（VLM）的评估仍然主要集中于西方。在这项工作中，我们引入了CulturalToM-VQA，这是一个涵盖多种文化背景、仪式和社会规范的5,095个视觉情境ToM探测的基准。该数据集通过前沿专有的MLLM和人工验证的流程构建，涵盖六种ToM任务和四个复杂性级别。我们对10个VLM（2023-2025）进行了基准测试，观察到显著的性能跃升：尽管早期模型表现不佳，前沿模型的准确率高达93%以上。然而，仍然存在显著的局限性：模型在错误信念推理方面表现不佳（准确率为19-83%），并且区域差异较大（20-30%的差距）。关键是，我们发现SOTA模型表现出社会期望偏见——系统性地偏向语义上积极的答案选择而非消极选择。消融实验表明，一些前沿模型在很大程度上依赖于参数化的社会先验，频繁默认安全对齐的预测。此外，虽然链式思维提示有助于旧模型，但对新模型的增益有限。总体而言，我们的工作为跨文化社会推理提供了一个测试平台，强调尽管架构上有所提升，实现稳健的、视觉基础的理解仍然是一个开放的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in Theory of Mind (ToM) evaluations of Vision-Language Models (VLMs), which have been predominantly Western-centric, by introducing CulturalToM-VQA, a benchmark consisting of 5,095 ToM probes across various cultural contexts. The study benchmarks 10 VLMs from 2023 to 2025, revealing that while newer models achieve high accuracy (over 93%), they still struggle with false belief reasoning and exhibit significant regional performance variance. Additionally, the findings indicate that state-of-the-art models display social desirability bias and rely on parametric social priors, with minimal improvements from Chain-of-Thought prompting for newer models, highlighting ongoing challenges in achieving robust cross-cultural social reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决理论心智（ToM）在视觉语言模型（VLM）评估中以西方为中心的局限性，动机是需要更全面地理解跨文化的社会智能。作者引入了CulturalToM-VQA，这是一个包含5,095个涵盖各种文化背景和社会规范的ToM探针的基准，并评估了2023年至2025年的10个VLM。研究结果表明，尽管较新的模型的准确率超过93%，但它们在错误信念推理方面仍然存在困难，并表现出显著的区域性能差异，同时倾向于选择社会上更受欢迎的回答，这表明尽管有进展，VLM在跨文化理解方面仍面临挑战。</div>
</details>
</div>
<div class="card">
<div class="title">I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing</div>
<div class="meta-line">Authors: Jinghan Yu, Junhao Xiao, Chenyu Zhu, Jiaming Li, Jia Li, HanMing Deng, Xirui Wang, Guoli Jia, Jianjun Li, Zhiyuan Ma, Xiang Bai, Bowen Zhou</div>
<div class="meta-line">First: 2026-01-07T09:29:57+00:00 · Latest: 2026-01-07T09:29:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03741v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03741v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel &quot;Decompose-then-Action&quot; paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I2E：从图像像素到可操作的交互环境用于文本引导的图像编辑</div>
<div class="mono" style="margin-top:8px">现有的文本引导图像编辑方法主要依赖于端到端的像素级修复范式。尽管在简单场景中取得了成功，但该范式在需要精确局部控制和复杂多对象空间推理的组合编辑任务中仍然面临重大挑战。该范式受到以下限制：1）规划与执行的隐式耦合，2）缺乏对象级控制粒度，3）依赖于非结构化的像素中心建模。为了解决这些限制，我们提出了I2E，一种新颖的“分解-再行动”范式，将图像编辑重新视为在结构化环境中的可操作交互过程。I2E利用分解器将非结构化图像转化为离散的、可操作的对象层，然后引入一个具有物理意识的视觉-语言-行动代理，通过思维链推理将复杂指令解析为一系列原子动作。此外，我们还构建了I2E-Bench，这是一个旨在多实例空间推理和高精度编辑的基准。I2E-Bench和多个公共基准上的实验结果表明，I2E在处理复杂组合指令、保持物理合理性和确保多轮编辑稳定性方面显著优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve text-guided image editing methods, which struggle with complex compositional tasks due to limitations in pixel-level inpainting approaches. The authors propose a novel &#x27;Decompose-then-Action&#x27; paradigm called I2E, which transforms images into discrete, manipulable object layers and employs a physics-aware Vision-Language-Action Agent to execute complex instructions through Chain-of-Thought reasoning. Experimental results on the newly constructed I2E-Bench and other public benchmarks show that I2E significantly outperforms existing methods in managing intricate compositional instructions, maintaining physical plausibility, and ensuring stability in multi-turn editing tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善文本引导的图像编辑方法，这些方法在处理复杂的组合任务时由于像素级修补方法的局限性而面临挑战。作者提出了一种新颖的“分解-再行动”范式I2E，该范式将非结构化图像转化为离散的可操作对象层，并利用物理感知的视觉-语言-行动代理将复杂指令解析为可管理的动作。实验结果表明，在新构建的I2E-Bench和其他公共基准上，I2E在执行复杂组合编辑时显著优于现有方法，同时保持物理合理性和多轮编辑的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">HyperCOD: The First Challenging Benchmark and Baseline for Hyperspectral Camouflaged Object Detection</div>
<div class="meta-line">Authors: Shuyan Bai, Tingfa Xu, Peifu Liu, Yuhao Qiu, Huiyan Bai, Huan Chen, Yanyan Peng, Jianan Li</div>
<div class="meta-line">First: 2026-01-07T09:26:32+00:00 · Latest: 2026-01-07T09:26:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03736v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03736v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">RGB-based camouflaged object detection struggles in real-world scenarios where color and texture cues are ambiguous. While hyperspectral image offers a powerful alternative by capturing fine-grained spectral signatures, progress in hyperspectral camouflaged object detection (HCOD) has been critically hampered by the absence of a dedicated, large-scale benchmark. To spur innovation, we introduce HyperCOD, the first challenging benchmark for HCOD. Comprising 350 high-resolution hyperspectral images, It features complex real-world scenarios with minimal objects, intricate shapes, severe occlusions, and dynamic lighting to challenge current models. The advent of foundation models like the Segment Anything Model (SAM) presents a compelling opportunity. To adapt the Segment Anything Model (SAM) for HCOD, we propose HyperSpectral Camouflage-aware SAM (HSC-SAM). HSC-SAM ingeniously reformulates the hyperspectral image by decoupling it into a spatial map fed to SAM&#x27;s image encoder and a spectral saliency map that serves as an adaptive prompt. This translation effectively bridges the modality gap. Extensive experiments show that HSC-SAM sets a new state-of-the-art on HyperCOD and generalizes robustly to other public HSI datasets. The HyperCOD dataset and our HSC-SAM baseline provide a robust foundation to foster future research in this emerging area.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperCOD：首个挑战性基准和高光谱伪装物体检测的基线</div>
<div class="mono" style="margin-top:8px">基于RGB的伪装物体检测在颜色和纹理线索模糊的现实场景中面临困难。高光谱图像通过捕捉细粒度光谱特征提供了强有力的替代方案，但高光谱伪装物体检测（HCOD）的进展因缺乏专门的大规模基准而受到严重阻碍。为促进创新，我们推出了HyperCOD，这是首个针对HCOD的挑战性基准。该基准包含350幅高分辨率高光谱图像，展示了复杂的现实场景，具有最小物体、复杂形状、严重遮挡和动态光照，以挑战当前模型。基础模型如Segment Anything Model（SAM）的出现提供了一个引人注目的机会。为了将Segment Anything Model（SAM）适应于HCOD，我们提出了高光谱伪装感知SAM（HSC-SAM）。HSC-SAM巧妙地通过将高光谱图像解耦为输入SAM图像编码器的空间图和作为自适应提示的光谱显著性图，重新构造高光谱图像。这一转换有效地弥合了模态差距。大量实验表明，HSC-SAM在HyperCOD上设定了新的最先进水平，并在其他公共HSI数据集上具有良好的泛化能力。HyperCOD数据集和我们的HSC-SAM基线为未来在这一新兴领域的研究提供了坚实的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by RGB-based camouflaged object detection in real-world scenarios where color and texture cues are often ambiguous. To facilitate advancements in hyperspectral camouflaged object detection (HCOD), the authors introduce HyperCOD, a large-scale benchmark consisting of 350 high-resolution hyperspectral images that depict complex scenarios with minimal objects and severe occlusions. They propose a novel method called HyperSpectral Camouflage-aware SAM (HSC-SAM), which reformulates hyperspectral images for use with the Segment Anything Model (SAM) by creating a spatial map and a spectral saliency map. Experimental results demonstrate that HSC-SAM achieves a new state-of-the-art performance on the HyperCOD benchmark and shows strong generalization capabilities across other public hyperspectral image datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决RGB基础的伪装物体检测在真实场景中面临的挑战，因为颜色和纹理线索往往模糊不清。为了促进高光谱伪装物体检测（HCOD）的进展，作者提出了HyperCOD，这是一个包含350张高分辨率高光谱图像的大规模基准数据集，展示了复杂场景、最小物体和严重遮挡的情况。他们提出了一种新方法，称为高光谱伪装感知SAM（HSC-SAM），通过将高光谱图像分离为空间图和光谱显著性图来增强Segment Anything Model（SAM）的性能。实验结果表明，HSC-SAM在HyperCOD基准上达到了最新的性能，并在其他公共高光谱图像数据集上表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion</div>
<div class="meta-line">Authors: Qingyao Tian, Bingyu Yang, Huai Liao, Xinyan Huang, Junyong Li, Dong Yi, Hongbin Liu</div>
<div class="meta-line">First: 2026-01-07T09:00:52+00:00 · Latest: 2026-01-07T09:00:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM&#x27;s ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BREATH-VL：基于视觉-语言引导的6自由度支气管镜定位通过语义-几何融合</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）最近在导航和定位任务中表现出色，利用大规模预训练进行语义理解。然而，将VLMs应用于6自由度内窥镜相机定位面临几个挑战：1）在真实医疗环境中缺乏大规模、高质量、密集标注和定位导向的视觉-语言数据集；2）细粒度姿态回归能力有限；3）从过去帧提取时间特征时计算延迟高。为了解决这些问题，我们首先构建了BREATH数据集，这是迄今为止最大的体内内窥镜定位数据集，收集于复杂的人类气道。基于该数据集，我们提出了BREATH-VL，一个混合框架，将VLMs的语义线索与基于视觉的配准方法的几何信息相结合，以实现准确的6自由度姿态估计。我们的动机在于两种方法的互补优势：VLMs提供可推广的语义理解，而配准方法提供精确的几何对齐。为了进一步增强VLM捕捉时间上下文的能力，我们引入了一种轻量级的上下文学习机制，将运动历史编码为语言提示，使得在不需要昂贵视频级计算的情况下实现高效的时间推理。大量实验表明，视觉-语言模块在具有挑战性的外科场景中提供了稳健的语义定位。在此基础上，我们的BREATH-VL在准确性和泛化能力上超越了最先进的仅视觉定位方法，与表现最佳的基线相比，减少了25.5%的平移误差，同时实现了竞争性的计算延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve 6-DoF endoscopic camera localization in medical settings, which is hindered by the lack of suitable datasets and the limitations of existing methods. To tackle these challenges, the authors constructed the BREATH dataset, the largest in-vivo endoscopic localization dataset, and developed BREATH-VL, a hybrid framework that combines semantic information from vision-language models with geometric data from vision-based registration techniques. Experimental results indicate that BREATH-VL significantly enhances localization accuracy and generalization, achieving a 25.5% reduction in translational error compared to the best-performing baseline while maintaining competitive computational latency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善复杂医疗环境中6自由度内窥镜相机定位，解决数据集不足和现有方法局限等挑战。作者构建了BREATH数据集，这是迄今为止最大的体内内窥镜定位数据集，并提出了BREATH-VL，这是一种将视觉-语言模型与几何配准技术相结合的混合框架，以增强姿态估计。实验结果表明，BREATH-VL显著优于最先进的仅基于视觉的定位方法，减少了25.5%的平移误差，同时保持了竞争性的计算延迟。</div>
</details>
</div>
<div class="card">
<div class="title">When in Doubt, Consult: Expert Debate for Sexism Detection via Confidence-Based Routing</div>
<div class="meta-line">Authors: Anwar Alajmi, Gabriele Pergola</div>
<div class="meta-line">First: 2025-12-21T05:48:57+00:00 · Latest: 2026-01-07T08:16:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23732v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.23732v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online sexism increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to better regularize supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. First, we stabilize the training combining class-balanced focal loss, class-aware batching, and post-hoc threshold calibration, strategies for the firs time adapted for this domain to mitigate label imbalance and noisy supervision. Second, we bridge the gap between efficiency and reasoning with a a dynamic routing mechanism that distinguishes between unambiguous instances and complex cases requiring a deliberative process. This reasoning process results in the novel Collaborative Expert Judgment (CEJ) module which prompts multiple personas and consolidates their reasoning through a judge model. Our approach outperforms existing approaches across several public benchmarks, with F1 gains of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% improvement in ICM on EXIST 2025 Task 1.1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有疑问时，请咨询：基于信心路由的性别歧视检测专家辩论</div>
<div class="mono" style="margin-top:8px">在线性别歧视越来越以微妙、依赖上下文的形式出现，逃避传统检测方法。其解释往往依赖于重叠的语言、心理、法律和文化维度，这在标注数据集中产生混合且有时矛盾的信号。这些不一致，加上标签稀缺和类别不平衡，导致决策边界不稳定，并使微调模型忽视更微妙、代表性不足的伤害形式。为了解决这些挑战，我们提出了一个两阶段框架，统一了（i）针对性训练程序，以更好地规范对稀缺和嘈杂数据的监督，以及（ii）选择性、基于推理的推断，以处理模糊或边界案例。首先，我们通过结合类别平衡的焦点损失、类别感知批处理和事后阈值校准来稳定训练，这些策略首次适应于该领域，以减轻标签不平衡和嘈杂监督。其次，我们通过动态路由机制弥合效率与推理之间的差距，区分明确实例和需要深思熟虑过程的复杂案例。这个推理过程产生了新颖的协作专家判断（CEJ）模块，该模块提示多个角色并通过评判模型整合他们的推理。我们的方法在多个公共基准测试中优于现有方法，在EDOS任务A和B上分别获得了+4.48%和+1.30%的F1提升，在EXIST 2025任务1.1上获得了+2.79%的ICM改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting subtle and context-dependent forms of online sexism that traditional methods struggle to identify due to inconsistencies in annotated datasets and class imbalances. The authors propose a two-stage framework that combines targeted training procedures with selective, reasoning-based inference to improve detection accuracy. Experimental results demonstrate that their approach, which includes a Collaborative Expert Judgment module for handling ambiguous cases, outperforms existing methods, achieving F1 score improvements of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% increase in ICM on EXIST 2025 Task 1.1.</div>
<div class="mono" style="margin-top:8px">该研究解决了传统方法难以识别的在线性别歧视的微妙和依赖于上下文的形式所带来的挑战，这些形式在标注数据集中存在不一致性和类别不平衡。作者提出了一种两阶段框架，包括针对稀缺数据的稳定监督的有针对性的训练程序和管理模糊案例的选择性推理方法。实验结果表明，他们的方法通过引入协作专家判断模块显著优于现有方法，在EDOS任务A和B上分别实现了F1分数提高4.48%和1.30%，在EXIST 2025任务1.1上提高了2.79%的ICM。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis</div>
<div class="meta-line">Authors: Jingguo Qu, Xinyang Han, Jia Ai, Juan Wu, Tong Zhao, Tonghuan Xiao, Sheng Ning, Yuqi Yang, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying</div>
<div class="meta-line">First: 2025-06-10T14:37:51+00:00 · Latest: 2026-01-07T07:58:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08849v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.08849v3">PDF</a> · <a href="https://github.com/jinggqu/NextGen-UIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities, yet their application to medical ultrasound remains constrained by the significant domain shift between natural images and sonographic data. The unique physics of ultrasound, manifesting as speckle noise, shadowing, and variable artifacts, often leads to suboptimal performance when applying off-the-shelf foundation models. To address this, we propose a novel Hybrid-tuning (HT) strategy for the efficient adaptation of CLIP-based models to ultrasound analysis. Our method introduces a lightweight adapter module integrated into the frozen visual backbone, featuring frequency-domain filtering to suppress periodic artifacts and dynamic noise estimation to calibrate feature representations. Furthermore, we design specialized segmentation and classification heads that employ multi-scale feature aggregation to maximize the utility of pre-trained semantic priors. Extensive evaluations across six multi-center datasets (covering lymph nodes, breast, thyroid, and prostate) reveal that our HT-enhanced models significantly outperform existing state-of-the-art methods, including BiomedCLIP and standard LoRA fine-tuning. The results highlight the superior data efficiency and robustness of our approach, paving the way for practical, foundational intelligence in automated ultrasound diagnosis. The source code is available at https://github.com/jinggqu/NextGen-UIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应视觉-语言基础模型以进行下一代医学超声图像分析</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）展示了显著的泛化能力，但其在医学超声中的应用仍受到自然图像与超声数据之间显著领域转移的限制。超声的独特物理特性表现为斑点噪声、阴影和可变伪影，常常导致现成基础模型应用时的性能不佳。为此，我们提出了一种新颖的混合调优（HT）策略，以高效适应基于CLIP的模型进行超声分析。我们的方法引入了一个轻量级适配器模块，集成在冻结的视觉主干中，采用频域滤波来抑制周期性伪影，并进行动态噪声估计以校准特征表示。此外，我们设计了专门的分割和分类头，采用多尺度特征聚合以最大化预训练语义先验的效用。在六个多中心数据集（涵盖淋巴结、乳腺、甲状腺和前列腺）上的广泛评估表明，我们的HT增强模型显著优于现有的最先进方法，包括BiomedCLIP和标准LoRA微调。结果突显了我们方法的优越数据效率和鲁棒性，为自动超声诊断中的实用基础智能铺平了道路。源代码可在https://github.com/jinggqu/NextGen-UIA获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenges posed by the domain shift between natural images and medical ultrasound data, which affects the performance of Vision-Language Models (VLMs) in ultrasound analysis. To tackle this issue, the authors propose a Hybrid-tuning (HT) strategy that integrates a lightweight adapter module into the frozen visual backbone of CLIP-based models, incorporating frequency-domain filtering and dynamic noise estimation. Experimental results across six multi-center datasets demonstrate that the HT-enhanced models significantly outperform existing methods, showcasing improved data efficiency and robustness for automated ultrasound diagnosis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视觉-语言模型（VLMs）在医学超声分析中的应用，因自然图像与声学数据之间存在显著差异而面临挑战。作者提出了一种混合调优（HT）策略，将轻量级适配器模块集成到CLIP基础模型的冻结视觉骨干中，利用频域滤波和动态噪声估计来改善特征表示。跨六个多中心数据集的实验结果表明，HT增强模型显著优于现有方法，展示了在自动超声诊断中的数据效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis</div>
<div class="meta-line">Authors: Haitao Li, Chunxiang Jin, Chenglin Li, Wenhao Guan, Zhengxing Huang, Xie Chen</div>
<div class="meta-line">First: 2026-01-07T06:23:23+00:00 · Latest: 2026-01-07T06:23:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03632v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot text-to-speech models can clone a speaker&#x27;s timbre from a short reference audio, but they also strongly inherit the speaking style present in the reference. As a result, synthesizing speech with a desired style often requires carefully selecting reference audio, which is impractical when only limited or mismatched references are available. While recent controllable TTS methods attempt to address this issue, they typically rely on absolute style targets and discrete textual prompts, and therefore do not support continuous and reference-relative style control. We propose ReStyle-TTS, a framework that enables continuous and reference-relative style control in zero-shot TTS. Our key insight is that effective style control requires first reducing the model&#x27;s implicit dependence on reference style before introducing explicit control mechanisms. To this end, we introduce Decoupled Classifier-Free Guidance (DCFG), which independently controls text and reference guidance, reducing reliance on reference style while preserving text fidelity. On top of this, we apply style-specific LoRAs together with Orthogonal LoRA Fusion to enable continuous and disentangled multi-attribute control, and introduce a Timbre Consistency Optimization module to mitigate timbre drift caused by weakened reference guidance. Experiments show that ReStyle-TTS enables user-friendly, continuous, and relative control over pitch, energy, and multiple emotions while maintaining intelligibility and speaker timbre, and performs robustly in challenging mismatched reference-target style scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReStyle-TTS：零样本语音合成的相对和连续风格控制</div>
<div class="mono" style="margin-top:8px">零样本文本到语音模型可以从短参考音频中克隆说话者的音色，但它们也强烈继承参考中的说话风格。因此，合成具有期望风格的语音通常需要仔细选择参考音频，而当仅有有限或不匹配的参考时，这种方法是不切实际的。虽然最近的可控TTS方法试图解决这个问题，但它们通常依赖于绝对风格目标和离散文本提示，因此不支持连续和相对参考的风格控制。我们提出了ReStyle-TTS，一个在零样本TTS中实现连续和相对参考风格控制的框架。我们的关键见解是，有效的风格控制需要首先减少模型对参考风格的隐性依赖，然后再引入显式控制机制。为此，我们引入了解耦分类器无关引导（DCFG），它独立控制文本和参考引导，减少对参考风格的依赖，同时保持文本的保真度。在此基础上，我们应用风格特定的LoRA以及正交LoRA融合，以实现连续和解耦的多属性控制，并引入音色一致性优化模块，以减轻因参考引导减弱而导致的音色漂移。实验表明，ReStyle-TTS使用户能够友好地、连续地和相对地控制音高、能量和多种情感，同时保持可懂性和说话者音色，并在具有挑战性的参考-目标风格不匹配场景中表现稳健。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve zero-shot text-to-speech (TTS) synthesis by allowing for continuous and reference-relative style control, addressing the limitations of existing methods that rely on absolute style targets and discrete prompts. The authors propose a framework called ReStyle-TTS, which incorporates Decoupled Classifier-Free Guidance (DCFG) to reduce the model&#x27;s dependence on reference style while maintaining text fidelity, along with style-specific LoRAs and Orthogonal LoRA Fusion for multi-attribute control. Experimental results demonstrate that ReStyle-TTS successfully enables continuous control over pitch, energy, and emotions, while preserving intelligibility and speaker timbre, even in scenarios with mismatched reference and target styles.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过实现更灵活的风格控制来改善零-shot文本到语音（TTS）合成，因为现有方法通常需要仔细选择的参考音频，而这些音频可能并不总是可用。作者提出了ReStyle-TTS框架，该框架结合了去耦分类器无指导（DCFG），以减少模型对参考风格的依赖，同时保持文本的保真度，并使用特定风格的LoRA和正交LoRA融合实现连续的多属性控制。实验结果表明，ReStyle-TTS允许用户友好地控制音调、能量和情感，同时在参考和目标风格不匹配的情况下仍能保持可懂性和说话者音色。</div>
</details>
</div>
<div class="card">
<div class="title">CNN-based Surface Temperature Forecasts with Ensemble Numerical Weather Prediction over Medium-range Forecast Periods</div>
<div class="meta-line">Authors: Takuya Inoue, Takuya Kawabata</div>
<div class="meta-line">First: 2025-07-25T04:19:05+00:00 · Latest: 2026-01-07T04:27:51+00:00</div>
<div class="meta-line">Comments: 41 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.18937v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.18937v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, a method that integrates convolutional neural networks (CNNs) with ensemble numerical weather prediction (NWP) models is proposed. This method enables surface temperature forecasting with lead times beyond the short-range, extending up to five days. Due to limited computational resources, operational medium-range temperature forecasts typically rely on low-resolution NWP models, which are prone to systematic and random errors. To resolve these limitations, the proposed method applies CNN-based post-processing (bias correction and spatial super-resolution) to an ensemble NWP system. First, the post-processing is applied to each ensemble member to reduce systematic errors and reconstruct high-resolution temperature fields from low-resolution model outputs. This approach reduces the systematic and random errors in NWP model outputs and outperforms operational post-processing. Second, the CNN is applied to all ensemble members to construct a new ensemble forecasting system, in which deterministic forecast accuracy, probabilistic reliability, and representation of ensemble spread are improved compared with those of the original system. We demonstrate that this CNN-based post-processing is fundamentally different from the artificial error reduction caused by smoothing inherent in ensemble averaging because the post-processing reduces forecast errors without degrading the forecast information. These results indicate that the proposed method provides a practical and scalable solution for improving medium-range temperature forecasts and is particularly valuable for use in operational centers with limited computational resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CNN的中期气温预测与集合数值天气预报结合</div>
<div class="mono" style="margin-top:8px">本研究提出了一种将卷积神经网络（CNN）与集合数值天气预报（NWP）模型相结合的方法。该方法能够进行超过短期的气温预测，预测期可延长至五天。由于计算资源有限，现有的中期气温预测通常依赖于低分辨率的NWP模型，这些模型容易出现系统性和随机性误差。为了解决这些局限性，所提方法对集合NWP系统应用基于CNN的后处理（偏差修正和空间超分辨率）。首先，后处理应用于每个集合成员，以减少系统性误差并从低分辨率模型输出重建高分辨率气温场。这种方法减少了NWP模型输出中的系统性和随机性误差，优于现有的后处理。其次，CNN应用于所有集合成员，构建新的集合预测系统，与原系统相比，确定性预测精度、概率可靠性和集合扩展的表现得到了改善。我们证明，这种基于CNN的后处理与集合平均中固有的平滑导致的人工误差减少根本不同，因为后处理在不降低预测信息的情况下减少了预测误差。这些结果表明，所提方法为改善中期气温预测提供了一个实用且可扩展的解决方案，特别适用于计算资源有限的操作中心。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of medium-range surface temperature forecasting, which often relies on low-resolution numerical weather prediction (NWP) models that exhibit systematic and random errors. The authors propose a method that integrates convolutional neural networks (CNNs) with ensemble NWP models to enhance forecast accuracy over lead times of up to five days. The experimental results show that applying CNN-based post-processing to each ensemble member significantly reduces errors and reconstructs high-resolution temperature fields, leading to improved deterministic accuracy, probabilistic reliability, and ensemble spread representation compared to the original system, thereby offering a scalable solution for operational centers with limited computational resources.</div>
<div class="mono" style="margin-top:8px">本研究解决了中期地表温度预报的挑战，这些预报通常由于依赖低分辨率的数值天气预报（NWP）模型而受到系统性和随机性错误的影响。作者提出了一种将卷积神经网络（CNN）与集合NWP模型相结合的方法，应用CNN基础的后处理技术来提高预报准确性。实验结果表明，该方法有效减少了NWP输出中的错误，并改善了确定性预报准确性、概率可靠性和集合扩展表示，展示了其作为计算资源有限的操作中心的实用解决方案的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Sortblock: Similarity-Aware Feature Reuse for Diffusion Model</div>
<div class="meta-line">Authors: Hanqi Chen, Xu Zhang, Xiaoliu Guan, Lielin Jiang, Guanzhong Wang, Zeyu Chen, Yi Liu</div>
<div class="meta-line">First: 2025-08-01T08:10:54+00:00 · Latest: 2026-01-07T03:25:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00412v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00412v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sortblock：基于相似性的特征重用用于扩散模型</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）展示了显著的生成能力，特别得益于增强视觉和艺术保真度的变换器架构。然而，它们固有的顺序去噪过程导致高推理延迟，限制了在实时场景中的部署。现有的无训练加速方法通常在固定时间步或层次上重用中间特征，忽视了去噪阶段和变换器块之间不断变化的语义焦点。为了解决这个问题，我们提出了Sortblock，一个无训练的推理加速框架，基于相邻时间步之间的相似性动态缓存块级特征。通过对残差演变进行排序，Sortblock自适应地确定重新计算比例，选择性地跳过冗余计算，同时保持生成质量。此外，我们还结合了一种轻量级线性预测机制，以减少跳过块中的累积误差。在各种任务和DiT架构上的广泛实验表明，Sortblock实现了超过2倍的推理加速，输出质量降级最小，为加速基于扩散的生成模型提供了有效且可推广的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the high inference latency of Diffusion Transformers (DiTs), which limits their use in real-time applications despite their strong generative capabilities. The authors propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps, allowing for selective skipping of redundant computations. Experimental results show that Sortblock achieves over 2× speedup in inference time with minimal degradation in output quality across various tasks and DiT architectures.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于减少扩散变换器（DiTs）高推理延迟的问题，这限制了它们在实时应用中的使用，尽管它们具有强大的生成能力。作者提出了Sortblock，这是一种训练无关的推理加速框架，基于相邻时间步的相似性动态缓存块级特征，从而选择性跳过冗余计算，同时保持生成质量。实验结果表明，Sortblock在各种任务和DiT架构中实现了超过2倍的推理速度提升，且输出质量几乎没有下降。</div>
</details>
</div>
<div class="card">
<div class="title">Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution</div>
<div class="meta-line">Authors: Zhicheng Zhao, Fengjiao Peng, Jinquan Yan, Wei Lu, Chenglong Li, Jin Tang</div>
<div class="meta-line">First: 2026-01-07T02:30:27+00:00 · Latest: 2026-01-07T02:30:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理约束的跨分辨率增强网络用于光学引导的热无人机图像超分辨率</div>
<div class="mono" style="margin-top:8px">光学引导的热无人机图像超分辨率因其在全天候监测应用中的潜力而受到广泛研究关注。然而，现有方法通常压缩光学特征以匹配热特征维度进行跨模态对齐和融合，这不仅导致有益于热超分辨率的高频信息丢失，还因忽视模态间成像物理差异而引入物理不一致的伪影，如纹理失真和边缘模糊。为了解决这些挑战，我们提出了PCNet，以实现光学和热模态之间的跨分辨率互增强，同时通过热传导物理约束光学引导过程，以实现稳健的热无人机图像超分辨率。特别地，我们设计了一个跨分辨率互增强模块（CRME），以联合优化热图像超分辨率和光学到热模态转换，促进跨分辨率的有效双向特征交互，同时保留高频光学先验。此外，我们提出了一个物理驱动的热传导模块（PDTM），将二维热传导纳入光学引导，建模空间变化的热传导特性，以防止不一致的伪影。此外，我们引入了温度一致性损失，强制区域分布一致性和边界梯度平滑性，以确保生成的热图像与现实世界的热辐射原理一致。在VGTSR2.0和DroneVehicle数据集上的大量实验表明，PCNet在重建质量和下游任务（包括语义分割和目标检测）上显著优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve optics-guided thermal UAV image super-resolution, which is crucial for all-weather monitoring applications, while addressing the limitations of existing methods that often lead to loss of high-frequency information and introduce artifacts. The authors propose a novel approach called PCNet, which employs a Cross-Resolution Mutual Enhancement Module (CRME) to optimize both thermal image super-resolution and optical-to-thermal modality conversion, and a Physics-Driven Thermal Conduction Module (PDTM) to incorporate heat conduction properties into the optical guidance process. Experimental results on the VGTSR2.0 and DroneVehicle datasets show that PCNet significantly enhances reconstruction quality and performs better in downstream tasks such as semantic segmentation and object detection compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善光学引导的热无人机图像超分辨率，这对全天候监测应用至关重要，同时解决现有方法在高频信息损失和引入伪影方面的局限性。作者提出了一种新颖的框架PCNet，该框架采用交叉分辨率互增强模块（CRME）来优化热图像超分辨率和光学到热转换，并结合物理驱动的热传导模块（PDTM），将热传导原理纳入其中以减轻伪影。VGTSR2.0和DroneVehicle数据集上的实验结果表明，PCNet在重建质量上显著提升，并在语义分割和目标检测等下游任务中优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Pattern Alignment Merging for Adaptive Reasoning</div>
<div class="meta-line">Authors: Zhaofeng Zhong, Wei Yuan, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin</div>
<div class="meta-line">First: 2026-01-07T01:36:39+00:00 · Latest: 2026-01-07T01:36:39+00:00</div>
<div class="meta-line">Comments: 16 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03506v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03506v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model&#x27;s intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应推理的推理模式对齐合并</div>
<div class="mono" style="margin-top:8px">最近的大型推理模型（LRMs）在复杂推理任务中取得了显著进展，但它们通常为每个查询生成冗长的推理路径，导致不必要的计算和延迟。现有的加速方法通常依赖于重新训练模型或设计复杂的提示，这要么成本过高，要么对输入和提示的构造高度敏感。在本研究中，我们将模型合并作为高效推理的轻量级替代方案：通过将长链推理（Long-CoT）模型与短链指令模型（Short-CoT）结合，我们获得了一个无需从头训练或大规模额外数据的自适应推理器。在此基础上，我们提出了推理模式对齐合并（RPAM），这是一个基于特征对齐的分层模型合并框架，以促进查询自适应推理。RPAM首先构建一个小型模式标记校准集，为每个查询分配适当的推理模式。然后，通过将合并模型的中间表示与所选模型的中间表示对齐，优化分层合并系数，同时对比目标明确地将它们推离未选择的模型。在七个广泛使用的推理基准上的实验表明，RPAM显著降低了推理成本，同时保持了强大的性能。文章接受后，我们将提供开源代码以重现RPAM的实验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies of large reasoning models (LRMs) that generate lengthy reasoning paths, leading to increased computation and latency. The authors propose a method called Reasoning Pattern Alignment Merging (RPAM), which merges a long chain-of-thought reasoning model with a short instruction model to create an adaptive reasoner without the need for extensive retraining or additional data. Experimental results demonstrate that RPAM significantly reduces inference costs while maintaining strong performance across seven widely used reasoning benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型推理模型（LRMs）生成冗长推理路径所带来的低效问题，这导致了不必要的计算和延迟。作者提出了一种名为推理模式对齐合并（RPAM）的方法，该方法将长链推理模型与短指令模型合并，创建了一种自适应推理器，无需大量重新训练或额外数据。实验结果表明，RPAM在七个广泛使用的推理基准上显著降低了推理成本，同时保持了强大的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GeoDiff-SAR: A Geometric Prior Guided Diffusion Model for SAR Image Generation</div>
<div class="meta-line">Authors: Fan Zhang, Xuanting Wu, Fei Ma, Qiang Yin, Yuxin Hu</div>
<div class="meta-line">First: 2026-01-07T01:27:20+00:00 · Latest: 2026-01-07T01:27:20+00:00</div>
<div class="meta-line">Comments: 22 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03499v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03499v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synthetic Aperture Radar (SAR) imaging results are highly sensitive to observation geometries and the geometric parameters of targets. However, existing generative methods primarily operate within the image domain, neglecting explicit geometric information. This limitation often leads to unsatisfactory generation quality and the inability to precisely control critical parameters such as azimuth angles. To address these challenges, we propose GeoDiff-SAR, a geometric prior guided diffusion model for high-fidelity SAR image generation. Specifically, GeoDiff-SAR first efficiently simulates the geometric structures and scattering relationships inherent in real SAR imaging by calculating SAR point clouds at specific azimuths, which serves as a robust physical guidance. Secondly, to effectively fuse multi-modal information, we employ a feature fusion gating network based on Feature-wise Linear Modulation (FiLM) to dynamically regulate the weight distribution of 3D physical information, image control parameters, and textual description parameters. Thirdly, we utilize the Low-Rank Adaptation (LoRA) architecture to perform lightweight fine-tuning on the advanced Stable Diffusion 3.5 (SD3.5) model, enabling it to rapidly adapt to the distribution characteristics of the SAR domain. To validate the effectiveness of GeoDiff-SAR, extensive comparative experiments were conducted on real-world SAR datasets. The results demonstrate that data generated by GeoDiff-SAR exhibits high fidelity and effectively enhances the accuracy of downstream classification tasks. In particular, it significantly improves recognition performance across different azimuth angles, thereby underscoring the superiority of physics-guided generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoDiff-SAR：一种几何先验引导的SAR图像生成扩散模型</div>
<div class="mono" style="margin-top:8px">合成孔径雷达（SAR）成像结果对观测几何和目标的几何参数高度敏感。然而，现有的生成方法主要在图像域内操作，忽视了显式的几何信息。这一局限性常常导致生成质量不佳，无法精确控制诸如方位角等关键参数。为了解决这些挑战，我们提出了GeoDiff-SAR，一种用于高保真SAR图像生成的几何先验引导扩散模型。具体而言，GeoDiff-SAR首先通过计算特定方位下的SAR点云，有效模拟真实SAR成像中固有的几何结构和散射关系，作为强有力的物理指导。其次，为了有效融合多模态信息，我们采用基于特征线性调制（FiLM）的特征融合门控网络，动态调节3D物理信息、图像控制参数和文本描述参数的权重分布。第三，我们利用低秩适应（LoRA）架构对先进的稳定扩散3.5（SD3.5）模型进行轻量级微调，使其能够快速适应SAR领域的分布特征。为了验证GeoDiff-SAR的有效性，我们在真实世界的SAR数据集上进行了广泛的对比实验。结果表明，GeoDiff-SAR生成的数据具有高保真度，并有效提高了下游分类任务的准确性。特别是，它显著改善了不同方位角下的识别性能，从而突显了物理引导生成的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the quality of Synthetic Aperture Radar (SAR) image generation, which is often hindered by the neglect of geometric information in existing generative methods. The authors propose GeoDiff-SAR, a diffusion model that incorporates geometric priors by simulating SAR point clouds at specific azimuths and using a feature fusion gating network to integrate multi-modal information. Experimental results on real-world SAR datasets show that GeoDiff-SAR generates high-fidelity images and enhances the accuracy of downstream classification tasks, particularly improving recognition performance across various azimuth angles.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善合成孔径雷达（SAR）图像生成的质量，而现有生成方法往往因缺乏明确的几何信息而受到限制。作者提出了GeoDiff-SAR，这是一种扩散模型，通过在特定方位角下模拟SAR点云并使用特征融合门控网络来整合多模态信息，从而引入几何先验。对真实SAR数据集的实验结果表明，GeoDiff-SAR生成的图像具有高保真度，并显著提高了分类准确性，尤其是在不同方位角下，展示了在生成过程中引入基于物理的指导的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Submodular Evaluation Subset Selection in Automatic Prompt Optimization</div>
<div class="meta-line">Authors: Jinming Nian, Zhiyuan Peng, Hongwei Shang, Dae Hoon Park, Yi Fang</div>
<div class="meta-line">First: 2026-01-07T01:12:45+00:00 · Latest: 2026-01-07T01:12:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03493v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatic prompt optimization reduces manual prompt engineering, but relies on task performance measured on a small, often randomly sampled evaluation subset as its main source of feedback signal. Despite this, how to select that evaluation subset is usually treated as an implementation detail. We study evaluation subset selection for prompt optimization from a principled perspective and propose SESS, a submodular evaluation subset selection method. We frame selection as maximizing an objective set function and show that, under mild conditions, it is monotone and submodular, enabling greedy selection with theoretical guarantees. Across GSM8K, MATH, and GPQA-Diamond, submodularly selected evaluation subsets can yield better optimized prompts than random or heuristic baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动提示优化中的子模块评估子集选择</div>
<div class="mono" style="margin-top:8px">自动提示优化减少了手动提示工程，但依赖于在小规模、通常是随机抽样的评估子集上测量的任务性能作为其主要反馈信号。尽管如此，如何选择该评估子集通常被视为实现细节。我们从原则的角度研究提示优化的评估子集选择，并提出了SESS，一种子模块评估子集选择方法。我们将选择框架化为最大化一个目标集合函数，并表明在温和条件下，它是单调和子模块的，从而使贪婪选择具有理论保证。在GSM8K、MATH和GPQA-Diamond上，子模块选择的评估子集可以产生比随机或启发式基线更优化的提示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve automatic prompt optimization, which traditionally relies on performance feedback from a small, randomly sampled evaluation subset. The authors propose a new method called SESS, which employs submodular evaluation subset selection to enhance this process. Their experiments demonstrate that using submodularly selected evaluation subsets leads to better-optimized prompts compared to random or heuristic selection methods across various tasks, including GSM8K, MATH, and GPQA-Diamond.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善自动提示优化，该过程通常依赖于来自小型随机抽样评估子集的性能反馈。作者提出了一种名为SESS的新方法，利用子模评估子集选择来增强这一过程。实验结果表明，在GSM8K、MATH和GPQA-Diamond等多个数据集上，使用子模选择的评估子集能够比随机或启发式选择方法产生更优化的提示。</div>
</details>
</div>
<div class="card">
<div class="title">CALM: Culturally Self-Aware Language Models</div>
<div class="meta-line">Authors: Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel</div>
<div class="meta-line">First: 2026-01-07T00:28:33+00:00 · Latest: 2026-01-07T00:28:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03483v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03483v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model&#x27;s original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Extensive experiments conducted on multiple cross-cultural benchmark datasets demonstrate that CALM consistently outperforms state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CALM：文化自觉语言模型</div>
<div class="mono" style="margin-top:8px">语言模型中的文化意识是理解和适应多样文化背景的能力。然而，大多数现有方法将文化视为静态背景知识，忽视其动态和不断发展的特性。这一局限性降低了它们在需要真正文化敏感性的下游任务中的可靠性。在本研究中，我们介绍了CALM，一个旨在赋予语言模型文化自觉的新框架。CALM将任务语义与显性文化概念和潜在文化信号解耦，通过对比学习将其塑造成结构化的文化集群。这些集群通过交叉注意力对齐，以建立相关文化特征之间的细粒度交互，并通过专家混合机制沿文化特定维度自适应整合。最终的统一表示与模型的原始知识融合，以构建一个文化基础的内部身份状态，并通过自我提示反思学习进一步增强，实现持续适应和自我修正。在多个跨文化基准数据集上进行的广泛实验表明，CALM始终优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance cultural awareness in language models, addressing the limitation of existing approaches that treat culture as static and fail to capture its dynamic nature. The authors propose CALM, a framework that employs contrastive learning to create structured cultural clusters by disentangling task semantics from cultural concepts and signals. Experimental results show that CALM significantly outperforms state-of-the-art methods across various cross-cultural benchmark datasets, demonstrating its effectiveness in fostering genuine cultural sensitivity in language models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强语言模型的文化意识，解决现有方法将文化视为静态背景知识而未能捕捉其动态特性的局限性。作者提出了CALM框架，通过对比学习将任务语义与文化概念和信号分离，从而创建结构化的文化集群。实验结果表明，CALM在多个跨文化基准数据集上显著优于最先进的方法，证明了其在促进语言模型真正文化敏感性方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
