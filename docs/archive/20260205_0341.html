<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 03:41</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0341</div>
    <div class="row"><div class="card">
<div class="title">Continuous Control of Editing Models via Adaptive-Origin Guidance</div>
<div class="meta-line">Authors: Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</div>
<div class="meta-line">First: 2026-02-03T18:33:39+00:00 · Latest: 2026-02-03T18:33:39+00:00</div>
<div class="meta-line">Comments: Project page at https://adaor-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03826v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adaor-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应原点引导实现编辑模型的连续控制</div>
<div class="mono" style="margin-top:8px">基于扩散的编辑模型已成为语义图像和视频处理的强大工具。然而，现有模型缺乏平滑控制文本引导编辑强度的机制。在标准的文本条件生成中，无分类器引导（CFG）影响提示遵循，暗示其作为编辑模型中编辑强度的潜在控制。然而，我们表明在这些模型中缩放CFG并未产生输入与编辑结果之间的平滑过渡。我们将这种行为归因于无条件预测，它作为引导原点并在低引导尺度下主导生成，同时代表对输入内容的任意操控。为了实现连续控制，我们引入了自适应原点引导（AdaOr），该方法使用与身份操控相对应的身份指令调整标准引导原点。通过根据编辑强度将此身份预测与标准无条件预测进行插值，我们确保从输入到编辑结果的连续过渡。我们在图像和视频编辑任务上评估了我们的方法，证明其提供了比当前基于滑块的编辑方法更平滑和一致的控制。我们的方法将身份指令纳入标准训练框架，使得在推理时能够进行细粒度控制，而无需每次编辑程序或依赖专门的数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the control of text-guided edits in diffusion-based editing models, which currently lack a smooth mechanism for adjusting edit intensity. The authors propose a novel method called Adaptive-Origin Guidance (AdaOr), which modifies the standard guidance origin by incorporating an identity-conditioned adaptive origin that aligns with the desired manipulation. Experimental results show that AdaOr enables a continuous transition from the input to the edited output, providing smoother and more consistent control in image and video editing tasks compared to existing slider-based methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有扩散基础编辑模型在控制图像和视频处理中的文本引导编辑强度方面的局限性。作者提出了一种新方法，称为自适应原点引导（AdaOr），该方法通过结合身份条件自适应原点来修改标准引导原点，以实现对编辑的连续控制。实验结果表明，AdaOr能够比传统的滑块编辑方法实现输入与编辑输出之间更平滑和一致的过渡，从而在不需要专门数据集或逐个编辑程序的情况下实现细粒度控制。</div>
</details>
</div>
<div class="card">
<div class="title">WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents</div>
<div class="meta-line">Authors: Xilong Wang, Yinuo Liu, Zhun Wang, Dawn Song, Neil Gong</div>
<div class="meta-line">First: 2026-02-03T17:55:04+00:00 · Latest: 2026-02-03T17:55:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03792v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03792v1">PDF</a> · <a href="https://github.com/wxl-lxw/WebSentinel">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user&#x27;s intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebSentinel：检测和定位网页代理的提示注入攻击</div>
<div class="mono" style="margin-top:8px">提示注入攻击操纵网页内容，使网页代理执行攻击者指定的任务，而不是用户意图的任务。现有的检测和定位此类攻击的方法效果有限，因为它们的基本假设在网页代理环境中往往不成立。在本研究中，我们提出了WebSentinel，一种用于检测和定位网页中提示注入攻击的两步方法。给定一个网页，第一步提取可能被污染的\emph{感兴趣片段}，第二步通过检查每个片段与网页内容的一致性作为上下文来评估每个片段。我们展示了WebSentinel的高效性，在我们收集的多个污染和干净网页的数据集上，显著优于基线方法。我们的代码可在以下网址获取：https://github.com/wxl-lxw/WebSentinel。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing methods for detecting and localizing prompt injection attacks on web agents, which often fail due to incorrect assumptions in the web-agent context. The authors propose WebSentinel, a two-step approach that first extracts potentially contaminated segments from a webpage and then evaluates their consistency with the overall webpage content. Experimental results demonstrate that WebSentinel significantly outperforms baseline methods across various datasets of both contaminated and clean webpages.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有检测和定位网页代理的提示注入攻击方法的局限性，这些方法通常因错误的基本假设而失败。作者提出了WebSentinel，这是一种两步法，首先从网页中提取可能被污染的片段，然后评估这些片段与整体网页内容的一致性。实验结果表明，WebSentinel在多个污染和干净网页的数据集上显著优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm</div>
<div class="meta-line">Authors: Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Anne Collins, Maarten Sap, Sydney Levine</div>
<div class="meta-line">First: 2026-01-13T19:41:11+00:00 · Latest: 2026-02-03T17:34:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08951v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08951v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions -- the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond &quot;one-size-fits-all&quot; safety toward pluralistically safe AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PluriHarms：基准测试人类对AI危害的全面判断</div>
<div class="mono" style="margin-top:8px">当前的AI安全框架通常将危害性视为二元的，缺乏处理人类有意义分歧的边界案例的灵活性。为了构建更具多元化的系统，必须超越共识，理解分歧产生的原因和地点。我们引入PluriHarms，这是一个旨在系统研究人类危害判断的基准，涵盖两个关键维度——危害轴（从良性到有害）和一致性轴（从一致到分歧）。我们的可扩展框架生成捕捉多样化AI危害和人类价值的提示，同时针对高分歧率的案例，经过人类数据验证。该基准包括150个提示，来自100名人类注释者的15,000条评分，丰富了人口统计和心理特征以及危害行为、影响和价值的提示级特征。我们的分析表明，与迫在眉睫的风险和具体危害相关的提示会增强感知的危害性，而注释者特征（例如，毒性经验、教育）及其与提示内容的互动解释了系统性分歧。我们在PluriHarms上对AI安全模型和对齐方法进行基准测试，发现个性化显著提高了对人类危害判断的预测，但未来仍有相当大的进步空间。通过明确针对价值多样性和分歧，我们的工作为超越“千篇一律”的安全提供了一个原则性基准，朝着多元安全的AI迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current AI safety frameworks that treat harmfulness as binary, which fails to accommodate the complexities of human judgments where disagreements exist. The authors introduce PluriHarms, a benchmark that systematically examines human harm judgments across the dimensions of harm and agreement, utilizing a scalable framework that generates prompts capturing diverse AI harms and human values. The study reveals that prompts associated with imminent risks and tangible harms increase perceived harmfulness, while individual annotator traits and their interactions with prompt content contribute to systematic disagreements; furthermore, the benchmarking of AI safety models indicates that while personalization enhances the prediction of human harm judgments, significant improvements are still needed for future advancements in AI safety.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前将有害性视为二元概念的AI安全框架的局限性，这种框架未能考虑人类在伤害判断上的复杂分歧。作者介绍了PluriHarms，一个系统探索人类伤害判断的基准，涵盖了伤害轴和一致性轴两个维度。该研究包含150个提示和来自100名标注者的15,000个评分，结果显示与迫在眉睫的风险相关的提示会增加感知的有害性，而个体标注者的特征则导致系统性分歧。研究结果表明，尽管个性化增强了对人类伤害判断的预测，但在AI安全模型和对齐方法上仍有显著的改进空间。</div>
</details>
</div>
<div class="card">
<div class="title">FOVI: A biologically-inspired foveated interface for deep vision models</div>
<div class="meta-line">Authors: Nicholas M. Blauch, George A. Alvarez, Talia Konkle</div>
<div class="meta-line">First: 2026-02-03T17:26:54+00:00 · Latest: 2026-02-03T17:26:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03766v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03766v1">PDF</a> · <a href="https://github.com/nblauch/fovi">Code1</a> · <a href="https://huggingface.co/fovi-pytorch">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FOVI：一种生物启发的深度视觉模型的注视接口</div>
<div class="mono" style="margin-top:8px">人类视觉是注视的，具有在大视场中心的可变分辨率；这反映了主动感知的高效权衡，允许眼动将世界的不同部分聚焦，同时保持其他部分的上下文。相比之下，大多数计算机视觉系统以均匀分辨率编码视觉世界，这给高分辨率全场图像的高效处理带来了挑战。我们提出了一种基于人类视网膜和初级视觉皮层的注视视觉接口（FOVI），将可变分辨率的类视网膜传感器阵列重新格式化为均匀密集的V1类传感器流形。感受野被定义为传感器流形上的k近邻（kNN），通过一种新颖的核映射技术实现kNN卷积。我们展示了两个用例：（1）端到端的kNN卷积架构，以及（2）基础DINOv3 ViT模型的注视适配，利用低秩适配（LoRA）。这些模型在计算成本仅为非注视基线的一小部分的情况下提供了竞争性能，为高分辨率自我中心视觉的高效和可扩展主动感知开辟了途径。代码和预训练模型可在https://github.com/nblauch/fovi和https://huggingface.co/fovi-pytorch获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inefficiencies in traditional computer vision systems that process images at a uniform resolution, unlike human vision which utilizes a foveated approach for efficient active sensing. The authors propose a foveated vision interface (FOVI) that mimics the human retina and primary visual cortex, employing a variable-resolution sensor array reformatted into a uniformly dense sensor manifold. Experimental results show that their end-to-end kNN-convolutional architecture and a foveated adaptation of the DINOv3 ViT model achieve competitive performance while significantly reducing computational costs compared to non-foveated baselines, thus enhancing the efficiency of high-resolution egocentric vision tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决计算机视觉系统在处理高分辨率图像时的低效问题，这些系统通常使用统一分辨率，而人类视觉则是以中心高分辨率为特征的凹视。作者提出了一种模仿人类视网膜和初级视觉皮层的凹视视觉接口（FOVI），通过将可变分辨率传感器阵列重新格式化为均匀密集的传感器流形，利用k近邻进行kNN卷积，并采用新颖的核映射技术。实验结果表明，他们的端到端kNN卷积架构和DINOv3 ViT模型的凹视适应在性能上具有竞争力，同时显著降低了与非凹视基线相比的计算成本，从而增强了高分辨率自我中心视觉的高效主动感知能力。</div>
</details>
</div>
<div class="card">
<div class="title">Test-Time Conditioning with Representation-Aligned Visual Features</div>
<div class="meta-line">Authors: Nicolas Sereyjol-Garros, Ellington Kirby, Victor Letzelter, Victor Besnier, Nermin Samet</div>
<div class="meta-line">First: 2026-02-03T17:15:03+00:00 · Latest: 2026-02-03T17:15:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03753v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03753v1">PDF</a> · <a href="https://github.com/valeoai/REPA-G">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于表示对齐视觉特征的测试时条件化</div>
<div class="mono" style="margin-top:8px">虽然与自监督模型的表示对齐已被证明可以改善扩散模型的训练，但其在增强推理时条件化方面的潜力仍然未被充分探索。我们引入了表示对齐引导（REPA-G），这是一个利用这些对齐表示的框架，具有丰富的语义属性，以实现从生成特征的测试时条件化。通过在推理时优化相似性目标（潜力），我们将去噪过程引导到从预训练特征提取器提取的条件表示。我们的方法在多个尺度上提供灵活的控制，从通过单个补丁进行细粒度纹理匹配到使用全局图像特征标记进行广泛的语义引导。我们进一步扩展到多概念组合，允许忠实地结合不同的概念。REPA-G 完全在推理时操作，提供了一种灵活而精确的替代方案，避免了模糊的文本提示或粗略的类别标签。我们从理论上证明了这种引导如何使得从潜力诱导的倾斜分布中进行采样成为可能。在 ImageNet 和 COCO 上的定量结果表明，我们的方法实现了高质量、多样化的生成。代码可在 https://github.com/valeoai/REPA-G 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve inference-time conditioning in diffusion models using representation alignment from self-supervised models. The authors introduce Representation-Aligned Guidance (REPA-G), a framework that utilizes aligned representations to condition generation at test time by optimizing a similarity objective during inference. Experimental results on ImageNet and COCO show that REPA-G enables versatile control over generation, achieving high-quality and diverse outputs while allowing for multi-concept composition, thus providing a more precise alternative to traditional text prompts and class labels.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于利用自监督模型中的表示对齐来增强扩散模型的推理时条件。作者提出了一种名为表示对齐引导（REPA-G）的框架，通过在推理时优化相似性目标，引导去噪过程基于来自预训练特征提取器的表示。对ImageNet和COCO的实验结果表明，REPA-G能够实现多样化的生成控制，生成高质量和多样化的输出，同时允许多概念组合，而无需依赖模糊的文本提示或粗略的类别标签。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives</div>
<div class="meta-line">Authors: Owen Dong, Lily Gao, Manish Kota, Bennett A. Landmana, Jelena Bekvalac, Gaynor Western, Katherine D. Van Schaik</div>
<div class="meta-line">First: 2026-02-03T17:14:23+00:00 · Latest: 2026-02-03T17:14:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03750v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03750v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本大规模视觉语言模型提示用于古放射学X光档案中的自动骨骼识别</div>
<div class="mono" style="margin-top:8px">古放射学是利用现代成像技术研究考古和人类学遗骸，为千年尺度的人类健康模式提供了新的视角。不幸的是，现场收集的放射线照片是异质的：骨骼是分离的，定位是临时的，且侧别标记通常缺失。此外，死亡年龄、骨骼年龄、性别和成像设备等因素引入了高度变异性。因此，内容导航，例如识别具有特定投影视图的图像子集，可能耗时且困难，使得高效的分流成为专家分析的瓶颈。我们报告了一种零样本提示策略，利用最先进的大规模视觉语言模型（LVLM）自动识别这些图像中的主要骨骼、投影视图和侧别。我们的流程将原始DICOM文件转换为骨窗PNG，提交给LVLM并使用精心设计的提示，接收结构化的JSON输出，这些输出被提取并格式化到电子表格中以准备验证。在由一位获得认证的古放射学专家审查的100张随机样本图像中，该系统实现了92%的主要骨骼准确率、80%的投影视图准确率和100%的侧别准确率，对于模糊案例则标记为低或中等置信度。这些结果表明，LVLM可以显著加速大型古放射学数据集的代码词开发，从而在未来的人类学工作流程中实现高效的内容导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of content navigation in paleoradiology, where the analysis of heterogeneous radiographs can be time-consuming due to various factors affecting bone identification. The authors developed a zero-shot prompting strategy utilizing a Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality from raw DICOM files. The experimental results demonstrated that the system achieved 92% accuracy in identifying the main bone, 80% in projection view, and 100% in laterality on a sample of 100 images reviewed by a certified paleoradiologist, indicating that LVLMs can significantly enhance the processing of large paleoradiology datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高古放射学中内容导航的效率，因为异质的放射图像使得骨骼及其特征的识别变得复杂。作者开发了一种利用大型视觉语言模型（LVLM）的零-shot提示策略，以自动识别X光图像中的主要骨骼、投影视图和侧向性。实验结果表明，该系统在100张由专家古放射学医师审查的图像样本中，主要骨骼识别准确率为92%，投影视图准确率为80%，侧向性准确率为100%，这表明LVLM在增强大型古放射学数据集分析中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Referring Industrial Anomaly Segmentation</div>
<div class="meta-line">Authors: Pengfei Yue, Xiaokang Jiang, Yilin Lu, Jianghang Lin, Shengchuan Zhang, Liujuan Cao</div>
<div class="meta-line">First: 2026-02-03T15:53:30+00:00 · Latest: 2026-02-03T15:53:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03673v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03673v1">PDF</a> · <a href="https://github.com/swagger-coder/RIAS-MVTec-Ref">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the &quot;One Anomaly Class, One Model&quot; limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only &quot;Anomaly&quot; and &quot;Background&quot; tokens for efficient visual-textual integration. Experiments demonstrate RIAS&#x27;s effectiveness in advancing IAD toward open-set capabilities. Code: https://github.com/swagger-coder/RIAS-MVTec-Ref.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参考工业异常分割</div>
<div class="mono" style="margin-top:8px">工业异常检测（IAD）对制造业至关重要，但传统方法面临重大挑战：无监督方法产生粗略的定位，需要手动阈值，而监督方法由于数据稀缺和不平衡而过拟合。两者都受到“一个异常类别，一个模型”的限制。为了解决这个问题，我们提出了参考工业异常分割（RIAS），一种利用语言指导检测的范式。RIAS从文本描述生成精确的掩膜，无需手动阈值，并使用通用提示通过单一模型检测多样的异常。我们引入了MVTec-Ref数据集，以支持这一点，该数据集设计了多样的参考表达，重点关注异常模式，特别是95%的小异常。我们还提出了双查询令牌与掩膜组变换器（DQFormer）基准，通过语言门控多级聚合（LMA）增强，以改善多尺度分割。与使用冗余查询的传统方法不同，DQFormer仅使用“异常”和“背景”令牌，以实现高效的视觉-文本集成。实验表明RIAS在推动IAD向开放集能力发展方面的有效性。代码：https://github.com/swagger-coder/RIAS-MVTec-Ref.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Industrial Anomaly Detection (IAD) in manufacturing, addressing the limitations of traditional methods that struggle with localization and overfitting due to data scarcity. The authors propose a new approach called Referring Industrial Anomaly Segmentation (RIAS), which utilizes language to guide the detection process, generating precise anomaly masks from text descriptions without the need for manual thresholds. Experimental results show that RIAS, supported by the MVTec-Ref dataset and the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, significantly enhances IAD capabilities, particularly in detecting small anomalies and achieving open-set performance with efficient visual-textual integration.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善制造业中的工业异常检测（IAD），传统方法面临手动阈值要求或因数据有限而过拟合的问题。作者提出了一种新的方法，称为参考工业异常分割（RIAS），利用语言指导检测过程，允许从文本描述中生成精确的异常掩码，而无需手动阈值。关键实验结果表明，RIAS在新引入的MVTec-Ref数据集和双查询令牌与掩码组变换器（DQFormer）的支持下，有效增强了IAD能力，能够更好地检测多样化的异常，并展示了开放集能力。</div>
</details>
</div>
<div class="card">
<div class="title">An Overview of Low-Rank Structures in the Training and Adaptation of Large Models</div>
<div class="meta-line">Authors: Laura Balzano, Tianjiao Ding, Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, Peng Wang, Zhangyang Wang, Can Yaras</div>
<div class="meta-line">First: 2025-03-25T17:26:09+00:00 · Latest: 2026-02-03T15:51:21+00:00</div>
<div class="meta-line">Comments: Authors are listed alphabetically; 37 pages, 15 figures; minor revision at IEEE Signal Processing Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.19859v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.19859v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The substantial computational demands of modern large-scale deep learning present significant challenges for efficient training and deployment. Recent research has revealed a widespread phenomenon wherein deep networks inherently learn low-rank structures in their weights and representations during training. This tutorial paper provides a comprehensive review of advances in identifying and exploiting these low-rank structures, bridging mathematical foundations with practical applications. We present two complementary theoretical perspectives on the emergence of low-rankness: viewing it through the optimization dynamics of gradient descent throughout training, and understanding it as a result of implicit regularization effects at convergence. Practically, these theoretical perspectives provide a foundation for understanding the success of techniques such as Low-Rank Adaptation (LoRA) in fine-tuning, inspire new parameter-efficient low-rank training strategies, and explain the effectiveness of masked training approaches like dropout and masked self-supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大模型训练与适应中的低秩结构概述</div>
<div class="mono" style="margin-top:8px">现代大规模深度学习的计算需求巨大，给高效训练和部署带来了显著挑战。最近的研究揭示了一个普遍现象，即深度网络在训练过程中固有地学习到其权重和表示中的低秩结构。本文提供了对识别和利用这些低秩结构的进展的全面回顾，桥接了数学基础与实际应用。我们提出了关于低秩性出现的两种互补理论视角：通过训练过程中梯度下降的优化动态来观察，以及理解其作为收敛时隐式正则化效应的结果。从实践角度来看，这些理论视角为理解低秩适应（LoRA）等技术在微调中的成功提供了基础，激发了新的参数高效低秩训练策略，并解释了诸如丢弃法和掩蔽自监督学习等掩蔽训练方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this paper is to address the significant computational challenges posed by large-scale deep learning models during training and deployment. The authors review recent findings that deep networks naturally learn low-rank structures in their weights and representations, employing a comprehensive analysis that combines theoretical insights with practical applications. Key experimental results highlight the emergence of low-rankness through optimization dynamics in gradient descent and implicit regularization effects, which inform the effectiveness of techniques like Low-Rank Adaptation (LoRA) and inspire new low-rank training strategies and masked training methods such as dropout and masked self-supervised learning.</div>
<div class="mono" style="margin-top:8px">本论文的动机在于解决与训练和部署大规模深度学习模型相关的计算挑战。作者采用综合评审的方法探讨深度网络在训练过程中学习到的权重和表示中的低秩结构现象。主要发现表明，这些低秩结构可以通过梯度下降的优化动态和隐式正则化效应来理解，这不仅阐明了低秩适应（LoRA）等技术的成功，还激发了新的低秩训练策略，并解释了诸如dropout和掩蔽自监督学习等方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection</div>
<div class="meta-line">Authors: Alireza Salehi, Ehsan Karami, Sepehr Noey, Sahand Noey, Makoto Yamada, Reshad Hosseini, Mohammad Sabokrou</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2026-02-03T14:48:11+00:00 · Latest: 2026-02-03T14:48:11+00:00</div>
<div class="meta-line">Comments: This is the extended version of the paper accepted in ICASSP&#x27;26, which will be publicly available in May. Authors&#x27; contributions may vary among the versions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03594v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03594v1">PDF</a> · <a href="http://github.com/AlirezaSalehy/Tipsomaly">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP&#x27;s coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP&#x27;s issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TIPS而非技巧：有效零-shot异常检测的简单提示</div>
<div class="mono" style="margin-top:8px">异常检测识别安全关键环境中与预期行为的偏离。当目标领域的正常数据不可用时，零-shot异常检测（ZSAD）利用视觉-语言模型（VLMs）。然而，CLIP的粗略图像-文本对齐由于（i）空间不对齐和（ii）对细粒度异常的敏感性较弱，限制了定位和检测；先前的工作通过复杂的辅助模块进行补偿，但在很大程度上忽视了主干的选择。我们重新审视主干，使用TIPS——一个以空间感知目标训练的VLM。虽然TIPS缓解了CLIP的问题，但它暴露了全局和局部特征之间的分布差距。我们通过解耦提示（固定用于图像级检测，学习用于像素级定位）以及将局部证据注入全局得分来解决这个问题。在没有CLIP特定技巧的情况下，我们基于TIPS的管道在七个工业数据集上提高了图像级性能1.1-3.9%和像素级性能1.5-6.9%，实现了强大的泛化能力和精简的架构。代码可在github.com/AlirezaSalehy/Tipsomaly获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of zero-shot anomaly detection (ZSAD) in safety-critical environments, particularly the limitations of existing vision-language models (VLMs) like CLIP, which struggle with spatial misalignment and sensitivity to fine-grained anomalies. The authors propose using TIPS, a VLM trained with spatially aware objectives, and introduce decoupled prompts to enhance both image-level detection and pixel-level localization. Experimental results demonstrate that the TIPS-based approach improves image-level performance by 1.1-3.9% and pixel-level performance by 1.5-6.9% across seven industrial datasets, showcasing strong generalization capabilities with a simplified architecture.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高在安全关键环境中零样本异常检测（ZSAD）的效果，特别是在目标领域缺乏正常数据的情况下。作者提出了一种使用TIPS的方案，这是一种经过空间感知目标训练的视觉语言模型（VLM），旨在解决CLIP的局限性，后者在图像-文本对齐和对细粒度异常的敏感性方面存在问题。关键实验结果表明，基于TIPS的方案在七个工业数据集上提高了1.1-3.9%的图像级性能和1.5-6.9%的像素级性能，展示了强大的泛化能力，并且采用了简化的架构，而不依赖复杂的辅助模块。</div>
</details>
</div>
<div class="card">
<div class="title">Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval</div>
<div class="meta-line">Authors: Tong Wang, Yunhan Zhao, Shu Kong</div>
<div class="meta-line">First: 2026-01-31T16:42:55+00:00 · Latest: 2026-02-03T14:05:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00813v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00813v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image&#x27;&#x27;, based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image&#x27;&#x27; is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image&#x27;&#x27; for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image&#x27;&#x27; for a given multimodal query and propose to use this ``mental image&#x27;&#x27; to search for the target image. As the ``mental image&#x27;&#x27; has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm&#x27;&#x27;, where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成用于无训练零-shot复合图像检索的虚构世界</div>
<div class="mono" style="margin-top:8px">复合图像检索（CIR）是从数据库中使用多模态查询检索目标图像的任务，该查询由参考图像和修改文本组成。文本指定如何改变参考图像以形成“心理图像”，CIR应基于此在数据库中找到目标图像。CIR的基本挑战在于“心理图像”并不存在于物理上，仅由查询隐式定义。当前文献追求零-shot方法，并使用大型多模态模型（LMM）为给定的多模态查询生成文本描述，然后利用视觉-语言模型（VLM）进行文本-视觉匹配以搜索目标图像。相较之下，我们从基本原理出发，直接生成“心理图像”以实现更准确的匹配。特别地，我们提示LMM为给定的多模态查询生成“心理图像”，并建议使用该“心理图像”搜索目标图像。由于“心理图像”与真实图像之间存在合成与真实的领域差距，我们还为数据库中的每个真实图像生成合成对应物以促进匹配。从这个意义上讲，我们的方法使用LMM构建一个“虚构世界”，在其中匹配多模态查询和数据库图像。因此，我们将此方法称为Paracosm。值得注意的是，Paracosm是一种无训练的零-shot CIR方法。在四个具有挑战性的基准测试中，它显著优于现有的零-shot方法，实现了零-shot CIR的最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Composed Image Retrieval (CIR), which involves retrieving a target image using a multimodal query that combines a reference image and modification text. The authors propose a novel method called Paracosm, which generates a &#x27;mental image&#x27; directly from the multimodal query using a Large Multimodal Model (LMM) and creates synthetic counterparts for real images to bridge the domain gap. Experimental results demonstrate that Paracosm significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance in zero-shot CIR.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高组合图像检索（CIR）的准确性，该任务涉及使用结合参考图像和修改文本的多模态查询来检索目标图像。作者提出了一种名为Paracosm的新方法，该方法直接从多模态查询生成“心理图像”，并使用大型多模态模型（LMM），然后利用生成的图像在数据库中搜索目标图像。实验结果表明，Paracosm在四个具有挑战性的基准测试中显著优于现有的零-shot方法，在零-shot CIR中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images</div>
<div class="meta-line">Authors: Xinyue Li, Zhiming Xu, Zhichao Zhang, Zhaolin Cai, Sijing Wu, Xiongkuo Min, Yitong Chen, Guangtao Zhai</div>
<div class="meta-line">First: 2026-02-03T14:04:51+00:00 · Latest: 2026-02-03T14:04:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03558v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03558v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ELIQ：一种无标签的演变AI生成图像质量评估框架</div>
<div class="mono" style="margin-top:8px">生成式文本到图像模型正以空前的速度发展，不断提升感知质量上限，使得之前收集的标签对新一代模型变得不可靠。为了解决这个问题，我们提出了ELIQ，一种无标签的演变AI生成图像质量评估框架。具体而言，ELIQ专注于视觉质量和提示-图像对齐，自动构建正样本和特定方面的负样本对，以覆盖传统失真和AIGC特定失真模式，实现无需人工标注的可转移监督。在这些样本对的基础上，ELIQ通过指令调优将预训练的多模态模型适配为质量感知的评估器，并使用轻量级门控融合和质量查询变换器预测二维质量。多个基准实验表明，ELIQ始终优于现有的无标签方法，能够在不修改的情况下从AI生成内容（AIGC）推广到用户生成内容（UGC）场景，并为在不断演变的生成模型下实现可扩展和无标签的质量评估铺平道路。代码将在发表时发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of generative text-to-image models has made traditional quality labels unreliable, motivating the development of ELIQ, a label-free framework for assessing the quality of evolving AI-generated images. ELIQ constructs positive and aspect-specific negative pairs to address both conventional and AIGC-specific distortions, allowing for transferable supervision without human annotations. Experimental results show that ELIQ outperforms existing label-free methods and effectively generalizes from AI-generated to user-generated content, facilitating scalable quality assessment in the context of continuously evolving generative models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是生成文本到图像模型的快速发展，使得以前收集的质量标签对于新一代图像变得不可靠。作者提出了ELIQ，一个无标签框架，通过自动创建正面和特定方面的负面配对来评估AI生成图像的视觉质量和提示-图像对齐，旨在解决传统和AIGC特有的失真。实验结果表明，ELIQ在性能上优于现有的无标签方法，并有效地从AI生成内容推广到用户生成内容，展示了其在不断发展的生成模型中进行可扩展质量评估的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs</div>
<div class="meta-line">Authors: Alessio Quercia, Arya Bangun, Ira Assent, Hanno Scharr</div>
<div class="meta-line">First: 2026-02-03T13:09:29+00:00 · Latest: 2026-02-03T13:09:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03493v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最小但不最后：微调中间主成分以获得更好的性能-遗忘权衡</div>
<div class="mono" style="margin-top:8px">低秩适应（LoRA）方法已成为在计算和内存限制下将大型预训练模型适应于下游任务的关键技术。然而，它们面临着在任务特定性能提升与预训练知识的灾难性遗忘之间平衡的基本挑战，现有方法提供的不一致建议。本文对使用主成分作为初始化的低秩适应中固有的性能-遗忘权衡进行了全面分析。我们的研究表明，微调中间成分比现有工作中的第一个（PiSSA）和最后一个（MiLoRA）成分更能实现更好的平衡，并对高学习率表现出更强的鲁棒性。基于这些发现，我们提供了一种LoRA初始化的实用方法，提供了更优的权衡。我们在多种计算机视觉和自然语言处理任务上的全面实证研究中证明了我们的方法提高了准确性并减少了遗忘，尤其是在持续学习场景中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of balancing task-specific performance improvements with the risk of catastrophic forgetting in low-rank adaptation (LoRA) methods for large pre-trained models. The authors conducted a comprehensive analysis of performance-forgetting trade-offs by fine-tuning intermediate principal components as initialization. Their findings indicate that this approach yields better performance-forgetting balance and greater robustness to high learning rates compared to existing methods that focus on first or last components, ultimately demonstrating improved accuracy and reduced forgetting across various computer vision and NLP tasks, including continual learning scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决低秩适应（LoRA）方法在大型预训练模型中平衡任务特定性能提升与灾难性遗忘风险的挑战。作者通过对中间主成分进行微调作为初始化，进行了全面的性能-遗忘权衡分析。研究结果表明，与现有方法关注首个或最后一个组件相比，这种方法在性能-遗忘平衡和对高学习率的鲁棒性方面表现更佳，并在各种计算机视觉和自然语言处理任务中，包括持续学习场景，展示了更高的准确性和更低的遗忘率。</div>
</details>
</div>
<div class="card">
<div class="title">PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers</div>
<div class="meta-line">Authors: Haopeng Li, Shitong Shao, Wenliang Zhong, Zikai Zhou, Lichen Bai, Hui Xiong, Zeke Xie</div>
<div class="meta-line">First: 2026-02-01T07:47:06+00:00 · Latest: 2026-02-03T13:02:26+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01077v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01077v2">PDF</a> · <a href="https://github.com/xie-lab-ml/piecewise-sparse-attention">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PISA：分段稀疏注意力更适合高效扩散变换器</div>
<div class="mono" style="margin-top:8px">扩散变换器是视频和图像生成的基础，但其效率受到注意力的二次复杂度的瓶颈。虽然块稀疏注意力通过仅关注关键的键值块来加速计算，但在高稀疏性下会因丢弃上下文而导致性能下降。在本研究中，我们发现非关键块的注意力分数表现出分布稳定性，使其能够被准确高效地近似，而不是被丢弃，这对稀疏注意力设计至关重要。基于这一关键见解，我们提出了PISA，一种无训练的分段稀疏注意力，具有亚二次复杂度，覆盖完整的注意力范围。与传统的保留或丢弃范式直接丢弃非关键块信息不同，PISA引入了一种新颖的精确或近似策略：对关键块保持精确计算，同时通过块级泰勒展开高效近似其余部分。该设计使PISA能够作为完整注意力的忠实代理，有效弥合速度与质量之间的差距。实验结果表明，PISA在Wan2.1-14B和Hunyuan-Video上分别实现了1.91倍和2.57倍的加速，同时在稀疏注意力方法中始终保持最高质量。值得注意的是，即使在FLUX上的图像生成中，PISA也实现了1.2倍的加速而不影响视觉质量。代码可在：https://github.com/xie-lab-ml/piecewise-sparse-attention获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inefficiencies in Diffusion Transformers due to the quadratic complexity of attention mechanisms, which hinder their application in video and image generation. The authors propose a novel method called PISA, a training-free Piecewise Sparse Attention that utilizes a unique exact-or-approximate strategy to maintain critical attention computations while efficiently approximating non-critical blocks using block-wise Taylor expansion. Experimental results show that PISA achieves significant speedups of 1.91 times and 2.57 times on Wan2.1-14B and Hunyuan-Video datasets, respectively, while also maintaining high quality, and it provides a 1.2 times acceleration in image generation on the FLUX dataset without compromising visual quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高扩散变换器的效率，这对于视频和图像生成至关重要，但由于注意力机制的平方复杂性面临挑战。作者提出了一种名为PISA的新方法，它实现了一种无训练的分段稀疏注意力方法，保持了完整的注意力范围，同时实现了亚平方复杂性。实验结果表明，PISA在Wan2.1-14B和Hunyuan-Video数据集上分别提供了1.91倍和2.57倍的显著加速，同时保持高质量，并且在FLUX数据集上的图像生成中也实现了1.2倍的加速而不影响视觉质量。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理约束编程</div>
<div class="mono" style="margin-top:8px">自然语言到正式约束模型的翻译需要在问题领域和建模框架方面的专业知识。为了探索代理工作流的有效性，我们提出了CP-Agent，一个使用ReAct框架和持久IPython内核的Python编码代理。我们提供相关领域知识作为不超过50行的项目提示。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果优化约束模型来工作。
我们在来自CP-Bench的101个约束编程问题上评估CP-Agent。我们对基准进行了小幅修改，以解决问题规范中的系统性模糊性和真实模型中的错误。在澄清的基准上，CP-Agent在所有101个问题上实现了完美的准确性。我们的实验表明，最小的指导优于详细的程序性支架。我们的实验还表明，明确的任务管理工具对集中建模任务可能有正面和负面影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the translation of natural language into formal constraint models, which typically requires specialized knowledge. The authors introduce CP-Agent, a Python coding agent that utilizes the ReAct framework and a persistent IPython kernel to iteratively execute code and refine constraint models based on solver feedback. The evaluation of CP-Agent on 101 constraint programming problems from CP-Bench demonstrates that it achieves perfect accuracy on all problems after addressing ambiguities in the specifications, and the findings indicate that minimal guidance is more effective than detailed procedural scaffolding, while explicit task management tools can have mixed effects on modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于将自然语言翻译为正式约束模型的挑战，这通常需要专业知识。作者提出了CP-Agent，一个利用ReAct框架和持久IPython内核的Python编码代理，通过迭代执行代码并根据求解器反馈来自动化这一过程。在对来自CP-Bench的101个约束编程问题进行评估时，CP-Agent在解决所有问题时达到了完美的准确率，解决了规范中的模糊性，证明了最小指导比详细说明更有效，并且任务管理工具对建模任务的影响是复杂的。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation</div>
<div class="meta-line">Authors: Yijia Xu, Zihao Wang, Jinshi Cui</div>
<div class="meta-line">First: 2026-02-03T12:13:29+00:00 · Latest: 2026-02-03T12:13:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03448v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03448v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多主体图像生成的层次化概念到外观指导</div>
<div class="mono" style="margin-top:8px">多主体图像生成旨在合成忠实保留多个参考主体身份的图像，同时遵循文本指令。然而，现有方法常常面临身份不一致和有限的组合控制，因为它们依赖扩散模型隐式地将文本提示与参考图像关联。在本研究中，我们提出了层次化概念到外观指导（CAG），这是一个提供从高层概念到细粒度外观的明确、结构化监督的框架。在概念层面，我们引入了一种VAE dropout训练策略，随机省略参考VAE特征，鼓励模型更多依赖来自视觉语言模型（VLM）的稳健语义信号，从而在缺乏完整外观线索的情况下促进一致的概念级生成。在外观层面，我们将VLM派生的对应关系集成到扩散变换器（DiT）中的一个关注对应关系的掩码注意模块中。该模块限制每个文本标记仅关注其匹配的参考区域，确保精确的属性绑定和可靠的多主体组合。大量实验表明，我们的方法在多主体图像生成上达到了最先进的性能，显著提高了提示遵循和主体一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of identity inconsistency and limited compositional control in multi-subject image generation, which often arise from existing methods that rely on diffusion models. The authors propose a Hierarchical Concept-to-Appearance Guidance (CAG) framework that utilizes a VAE dropout training strategy to enhance semantic signal reliance and a correspondence-aware masked attention module within the Diffusion Transformer to ensure precise attribute binding. Experimental results indicate that this approach achieves state-of-the-art performance in multi-subject image generation, significantly improving prompt adherence and subject consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多主体图像生成中的身份不一致和有限的组合控制问题，这些问题通常源于依赖扩散模型的现有方法。作者提出了一种层次化概念到外观引导（CAG）框架，该框架结合了从高层次概念到细节外观的结构化监督。关键实验结果表明，这种方法通过引入VAE丢弃训练策略和对应感知的掩蔽注意模块，显著提高了提示遵循和主体一致性，在多主体图像生成任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</div>
<div class="meta-line">Authors: Piotr Wójcik, Maksym Petrenko, Wojciech Gromski, Przemysław Spurek, Maciej Zieba</div>
<div class="meta-line">First: 2026-02-03T11:37:08+00:00 · Latest: 2026-02-03T11:37:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03410v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03410v1">PDF</a> · <a href="https://github.com/gmum/UnHype">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: https://github.com/gmum/UnHype.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UnHype：基于CLIP的超网络动态LoRA遗忘</div>
<div class="mono" style="margin-top:8px">大型扩散模型的最新进展加剧了人们对其潜在滥用的担忧，特别是在生成现实但有害或社会破坏性内容方面。这个挑战激发了对有效机器遗忘的日益关注，即在不影响模型整体生成能力的情况下，有选择地移除模型中的特定知识或概念。在各种方法中，低秩适应（LoRA）已成为一种有效且高效的微调模型以实现目标遗忘的方法。然而，基于LoRA的方法通常在概念语义的适应性方面表现有限，并且在移除密切相关的概念与保持更广泛意义的泛化之间难以平衡。此外，当必须同时删除多个概念时，这些方法面临可扩展性挑战。为了解决这些局限性，我们引入了UnHype，一个将超网络纳入单一和多概念LoRA训练的框架。所提出的架构可以直接插入到稳定扩散以及现代基于流的文本到图像模型中，在这些模型中展示了稳定的训练行为和有效的概念控制。在推理过程中，超网络根据CLIP嵌入动态生成自适应的LoRA权重，从而实现更具上下文感知的可扩展遗忘。我们在多个具有挑战性的任务中评估UnHype，包括对象删除、名人删除和显式内容移除，展示了其有效性和多样性。代码库：https://github.com/gmum/UnHype。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the growing concerns regarding the misuse of large-scale diffusion models in generating harmful content, which highlights the need for effective machine unlearning techniques. The authors propose UnHype, a framework that integrates hypernetworks into Low-Rank Adaptation (LoRA) for both single and multi-concept unlearning, allowing for improved adaptability and scalability. Experimental results show that UnHype effectively manages tasks such as object erasure, celebrity erasure, and explicit content removal, demonstrating stable training behavior and enhanced concept control when applied to models like Stable Diffusion and modern flow-based text-to-image systems.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要有效的机器遗忘，以应对大规模扩散模型在生成有害内容方面的误用问题。作者提出了UnHype框架，将超网络集成到低秩适应（LoRA）中，以实现单一和多概念的遗忘，从而提高适应性和可扩展性。实验结果表明，UnHype在对象删除、名人删除和显式内容移除等任务中表现出色，展示了在包括稳定扩散在内的各种模型中稳定的训练行为和增强的概念控制能力。</div>
</details>
</div>
<div class="card">
<div class="title">LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence</div>
<div class="meta-line">Authors: Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum</div>
<div class="meta-line">First: 2025-09-15T17:59:47+00:00 · Latest: 2026-02-03T11:16:36+00:00</div>
<div class="meta-line">Comments: https://zxyin.github.io/LazyDrag</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12203v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.12203v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zxyin.github.io/LazyDrag">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball&#x27;&#x27;, or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LazyDrag：通过显式对应实现多模态扩散变换器上的稳定拖拽编辑</div>
<div class="mono" style="margin-top:8px">依赖于通过注意力进行隐式点匹配已成为拖拽编辑的核心瓶颈，导致反演强度减弱和昂贵的测试时间优化（TTO）之间的根本妥协。这种妥协严重限制了扩散模型的生成能力，抑制了高保真修补和文本引导创作。在本文中，我们介绍了LazyDrag，这是首个针对多模态扩散变换器的拖拽图像编辑方法，直接消除了对隐式点匹配的依赖。具体而言，我们的方法从用户拖拽输入生成显式对应图，作为可靠参考以增强注意力控制。这一可靠参考为稳定的全强度反演过程打开了潜力，这是拖拽编辑任务中的首次实现。它消除了对TTO的必要性，并解锁了模型的生成能力。因此，LazyDrag自然将精确的几何控制与文本引导统一，能够实现以前无法达到的复杂编辑：打开狗的嘴并修补其内部，生成新物体如“网球”，或对于模糊的拖拽，进行上下文感知的更改，如将手移入口袋。此外，LazyDrag支持多轮工作流程，同时进行移动和缩放操作。在DragBench上的评估中，我们的方法在拖拽准确性和感知质量上超越了基线，得到了VIEScore和人工评估的验证。LazyDrag不仅建立了新的最先进性能，还为编辑范式开辟了新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of implicit point matching in drag-based editing, which compromises inversion strength and increases optimization costs in diffusion models. The authors propose LazyDrag, a novel method that generates an explicit correspondence map from user drag inputs, enhancing attention control and enabling a stable inversion process without the need for costly test-time optimization. Experimental results demonstrate that LazyDrag significantly improves drag accuracy and perceptual quality compared to existing methods, allowing for complex edits and multi-round workflows, thereby establishing new state-of-the-art performance in the field of image editing with Multi-Modal Diffusion Transformers.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决扩散模型中基于拖动的编辑的局限性，特别是隐式点匹配带来的问题，这些问题削弱了反演强度并增加了测试时优化的成本。作者提出了LazyDrag，这是一种新颖的方法，通过用户拖动输入生成显式对应图，增强注意力控制，并实现稳定的反演过程，无需昂贵的优化。实验结果表明，LazyDrag在拖动精度和感知质量方面显著优于现有方法，允许复杂编辑和多轮工作流程，从而在基于拖动的图像编辑中设定了新的标准。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</div>
<div class="meta-line">Authors: Zixin Yin, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, Duomin Wang, Gang Yu, Lionel M. Ni, Lei Zhang, Heung-Yeung Shum</div>
<div class="meta-line">First: 2025-08-12T17:57:04+00:00 · Latest: 2026-02-03T11:16:25+00:00</div>
<div class="meta-line">Comments: https://zxyin.github.io/ColorCtrl</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09131v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.09131v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zxyin.github.io/ColorCtrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无训练的文本引导颜色编辑与多模态扩散变换器</div>
<div class="mono" style="margin-top:8px">图像和视频中的文本引导颜色编辑是一个基本但尚未解决的问题，需要对颜色属性进行细致的操控，包括反照率、光源颜色和环境光，同时保持几何形状、材料属性和光物质交互的物理一致性。现有的无训练方法在编辑任务中具有广泛的适用性，但在精确颜色控制方面存在困难，并且常常在编辑和未编辑区域引入视觉不一致性。在这项工作中，我们提出了ColorCtrl，一种无训练的颜色编辑方法，利用现代多模态扩散变换器（MM-DiT）的注意力机制。通过针对性地操控注意力图和数值标记，我们的方法实现了准确且一致的颜色编辑，并能够在属性强度上进行单词级控制。我们的方法仅修改提示中指定的目标区域，保持无关区域不变。在SD3和FLUX.1-dev上的大量实验表明，ColorCtrl在编辑质量和一致性方面优于现有的无训练方法，并在这两个方面达到了最先进的性能。此外，我们的方法在一致性方面超越了强大的商业模型，如FLUX.1 Kontext Max和GPT-4o图像生成。当扩展到视频模型如CogVideoX时，我们的方法表现出更大的优势，特别是在保持时间一致性和编辑稳定性方面。最后，我们的方法还可以推广到基于指令的编辑扩散模型，如Step1X-Edit和FLUX.1 Kontext dev，进一步展示了其多功能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of text-guided color editing in images and videos, particularly the need for precise manipulation of color attributes while maintaining physical consistency. The authors propose ColorCtrl, a training-free color editing method that utilizes Multi-Modal Diffusion Transformers to manipulate attention maps and value tokens for accurate color editing. Experimental results show that ColorCtrl outperforms existing training-free methods and achieves state-of-the-art performance in edit quality and consistency, even surpassing strong commercial models in terms of consistency, and demonstrating advantages in video editing by maintaining temporal coherence and stability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图像和视频中的文本引导颜色编辑所面临的挑战，这需要在保持物理一致性的同时精确操控颜色属性。作者提出了一种名为ColorCtrl的无训练颜色编辑方法，该方法利用多模态扩散变换器，通过操控注意力图和数值标记来解耦结构和颜色，从而实现准确的颜色编辑。实验结果表明，ColorCtrl在编辑质量和一致性方面超越了现有的无训练方法，并在视频编辑中保持时间一致性方面超过了商业模型，达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Z3D: Zero-Shot 3D Visual Grounding from Images</div>
<div class="meta-line">Authors: Nikita Drozdov, Andrey Lemeshko, Nikita Gavrilov, Anton Konushin, Danila Rukhovich, Maksim Kolodiazhnyi</div>
<div class="meta-line">First: 2026-02-03T10:35:18+00:00 · Latest: 2026-02-03T10:35:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03361v1">PDF</a> · <a href="https://github.com/col14m/z3d">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at https://github.com/col14m/z3d .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Z3D：基于图像的零样本3D视觉定位</div>
<div class="mono" style="margin-top:8px">3D视觉定位（3DVG）旨在根据自然语言查询在3D场景中定位物体。在这项工作中，我们探索仅通过多视角图像进行零样本3DVG，而无需任何几何监督或物体先验。我们引入Z3D，一个通用的定位管道，灵活地处理多视角图像，同时可选地结合相机姿态和深度图。我们识别出先前零样本方法中的关键瓶颈，导致显著的性能下降，并通过（i）一种最先进的零样本3D实例分割方法生成高质量的3D边界框提议，以及（ii）通过基于提示的分割进行高级推理，充分利用现代VLM的能力，来解决这些问题。在ScanRefer和Nr3D基准上的大量实验表明，我们的方法在零样本方法中实现了最先进的性能。代码可在https://github.com/col14m/z3d获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve 3D visual grounding (3DVG) by localizing objects in 3D scenes using natural language queries without relying on geometric supervision or object priors. The authors introduce Z3D, a universal grounding pipeline that processes multi-view images and optionally integrates camera poses and depth maps. Key experimental findings indicate that Z3D outperforms existing zero-shot methods by addressing performance bottlenecks through a state-of-the-art zero-shot 3D instance segmentation method and advanced reasoning techniques, achieving state-of-the-art results on the ScanRefer and Nr3D benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过自然语言查询在3D场景中定位物体，从而增强3D视觉定位（3DVG），而无需依赖几何监督或物体先验。作者提出了Z3D，这是一种通用的定位管道，处理多视角图像，并可选地集成相机姿态和深度图。实验结果表明，Z3D通过一种最先进的零样本3D实例分割技术和基于提示的分割高级推理，解决了先前零样本方法中的显著性能问题，在ScanRefer和Nr3D基准测试中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer</div>
<div class="meta-line">Authors: Junmo Cho, Suhan Kim, Sangjune An, Minsu Kim, Dong Bok Lee, Heejun Lee, Sung Ju Hwang, Hae Beom Lee</div>
<div class="meta-line">First: 2026-02-03T10:30:03+00:00 · Latest: 2026-02-03T10:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03358v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GFlowPO：生成流网络作为语言模型提示优化器</div>
<div class="mono" style="margin-top:8px">为语言模型（LM）寻找有效的提示至关重要，但却 notoriously difficult：提示空间组合庞大，由于目标LM评估成本高，奖励稀疏。然而，现有的基于RL的提示优化器通常依赖于策略更新和从固定分布中采样的元提示，导致样本效率低下。我们提出GFlowPO，一种将提示搜索视为潜在提示的后验推断问题的概率提示优化框架，受元提示参考LM先验的正则化。在第一步中，我们使用基于重放的训练策略微调轻量级提示LM，采用离策略生成流网络（GFlowNet）目标，重用过去的提示评估以实现样本高效探索。在第二步中，我们引入动态记忆更新（DMU），这是一种无训练机制，通过注入（i）来自重放缓冲区的多样化提示和（ii）来自小优先队列的高性能提示，更新元提示，从而逐步将搜索过程集中在高奖励区域。在少量样本文本分类、指令诱导基准和问答任务中，GFlowPO始终优于最近的离散提示优化基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of finding effective prompts for language models, which is challenging due to the vast prompt space and sparse rewards from evaluations. The authors propose GFlowPO, a probabilistic prompt optimization framework that treats prompt search as a posterior inference problem, utilizing a Generative Flow Network for off-policy updates and a Dynamic Memory Update mechanism to enhance sample efficiency. Experimental results demonstrate that GFlowPO outperforms existing discrete prompt optimization methods across various tasks, including few-shot text classification and question answering.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高语言模型的提示优化效率，这一过程因提示空间庞大和评估稀疏奖励而具有挑战性。作者提出了GFlowPO，这是一种将提示搜索视为后验推断问题的概率框架，利用生成流网络进行离线训练以增强样本效率。实验结果表明，GFlowPO在少量样本文本分类和问答等多种任务中，优于现有的离散提示优化方法，通过动态记忆更新机制有效集中搜索于高奖励区域。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T03:50:01+00:00 · Latest: 2026-02-03T10:25:06+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01601v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01601v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可验证奖励的在线强化学习的自适应回合分配</div>
<div class="mono" style="margin-top:8px">采样效率是具有可验证奖励的强化学习中的一个关键瓶颈。现有的基于组的策略优化方法，如GRPO，为所有训练提示分配固定数量的回合。这种统一分配隐含地将所有提示视为同等信息量，可能导致计算预算使用效率低下并阻碍训练进展。我们引入了VIP，一种方差信息预测分配策略，它将给定的回合预算分配给现有批次中的提示，以最小化策略更新的期望梯度方差。在每次迭代中，VIP使用轻量级高斯过程模型根据最近的回合预测每个提示的成功概率。这些概率预测被转化为方差估计，然后输入到一个凸优化问题中，以确定在严格计算预算约束下的最佳回合分配。实证结果表明，VIP在多个基准测试中始终提高了采样效率，并实现了比统一或启发式分配策略更高的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the sampling efficiency challenges in reinforcement learning with verifiable rewards, where existing methods allocate rollouts uniformly across training prompts, potentially leading to inefficient resource use. The authors propose a new strategy called Variance-Informed Predictive (VIP) allocation, which employs a Gaussian process model to predict success probabilities for each prompt and optimally allocate rollouts to minimize expected gradient variance. Experimental results demonstrate that VIP significantly enhances sampling efficiency and outperforms traditional uniform and heuristic allocation methods across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决可验证奖励的强化学习中采样效率的挑战，特别是现有的基于组的策略优化方法在训练提示上均匀分配固定数量的回合的局限性。作者提出了一种新的策略，称为方差信息预测（VIP）分配，该策略利用轻量级高斯过程模型根据最近的回合预测每个提示的成功概率，从而实现更为合理的回合预算分配。实验结果表明，VIP显著提高了采样效率，并在多个基准测试中优于传统的均匀和启发式分配方法。</div>
</details>
</div>
<div class="card">
<div class="title">V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction</div>
<div class="meta-line">Authors: Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Xikun Bao, Lin Chen, Zehui Chen, Qing Miao, Chenxi Liu, Jie Zhao, Feng Zhao</div>
<div class="meta-line">First: 2025-03-22T11:30:46+00:00 · Latest: 2026-02-03T10:18:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://vlm-reasoning.github.io/V2P-Bench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17736v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17736v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vlm-reasoning.github.io/V2P-Bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have made significant strides in the field of video understanding in recent times. Nevertheless, existing video benchmarks predominantly rely on text prompts for evaluation, which often require complex referential language and diminish both the accuracy and efficiency of human model interaction in turn. To address this limitation, we propose V2P-Bench, a robust and comprehensive benchmark for evaluating the ability of LVLMs to understand Video Visual Prompts in human model interaction scenarios. V2P-Bench consists of 980 videos and 1172 well-structured high-quality QA pairs, each paired with manually annotated visual prompt frames. The benchmark spans three main tasks and twelve categories, thereby enabling fine-grained, instance-level evaluation. Through an in-depth analysis of current LVLMs, we identify several key findings: 1) Visual prompts are both more model-friendly and user-friendly in interactive scenarios than text prompts, leading to significantly improved model performance and enhanced user experience. 2) Models are reasonably capable of zero-shot understanding of visual prompts, but struggle with spatiotemporal understanding. Even o1 achieves only 71.8%, far below the human expert score of 88.3%, while most open-source models perform below 60%. 3) LVLMs exhibit pervasive Hack Phenomena in video question answering tasks, which become more pronounced as video length increases and frame sampling density decreases, thereby inflating performance scores artificially. We anticipate that V2P-Bench will not only shed light on these challenges but also serve as a foundational tool for advancing human model interaction and improving the evaluation of video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V2P-Bench：通过视觉提示评估视频语言理解以改善人机交互</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）在视频理解领域取得了显著进展。然而，现有的视频基准主要依赖文本提示进行评估，这通常需要复杂的指代语言，从而降低了人机交互的准确性和效率。为了解决这一局限性，我们提出了V2P-Bench，这是一个强大而全面的基准，用于评估LVLMs在与人机交互场景中理解视频视觉提示的能力。V2P-Bench包含980个视频和1172对结构良好的高质量问答对，每对都配有手动注释的视觉提示帧。该基准涵盖三个主要任务和十二个类别，从而实现细粒度的实例级评估。通过对当前LVLMs的深入分析，我们发现了几个关键发现：1）在交互场景中，视觉提示比文本提示更适合模型和用户，显著提高了模型性能和用户体验。2）模型在零样本理解视觉提示方面具有合理能力，但在时空理解上存在困难。即使o1的得分也仅为71.8%，远低于人类专家的88.3%，而大多数开源模型的表现低于60%。3）LVLMs在视频问答任务中普遍存在黑客现象，随着视频长度增加和帧采样密度降低，这种现象愈发明显，从而人为地抬高了性能得分。我们预计V2P-Bench不仅能揭示这些挑战，还将作为推动人机交互和改善视频理解评估的基础工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the evaluation of large vision-language models (LVLMs) in video understanding, as existing benchmarks primarily use text prompts that complicate human-model interactions. The authors introduce V2P-Bench, a new benchmark consisting of 980 videos and 1172 high-quality question-answer pairs with visual prompts, designed for fine-grained evaluation across three tasks and twelve categories. Key findings reveal that visual prompts enhance model performance and user experience compared to text prompts, while models show reasonable zero-shot understanding of visual prompts but struggle with spatiotemporal comprehension, achieving only 71.8% accuracy compared to 88.3% for human experts, and exhibit performance inflation due to Hack Phenomena in longer videos with lower frame sampling density.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善大型视觉语言模型（LVLMs）在视频理解方面的评估，因为当前基准主要使用文本提示，导致人机交互复杂化。作者提出了V2P-Bench，这是一个新的基准，包含980个视频和1172个高质量问答对，每个都与视觉提示相关，以便更好地进行评估。主要发现表明，与文本提示相比，视觉提示提高了模型性能和用户体验，而模型在视觉提示的零-shot 理解方面表现合理，但在时空理解上存在困难，仅达到71.8%的准确率，而人类专家的准确率为88.3%，此外，LVLMs在视频问答任务中表现出性能膨胀现象，尤其是在视频较长和帧采样密度较低的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">No time to train! Training-Free Reference-Based Instance Segmentation</div>
<div class="meta-line">Authors: Miguel Espinosa, Chenhongyi Yang, Linus Ericsson, Steven McDonagh, Elliot J. Crowley</div>
<div class="meta-line">First: 2025-07-03T16:59:01+00:00 · Latest: 2026-02-03T10:17:35+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02798v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.02798v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无时间训练！无训练参考基础的实例分割</div>
<div class="mono" style="margin-top:8px">图像分割模型的性能历来受到收集大规模标注数据的高成本的限制。Segment Anything Model (SAM) 通过可提示的、与语义无关的分割范式缓解了这一原始问题，但仍然需要手动视觉提示或复杂的领域依赖提示生成规则来处理新图像。为了减少这一新负担，我们的工作研究了在仅提供一小组参考图像的情况下进行物体分割的任务。我们的关键见解是利用基础模型学习到的强语义先验，识别参考图像与目标图像之间的对应区域。我们发现，建立对应关系可以自动生成实例级分割掩码，用于下游任务，并通过一种多阶段、无训练的方法实现我们的想法，该方法包括 (1) 内存库构建；(2) 表示聚合；(3) 语义感知特征匹配。我们的实验显示分割指标显著改善，在 COCO FSOD 上达到最先进的性能 (36.8% nAP)，在 PASCAL VOC Few-Shot 上达到 (71.2% nAP50)，并在跨领域 FSOD 基准上超越现有的无训练方法 (22.4% nAP)。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of high costs associated with collecting large-scale annotated data for image segmentation models. The authors propose a training-free method that utilizes a small set of reference images to perform object segmentation by leveraging strong semantic priors from foundation models to establish correspondences between reference and target images. Experimental results demonstrate significant improvements in segmentation metrics, achieving state-of-the-art performance on COCO FSOD, PASCAL VOC Few-Shot, and surpassing existing training-free methods on the Cross-Domain FSOD benchmark.</div>
<div class="mono" style="margin-top:8px">本研究解决了图像分割模型在收集大规模标注数据时面临的高成本挑战。作者提出了一种无训练的方法，利用一小组参考图像通过利用基础模型的强语义先验来进行物体分割，从而建立参考图像与目标图像之间的对应关系。实验结果表明，在分割指标上显著改善，在COCO FSOD上达到36.8%的nAP，在PASCAL VOC少样本上达到71.2%的nAP50，并在跨域FSOD基准上超过现有的无训练方法，获得22.4%的nAP。</div>
</details>
</div>
<div class="card">
<div class="title">Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution</div>
<div class="meta-line">Authors: Bryan Sangwoo Kim, Jonghyun Park, Jong Chul Ye</div>
<div class="meta-line">First: 2026-02-03T10:09:27+00:00 · Latest: 2026-02-03T10:09:27+00:00</div>
<div class="meta-line">Comments: 13 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平铺提示：克服图像和视频超分辨率中的提示不足问题</div>
<div class="mono" style="margin-top:8px">文本条件扩散模型通过将提示作为语义先验，推动了图像和视频超分辨率的发展，但现代超分辨率流程通常依赖于潜在平铺以扩展到高分辨率，其中单一的全局标题导致提示不足。粗略的全局提示往往错过局部细节（提示稀疏性），并提供局部无关的指导（提示误导），这可能会被无分类器指导放大。我们提出了平铺提示，这是一种统一的图像和视频超分辨率框架，为每个潜在平铺生成特定的提示，并在局部文本条件后验下执行超分辨率，提供高信息指导，以最小的开销解决提示不足问题。在高分辨率真实图像和视频上的实验显示，在感知质量和文本对齐方面持续提高，同时相对于全局提示基线减少了幻觉和平铺级伪影。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of prompt underspecification in image and video super-resolution, which arises when using a single global caption that fails to capture localized details. The authors propose a method called Tiled Prompts, which generates specific prompts for each latent tile, allowing for super-resolution based on locally text-conditioned posteriors. Experimental results demonstrate that this approach consistently improves perceptual quality and text alignment in high-resolution images and videos, while also reducing hallucinations and tile-level artifacts compared to traditional global-prompt methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图像和视频超分辨率中的提示不足问题，该问题源于依赖单一全局提示而无法捕捉局部细节。作者提出了一种名为Tiled Prompts的方法，为每个潜在瓦片生成特定提示，从而允许在局部文本条件后验的指导下进行超分辨率。实验结果表明，该方法在高分辨率图像和视频中持续提高了感知质量和文本对齐，同时相较于传统的全局提示方法，减少了幻觉和瓦片级伪影。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Thought Hijacking</div>
<div class="meta-line">Authors: Jianli Zhao, Tingchen Fu, Rylan Schaeffer, Mrinank Sharma, Fazl Barez</div>
<div class="meta-line">First: 2025-10-30T12:10:03+00:00 · Latest: 2026-02-03T10:05:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26418v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26418v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) improve task performance through extended inference-time reasoning. While prior work suggests this should strengthen safety, we find evidence to the contrary. Long reasoning sequences can be exploited to systematically weaken them. We introduce Chain-of-Thought Hijacking, a jailbreak attack that prepends harmful instructions with extended sequences of benign puzzle reasoning. Across HarmBench, CoT Hijacking achieves attack success rates of 99\%, 94\%, 100\%, and 94\% on Gemini 2.5 Pro, ChatGPT o4 Mini, Grok 3 Mini, and Claude 4 Sonnet. To understand this mechanism, we apply activation probing, attention analysis, and causal interventions. We find that refusal depends on a low-dimensional safety signal that becomes diluted as reasoning grows: mid-layers encode the strength of safety checking, while late layers encode the refusal outcome. These findings demonstrate that explicit chain-of-thought reasoning introduces a systematic vulnerability when combined with answer-prompting cues. We release all evaluation materials to facilitate replication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维链劫持</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过扩展推理时间推理来提高任务性能。尽管先前的研究表明这应该增强安全性，但我们发现相反的证据。长推理序列可以被利用来系统性地削弱它们。我们引入了思维链劫持，这是一种越狱攻击，通过扩展的无害谜题推理序列前置有害指令。在HarmBench上，思维链劫持在Gemini 2.5 Pro、ChatGPT o4 Mini、Grok 3 Mini和Claude 4 Sonnet上的攻击成功率分别为99%、94%、100%和94%。为了理解这一机制，我们应用了激活探测、注意力分析和因果干预。我们发现拒绝依赖于一个低维的安全信号，随着推理的增长而被稀释：中间层编码安全检查的强度，而后期层编码拒绝结果。这些发现表明，显式的思维链推理在与答案提示线索结合时引入了系统性脆弱性。我们发布所有评估材料以促进复制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to investigate the safety implications of Large Reasoning Models (LRMs) that utilize extended inference-time reasoning, as prior studies suggested that such reasoning would enhance safety. The authors introduce a novel attack method called Chain-of-Thought Hijacking, which involves prepending harmful instructions with lengthy sequences of benign reasoning to exploit vulnerabilities in LRMs. Experimental results show that this method achieves high attack success rates of 99%, 94%, 100%, and 94% on various models, including Gemini 2.5 Pro and ChatGPT o4 Mini, revealing that the effectiveness of safety signals diminishes with longer reasoning sequences, thus highlighting a systematic vulnerability in the models when using explicit reasoning combined with answer prompts.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型推理模型（LRMs）在扩展推理序列中被利用的脆弱性，这与认为这种推理增强安全性的观点相反。作者提出了一种新攻击方法，称为思维链劫持，该方法通过将有害指令附加到良性推理序列中来实施攻击。实验结果表明，该方法在包括Gemini 2.5 Pro和ChatGPT o4 Mini等多个模型上实现了99%、94%、100%和94%的高攻击成功率。通过激活探测和注意力分析等技术，研究揭示了安全信号的有效性随着推理时间的延长而减弱，表明模型在显式推理与提示线索结合时存在系统性脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">LaVPR: Benchmarking Language and Vision for Place Recognition</div>
<div class="meta-line">Authors: Ofer Idan, Dan Badur, Yosi Keller, Yoli Shavit</div>
<div class="meta-line">First: 2026-02-03T08:38:38+00:00 · Latest: 2026-02-03T08:38:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03253v1">PDF</a> · <a href="https://github.com/oferidan1/LaVPR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &quot;blind&quot; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LaVPR：语言与视觉在地点识别中的基准测试</div>
<div class="mono" style="margin-top:8px">视觉地点识别（VPR）在极端环境变化和感知别名下常常失败。此外，标准系统无法仅通过口头描述进行“盲”定位，这对于紧急响应等应用至关重要。为了解决这些挑战，我们引入了LaVPR，这是一个大规模基准，扩展了现有的VPR数据集，包含超过650,000个丰富的自然语言描述。使用LaVPR，我们研究了两种范式：多模态融合以增强鲁棒性和跨模态检索以实现基于语言的定位。我们的结果表明，语言描述在视觉退化条件下带来了持续的提升，尤其对较小的骨干网络影响显著。值得注意的是，添加语言使紧凑模型的性能能够与更大规模的仅视觉架构相媲美。对于跨模态检索，我们建立了一个基线，使用低秩适应（LoRA）和多相似性损失，显著优于标准对比方法在视觉-语言模型中的表现。最终，LaVPR使得一种新的定位系统成为可能，这种系统既能抵御现实世界的随机性，又适合资源受限的部署。我们的数据集和代码可在https://github.com/oferidan1/LaVPR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Visual Place Recognition (VPR) systems that struggle with environmental changes and cannot localize based solely on verbal descriptions, which is crucial for applications like emergency response. The authors introduce LaVPR, a benchmark that enhances existing VPR datasets with over 650,000 natural-language descriptions, and explore two approaches: Multi-Modal Fusion for robustness and Cross-Modal Retrieval for language-based localization. Experimental results indicate that incorporating language descriptions significantly improves performance in visually degraded conditions, particularly for smaller models, and that compact models can achieve performance comparable to larger vision-only systems. Additionally, a baseline for cross-modal retrieval using Low-Rank Adaptation and Multi-Similarity loss is established, outperforming traditional contrastive methods in vision-language models, thus paving the way for more resilient localization systems suitable for resource-limited environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉地点识别（VPR）系统，这些系统在极端环境变化下表现不佳，且无法仅依靠语言描述进行定位。作者提出了LaVPR，这是一个大型基准，增强了现有的VPR数据集，包含超过65万个自然语言描述，并研究了多模态融合和跨模态检索方法。实验结果表明，结合语言描述显著提高了在视觉退化条件下的性能，尤其是对较小模型的益处，而使用低秩适应和多相似性损失的跨模态检索方法则优于传统的对比方法，为更强大的定位系统铺平了道路，这些系统适用于现实世界应用和资源限制。</div>
</details>
</div>
<div class="card">
<div class="title">Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization</div>
<div class="meta-line">Authors: Xin Ding, Yun Chen, Yongwei Wang, Kao Zhang, Sen Zhang, Peibei Cao, Xiangxue Wang</div>
<div class="meta-line">First: 2025-08-03T11:36:00+00:00 · Latest: 2026-02-03T08:26:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01725v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.01725v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. To address these issues, we propose CcGAN-AVAR, an enhanced CcGAN framework featuring (1) two novel components for handling data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity size and a multi-task discriminator that enhances generator training through auxiliary regression and density ratio estimation - and (2) the GAN framework&#x27;s native one-step generator, enable 30x-2000x faster inference than CCDM. Extensive experiments on four benchmark datasets (64x64 to 256x256 resolution) across eleven challenging settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应邻域和辅助正则化的失衡鲁棒和采样高效的连续条件GAN</div>
<div class="mono" style="margin-top:8px">最近在条件生成建模方面的进展引入了连续条件生成对抗网络（CcGAN）和连续条件扩散模型（CCDM），用于估计基于标量、连续回归标签（例如角度、年龄或温度）的高维数据分布。然而，这些方法面临着根本性的限制：CcGAN由于固定大小的邻域约束而遭受数据失衡，而CCDM则需要计算成本高昂的迭代采样。为了解决这些问题，我们提出了CcGAN-AVAR，这是一个增强的CcGAN框架，具有（1）两个新组件来处理数据失衡 - 一个动态调整邻域大小的自适应邻域机制和一个通过辅助回归和密度比估计增强生成器训练的多任务判别器 - 以及（2）GAN框架的原生一步生成器，使得推理速度比CCDM快30倍至2000倍。在四个基准数据集（64x64到256x256分辨率）上的广泛实验，涵盖了十一种具有挑战性的设置，证明了CcGAN-AVAR在保持采样效率的同时实现了最先进的生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing conditional generative models, specifically the Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM), which struggle with data imbalance and high computational costs. The authors propose CcGAN-AVAR, an improved CcGAN framework that incorporates an adaptive vicinity mechanism to dynamically adjust vicinity size and a multi-task discriminator to enhance generator training. Experimental results across four benchmark datasets show that CcGAN-AVAR achieves state-of-the-art generation quality while significantly improving sampling efficiency, with inference speeds 30 to 2000 times faster than CCDM.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善条件生成建模，特别是解决连续条件生成对抗网络（CcGAN）和连续条件扩散模型（CCDM）在处理数据不平衡和计算效率低下方面的局限性。作者提出了一种新的框架CcGAN-AVAR，该框架结合了自适应邻域机制以动态调整邻域大小和多任务判别器以增强生成器训练。对四个基准数据集的实验结果表明，CcGAN-AVAR在生成质量上优于其他方法，同时在推理速度上比CCDM快30到2000倍。</div>
</details>
</div>
<div class="card">
<div class="title">PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation</div>
<div class="meta-line">Authors: Jingbang Tang</div>
<div class="meta-line">First: 2026-02-03T07:44:01+00:00 · Latest: 2026-02-03T07:44:01+00:00</div>
<div class="meta-line">Comments: 7 pages, 5 figures. Under review at IJCNN 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03220v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment flexibility.We propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully frozen.PokeFusion Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different backbones.Experiments on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PokeFusion 注意力：增强无参考风格条件生成</div>
<div class="mono" style="margin-top:8px">本文研究了文本到图像扩散模型中的无参考风格条件角色生成，其中高质量合成需要在多样化提示中保持稳定的角色结构和一致的细粒度风格表达。现有方法主要依赖于仅文本提示，这通常对视觉风格的描述不足，容易产生明显的风格漂移和几何不一致，或者引入依赖外部图像的基于参考的适配器，增加了架构复杂性并限制了部署灵活性。我们提出了PokeFusion注意力，这是一种轻量级解码器级交叉注意力机制，直接在扩散解码器内部融合文本语义和学习的风格嵌入。通过在注意力层面解耦文本和风格条件，我们的方法实现了有效的无参考风格化生成，同时保持预训练的扩散主干完全冻结。PokeFusion注意力仅训练解码器交叉注意力层以及一个紧凑的风格投影模块，形成一个参数高效且即插即用的控制组件，可以轻松集成到现有的扩散管道中，并在不同主干之间转移。在一个风格化角色生成基准（宝可梦风格）上的实验表明，与代表性的基于适配器的基线相比，我们的方法在风格保真度、语义对齐和角色形状一致性方面始终有所改善，同时保持低参数开销和推理时的简单性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of reference-free style-conditioned character generation in text-to-image diffusion models, where achieving high-quality synthesis is hindered by issues like style drift and geometric inconsistency. The authors introduce PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that integrates textual semantics with learned style embeddings within the diffusion decoder, allowing for effective stylized generation without relying on external references. Experimental results on a Pokemon-style character generation benchmark show that this method significantly enhances style fidelity, semantic alignment, and character shape consistency compared to existing adapter-based approaches, while also being parameter-efficient and easy to integrate into current diffusion frameworks.</div>
<div class="mono" style="margin-top:8px">本文研究了文本到图像扩散模型中无参考风格条件角色生成的挑战，这些模型通常由于依赖仅基于文本的提示或复杂的基于参考的适配器而遭受风格漂移和几何不一致的问题。作者提出了PokeFusion Attention，这是一种轻量级的解码器级交叉注意机制，它将文本语义与学习到的风格嵌入直接集成到扩散解码器中，从而实现有效的无参考风格生成。实验结果表明，在Pokemon风格角色生成基准测试中，该方法显著提高了风格保真度、语义对齐和角色形状一致性，相比现有的适配器方法具有更高的参数效率，并且易于集成到当前的扩散框架中。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification</div>
<div class="meta-line">Authors: Qinghao Gao, Jiahui Qu, Wenqian Dong</div>
<div class="meta-line">First: 2025-11-14T16:31:37+00:00 · Latest: 2026-02-03T07:34:31+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11460v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.11460v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal remote sensing classification often suffers from missing modalities caused by sensor failures and environmental interference, leading to severe performance degradation. In this work, we rethink missing-modality learning from a conditional computation perspective and investigate whether Mixture-of-Experts (MoE) models can inherently adapt to diverse modality-missing scenarios. We first conduct a systematic study of representative MoE paradigms under various missing-modality settings, revealing both their potential and limitations. Building on these insights, we propose a Missing-aware Mixture-of-LoRAs (MaMOL), a parameter-efficient MoE framework that unifies multiple modality-missing cases within a single model. MaMOL introduces a dual-routing mechanism to decouple modality-invariant shared experts and modality-aware dynamic experts, enabling automatic expert activation conditioned on available modalities. Extensive experiments on multiple remote sensing benchmarks demonstrate that MaMOL significantly improves robustness and generalization under diverse missing-modality scenarios with minimal computational overhead. Transfer experiments on natural image datasets further validate its scalability and cross-domain applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考高效的专家混合模型用于遥感模态缺失分类</div>
<div class="mono" style="margin-top:8px">多模态遥感分类常常受到传感器故障和环境干扰导致的模态缺失的影响，从而导致严重的性能下降。在这项工作中，我们从条件计算的角度重新思考缺失模态学习，并研究专家混合模型（MoE）是否能够自适应于多样的模态缺失场景。我们首先在各种缺失模态设置下对代表性的MoE范式进行系统研究，揭示了它们的潜力和局限性。在这些见解的基础上，我们提出了一种缺失感知的LoRA混合模型（MaMOL），这是一种参数高效的MoE框架，能够在单一模型中统一多种模态缺失情况。MaMOL引入了一种双路由机制，以解耦模态不变的共享专家和模态感知的动态专家，实现基于可用模态的自动专家激活。在多个遥感基准上的广泛实验表明，MaMOL在多样的缺失模态场景下显著提高了鲁棒性和泛化能力，同时计算开销最小。对自然图像数据集的迁移实验进一步验证了其可扩展性和跨领域适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation in multimodal remote sensing classification caused by missing modalities due to sensor failures and environmental factors. The authors propose a Missing-aware Mixture-of-LoRAs (MaMOL) framework, which utilizes a dual-routing mechanism to manage modality-invariant and modality-aware experts, allowing for automatic expert activation based on available modalities. Experimental results on various remote sensing benchmarks show that MaMOL enhances robustness and generalization in scenarios with missing modalities while maintaining low computational costs, and transfer experiments on natural image datasets confirm its scalability and applicability across domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多模态遥感分类中由于传感器故障和环境干扰导致的缺失模态引起的性能下降。作者提出了一种新的框架，称为缺失感知混合LoRA（MaMOL），该框架采用双路由机制来管理模态不变的共享专家和模态感知的动态专家，允许根据可用模态自动激活专家。对多个遥感基准的实验结果表明，MaMOL在不同缺失模态场景下显著提高了鲁棒性和泛化能力，同时保持了较低的计算开销，且在自然图像数据集上的迁移实验确认了其可扩展性和跨领域适用性。</div>
</details>
</div>
<div class="card">
<div class="title">VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers</div>
<div class="meta-line">Authors: Zhiwen Li, Zhongjie Duan, Jinyan Ye, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</div>
<div class="meta-line">First: 2026-02-03T07:27:23+00:00 · Latest: 2026-02-03T07:27:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03210v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at https://anonymous.4open.science/r/VIRAL-744A</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRAL：通过类比在扩散变换器中进行视觉上下文推理</div>
<div class="mono" style="margin-top:8px">在计算机视觉中复制上下文学习（ICL）仍然具有挑战性，因为任务异质性。我们提出了\textbf{VIRAL}，一个通过将ICL表述为通过视觉类比的条件生成（$x_s : x_t :: x_q : y_q$）来引发预训练图像编辑模型的视觉推理的框架。我们使用角色感知的多图像条件适配了一个冻结的扩散变换器（DiT），并引入了混合专家LoRA以减轻不同任务之间的梯度干扰。此外，为了弥补当前视觉上下文数据集的不足，我们策划了一个涵盖感知、恢复和编辑的大规模数据集。实验表明，VIRAL的表现优于现有方法，验证了统一的V-ICL范式能够处理大多数视觉任务，包括开放域编辑。我们的代码可在https://anonymous.4open.science/r/VIRAL-744A获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of replicating In-Context Learning (ICL) in computer vision due to the diversity of tasks. The authors propose VIRAL, a framework that utilizes a pre-trained image editing model to perform visual reasoning through conditional generation via visual analogy. They adapt a frozen Diffusion Transformer and introduce a Mixture-of-Experts LoRA to reduce gradient interference across various tasks. Experimental results indicate that VIRAL surpasses existing methods, demonstrating its effectiveness in managing a wide range of visual tasks, including open-domain editing.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于任务多样性而导致的在计算机视觉中复制上下文学习（ICL）的挑战。作者提出了一个名为VIRAL的框架，该框架利用预训练的图像编辑模型，通过基于视觉类比的条件生成来促进视觉推理。通过适应冻结的扩散变换器，并使用角色感知的多图像条件和实施混合专家LoRA以减少梯度干扰，作者还策划了一个包含各种视觉任务的大规模数据集。实验结果表明，VIRAL超越了现有方法，证明了统一的V-ICL方法在广泛的视觉任务中，包括开放域编辑的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
