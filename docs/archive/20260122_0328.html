<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 03:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0328</div>
    <div class="row"><div class="card">
<div class="title">APEX-Agents</div>
<div class="meta-line">Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</div>
<div class="meta-line">First: 2026-01-20T18:53:44+00:00 · Latest: 2026-01-20T18:53:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-Agents</div>
<div class="mono" style="margin-top:8px">我们介绍了代理的人工智能生产力指数（APEX-Agents），这是一个评估人工智能代理是否能够执行由投资银行分析师、管理顾问和企业律师创建的长期跨应用任务的基准。APEX-Agents要求代理在具有文件和工具的现实工作环境中导航。我们使用Pass@1测试了八个代理以进行排行榜。Gemini 3 Flash（思维=高）获得最高分24.0%，其次是GPT-5.2（思维=高）、Claude Opus 4.5（思维=高）和Gemini 3 Pro（思维=高）。我们开源了APEX-Agents基准（n=480），包括所有提示、评分标准、金标准输出、文件和元数据。我们还开源了Archipelago，这是我们用于代理执行和评估的基础设施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation behind APEX-Agents is to create a benchmark for evaluating AI agents&#x27; ability to perform complex, long-term tasks typical in fields such as investment banking and corporate law. The main method involves testing eight different AI agents in realistic work environments using a metric called Pass@1. The key experimental findings reveal that Gemini 3 Flash (Thinking=High) achieved the highest score of 24.0%, outperforming other agents like GPT-5.2 and Claude Opus 4.5, with the benchmark and all related materials being made publicly available for further research.</div>
<div class="mono" style="margin-top:8px">APEX-Agents研究的动机在于评估人工智能代理在执行投资银行、管理咨询和企业法律等专业人士通常承担的复杂长期任务方面的能力。研究人员开发了人工智能生产力指数代理（APEX-Agents）作为基准，要求代理在使用各种文件和工具的现实工作环境中操作。在实验中，测试了八个代理，其中Gemini 3 Flash（Thinking=High）以24.0%的最高分领先，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro，均在相同的评估条件下进行。该基准包含480个任务及所有相关材料，已开源以供进一步研究和开发。</div>
</details>
</div>
<div class="card">
<div class="title">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</div>
<div class="meta-line">Authors: Rotem Gatenyo, Ohad Fried</div>
<div class="meta-line">First: 2026-01-20T18:12:55+00:00 · Latest: 2026-01-20T18:12:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复制-变换-粘贴：由视觉-语言和几何约束引导的零-shot物体-物体对齐</div>
<div class="mono" style="margin-top:8px">我们研究了给定两个网格的零-shot 3D 对齐，使用描述其空间关系的文本提示——这是内容创作和场景组装的基本能力。早期的方法主要依赖几何对齐程序，而最近的工作利用预训练的2D扩散模型来建模语言条件的物体-物体空间关系。相比之下，我们在测试时直接优化相对姿态，通过可微渲染器使用CLIP驱动的梯度更新平移、旋转和各向同性缩放，而无需训练新模型。我们的框架通过几何感知目标增强语言监督：一种软迭代最近点（ICP）项的变体以鼓励表面附着，以及一个穿透损失以抑制相互穿透。分阶段的调度随着时间的推移加强接触约束，摄像机控制将优化集中在交互区域。为了实现评估，我们策划了一个包含多样类别和关系的基准，并与基线进行比较。我们的方法优于所有替代方案，产生语义上可信和物理上合理的对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance zero-shot 3D alignment of meshes using text prompts that describe their spatial relations, which is crucial for content creation and scene assembly. The authors propose a method that optimizes the relative pose of the meshes at test time by updating translation, rotation, and isotropic scale using CLIP-driven gradients through a differentiable renderer, without the need for retraining a model. Experimental results demonstrate that their approach, which incorporates language supervision and geometry-aware objectives, outperforms existing methods, achieving semantically accurate and physically plausible alignments across a curated benchmark of diverse categories and relations.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用文本提示描述空间关系的网格零-shot 3D 对齐问题，这对于内容创作和场景组装至关重要。作者提出了一种方法，通过在测试时优化网格的相对姿态，利用 CLIP 驱动的梯度通过可微渲染器更新平移、旋转和缩放，而无需重新训练模型。实验结果表明，他们的方法结合了语言监督和几何感知目标，显著优于现有方法，在多样化类别和关系的基准测试中实现了语义准确和物理合理的对齐。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionAgent: Navigating Expert Models for Agentic Image Generation</div>
<div class="meta-line">Authors: Jie Qin, Jie Wu, Weifeng Chen, Yueming Lyu</div>
<div class="meta-line">First: 2024-01-18T15:30:58+00:00 · Latest: 2026-01-20T18:02:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.10061v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.10061v2">PDF</a> · <a href="https://github.com/DiffusionAgent/DiffusionAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the accelerating era of human-instructed visual content creation, diffusion models have demonstrated remarkable generative potential. Yet their deployment is constrained by a dual bottleneck: semantic ambiguity in diverse prompts and the narrow specialization of individual models. A single diffusion architecture struggles to maintain optimal performance across heterogeneous prompts, while conventional &quot;parse-then-call&quot; pipelines artificially separate semantic understanding from generative execution. To bridge this gap, we introduce DiffusionAgent, a unified, language-model-driven agent that casts the entire &quot;prompt comprehension-expert routing-image synthesis&quot; loop into a agentic framework. Our contributions are three-fold: (1) a tree-of-thought-powered expert navigator that performs fine-grained semantic parsing and zero-shot matching to the most suitable diffusion model via an extensible prior-knowledge tree; (2) an advantage database updated with human-in-the-loop feedback, continually aligning model-selection policy with human aesthetic and semantic preferences; and (3) a fully decoupled agent architecture that activates the optimal generative path for open-domain prompts without retraining or fine-tuning any expert. Extensive experiments show that DiffusionAgent retains high generation quality while significantly broadening prompt coverage, establishing a new performance and generality benchmark for multi-domain image synthesis. The code is available at https://github.com/DiffusionAgent/DiffusionAgent</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionAgent：为代理图像生成导航专家模型</div>
<div class="mono" style="margin-top:8px">在人类指导的视觉内容创作加速发展的时代，扩散模型展现了显著的生成潜力。然而，它们的应用受到双重瓶颈的限制：多样提示中的语义模糊性和单个模型的狭窄专业化。单一的扩散架构难以在异构提示中保持最佳性能，而传统的“解析-再调用”管道人为地将语义理解与生成执行分开。为了解决这一问题，我们引入了DiffusionAgent，一个统一的、基于语言模型的代理，将整个“提示理解-专家路由-图像合成”循环转化为一个代理框架。我们的贡献有三方面：（1）一个基于思维树的专家导航器，通过可扩展的先验知识树进行细粒度语义解析和零-shot匹配到最合适的扩散模型；（2）一个更新了人类反馈的优势数据库，持续将模型选择策略与人类的审美和语义偏好对齐；（3）一个完全解耦的代理架构，激活开放域提示的最佳生成路径，而无需重新训练或微调任何专家。大量实验表明，DiffusionAgent在显著扩大提示覆盖范围的同时保持了高生成质量，为多领域图像合成建立了新的性能和通用性基准。代码可在 https://github.com/DiffusionAgent/DiffusionAgent 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of diffusion models in generating images from diverse prompts due to semantic ambiguity and the specialization of individual models. The authors propose DiffusionAgent, a unified agent that integrates prompt comprehension, expert routing, and image synthesis into a single framework. Key experimental findings indicate that DiffusionAgent maintains high image generation quality while significantly expanding the range of prompts it can effectively handle, thereby setting a new benchmark for performance and generality in multi-domain image synthesis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决扩散模型在处理多样化提示时，由于语义模糊和单一模型的专业化而导致的图像生成限制。作者提出了DiffusionAgent，这是一种将提示理解、专家路由和图像合成整合为一个统一框架的代理。关键实验结果表明，DiffusionAgent在保持高图像生成质量的同时，显著扩展了其有效处理的提示范围，从而为多领域图像合成设定了新的性能和通用性基准。</div>
</details>
</div>
<div class="card">
<div class="title">DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision</div>
<div class="meta-line">Authors: Wenlin Chen, Mingtian Zhang, Jiajun He, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber</div>
<div class="meta-line">First: 2025-02-11T23:02:14+00:00 · Latest: 2026-01-20T17:24:34+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures, 5 tables, 2 algorithms</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.08005v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.08005v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based distillation methods (e.g., variational score distillation) train one-step diffusion models by first pre-training a teacher score model and then distilling it into a one-step student model. However, the gradient estimator in the distillation stage usually suffers from two sources of bias: (1) biased teacher supervision due to score estimation error incurred during pre-training, and (2) the student model&#x27;s score estimation error during distillation. These biases can degrade the quality of the resulting one-step diffusion model. To address this, we propose DiffRatio, a new framework for training one-step diffusion models: instead of estimating the teacher and student scores independently and then taking their difference, we directly estimate the score difference as the gradient of a learned log density ratio between the student and data distributions across diffusion time steps. This approach greatly simplifies the training pipeline, significantly reduces gradient estimation bias, and improves one-step generation quality. Additionally, it also reduces auxiliary network size by using a lightweight density-ratio network instead of two full score networks, which improves computational and memory efficiency. DiffRatio achieves competitive one-step generation results on CIFAR-10 and ImageNet (64x64 and 512x512), outperforming most teacher-supervised distillation approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffRatio：无教师监督训练一步扩散模型</div>
<div class="mono" style="margin-top:8px">基于评分的蒸馏方法（例如，变分评分蒸馏）通过首先预训练教师评分模型，然后将其蒸馏为一步学生模型来训练一步扩散模型。然而，蒸馏阶段的梯度估计器通常受到两种偏差的影响：（1）由于预训练期间产生的评分估计误差导致的偏倚教师监督，以及（2）蒸馏过程中学生模型的评分估计误差。这些偏差会降低最终一步扩散模型的质量。为了解决这个问题，我们提出了DiffRatio，一个新的训练一步扩散模型的框架：我们直接估计学生和数据分布在扩散时间步长之间的对数密度比的梯度作为评分差，而不是独立估计教师和学生的评分然后取其差值。这种方法大大简化了训练流程，显著减少了梯度估计偏差，并提高了一步生成质量。此外，它还通过使用轻量级密度比网络而不是两个完整的评分网络来减少辅助网络的大小，从而提高计算和内存效率。DiffRatio在CIFAR-10和ImageNet（64x64和512x512）上实现了具有竞争力的一步生成结果，超越了大多数教师监督的蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the training of one-step diffusion models, which often suffer from biases due to errors in teacher supervision and student score estimation during the distillation process. The authors propose a novel framework called DiffRatio, which directly estimates the score difference as the gradient of a learned log density ratio between the student and data distributions, thereby simplifying the training process and reducing gradient estimation bias. Experimental results demonstrate that DiffRatio achieves competitive one-step generation performance on CIFAR-10 and ImageNet datasets, surpassing most existing teacher-supervised distillation methods while also enhancing computational and memory efficiency by utilizing a lightweight density-ratio network.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有的基于分数的蒸馏方法在训练一步扩散模型时存在的局限性，这些方法由于教师监督和学生分数估计中的错误而受到偏差的影响。作者提出了DiffRatio，这是一种新颖的框架，通过直接估计学生和数据分布之间的学习对数密度比的梯度来简化训练过程并减少梯度估计偏差。实验结果表明，DiffRatio在CIFAR-10和ImageNet数据集上实现了具有竞争力的一步生成性能，超越了大多数传统的教师监督蒸馏方法，同时通过减少辅助网络的规模提高了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion</div>
<div class="meta-line">Authors: Yitong Dong, Qi Zhang, Minchao Jiang, Zhiqiang Wu, Qingnan Fan, Ying Feng, Huaqi Zhang, Hujun Bao, Guofeng Zhang</div>
<div class="meta-line">First: 2026-01-20T17:11:55+00:00 · Latest: 2026-01-20T17:11:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14161v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一-shot 精炼器：通过一步扩散提升前馈新视图合成</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的高保真新视图合成（NVS）框架，旨在从稀疏图像中解决近期基于视觉变换器（ViT）骨干的前馈3D高斯点云（3DGS）方法中的关键限制。虽然基于ViT的管道提供了强大的几何先验，但由于计算成本，它们通常受到低分辨率输入的限制。此外，现有的生成增强方法往往对3D无关，导致视图间结构不一致，尤其是在未见区域。为克服这些挑战，我们设计了一个双域细节感知模块，使得在不受ViT骨干限制的情况下处理高分辨率图像，并赋予高斯额外特征以存储高频细节。我们开发了一个特征引导的扩散网络，可以在恢复过程中保持高频细节。我们引入了一种统一的训练策略，使得基于ViT的几何骨干和基于扩散的精炼模块能够联合优化。实验表明，我们的方法能够在多个数据集上保持优越的生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing feed-forward 3D Gaussian Splatting methods in novel view synthesis from sparse images, particularly their reliance on low-resolution inputs and 3D-agnostic generative enhancements. The authors propose a Dual-Domain Detail Perception Module to manage high-resolution images and enhance Gaussian features for high-frequency detail retention, alongside a feature-guided diffusion network to preserve these details during restoration. Experimental results indicate that their unified training strategy significantly improves generation quality across various datasets.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高稀疏图像的高保真新视图合成（NVS），解决现有基于视觉变换器（ViT）骨干网的前馈3D高斯点云方法在低分辨率输入和3D无关生成增强方面的局限性。作者提出了一种双域细节感知模块，以处理高分辨率图像，并为高斯分布增强特征以保留高频细节，同时引入了一种特征引导的扩散网络，以在恢复过程中保留这些细节。实验结果表明，所提出的方法在多个数据集上实现了优越的生成质量，有效克服了识别出的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">GalaxyEdit: Large-Scale Image Editing Dataset with Enhanced Diffusion Adapter</div>
<div class="meta-line">Authors: Aniruddha Bala, Rohan Jaiswal, Siddharth Roheda, Rohit Chowdhury, Loay Rashid</div>
<div class="meta-line">First: 2024-11-21T02:48:38+00:00 · Latest: 2026-01-20T17:00:13+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.13794v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.13794v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training of large-scale text-to-image and image-to-image models requires a huge amount of annotated data. While text-to-image datasets are abundant, data available for instruction-based image-to-image tasks like object addition and removal is limited. This is because of the several challenges associated with the data generation process, such as, significant human effort, limited automation, suboptimal end-to-end models, data diversity constraints and high expenses. We propose an automated data generation pipeline aimed at alleviating such limitations, and introduce GalaxyEdit - a large-scale image editing dataset for add and remove operations. We fine-tune the SD v1.5 model on our dataset and find that our model can successfully handle a broader range of objects and complex editing instructions, outperforming state-of-the-art methods in FID scores by 11.2\% and 26.1\% for add and remove tasks respectively. Furthermore, in light of on-device usage scenarios, we expand our research to include task-specific lightweight adapters leveraging the ControlNet-xs architecture. While ControlNet-xs excels in canny and depth guided generation, we propose to improve the communication between the control network and U-Net for more intricate add and remove tasks. We achieve this by enhancing ControlNet-xs with non-linear interaction layers based on Volterra filters. Our approach outperforms ControlNet-xs in both add/remove and canny-guided image generation tasks, highlighting the effectiveness of the proposed enhancement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GalaxyEdit：增强扩散适配器的大规模图像编辑数据集</div>
<div class="mono" style="margin-top:8px">大规模文本到图像和图像到图像模型的训练需要大量标注数据。虽然文本到图像的数据集丰富，但用于基于指令的图像到图像任务（如对象添加和移除）的数据有限。这是由于数据生成过程中的多个挑战，例如，显著的人力投入、有限的自动化、次优的端到端模型、数据多样性限制和高昂的费用。我们提出了一种自动化数据生成管道，旨在缓解这些限制，并引入GalaxyEdit——一个用于添加和移除操作的大规模图像编辑数据集。我们在我们的数据集上微调SD v1.5模型，发现我们的模型能够成功处理更广泛的对象和复杂的编辑指令，在添加和移除任务中，FID分数分别超越了最先进的方法11.2%和26.1%。此外，考虑到设备上的使用场景，我们将研究扩展到包括利用ControlNet-xs架构的任务特定轻量级适配器。虽然ControlNet-xs在Canny和深度引导生成方面表现出色，但我们提出改善控制网络与U-Net之间的通信，以应对更复杂的添加和移除任务。我们通过基于Volterra滤波器的非线性交互层增强ControlNet-xs，取得了在添加/移除和Canny引导图像生成任务中的优于ControlNet-xs的表现，突显了所提增强的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the scarcity of annotated data for instruction-based image-to-image tasks, particularly for object addition and removal, which poses challenges in training large-scale models. To address this issue, the authors propose an automated data generation pipeline and introduce GalaxyEdit, a large-scale dataset specifically designed for these editing operations. Experimental results show that fine-tuning the SD v1.5 model on GalaxyEdit leads to significant improvements, achieving 11.2% and 26.1% better FID scores than state-of-the-art methods for add and remove tasks, respectively. Additionally, the study explores lightweight adapters using the ControlNet-xs architecture, enhancing communication between the control network and U-Net with non-linear interaction layers, resulting in superior performance in both add/remove and canny-guided image generation tasks compared to ControlNet-xs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于指令基础的图像到图像任务（特别是物体添加和移除操作）缺乏标注数据，这带来了高成本和有限自动化等挑战。作者提出了一种自动化数据生成管道，并引入了GalaxyEdit，一个专门为这些任务设计的大规模图像编辑数据集。他们在该数据集上微调了SD v1.5模型，取得了显著的性能提升，添加和移除任务的FID分数分别比最先进的方法提高了11.2%和26.1%。此外，他们通过非线性交互层增强ControlNet-xs架构，以改进任务特定的轻量级适配器，在添加/移除和基于边缘引导的图像生成任务中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion</div>
<div class="meta-line">Authors: Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe</div>
<div class="meta-line">First: 2026-01-20T15:13:43+00:00 · Latest: 2026-01-20T15:13:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14056v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POCI-Diff：基于3D布局引导扩散的一致性和交互性物体定位</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于扩散的文本到图像（T2I）生成方法，具有一致性和交互性的3D布局控制和编辑。虽然先前的方法通过使用2D线索或迭代的复制-扭曲-粘贴策略来改善空间遵循性，但它们往往会扭曲物体几何形状，并未能在编辑中保持一致性。为了解决这些局限性，我们引入了一种一致性和交互性物体定位框架（POCI-Diff），这是一种在统一扩散过程中共同强制执行3D几何约束和实例级语义绑定的新颖公式。我们的方法通过将单个文本描述绑定到特定的3D边界框，允许通过混合潜在扩散实现每个物体的显式语义控制，从而实现复杂多物体场景的一次性合成。我们进一步提出了一种无扭曲的生成编辑管道，支持通过再生而非像素变形进行物体插入、移除和变换。为了在编辑中保持物体的身份和一致性，我们使用IP-Adapter对扩散过程进行条件化，确保在交互式3D编辑中物体外观的一致性，同时保持全局场景的一致性。实验结果表明，POCI-Diff生成的图像与指定的3D布局和编辑一致，且在视觉保真度和布局遵循性方面超越了最先进的方法，同时消除了扭曲引起的几何伪影。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the consistency and interactivity of 3D layout control in Text-to-Image generation, addressing limitations of previous methods that distort object geometry and lack coherence across edits. The authors propose a novel diffusion-based framework called POCI-Diff, which integrates 3D geometric constraints and semantic binding within a unified process, allowing for explicit control of individual objects through Blended Latent Diffusion. Experimental results indicate that POCI-Diff generates high-quality images that adhere to specified 3D layouts and edits, outperforming existing methods in visual fidelity and layout consistency while avoiding warping artifacts.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有的文本到图像生成方法在编辑过程中难以保持物体几何形状和一致性。作者提出了一种名为POCI-Diff的新型扩散框架，该框架结合了3D几何约束和语义绑定，以增强空间遵循性和交互性。实验结果表明，POCI-Diff生成的高质量图像与指定的3D布局和编辑一致，在视觉保真度和布局一致性方面超越了当前的最先进技术，同时避免了与变形相关的几何失真。</div>
</details>
</div>
<div class="card">
<div class="title">Kakugo: Distillation of Low-Resource Languages into Small Language Models</div>
<div class="meta-line">Authors: Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga, Barry Haddow</div>
<div class="meta-line">First: 2026-01-20T15:05:44+00:00 · Latest: 2026-01-20T15:05:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14051v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14051v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Kakugo：将低资源语言提炼为小型语言模型</div>
<div class="mono" style="margin-top:8px">我们提出了Kakugo，这是一种新颖且具有成本效益的流程，旨在仅使用语言名称作为输入，为低资源语言训练通用小型语言模型（SLMs）。通过使用大型教师模型生成合成提示和翻译指令数据集，我们为54种低资源语言生成了训练数据和SLMs。在包括翻译、分类和问答在内的多样化自然语言处理任务的评估中，我们的流程始终提高了基础模型的性能。Kakugo以每种语言不到50美元的总生成和训练成本，为社区提供了一种可访问的方法来开发特定语言的人工智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create an efficient and cost-effective method for training Small Language Models (SLMs) for low-resource languages. The authors developed Kakugo, a pipeline that utilizes a large teacher model to generate synthetic prompts and translate instruction datasets, enabling the production of training data for 54 low-resource languages. Experimental results show that the models trained using Kakugo outperform base models across various natural language processing tasks, with a total cost of under $50 per language, making it a viable option for communities to develop language-specific AI solutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为低资源语言创建一种高效的训练小型语言模型（SLM）的方法，这些语言通常缺乏足够的数据进行有效的模型训练。作者开发了Kakugo，一个利用大型教师模型生成合成提示和翻译指令数据集的管道，从而为54种低资源语言生成训练数据。实验结果表明，使用Kakugo训练的模型在各种自然语言处理任务中显著优于基础模型，总成本低于每种语言50美元，使其成为社区开发特定语言AI的可行解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis</div>
<div class="meta-line">Authors: Andrea Rigo, Luca Stornaiuolo, Mauro Martino, Bruno Lepri, Nicu Sebe</div>
<div class="meta-line">First: 2025-04-18T15:21:37+00:00 · Latest: 2026-01-20T15:03:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.13745v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.13745v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have revolutionized text-to-image (T2I) synthesis, producing high-quality, photorealistic images. However, they still struggle to properly render the spatial relationships described in text prompts. To address the lack of spatial information in T2I generations, existing methods typically use external network conditioning and predefined layouts, resulting in higher computational costs and reduced flexibility. Our approach builds upon a curated dataset of spatially explicit prompts, meticulously extracted and synthesized from LAION-400M to ensure precise alignment between textual descriptions and spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible fine-tuning framework based on Low-Rank Adaptation, specifically designed to enhance spatial consistency in generative models without increasing generation time or compromising the quality of the outputs. In addition to ESPLoRA, we propose refined evaluation metrics grounded in geometric constraints, capturing 3D spatial relations such as &quot;in front of&quot; or &quot;behind&quot;. These metrics also expose spatial biases in T2I models which, even when not fully mitigated, can be strategically exploited by our TORE algorithm to further improve the spatial consistency of generated images. Our method outperforms CoMPaSS, the current baseline framework, on spatial consistency benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESPLoRA：在文本到图像扩散模型中通过低秩适应增强空间精度以实现高清合成</div>
<div class="mono" style="margin-top:8px">扩散模型彻底改变了文本到图像（T2I）合成，生成高质量、逼真的图像。然而，它们在正确渲染文本提示中描述的空间关系方面仍然存在困难。为了解决T2I生成中缺乏空间信息的问题，现有方法通常使用外部网络条件和预定义布局，导致更高的计算成本和降低的灵活性。我们的方法基于一个经过精心策划的空间明确提示数据集，仔细提取和合成自LAION-400M，以确保文本描述与空间布局之间的精确对齐。除了这个数据集，我们还提出了ESPLoRA，一个基于低秩适应的灵活微调框架，专门设计用于增强生成模型中的空间一致性，而不增加生成时间或影响输出质量。除了ESPLoRA，我们还提出了基于几何约束的精细评估指标，捕捉诸如“在前面”或“在后面”等3D空间关系。这些指标还揭示了T2I模型中的空间偏差，即使在未完全缓解的情况下，也可以通过我们的TORE算法进行战略性利用，以进一步改善生成图像的空间一致性。我们的方法在空间一致性基准测试中优于当前基线框架CoMPaSS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the spatial relationships in text-to-image synthesis, which are often inadequately rendered by existing diffusion models. The authors introduce ESPLoRA, a fine-tuning framework based on Low-Rank Adaptation, utilizing a curated dataset of spatially explicit prompts from LAION-400M to enhance spatial consistency without increasing generation time or compromising image quality. Experimental results demonstrate that ESPLoRA outperforms the current baseline framework, CoMPaSS, on benchmarks measuring spatial consistency, while also revealing spatial biases in T2I models that can be strategically addressed using the proposed TORE algorithm.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高文本到图像合成中的空间准确性，尽管扩散模型已经取得了进展，但这一挑战依然存在。作者提出了一种名为ESPLoRA的新框架，该框架利用从LAION-400M提取的空间明确提示的策划数据集，并采用低秩适应进行生成模型的微调。实验结果表明，ESPLoRA显著提高了生成图像的空间一致性，而不增加生成时间或牺牲输出质量，在空间一致性基准测试中超越了现有的基线框架CoMPaSS。</div>
</details>
</div>
<div class="card">
<div class="title">Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution</div>
<div class="meta-line">Authors: Samuel W. Remedios, Zhangxing Bian, Shuwen Wei, Aaron Carass, Jerry L. Prince, Blake E. Dewey</div>
<div class="meta-line">First: 2026-01-20T14:53:20+00:00 · Latest: 2026-01-20T14:53:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14030v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多图像MRI超分辨率的可分离扩散推断</div>
<div class="mono" style="margin-top:8px">扩散模型是解决成像逆问题的当前最先进技术。它们令人印象深刻的生成能力使其能够近似从先验分布中采样，这与已知的似然函数结合，允许在不重新训练模型的情况下进行后验采样。尽管最近的方法在提高后验采样的准确性方面取得了进展，但大多数集中于单图像逆问题。然而，对于磁共振成像（MRI）等模态，通常会获取多个互补测量，每个测量在不同轴上都是低分辨率。在这项工作中，我们将常见的基于扩散的单图像逆问题求解器推广到多图像超分辨率（MISR）MRI。我们表明，DPS似然校正允许在独立获取的测量之间进行完全可分离的梯度分解，从而实现MISR，而无需构建联合算子、修改扩散模型或增加网络函数评估。我们推导了DPS、DMAP、DPPS和基于扩散的PnP/ADMM的MISR版本，并在$4\times/8\times/16\times$各向异性降解上展示了相较于SISR的显著提升。我们的结果实现了各向异性MRI体积的最先进超分辨率，并且关键的是，能够从常规的2D多切片采集中重建近各向同性的解剖结构，而这些在正交视图中通常高度退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance super-resolution techniques for multi-image magnetic resonance imaging (MRI), as existing methods primarily address single-image inverse problems. The authors propose a generalization of diffusion-based models to facilitate multi-image super-resolution (MISR) without the need for joint operator construction or modifications to the diffusion model. Key experimental findings indicate that their approach, utilizing the DPS likelihood correction, achieves significant improvements over single-image super-resolution (SISR) methods, resulting in state-of-the-art super-resolution for anisotropic MRI volumes and enabling the reconstruction of near-isotropic anatomy from standard 2D multi-slice acquisitions, which typically suffer from degradation in orthogonal views.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高多图像磁共振成像（MRI）的超分辨率技术，因为现有方法主要针对单图像逆问题。作者提出了一种使用扩散模型的通用方法，允许在不需要联合算子构建或对扩散模型进行修改的情况下实现多图像超分辨率（MISR）。关键实验结果表明，他们的方法通过引入梯度分解的似然校正，在各种各向异性降解下超分辨率性能显著优于单图像方法，成功从低分辨率的二维多切片MRI获取中重建近各向同性的解剖结构。</div>
</details>
</div>
<div class="card">
<div class="title">SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration</div>
<div class="meta-line">Authors: Jiangwei Xie, Zhang Wen, Mike Davies, Dongdong Chen</div>
<div class="meta-line">First: 2026-01-20T14:01:13+00:00 · Latest: 2026-01-20T14:01:13+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13987v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13987v1">PDF</a> · <a href="https://github.com/xuwayyy/SHARE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE&#x27;s core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SHARE：一种完全无监督的单幅高光谱图像恢复框架</div>
<div class="mono" style="margin-top:8px">高光谱图像（HSI）恢复是计算成像和计算机视觉中的一个基本挑战。它涉及不适定的逆问题，如图像修复和超分辨率。尽管深度学习方法通过数据驱动学习改变了这一领域，但其有效性依赖于对精心策划的真实数据集的访问。这在现实场景中限制了其适用性，因为此类数据往往不可用。本文提出了SHARE（具有等变性的单幅高光谱图像恢复），这是一个完全无监督的框架，将几何等变性原理与低秩光谱建模相结合，以消除对真实数据的需求。SHARE的核心概念是利用高光谱结构在可微几何变换（如旋转和缩放）下的内在不变性，通过等变性一致性约束推导自我监督信号。我们新颖的动态自适应光谱注意力（DASA）模块通过显式编码HSI的全局低秩特性，并通过可学习的注意力机制自适应地细化局部光谱-空间相关性，进一步增强了这一范式转变。在HSI图像修复和超分辨率任务上的大量实验表明SHARE的有效性。我们的方法超越了许多最先进的无监督方法，并达到了与监督方法相当的性能。我们希望我们的方法能为HSI恢复和更广泛的科学成像场景带来新的启示。代码将发布在https://github.com/xuwayyy/SHARE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of hyperspectral image (HSI) restoration, which often relies on curated ground-truth datasets that are not available in real-world applications. The authors propose SHARE, a fully unsupervised framework that integrates geometric equivariance principles with low-rank spectral modeling to eliminate the need for such datasets. Experimental results show that SHARE effectively performs HSI inpainting and super-resolution, outperforming many state-of-the-art unsupervised methods and achieving performance levels comparable to supervised techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决高光谱图像（HSI）恢复中的挑战，这通常依赖于在现实世界中不可用的精心策划的真实数据集。作者提出了SHARE，一个完全无监督的框架，将几何等变原理与低秩光谱建模相结合，以消除对这些数据集的需求。实验结果表明，SHARE在HSI修复和超分辨率任务中表现出色，超越了许多现有的无监督方法，并取得了与监督技术相当的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models</div>
<div class="meta-line">Authors: Nikita Kuzmin, Songting Liu, Kong Aik Lee, Eng Siong Chng</div>
<div class="meta-line">First: 2026-01-20T13:23:44+00:00 · Latest: 2026-01-20T13:23:44+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13948v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Stream-Voice-Anon：通过神经音频编解码器和语言模型增强实时说话者匿名化的实用性</div>
<div class="mono" style="margin-top:8px">保护说话者身份对在线语音应用至关重要，但流式说话者匿名化（SA）仍然未被充分探索。最近的研究表明，神经音频编解码器（NAC）提供了优越的说话者特征解耦和语言保真度。NAC还可以与因果语言模型（LM）结合使用，以增强语言保真度和流式任务的提示控制。然而，现有的基于NAC的在线LM系统是为语音转换（VC）而设计的，缺乏隐私保护所需的技术。在这些进展的基础上，我们提出了Stream-Voice-Anon，它通过整合匿名化技术，专门为流式SA调整现代基于因果LM的NAC架构。我们的匿名化方法结合了伪说话者表示采样、说话者嵌入混合和多样的提示选择策略，以利用量化内容代码的解耦特性，防止说话者信息泄露。此外，我们比较了动态和固定延迟配置，以探索实时场景中的延迟-隐私权衡。在VoicePrivacy 2024挑战协议下，Stream-Voice-Anon在可懂度（相对WER减少高达46%）和情感保留（相对UAR高达28%）方面相比于之前的最先进流式方法DarkStream取得了显著改善，同时保持了可比的延迟（180ms对比200ms）和对懒惰知情攻击者的隐私保护，尽管在对半知情攻击者的情况下显示出15%的相对降级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance speaker anonymity in real-time voice applications, an area that has not been thoroughly explored despite its importance. The authors developed Stream-Voice-Anon, which adapts neural audio codec (NAC) architectures for streaming speaker anonymization by integrating techniques such as pseudo-speaker representation sampling and speaker embedding mixing. Experimental results demonstrate that Stream-Voice-Anon significantly improves intelligibility with a 46% relative reduction in word error rate and emotion preservation with a 28% relative increase in unweighted accuracy compared to the previous method, DarkStream, while maintaining similar latency and effective privacy protection against lazy-informed attackers, although it shows some degradation against semi-informed attackers.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强实时语音应用中的说话人匿名化，尽管这一领域对隐私保护至关重要，但尚未得到充分探索。作者提出了Stream-Voice-Anon，该方法结合了神经音频编解码器（NAC）架构和因果语言模型（LM），通过整合隐私保护技术来改善流媒体说话人匿名化。实验结果表明，Stream-Voice-Anon在可懂度方面显著提高，相较于之前的方法DarkStream，词错误率减少了46%，情感保留率提高了28%，同时保持了相似的延迟，并有效保护了对懒惰知情攻击者的隐私，尽管在对半知情攻击者的情况下显示出15%的相对降级。</div>
</details>
</div>
<div class="card">
<div class="title">OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting</div>
<div class="meta-line">Authors: Michail Spanakis, Iason Oikonomidis, Antonis Argyros</div>
<div class="meta-line">First: 2026-01-20T11:36:38+00:00 · Latest: 2026-01-20T11:36:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13871v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mikespanak.github.io/OCCAM_counter">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCCAM：无类别、无训练、无先验且多类别的物体计数</div>
<div class="mono" style="margin-top:8px">无类别物体计数（CAC）涉及在图像中计数任意类别的物体实例。由于其实际重要性，CAC近年来受到越来越多的关注。大多数现有方法假设每个图像只有一个物体类别，依赖于对大型深度学习模型的广泛训练，并通过结合额外信息（如视觉示例或文本提示）来解决问题。本文提出了OCCAM，这是首个无训练的CAC方法，无需任何补充信息。此外，我们的方法解决了问题的多类别变体，能够计数图像中任意物体类别的每个类别的物体实例。我们利用了Segment Anything Model 2（SAM2）这一基础模型，以及基于阈值的自定义版本的第一整数邻居聚类层次（FINCH）算法，在广泛使用的基准数据集FSC-147和CARPK上实现了竞争性能。我们提出了一个合成多类别数据集和F1分数作为更合适的评估指标。我们的方法代码和提出的合成数据集将公开发布在https://mikespanak.github.io/OCCAM_counter。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of Class-Agnostic object Counting (CAC), which is crucial for various applications but often relies on extensive training and additional information. The authors introduce OCCAM, a novel training-free method that counts object instances across multiple classes in an image without requiring supplementary data. Experimental results demonstrate that OCCAM achieves competitive performance on benchmark datasets FSC-147 and CARPK, utilizing the Segment Anything Model 2 and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy algorithm, while also proposing a synthetic multi-class dataset and F1 score as a more appropriate evaluation metric.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决类无关物体计数（CAC）面临的挑战，这对于各种应用至关重要，但通常依赖于大量训练和额外信息。作者提出了OCCAM，这是一种新颖的无训练方法，可以在图像中计数多个物体类别的实例，而无需补充数据。OCCAM利用Segment Anything Model 2（SAM2）和修改版的第一整数邻居聚类层次（FINCH）算法，在基准数据集FSC-147和CARPK上展示了竞争力的性能，同时还提出了一个合成多类数据集和F1分数作为更合适的评估指标。</div>
</details>
</div>
<div class="card">
<div class="title">Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments</div>
<div class="meta-line">Authors: Glinskaya Maria</div>
<div class="meta-line">First: 2026-01-20T10:59:44+00:00 · Latest: 2026-01-20T10:59:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13846v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13846v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>虚拟城市主义：一个基于人工智能的城市身份量化框架。基于东京的扩散生成合成环境的试点研究</div>
<div class="mono" style="margin-top:8px">本文介绍了虚拟城市主义（VU），这是一个多模态的人工智能驱动分析框架，通过合成城市复制品量化城市身份。该框架旨在推进计算上可处理的城市身份指标。为了证明可行性，提出了试点研究虚拟城市主义与东京微观世界。使用集成稳定扩散和LoRA模型的管道生成了九个东京地区的合成复制品，呈现为动态合成城市序列，排除了现有的方向标，以引发核心身份形成元素。人类评估实验（I）评估了复制品的感知合法性；（II）量化了区域级身份；（III）推导了核心身份形成元素。结果显示平均识别准确率约为81%，确认了复制品的有效性。城市身份水平（UIL）指标使得能够评估各区域的身份水平，而语义分析揭示了文化嵌入的类型作为核心身份形成元素，使VU成为一个可行的人工智能增强城市分析框架，勾勒出通向自动化、多参数身份指标的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivates the need for a systematic approach to quantify urban identity using synthetic urban replicas. The authors developed the Virtual Urbanism (VU) framework, which integrates Stable Diffusion and LoRA models to create dynamic synthetic representations of nine areas in Tokyo, intentionally omitting orientation markers to focus on core identity elements. Experimental results indicated a mean identification accuracy of approximately 81%, validating the synthetic replicas, while the Urban Identity Level (UIL) metric facilitated the assessment of identity levels across different areas, revealing culturally embedded typologies as key identity-forming elements and establishing VU as a promising tool for AI-enhanced urban analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个框架，通过合成城市复制品来量化城市身份。作者介绍了虚拟城市主义（VU），这是一个基于人工智能的分析框架，并在东京进行了一项试点研究，使用集成稳定扩散和LoRA模型的管道创建九个东京地区的动态合成表示。主要实验结果表明，合成复制品的平均识别准确率约为81%，验证了其感知合法性，而城市身份水平（UIL）指标则允许对不同地区的身份水平进行评估，揭示了作为核心身份形成元素的文化嵌入类型。</div>
</details>
</div>
<div class="card">
<div class="title">DroneVLA: VLA based Aerial Manipulation</div>
<div class="meta-line">Authors: Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:08:00+00:00 · Latest: 2026-01-20T10:08:00+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system&#x27;s efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DroneVLA：基于VLA的空中操作</div>
<div class="mono" style="margin-top:8px">随着空中平台从被动观察者演变为主动操作者，挑战转向设计直观的界面，使非专业用户能够自然地指挥这些系统。本研究介绍了一种新颖的自主空中操作系统，能够解释高层次自然语言命令以检索物体并将其交付给人类用户。该系统旨在集成基于Grounding DINO的MediaPipe和视觉-语言-动作（VLA）模型与一架配备1-DOF抓手和Intel RealSense RGB-D相机的定制无人机。VLA执行语义推理以解释用户提示的意图，并生成优先任务队列以抓取场景中的相关物体。使用Grounding DINO和动态A*规划算法进行导航和安全重新定位物体。为了确保在交接阶段的安全和自然交互，系统采用由MediaPipe驱动的人本控制器。该模块提供实时人类姿态估计，使无人机能够利用视觉伺服保持在用户正前方的稳定、清晰位置，促进舒适的交接。我们通过实际实验展示了系统在定位和导航方面的有效性，结果显示最大、平均欧几里得误差和均方根误差分别为0.164m、0.070m和0.084m，突显了VLA在空中操作中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for intuitive interfaces that enable non-expert users to control aerial manipulation systems. The authors developed an autonomous aerial manipulation system that interprets high-level natural language commands using a combination of a MediaPipe-based Grounding DINO and a Vision-Language-Action (VLA) model integrated with a custom drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. Experimental results demonstrated the system&#x27;s effectiveness in localization and navigation, achieving maximum, mean, and root-mean squared errors of 0.164m, 0.070m, and 0.084m respectively, indicating the practical applicability of VLA in aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强空中平台的能力，使其从被动观察者转变为能够被非专业用户轻松控制的主动操作者。作者开发了一种自主空中操控系统，该系统能够解释高级自然语言命令以检索和交付物体，整合了基于MediaPipe的Grounding DINO和视觉-语言-动作（VLA）模型，并与一架配备1-DOF抓手和Intel RealSense RGB-D相机的定制无人机相结合。实验结果表明，该系统在定位和导航方面的有效性，最大、平均和均方根误差分别为0.164米、0.070米和0.084米，确认了VLA在空中操控任务中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Latent Diffusion for Graphs via Laplacian Autoencoders</div>
<div class="meta-line">Authors: Antoine Siraudin, Christopher Morris</div>
<div class="meta-line">First: 2026-01-20T09:37:53+00:00 · Latest: 2026-01-20T09:37:53+00:00</div>
<div class="meta-line">Comments: Preprint, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\times$ speed-up.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于拉普拉斯自编码器的图的原则性潜在扩散</div>
<div class="mono" style="margin-top:8px">图扩散模型在图生成中实现了最先进的性能，但在节点数量上存在二次复杂性——而且它们的大部分能力在稀疏图中建模边缺失时被浪费。受到其他模态中潜在扩散的启发，一个自然的想法是将图压缩到低维潜在空间并在其中进行扩散。然而，与图像或文本不同，图生成需要几乎无损的重构，因为即使在解码邻接矩阵时出现一个错误也会使整个样本无效。这个挑战在很大程度上仍未得到解决。我们提出了LG-Flow，一个潜在图扩散框架，直接克服这些障碍。一个置换等变自编码器将每个节点映射到一个固定维度的嵌入，从中可以证明完整的邻接矩阵是可恢复的，从而实现无损重构，适用于无向图和有向无环图（DAG）。这种潜在表示的维度与节点数量线性缩放，消除了二次瓶颈，使得训练更大和更具表现力的模型成为可能。在这个潜在空间中，我们训练了一个具有流匹配的扩散变换器，实现高效且富有表现力的图生成。我们的方法在与最先进的图扩散模型的竞争中取得了可比的结果，同时实现了高达$1000\times$的加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies of graph diffusion models in generating graphs, particularly their quadratic complexity in relation to the number of nodes and the challenge of ensuring nearly lossless reconstruction of adjacency matrices. The authors propose LG-Flow, a latent graph diffusion framework that utilizes a permutation-equivariant autoencoder to compress graphs into a low-dimensional latent space, allowing for efficient diffusion and near-lossless reconstruction of both undirected graphs and directed acyclic graphs (DAGs). Experimental results demonstrate that LG-Flow achieves competitive performance against existing state-of-the-art graph diffusion models while providing up to a 1000-fold increase in speed, effectively overcoming the limitations of previous approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图扩散模型在节点数量上存在的二次复杂性问题，并且通常在稀疏图中浪费容量来建模缺失的边。作者提出了LG-Flow，一种潜在图扩散框架，利用置换等变自编码器将图压缩到低维潜在空间，从而实现邻接矩阵的近乎无损重构。实验结果表明，该方法不仅消除了传统方法的二次瓶颈，还在性能上与最先进的模型相当，速度提升高达1000倍。</div>
</details>
</div>
<div class="card">
<div class="title">ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars</div>
<div class="meta-line">Authors: Ziqiao Peng, Yi Chen, Yifeng Ma, Guozhen Zhang, Zhiyao Sun, Zixiang Zhou, Youliang Zhang, Zhengguang Zhou, Zhaoxin Fan, Hongyan Liu, Yuan Zhou, Qinglin Lu, Jun He</div>
<div class="meta-line">First: 2025-12-22T16:28:27+00:00 · Latest: 2026-01-20T08:45:35+00:00</div>
<div class="meta-line">Comments: Project Page: https://ziqiaopeng.github.io/ActAvatar/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19546v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19546v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ziqiaopeng.github.io/ActAvatar/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model&#x27;s text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActAvatar：面向时间的精确动作控制框架用于对话头像</div>
<div class="mono" style="margin-top:8px">尽管对话头像生成取得了显著进展，但现有方法面临关键挑战：对多样化动作的文本跟随能力不足、动作与音频内容之间缺乏时间对齐，以及对额外控制信号（如姿态骨架）的依赖。我们提出了ActAvatar，一个通过捕捉动作语义和时间上下文实现动作控制相位级精度的框架。我们的方法引入了三个核心创新：（1）相位感知交叉注意力（PACA），将提示分解为全局基础块和时间锚定的相位块，使模型能够集中于与相位相关的标记，实现精确的时间-语义对齐；（2）渐进式音频-视觉对齐，将模态影响与分层特征学习过程对齐——早期层优先考虑文本以建立动作结构，而深层则强调音频以细化唇部动作，防止模态干扰；（3）一种两阶段训练策略，首先在多样化数据上建立稳健的音频-视觉对应关系，然后通过对结构化注释的微调注入动作控制，保持音频-视觉对齐和模型的文本跟随能力。大量实验表明，ActAvatar在动作控制和视觉质量方面显著优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing talking avatar generation methods, particularly their inadequate text-following capabilities, poor temporal alignment between actions and audio, and reliance on additional control signals. The authors introduce ActAvatar, a framework that utilizes Phase-Aware Cross-Attention (PACA) for precise action control by decomposing prompts into relevant components, and employs Progressive Audio-Visual Alignment to enhance the interaction between text and audio features during model training. Experimental results indicate that ActAvatar significantly surpasses current state-of-the-art techniques in both action control accuracy and visual quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有谈话头像生成方法的局限性，特别是它们在文本跟随能力、动作与音频的时间对齐不足以及对额外控制信号的依赖。作者提出了ActAvatar，一个利用文本指导实现精确阶段级动作控制的框架，通过捕捉动作语义和时间上下文来实现。主要创新包括：相位感知交叉注意力以改善时间语义对齐，渐进式音频-视觉对齐以平衡特征学习过程中的模态影响，以及两阶段训练策略以增强音频-视觉对应关系和动作控制。实验结果表明，ActAvatar在动作控制和视觉质量方面显著超越了最先进的技术。</div>
</details>
</div>
<div class="card">
<div class="title">Edit2Restore:Few-Shot Image Restoration via Parameter-Efficient Adaptation of Pre-trained Editing Models</div>
<div class="meta-line">Authors: M. Akın Yılmaz, Ahmet Bilican, Burak Can Biner, A. Murat Tekalp</div>
<div class="meta-line">First: 2026-01-06T19:56:16+00:00 · Latest: 2026-01-20T08:08:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03391v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03391v2">PDF</a> · <a href="https://github.com/makinyilmaz/Edit2Restore">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image restoration has traditionally required training specialized models on thousands of paired examples per degradation type. We challenge this paradigm by demonstrating that powerful pre-trained text-conditioned image editing models can be efficiently adapted for multiple restoration tasks through parameter-efficient fine-tuning with remarkably few examples. Our approach fine-tunes LoRA adapters on FLUX.1 Kontext, a state-of-the-art 12B parameter flow matching model for image-to-image translation, using only 16-128 paired images per task, guided by simple text prompts that specify the restoration operation. Unlike existing methods that train specialized restoration networks from scratch with thousands of samples, we leverage the rich visual priors already encoded in large-scale pre-trained editing models, dramatically reducing data requirements while maintaining high perceptual quality. A single unified LoRA adapter, conditioned on task-specific text prompts, effectively handles multiple degradations including denoising, deraining, and dehazing. Through comprehensive ablation studies, we analyze: (i) the impact of training set size on restoration quality, (ii) trade-offs between task-specific versus unified multi-task adapters, (iii) the role of text encoder fine-tuning, and (iv) zero-shot baseline performance. While our method prioritizes perceptual quality over pixel-perfect reconstruction metrics like PSNR/SSIM, our results demonstrate that pre-trained image editing models, when properly adapted, offer a compelling and data-efficient alternative to traditional image restoration approaches, opening new avenues for few-shot, prompt-guided image enhancement. The code to reproduce our results are available at: https://github.com/makinyilmaz/Edit2Restore</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Edit2Restore：通过参数高效适应预训练编辑模型实现少样本图像修复</div>
<div class="mono" style="margin-top:8px">图像修复传统上需要在每种退化类型上训练数千个配对示例的专用模型。我们挑战这一范式，展示强大的预训练文本条件图像编辑模型可以通过参数高效的微调，使用极少的示例有效适应多种修复任务。我们的方法在FLUX.1 Kontext上微调LoRA适配器，这是一个用于图像到图像翻译的最先进的12B参数流匹配模型，每个任务仅使用16-128个配对图像，并通过简单的文本提示指导修复操作。与现有方法从头开始训练数千个样本的专用修复网络不同，我们利用已经在大规模预训练编辑模型中编码的丰富视觉先验，显著减少数据需求，同时保持高感知质量。一个统一的LoRA适配器，基于特定任务的文本提示，有效处理多种退化，包括去噪、去雨和去雾。通过全面的消融研究，我们分析了：（i）训练集大小对修复质量的影响，（ii）任务特定适配器与统一多任务适配器之间的权衡，（iii）文本编码器微调的作用，以及（iv）零样本基线性能。虽然我们的方法优先考虑感知质量而非像素完美重建指标如PSNR/SSIM，但我们的结果表明，经过适当调整的预训练图像编辑模型提供了一种引人注目且数据高效的替代传统图像修复方法的新途径，为少样本、提示引导的图像增强开辟了新方向。重现我们结果的代码可在：https://github.com/makinyilmaz/Edit2Restore</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to challenge the traditional requirement of training specialized models on extensive datasets for image restoration tasks. The authors propose a method that utilizes parameter-efficient fine-tuning of pre-trained text-conditioned image editing models, specifically adapting LoRA adapters on the FLUX.1 Kontext model with only 16-128 paired images per restoration task. Key experimental findings indicate that this approach significantly reduces data requirements while maintaining high perceptual quality, effectively handling multiple degradation types such as denoising, deraining, and dehazing, and demonstrating that pre-trained models can serve as a viable alternative to conventional restoration methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于挑战传统上需要在大量数据集上训练专门模型的图像修复任务。作者提出了一种方法，利用预训练的文本条件图像编辑模型的参数高效微调，特别是在FLUX.1 Kontext模型上适应LoRA适配器，仅需16-128对图像进行每个任务的训练。关键实验结果表明，这种方法显著减少了数据需求，同时在去噪、去雨和去雾等各种修复任务中保持了高感知质量，展示了利用大规模模型中现有视觉先验进行少样本图像增强的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation</div>
<div class="meta-line">Authors: Boyuan Cao, Xingbo Yao, Chenhui Wang, Jiaxin Ye, Yujie Wei, Hongming Shan</div>
<div class="meta-line">First: 2026-01-20T07:33:16+00:00 · Latest: 2026-01-20T07:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13683v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13683v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态差分线性注意力：增强线性扩散变换器以实现高质量图像生成</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）已成为高保真图像生成的强大架构，但自注意力的二次成本构成了主要的可扩展性瓶颈。为了解决这个问题，采用了线性注意力机制以降低计算成本；不幸的是，结果线性扩散变换器（LiTs）模型往往以生成性能为代价，常常产生过于平滑的注意力权重，限制了表现力。在本研究中，我们引入了动态差分线性注意力（DyDiLA），这是一种新颖的线性注意力公式，通过缓解过平滑问题和提高生成质量来增强LiTs的有效性。具体而言，DyDiLA的新颖性体现在三个关键设计上：（i）动态投影模块，通过动态分配知识来促进令牌表示的解耦；（ii）动态测量核，通过动态分配核函数进行令牌处理，提供更好的相似性测量，以捕捉令牌之间的细粒度语义差异；（iii）令牌差分算子，通过计算令牌之间的差异及其对应的动态测量核产生的信息冗余，能够实现更强大的查询到键的检索。为了利用DyDiLA，我们引入了一种精细化的LiT，称为DyDi-LiT，系统地整合了我们的进展。大量实验表明，DyDi-LiT在多个指标上始终优于当前的最先进（SOTA）模型，突显了其强大的实际潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generative performance of linear diffusion transformers (LiTs) for high-fidelity image generation, addressing the scalability issues posed by the quadratic cost of self-attention. The authors propose a novel linear attention formulation called Dynamic Differential Linear Attention (DyDiLA), which includes a dynamic projection module, a dynamic measure kernel, and a token differential operator to enhance the effectiveness of LiTs by mitigating oversmoothing and improving generation quality. Experimental results demonstrate that the refined LiT, DyDi-LiT, consistently outperforms existing state-of-the-art models across various metrics, highlighting its significant practical potential.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高线性扩散变换器（LiTs）在高保真图像生成中的生成性能，而LiTs由于自注意力的平方成本而面临可扩展性问题。作者提出了一种新颖的线性注意机制，称为动态差分线性注意（DyDiLA），该机制包括动态投影模块、动态测量核和令牌差分算子，以通过解决过平滑问题和提高生成质量来增强LiTs的有效性。实验结果表明，改进后的LiT，DyDi-LiT，在多个指标上始终优于现有的最先进模型，突显了其显著的实际潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models</div>
<div class="meta-line">Authors: Zhenhua Xu, Haobo Zhang, Zhebo Wang, Qichen Liu, Haitao Xu, Wenpeng Xing, Meng Han</div>
<div class="meta-line">First: 2026-01-13T03:41:28+00:00 · Latest: 2026-01-20T06:49:45+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08189v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08189v2">PDF</a> · <a href="https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark">Code1</a> · <a href="https://github.com/Xuzhenhua55/ForgetMark">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce \textsc{ForgetMark}, a stealthy fingerprinting framework that encodes provenance via targeted unlearning. It builds a compact, human-readable key--value set with an assistant model and predictive-entropy ranking, then trains lightweight LoRA adapters to suppress the original values on their keys while preserving general capabilities. Ownership is verified under black/gray-box access by aggregating likelihood and semantic evidence into a fingerprint success rate. By relying on probabilistic forgetting traces rather than fixed trigger--response patterns, \textsc{ForgetMark} avoids high-perplexity triggers, reduces detectability, and lowers false triggers. Across diverse architectures and settings, it achieves 100\% ownership verification on fingerprinted models while maintaining standard performance, surpasses backdoor baselines in stealthiness and robustness to model merging, and remains effective under moderate incremental fine-tuning. Our code and data are available at \href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ForgetMark：通过目标性遗忘在语言模型中隐秘地嵌入指纹</div>
<div class="mono" style="margin-top:8px">现有的侵入性（后门）指纹面临高困惑度触发器易被过滤、启发式检测器暴露的固定响应模式以及在良性输入上的虚假激活等问题。我们提出了\textsc{ForgetMark}，一个通过目标性遗忘编码来源的隐秘指纹框架。它构建了一个紧凑的人类可读的键值集，使用辅助模型和预测熵排名，然后训练轻量级LoRA适配器，在保留一般能力的同时抑制其键上的原始值。在黑/灰盒访问下，通过将可能性和语义证据聚合为指纹成功率来验证所有权。通过依赖概率遗忘痕迹而非固定的触发-响应模式，\textsc{ForgetMark}避免了高困惑度触发器，降低了可检测性，并减少了虚假触发。在多种架构和设置中，它在指纹模型上实现了100\%的所有权验证，同时保持标准性能，超越了在隐秘性和对模型合并的鲁棒性方面的后门基线，并在适度增量微调下仍然有效。我们的代码和数据可在\href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing backdoor fingerprinting methods, which are often detectable and ineffective due to high-perplexity triggers and fixed response patterns. The authors propose ForgetMark, a fingerprinting framework that utilizes targeted unlearning to create a compact key-value set and employs lightweight LoRA adapters to suppress original values while maintaining model performance. The experimental results demonstrate that ForgetMark achieves 100% ownership verification across various architectures, outperforms traditional backdoor methods in terms of stealthiness and robustness, and remains effective even with moderate incremental fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有后门指纹方法的局限性，这些方法通常容易被检测且表现出固定的响应模式。作者提出了ForgetMark，这是一种利用目标遗忘的指纹框架，创建紧凑的键值集，并采用轻量级的LoRA适配器来抑制原始值，同时保持模型性能。实验结果表明，ForgetMark在各种架构中实现了100%的所有权验证，在隐蔽性和鲁棒性方面超越了传统的后门方法，并且即使在适度的增量微调下仍然有效。</div>
</details>
</div>
<div class="card">
<div class="title">Academic journals&#x27; AI policies fail to curb the surge in AI-assisted academic writing</div>
<div class="meta-line">Authors: Yongyuan He, Yi Bu</div>
<div class="meta-line">First: 2025-12-07T07:30:53+00:00 · Latest: 2026-01-20T04:40:14+00:00</div>
<div class="meta-line">Comments: 39 pages, 10 figures, and 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.06705v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.06705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers&#x27; use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (~0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学术期刊的人工智能政策未能遏制人工智能辅助学术写作的激增</div>
<div class="mono" style="margin-top:8px">生成性人工智能迅速融入学术写作，促使期刊和出版商广泛响应政策。然而，这些政策的有效性仍不明确。我们分析了5114本期刊和超过520万篇论文，以评估人工智能使用指南的实际影响。我们发现，尽管70%的期刊采用了人工智能政策（主要要求披露），研究人员在各学科中使用人工智能写作工具的情况却显著增加，且有无政策的期刊之间没有显著差异。非英语国家、物理科学和高开放获取期刊的增长率最高。关键的是，对164,000篇科学出版物的全文分析揭示了显著的透明度差距：自2023年以来发表的75,000篇论文中，仅有76篇（约0.1%）明确披露了人工智能的使用。我们的研究结果表明，当前政策在促进透明度或限制人工智能采用方面基本失败。我们呼吁重新评估伦理框架，以促进人工智能在科学中的负责任整合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of generative AI in academic writing has led to the implementation of policies by journals and publishers, yet their effectiveness is questionable. This study analyzes 5,114 journals and over 5.2 million papers to assess the impact of AI usage guidelines. The findings reveal that while 70% of journals have adopted AI policies, the use of AI writing tools has surged across various disciplines, with no significant difference in usage between journals with and without such policies. Notably, only 0.1% of the 75,000 papers published since 2023 disclosed AI use, indicating a significant transparency gap and suggesting that existing policies have not succeeded in promoting transparency or limiting AI adoption, prompting a call for a reassessment of ethical frameworks for responsible AI integration in academia.</div>
<div class="mono" style="margin-top:8px">随着生成性人工智能在学术写作中的广泛应用，各个期刊和出版商实施了多项政策，但其有效性仍然存在疑问。本研究分析了5114个期刊和超过520万篇论文，以评估人工智能使用指南的影响。研究结果显示，尽管70%的期刊已采用人工智能政策，但各学科中人工智能写作工具的使用急剧增加，且有无政策的期刊之间并无显著差异，尤其是在非英语国家和物理科学领域。值得注意的是，对164,000篇出版物的全文分析显示透明度存在显著差距，自2023年以来发布的75,000篇论文中仅有76篇（约0.1%）明确披露了人工智能的使用，这表明当前政策未能有效促进透明度或限制人工智能的采用，呼吁重新评估学术界负责任的人工智能整合的伦理框架。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models</div>
<div class="meta-line">Authors: Ahmad Al-Zuraiqi</div>
<div class="meta-line">First: 2026-01-20T04:10:57+00:00 · Latest: 2026-01-20T04:10:57+00:00</div>
<div class="meta-line">Comments: 27 pages, 8 figures, 16 tables. Decoder-only transformers (124M-20B parameters). Complete experimental results and reproducibility details in appendices. Code and checkpoints: https://github.com/zuraiqi/neural-organ-transplant</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13580v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13580v1">PDF</a> · <a href="https://github.com/zuraiqi/neural-organ-transplant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets (&quot;donor organs&quot;) from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经器官移植（NOT）：基于检查点的变换器模型模块适应</div>
<div class="mono" style="margin-top:8px">我们介绍了神经器官移植（NOT），一种模块适应框架，使训练好的变换器层能够作为可重用的可转移检查点用于领域适应。与传统的微调方法将训练参数紧密耦合到特定模型实例和训练数据不同，NOT 从预训练模型中提取连续的层子集（“供体器官”），在领域特定数据上独立训练，并将其保存为独立的检查点文件，可以在不访问原始训练数据的情况下移植到兼容的接收模型中。通过对三种仅解码器的变换器架构（参数范围从124M到20B，GPT-2、TinyLlama和GPT-OSS）的实验，我们证明供体移植显著优于现有的适应方法，在困惑度上实现了比LoRA高一个数量级的改进，同时训练速度显著更快。该方法表现出位置依赖性，早期插入位置产生最佳结果。在十亿参数规模的跨领域转移中揭示了意想不到的正则化效益。这些发现表明，变换器中间层可以支持仅解码器架构的高效模块转移，通过检查点分发实现隐私保护的专业知识共享。我们注意到该方法目前仅限于仅解码器模型；对基于编码器的架构的初步实验显示效果降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more efficient domain adaptation methods for transformer models, moving beyond traditional fine-tuning approaches. The authors propose a modular adaptation framework called Neural Organ Transplantation (NOT), which allows for the extraction of trained transformer layer subsets, termed &#x27;donor organs,&#x27; that can be independently trained on domain-specific data and later transplanted into compatible models. Experimental results across three decoder-only transformer architectures demonstrate that this method significantly outperforms existing adaptation techniques, achieving substantial improvements in perplexity and training speed, particularly when donor organs are inserted at optimal positions, while also revealing unexpected benefits in cross-domain transfer at a large scale.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要更高效的变换器模型领域适应方法，这些方法不依赖于紧密耦合的训练参数。作者提出了神经器官移植（NOT），这是一种模块化适应框架，允许将训练好的变换器层提取为可重用的检查点，可以在特定领域数据上独立训练，并移植到兼容模型中。对三种仅解码器的变换器架构的实验结果表明，供体移植显著优于现有的适应方法，在困惑度上取得了显著改善，并且训练时间更快，最佳结果出现在早期插入位置，并且在十亿参数规模的跨领域转移中显示出意想不到的正则化效益。</div>
</details>
</div>
<div class="card">
<div class="title">Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs</div>
<div class="meta-line">Authors: Jackson Kaunismaa, Avery Griffin, John Hughes, Christina Q. Knight, Mrinank Sharma, Erik Jones</div>
<div class="meta-line">First: 2026-01-20T02:24:44+00:00 · Latest: 2026-01-20T02:24:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13528v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13528v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过微调受保护输出引发有害能力</div>
<div class="mono" style="margin-top:8px">模型开发者在前沿模型中实施安全措施以防止误用，例如，通过使用分类器过滤危险输出。在这项工作中，我们展示了即使是经过强有力保护的模型也可以通过引发攻击来引出开源模型中的有害能力。我们的引发攻击包括三个阶段：（i）构建与目标有害任务相邻领域的提示，这些提示不请求危险信息；（ii）从受保护的前沿模型获取这些提示的响应；（iii）在这些提示-输出对上微调开源模型。由于请求的提示不能直接造成伤害，因此前沿模型的安全措施不会拒绝它们。我们在危险化学合成和处理领域评估这些引发攻击，并展示我们的攻击恢复了基础开源模型与不受限制的前沿模型之间约40%的能力差距。然后我们展示，引发攻击的有效性与前沿模型的能力和生成的微调数据量成比例。我们的工作展示了通过输出级安全措施减轻生态系统级风险的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to understand the limitations of safeguards implemented in frontier models to prevent misuse. The authors employ a method involving three stages of elicitation attacks: creating prompts in related domains that do not request harmful information, obtaining responses from safeguarded models, and fine-tuning open-source models on these prompt-output pairs. The findings reveal that these attacks can recover approximately 40% of the capability gap between open-source models and unrestricted frontier models, highlighting the challenges in mitigating risks associated with output-level safeguards in model ecosystems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨受保护模型在引发开源模型有害能力方面的脆弱性。作者采用的方法包括三个阶段：构建与目标危险任务相关的非有害提示，从受保护的前沿模型获取响应，以及使用这些提示-输出对对开源模型进行微调。关键实验结果表明，他们的引导攻击可以恢复基础开源模型与不受限制的前沿模型之间约40%的能力差距，突显了输出级别保护措施在减轻生态系统风险方面的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation</div>
<div class="meta-line">Authors: Amine Rostane</div>
<div class="meta-line">First: 2026-01-19T23:37:10+00:00 · Latest: 2026-01-19T23:37:10+00:00</div>
<div class="meta-line">Comments: 19 pages, includes figures and tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13462v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker&#x27;s abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialBench-UC：基于不确定性的空间提示跟随评估在文本到图像生成中的应用</div>
<div class="mono" style="margin-top:8px">评估文本到图像模型是否遵循明确的空间指令难以自动化。物体检测器可能会漏掉目标或返回多个合理的检测结果，简单的几何测试在边界情况下可能变得模糊。空间评估本质上是一个选择性预测问题，检查者在证据薄弱时可能会选择不作判断，并报告置信度，以便结果可以被解释为风险覆盖权衡，而不是单一分数。我们引入了SpatialBench-UC，这是一个小型、可重复的成对空间关系基准。该基准包含200个提示（50个物体对乘以4个关系），分为100个通过交换物体角色获得的反事实对。我们发布了基准包、版本化提示、固定配置、每个样本的检查器输出和报告表，使得跨模型的可重复和可审计比较成为可能。我们还包括一个轻量级的人类审计，用于校准检查者的弃权边际和置信阈值。我们评估了三个基线，Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN。检查者报告通过率和覆盖率，以及对已决定样本的条件通过率。结果表明，基础方法显著提高了通过率和覆盖率，而弃权仍然是一个主要因素，主要由于漏检。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in automating the evaluation of text-to-image models&#x27; adherence to spatial instructions, as existing methods often struggle with ambiguous cases. The authors introduce SpatialBench-UC, a benchmark designed for assessing pairwise spatial relations, which includes 200 prompts organized into counterfactual pairs, and they provide a comprehensive package for reproducibility. Experimental results indicate that grounding methods significantly enhance both the pass rate and coverage of the evaluations, although abstention remains a prevalent issue primarily due to missed detections.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决自动评估文本到图像模型遵循空间指令的挑战，因为现有方法在模糊检测方面常常面临困难。作者提出了SpatialBench-UC，一个用于评估成对空间关系的基准，通过200个提示集进行评估，并包括人类审核以校准评估参数。实验结果表明，基础方法显著提高了被评估模型的通过率和覆盖率，但由于漏检，弃权仍然是一个主要问题。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</div>
<div class="meta-line">Authors: Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</div>
<div class="meta-line">First: 2026-01-19T22:55:30+00:00 · Latest: 2026-01-19T22:55:30+00:00</div>
<div class="meta-line">Comments: 10 pages,4 images</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13440v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的异常分类与分割方法分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM），特别是CLIP，通过实现零样本和少样本缺陷识别，彻底改变了异常检测，无需大量标注数据集。通过学习图像和文本的对齐表示，VLM通过自然语言描述正常和异常状态，促进异常分类和分割，消除了对特定任务训练或缺陷示例的传统要求。本项目对基于VLM的异常分类（AC）和异常分割（AS）方法进行了全面分析。我们系统地研究了关键架构范式，包括基于滑动窗口的密集特征提取（WinCLIP）、具有可学习投影的多阶段特征对齐（AprilLab框架）和组合提示集成策略。我们的分析在关键维度上评估这些方法：特征提取机制、文本-视觉对齐策略、提示工程技术、零样本与少样本的权衡、计算效率和跨领域泛化。通过在MVTec AD和VisA等基准上的严格实验，我们比较了分类准确性、分割精度和推理效率。主要贡献是对VLM在异常检测中成功的基础理解，综合了方法选择的实用见解并识别当前的局限性。本研究旨在促进VLM方法在工业质量控制中的知情采用，并指导未来的研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective anomaly detection methods that do not rely on extensive labeled datasets. The study employs a comprehensive analysis of Vision-Language Models (VLMs), particularly focusing on CLIP, to explore various architectural paradigms for anomaly classification and segmentation. Key experimental findings reveal that VLMs can successfully facilitate zero-shot and few-shot defect identification, with evaluations on benchmarks like MVTec AD and VisA demonstrating significant insights into classification accuracy, segmentation precision, and computational efficiency, ultimately providing a foundational understanding of VLMs&#x27; effectiveness in anomaly detection and guiding future research directions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于寻找不依赖于大量标注数据的有效异常检测方法。研究采用了对视觉-语言模型（VLMs）的全面分析，特别关注CLIP，以探索异常分类和分割的各种架构范式。关键实验结果表明，VLMs能够成功执行零样本和少样本缺陷识别，评估显示对特征提取机制、文本-视觉对齐策略以及零样本与少样本方法之间的权衡有显著的见解，最终增强了对VLM在工业应用中有效性的理解。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精度推理：QVLM架构和SQuID数据集用于定量地理空间分析</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLM）在定量空间推理方面表现不佳，因为它们的架构破坏了计数和测量所需的像素级信息。视觉编码器通过补丁嵌入压缩图像，减少空间索引并丢失准确计数所需的精确像素级跟踪。我们提出了两个贡献来解决这一根本限制。首先，我们引入SQuID（卫星定量智能数据集），这是一个包含2000对卫星图像问答对的基准数据集，具有数值范围和分类答案，旨在评估定量空间推理。该数据集涵盖三个难度级别，注释由人类标签及其学习的变异性自动生成。其次，我们提出QVLM（定量视觉-语言模型），这是一种代码生成架构，通过将语言理解与视觉分析解耦来保持像素精度。QVLM生成可执行代码，首先调用分割模型以获取像素级掩膜，然后直接在这些掩膜上操作，在整个推理过程中保持空间索引。我们的实验表明，使用GPT-5作为编码器的QVLM在SQuID上的准确率为42.0%，而使用图像-问题对提示的VLM的准确率为28.1%。我们的工作揭示，对于定量空间推理，架构解耦能够在定量任务上实现更好的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current Vision-Language Models (VLMs) in performing quantitative spatial reasoning due to the loss of pixel-level information during image processing. The authors introduce the SQuID dataset, which consists of 2,000 satellite image question-answer pairs designed to evaluate quantitative spatial reasoning across varying difficulty levels. They also propose the QVLM architecture, which maintains pixel precision by generating executable code that interacts with pixel-level masks instead of relying on traditional image embeddings. Experimental results demonstrate that QVLM, utilizing GPT-5, achieves an accuracy of 42.0% on the SQuID dataset, significantly outperforming the 28.1% accuracy of a conventional VLM using image-question pairs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前视觉语言模型（VLM）在进行定量空间推理时由于图像处理过程中像素级信息丢失而导致的局限性。作者介绍了SQuID数据集，该数据集由2000个卫星图像问答对组成，旨在评估不同难度水平的定量空间推理能力。他们还提出了QVLM架构，该架构通过生成可执行代码与像素级掩模进行交互，从而保持像素精度，而不是依赖传统的图像嵌入。实验结果表明，QVLM在SQuID数据集上的准确率为42.0%，显著高于传统VLM的28.1%准确率，突显了架构解耦在定量任务中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision</div>
<div class="meta-line">Authors: Bingsen Chen, Boyan Li, Ping Nie, Yuyu Zhang, Xi Ye, Chen Zhao</div>
<div class="meta-line">First: 2026-01-19T16:48:45+00:00 · Latest: 2026-01-19T16:48:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13217v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13217v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback&#x27;s scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次写作：深度研究代理在多轮报告修订中的不可靠性</div>
<div class="mono" style="margin-top:8px">现有的深度研究代理（DRA）基准将报告生成视为单次写作任务，这与人类研究者通过自我反思或同行反馈迭代起草和修订报告的方式根本不同。DRA能否可靠地根据用户反馈修订报告尚未被探索。我们引入了Mr Dre，一个评估套件，将多轮报告修订确立为DRA的新评估维度。Mr Dre包括（1）一个统一的长篇报告评估协议，涵盖全面性、事实性和呈现，和（2）一个经过人工验证的反馈模拟管道，用于多轮修订。我们对五个不同DRA的分析揭示了一个关键限制：尽管代理可以处理大多数用户反馈，但它们在16-27%的先前覆盖内容和引用质量上出现退步。在多次修订中，即使是表现最好的代理也留下了显著的提升空间，因为它们继续干扰反馈范围之外的内容，并未能保留早期的编辑。我们进一步表明，这些问题并不容易通过推理时的修复（如提示工程和专门的报告修订子代理）来解决。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the gap in evaluating Deep Research Agents (DRAs) for multi-turn report revision, as existing benchmarks treat report generation as a single-shot task, unlike the iterative process used by human researchers. The authors introduce Mr Dre, an evaluation suite that includes a comprehensive long-form report evaluation protocol and a feedback simulation pipeline for multi-turn revisions. The findings indicate that while DRAs can respond to user feedback, they often regress on 16-27% of previously covered content and citation quality, and even the best agents struggle to maintain content integrity across revisions, highlighting that these challenges cannot be easily fixed with inference-time adjustments like prompt engineering.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决对深度研究代理（DRA）进行报告生成评估时存在的差距，通常将其视为单次任务，而与人类研究人员的迭代过程不同。作者引入了Mr Dre，一个评估套件，旨在评估多轮报告修订，结合了全面的评估协议和反馈模拟管道。研究结果表明，尽管DRA能够响应用户反馈，但它们往往在之前覆盖的内容和引用质量上出现退步，表现最好的代理仍然无法保持早期编辑，并且在反馈范围之外干扰内容，突显了它们在多轮修订中的可靠性存在重大局限性。</div>
</details>
</div>
<div class="card">
<div class="title">LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations</div>
<div class="meta-line">Authors: Vittoria De Pellegrini, Tariq Alkhalifah</div>
<div class="meta-line">First: 2026-01-19T16:12:41+00:00 · Latest: 2026-01-19T16:12:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13190v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13190v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAViG-FLOW：用于流体流动模拟的潜在自回归视频生成</div>
<div class="mono" style="margin-top:8px">建模和预测地下多相流体流动场是从地质二氧化碳封存（GCS）操作到地热生产等应用的基础。这对于确保操作性能和长期安全至关重要。虽然高保真多相模拟器广泛用于此目的，但一旦需要进行多次正向运行以进行反演和量化不确定性，它们的成本就会变得过于昂贵。为了解决这个挑战，我们提出了LAViG-FLOW，这是一种潜在自回归视频生成扩散框架，明确学习饱和度和压力场的耦合演变。每个状态变量通过专用的2D自编码器进行压缩，视频扩散变换器（VDiT）建模它们随时间变化的耦合分布。我们首先在给定的时间范围内训练模型，以学习它们的耦合关系，然后自回归微调，以便它可以超出观察时间窗口进行外推。在一个开源的二氧化碳封存数据集上进行评估，LAViG-FLOW生成的饱和度和压力场在时间上保持一致，同时运行速度比传统数值求解器快几个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the modeling and forecasting of subsurface multiphase fluid flow fields, which is crucial for applications like geological CO2 sequestration and geothermal production. The authors propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that learns the coupled evolution of saturation and pressure fields using a 2D autoencoder for compression and a Video Diffusion Transformer for modeling their distribution over time. Experimental results on an open-source CO2 sequestration dataset demonstrate that LAViG-FLOW generates consistent saturation and pressure fields over time while significantly outperforming traditional numerical solvers in terms of speed.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高对地下多相流体流动场建模和预测的效率，这对地质二氧化碳封存和地热生产等应用至关重要。作者提出了LAViG-FLOW，这是一种潜在自回归视频生成扩散框架，利用2D自编码器进行压缩，并通过视频扩散变换器建模饱和度和压力场的耦合演变。实验结果表明，在开放源代码的二氧化碳封存数据集上，LAViG-FLOW能够生成时间上保持一致的饱和度和压力场，同时运行速度显著快于传统数值求解器。</div>
</details>
</div>
<div class="card">
<div class="title">Building Production-Ready Probes For Gemini</div>
<div class="meta-line">Authors: János Kramár, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy</div>
<div class="meta-line">First: 2026-01-16T18:54:29+00:00 · Latest: 2026-01-19T16:05:05+00:00</div>
<div class="meta-line">Comments: v2 (minor typo fixes)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11516v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11516v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architectures that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant distribution shifts, including multi-turn conversations, long context prompts, and adaptive red teaming. Our results demonstrate that while our novel architectures address context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google&#x27;s frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为Gemini构建生产就绪探针</div>
<div class="mono" style="margin-top:8px">前沿语言模型的能力正在迅速提升。因此，我们需要更强的措施来防止不法分子滥用日益强大的系统。先前的研究表明，激活探针可能是一种有前景的滥用缓解技术，但我们识别出一个关键的挑战：探针在重要的生产分布变化下无法泛化。特别是，我们发现从短上下文到长上下文输入的转变对现有探针架构来说是困难的。我们提出了几种新的探针架构，以处理这种长上下文分布变化。
我们在网络攻击领域评估这些探针，测试它们在各种与生产相关的分布变化下的鲁棒性，包括多轮对话、长上下文提示和自适应红队。我们的结果表明，尽管我们的新架构解决了上下文长度的问题，但架构选择与在多样化分布上训练的结合是广泛泛化所必需的。此外，我们还表明，将探针与提示分类器配对可以以低成本实现最佳准确性，因为探针的计算效率。
这些发现为在用户面向的Gemini实例中成功部署滥用缓解探针提供了依据，Gemini是谷歌的前沿语言模型。最后，我们发现使用AlphaEvolve自动化改进探针架构搜索和自适应红队的早期积极结果，表明自动化某些AI安全研究已经成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the rapid improvement of frontier language model capabilities, which necessitates stronger mitigations against potential misuse by bad actors. The authors propose new probe architectures to address the challenge of existing probes failing to generalize under production distribution shifts, particularly from short-context to long-context inputs. Their evaluation in the cyber-offensive domain reveals that while the new architectures improve handling of context length, achieving broad generalization requires a combination of architecture choice and training on diverse distributions. Additionally, they find that combining probes with prompted classifiers enhances accuracy efficiently, and their findings have led to successful deployment of these probes in Gemini, Google&#x27;s language model, with promising results from automating improvements in probe architecture and adaptive red teaming using AlphaEvolve.</div>
<div class="mono" style="margin-top:8px">本研究的动机是前沿语言模型能力的快速提升，这需要更强的措施来防止恶意行为者的潜在滥用。作者提出了新的探针架构，以应对生产分布变化下的泛化挑战，特别是从短上下文到长上下文输入的转变。他们在网络攻防领域的实验表明，尽管新架构改善了上下文长度的处理，但实现广泛的泛化需要架构选择和多样化分布训练的结合。此外，他们发现将探针与提示分类器结合使用可以高效地提高准确性，他们的研究结果已成功应用于谷歌的Gemini语言模型中，并在使用AlphaEvolve自动改进探针架构和自适应红队方面取得了积极的初步结果。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</div>
<div class="meta-line">Authors: Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan</div>
<div class="meta-line">First: 2026-01-19T15:19:28+00:00 · Latest: 2026-01-19T15:19:28+00:00</div>
<div class="meta-line">Comments: Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于CLIP的可适应自监督学习用于以人为中心的视觉任务</div>
<div class="mono" style="margin-top:8px">以人为中心的视觉分析在监控、医疗保健和人机交互等多种应用中发挥着关键作用。随着大规模无标签人类图像数据集的出现，对能够支持多样化以人为中心的下游任务的一般无监督预训练模型的需求日益增加。为实现这一目标，我们提出了CLASP（基于CLIP的可适应自监督学习），这是一个旨在进行以人为中心的视觉任务的无监督预训练的新框架。CLASP利用强大的视觉-语言模型CLIP生成低级（例如，身体部位）和高级（例如，属性）语义伪标签。这些多层次的语义线索被整合到学习的视觉表示中，丰富了它们的表现力和泛化能力。考虑到不同的下游任务对语义粒度的需求不同，CLASP结合了一个提示控制的专家混合（MoE）模块。MoE根据特定任务的提示动态调整特征提取，减轻潜在的特征冲突并增强可迁移性。此外，CLASP采用多任务预训练策略，其中来自CLIP的部分和属性级伪标签指导表示学习过程。在多个基准上的广泛实验表明，CLASP始终优于现有的无监督预训练方法，推动了以人为中心的视觉分析领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for a general unsupervised pre-training model for human-centric visual tasks, which are crucial in applications like surveillance and healthcare. The authors propose CLASP, a framework that utilizes the CLIP model to generate low-level and high-level semantic pseudo-labels, which are integrated into visual representations to enhance their expressiveness. Experimental results show that CLASP, through its Prompt-Controlled Mixture-of-Experts module and multi-task pre-training strategy, consistently outperforms existing unsupervised pre-training methods across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是应对随着大规模无标记人类图像数据集的出现，对能够支持各种以人为中心的视觉任务的通用无监督预训练模型的需求。作者提出了CLASP框架，该框架利用CLIP视觉-语言模型生成多层次的语义伪标签，并将其整合到视觉表示中，以增强其表现力。实验结果表明，CLASP在多个基准测试中优于现有的无监督预训练方法，证明了其在改善以人为中心的视觉分析方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
