<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-10 04:06</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260210_0406</div>
    <div class="row"><div class="card">
<div class="title">Agentic Uncertainty Reveals Agentic Overconfidence</div>
<div class="meta-line">Authors: Jean Kaddour, Srijan Patel, Gbètondji Dovonon, Leo Richter, Pasquale Minervini, Matt J. Kusner</div>
<div class="meta-line">First: 2026-02-06T18:49:35+00:00 · Latest: 2026-02-06T18:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06948v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理不确定性揭示代理过度自信</div>
<div class="mono" style="margin-top:8px">AI代理能否预测自己在任务中是否会成功？我们通过在任务执行前、中、后引导成功概率估计来研究代理不确定性。所有结果都表现出代理过度自信：一些成功率仅为22%的代理预测成功率为77%。反直觉的是，执行前的评估在信息严格较少的情况下往往比标准的执行后评审产生更好的区分，尽管差异并不总是显著。对抗性提示将评估重新框定为缺陷发现实现了最佳校准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the concept of agentic uncertainty in AI agents by examining their ability to predict their success at tasks. The method involves eliciting success probability estimates from agents before, during, and after task execution. The findings reveal a phenomenon of agentic overconfidence, where agents that succeed only 22% of the time predict a 77% success rate, and interestingly, pre-execution assessments often provide better discrimination than post-execution reviews, although the differences are not always statistically significant. Additionally, using adversarial prompting to frame the assessment as bug-finding leads to improved calibration of success predictions.</div>
<div class="mono" style="margin-top:8px">本研究通过评估人工智能代理在任务执行前、中、后对成功的预测，探讨了代理不确定性的概念。主要方法是从代理中引出成功概率估计，结果显示代理存在代理过度自信的倾向，成功率仅为22%的代理预测成功率为77%。有趣的是，研究发现尽管信息较少，执行前的评估往往比传统的执行后评估提供更好的区分度，而通过将评估重新框定为寻找错误的对抗性提示则实现了最佳的校准。</div>
</details>
</div>
<div class="card">
<div class="title">Dataset Distillation as Pushforward Optimal Quantization</div>
<div class="meta-line">Authors: Hong Ye Tan, Emma Slade</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-13T20:41:52+00:00 · Latest: 2026-02-06T18:38:03+00:00</div>
<div class="meta-line">Comments: ICLR 2026, https://openreview.net/forum?id=FMSp8AUF3m</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.07681v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.07681v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose Dataset Distillation by Optimal Quantization, based on clustering in a latent space. Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better performance and inter-model generalization on the ImageNet-1K dataset with trivial additional computation, and SOTA performance in higher image-per-class settings. Using the distilled noise initializations in a stronger diffusion transformer model, we obtain SOTA distillation performance on ImageNet-1K and its subsets, outperforming diffusion guidance methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据集蒸馏作为推前最优量化</div>
<div class="mono" style="margin-top:8px">数据集蒸馏旨在找到一个合成训练集，使得在合成数据上训练的性能与在真实数据上训练的性能相似，同时计算需求减少几个数量级。现有方法大致可分为两类：一种是将神经网络训练启发式作为下层问题的双层优化问题，另一种是通过匹配数据分布来绕过双层优化的解耦方法。后者在训练和蒸馏数据集的规模方面具有速度和可扩展性的主要优势。我们证明，当配备编码器-解码器结构时，经验上成功的解耦方法可以重新表述为最优量化问题，其中通过最小化期望投影距离找到有限的点集来近似基础概率测度。特别地，我们将现有的解耦数据集蒸馏方法与经典的最优量化和Wasserstein重心问题联系起来，展示了蒸馏数据集在基于扩散的生成先验中的一致性。我们提出了基于潜在空间聚类的最优量化数据集蒸馏。与之前的SOTA方法D⁴M相比，我们在ImageNet-1K数据集上实现了更好的性能和模型间泛化，且额外计算量微不足道，在每类图像设置中实现了SOTA性能。使用蒸馏噪声初始化在更强的扩散变换模型中，我们在ImageNet-1K及其子集上获得了SOTA蒸馏性能，超越了扩散引导方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve dataset distillation, which seeks to create synthetic training sets that perform comparably to real data while significantly reducing computational costs. The authors propose a novel method that reformulates existing disentangled dataset distillation techniques as an optimal quantization problem, utilizing an encoder-decoder structure to minimize expected projection distances. Experimental results show that their approach, Dataset Distillation by Optimal Quantization, outperforms the previous state-of-the-art method D4M on the ImageNet-1K dataset, achieving superior performance and inter-model generalization with minimal additional computation, and setting new records in distillation performance using distilled noise initializations in a stronger diffusion transformer model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善数据集蒸馏，旨在创建合成训练集，以复制真实数据集的性能，同时显著降低计算成本。作者提出了一种方法，将现有的解耦数据集蒸馏技术重新表述为最优量化问题，利用编码器-解码器结构来最小化期望投影距离。实验结果表明，他们的方法“基于最优量化的数据集蒸馏”在ImageNet-1K数据集上优于之前的最先进方法D4M，以最小的额外计算实现更好的性能和模型间泛化，并在每类图像数量较高的情况下设定了新的标准。</div>
</details>
</div>
<div class="card">
<div class="title">DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</div>
<div class="meta-line">Authors: Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</div>
<div class="meta-line">First: 2025-10-05T19:27:48+00:00 · Latest: 2026-02-06T17:31:48+00:00</div>
<div class="meta-line">Comments: Nghiem T. Diep, Hien Dang, and Tuan Truong contributed equally to this work</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04331v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04331v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new technique designed to stabilize training and boost the sample efficiency of DoRA. Our framework introduces two key components: (i) the injection of learnable noise into the denominator of DoRA weight decomposition, which serves as an adaptive regularizer to mitigate instabilities and improve the estimation rate of low-rank matrices; and (ii) the replacement of static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling between the query and value projection matrices, leading to improved sample efficiency both theoretically and empirically. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines, underscoring the effectiveness of combining noise-based regularization with network-based parameter generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DoRAN：通过噪声注入和辅助网络稳定权重分解低秩适应</div>
<div class="mono" style="margin-top:8px">参数高效微调（PEFT）方法已成为适应大规模模型的标准范式。在这些技术中，权重分解低秩适应（DoRA）通过将预训练权重显式分解为幅度和方向分量，已被证明可以提高低秩适应（LoRA）方法的学习能力和训练稳定性。在本研究中，我们提出了DoRAN，一种旨在稳定训练并提高DoRA样本效率的新技术。我们的框架引入了两个关键组件：（i）将可学习噪声注入到DoRA权重分解的分母中，作为自适应正则化器，以减轻不稳定性并提高低秩矩阵的估计速率；（ii）用动态生成低秩矩阵的辅助网络替代静态低秩矩阵，实现查询和价值投影矩阵之间的参数耦合，从理论和实证上提高样本效率。在视觉和语言基准上的全面实验表明，DoRAN始终优于LoRA、DoRA和其他PEFT基线，强调了将基于噪声的正则化与基于网络的参数生成相结合的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the stability and sample efficiency of Weight-Decomposed Low-Rank Adaptation (DoRA), a parameter-efficient fine-tuning method for large-scale models. The authors propose DoRAN, which incorporates learnable noise into the weight decomposition process and utilizes auxiliary networks for dynamic low-rank matrix generation. Experimental results demonstrate that DoRAN consistently outperforms existing methods such as LoRA and DoRA across various vision and language benchmarks, highlighting the benefits of combining noise-based regularization with dynamic parameter generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高权重分解低秩适应（DoRA）这一大规模模型的参数高效微调方法的稳定性和样本效率。作者提出了DoRAN，该方法在权重分解过程中引入可学习噪声，并用动态辅助网络替代静态低秩矩阵。实验结果表明，DoRAN在各种视觉和语言基准测试中始终优于LoRA、DoRA及其他参数高效微调基线，突显了所提噪声正则化和动态参数生成方法的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Inverse problems with diffusion models: MAP estimation via mode-seeking loss</div>
<div class="meta-line">Authors: Sai Bharath Chandra Gutha, Ricardo Vinuesa, Hossein Azizpour</div>
<div class="meta-line">First: 2025-12-11T10:51:34+00:00 · Latest: 2026-02-06T17:30:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10524v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10524v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can also be computationally demanding. In this work, we propose a new MAP estimation strategy for solving inverse problems with a pre-trained unconditional diffusion model. Specifically, we introduce the variational mode-seeking loss (VML) and show that its minimization at each reverse diffusion step guides the generated sample towards the MAP estimate (modes in practice). VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived without any modeling approximations. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems via VML minimization, and validate its efficacy in both performance and computational time through extensive experiments on diverse image-restoration tasks across multiple datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的逆问题：通过模式寻求损失进行MAP估计</div>
<div class="mono" style="margin-top:8px">一个预训练的无条件扩散模型，结合后验采样或最大后验（MAP）估计技术，可以在没有特定任务训练或微调的情况下解决任意逆问题。然而，现有的后验采样和MAP估计方法通常依赖于建模近似，并且计算上也可能要求较高。在这项工作中，我们提出了一种新的MAP估计策略，用于利用预训练的无条件扩散模型解决逆问题。具体而言，我们引入了变分模式寻求损失（VML），并展示了在每个反向扩散步骤中最小化VML如何引导生成样本朝向MAP估计（实际中的模式）。VML源于最小化扩散后验$p(\mathbf{x}_0|\mathbf{x}_t)$与测量后验$p(\mathbf{x}_0|\mathbf{y})$之间的Kullback-Leibler（KL）散度的新视角，其中$\mathbf{y}$表示测量。重要的是，对于线性逆问题，VML可以在没有任何建模近似的情况下进行解析推导。基于进一步的理论见解，我们提出了VML-MAP，这是一种通过VML最小化解决逆问题的经验有效算法，并通过在多个数据集上的多样图像恢复任务的广泛实验验证了其在性能和计算时间上的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of solving inverse problems using pre-trained unconditional diffusion models without the need for task-specific training. The authors propose a new MAP estimation strategy that employs a variational mode-seeking loss (VML) to guide the generated samples towards the MAP estimate by minimizing the Kullback-Leibler divergence between the diffusion posterior and the measurement posterior. Experimental results demonstrate that the VML-MAP algorithm effectively addresses linear inverse problems, showing improvements in both performance and computational time across various image-restoration tasks on multiple datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用预训练无条件扩散模型解决逆问题的挑战，特别是现有后验采样和最大后验估计方法的局限性。作者提出了一种新的最大后验估计策略，结合了变分模式寻求损失（VML），通过最小化扩散后验和测量后验之间的Kullback-Leibler散度，引导生成样本朝向最大后验估计。实验结果表明，VML-MAP算法在多个数据集上的各种图像恢复任务中有效解决线性逆问题，表现出更好的性能和更低的计算时间。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</div>
<div class="meta-line">Authors: Yuxuan Yao, Yuxuan Chen, Hui Li, Kaihui Cheng, Qipeng Guo, Yuwei Sun, Zilong Dong, Jingdong Wang, Siyu Zhu</div>
<div class="meta-line">First: 2026-02-06T17:19:53+00:00 · Latest: 2026-02-06T17:19:53+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06886v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06886v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示再注入：缓解多模态扩散变换器中的提示遗忘</div>
<div class="mono" style="margin-top:8px">多模态扩散变换器（MMDiTs）用于文本到图像生成，保持文本和图像分支的独立性，在去噪过程中实现文本标记与视觉潜变量之间的双向信息流。在这种设置中，我们观察到提示遗忘现象：随着深度的增加，文本分支中提示表示的语义逐渐被遗忘。我们进一步通过探测文本分支中各层表示的语言属性，验证了这一效应在三个代表性MMDiTs（SD3、SD3.5和FLUX.1）上的存在。基于这些发现，我们提出了一种无训练的方法，提示再注入，将早期层的提示表示再注入到后期层，以缓解这种遗忘。在GenEval、DPG和T2I-CompBench++上的实验显示，指令跟随能力持续提升，同时在捕捉偏好、美学和整体文本-图像生成质量的指标上也有所改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of prompt forgetting in Multimodal Diffusion Transformers (MMDiTs) used for text-to-image generation, where the semantic representation of prompts diminishes as the model depth increases. To investigate this phenomenon, the authors analyzed three MMDiTs—SD3, SD3.5, and FLUX.1—by examining the linguistic attributes of text representations across different layers. They proposed a novel method called prompt reinjection, which involves reinjecting prompt representations from earlier layers into later ones, and demonstrated through experiments on GenEval, DPG, and T2I-CompBench++ that this approach significantly enhances instruction-following capabilities and improves various quality metrics related to text-image generation.</div>
<div class="mono" style="margin-top:8px">本研究解决了在用于文本到图像生成的多模态扩散变换器（MMDiTs）中出现的提示遗忘问题，即随着模型深度的增加，提示表示的语义逐渐减弱。作者通过分析三个MMDiTs（SD3、SD3.5和FLUX.1）中不同层次的文本分支表示的语言属性，调查了这一现象。为了应对这一问题，他们提出了一种无训练的方法，称为提示再注入，该方法将早期层的提示表示重新注入到后期层中。对GenEval、DPG和T2I-CompBench++的实验结果表明，在指令遵循能力、偏好、美学和文本-图像生成质量的整体提升方面取得了显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</div>
<div class="meta-line">Authors: Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Abhinav Mehrotra</div>
<div class="meta-line">First: 2026-02-06T17:05:56+00:00 · Latest: 2026-02-06T17:05:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NanoFLUX：基于蒸馏的移动设备大型文本到图像生成模型压缩</div>
<div class="mono" style="margin-top:8px">尽管大规模文本到图像扩散模型在视觉质量上不断提高，但其规模的增加加大了最先进模型与设备端解决方案之间的差距。为了解决这一问题，我们推出了NanoFLUX，这是一个从17B FLUX.1-Schnell蒸馏而来的2.4B文本到图像流匹配模型，采用了旨在保持生成质量的渐进压缩管道。我们的贡献包括：（1）一种通过修剪扩散变换器中冗余组件驱动的模型压缩策略，将其大小从12B减少到2B；（2）一种基于ResNet的令牌下采样机制，通过允许中间块在较低分辨率的令牌上操作来减少延迟，同时在其他地方保持高分辨率处理；（3）一种新颖的文本编码器蒸馏方法，在采样过程中利用去噪器早期层的视觉信号。实证结果表明，NanoFLUX在移动设备上生成512 x 512图像的时间约为2.5秒，展示了高质量设备端文本到图像生成的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to bridge the gap between large-scale text-to-image diffusion models and on-device solutions, which have lagged in visual quality due to their size. The authors present NanoFLUX, a 2.4B text-to-image flow-matching model distilled from a larger 17B model using a progressive compression pipeline aimed at maintaining generation quality. Key experimental findings indicate that NanoFLUX can generate 512 x 512 images in approximately 2.5 seconds on mobile devices, showcasing its potential for high-quality text-to-image generation in a compact form factor.</div>
<div class="mono" style="margin-top:8px">本研究解决了在移动设备上部署大规模文本到图像扩散模型的挑战，因为现有模型过于庞大，无法实际使用。作者提出了NanoFLUX，这是一种从更大17B模型中提炼出的2.4B文本到图像流匹配模型，采用渐进压缩管道以保持生成质量。关键实验结果表明，NanoFLUX能够在移动设备上大约2.5秒内生成512 x 512的图像，展示了其在高质量设备端文本到图像生成中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</div>
<div class="meta-line">Authors: Mohammadreza Salehi, Mehdi Noroozi, Luca Morreale, Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Ramos, Abhinav Mehrotra</div>
<div class="meta-line">First: 2026-02-06T16:56:30+00:00 · Latest: 2026-02-06T16:56:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06871v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://smsd75.github.io/RFDM_page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model&#x27;s prediction at t-1. To leverage videos&#x27; temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RFDM：高效因果视频编辑的残差流扩散模型</div>
<div class="mono" style="margin-top:8px">指令性视频编辑仅使用文本提示对输入视频进行编辑，实现直观的自然语言控制。尽管取得了快速进展，但大多数方法仍需固定长度的输入和大量计算。同时，自回归视频生成实现了高效的可变长度合成，但在视频编辑中仍未得到充分探索。我们提出了一种因果的高效视频编辑模型，逐帧编辑可变长度视频。为了提高效率，我们从二维图像到图像（I2I）扩散模型开始，并通过在时间步t上将编辑条件化于模型在t-1时的预测，将其适应于视频到视频（V2V）编辑。为了利用视频的时间冗余，我们提出了一种新的I2I扩散前向过程公式，鼓励模型预测目标输出与先前预测之间的残差。我们称之为残差流扩散模型（RFDM），它将去噪过程集中在连续帧之间的变化上。此外，我们提出了一个新的基准，更好地对编辑任务的最先进方法进行排名。在全球/局部风格转移和物体移除的配对视频数据上训练，RFDM超越了基于I2I的方法，并与完全时空（3D）V2V模型竞争，同时匹配图像模型的计算并独立于输入视频长度进行扩展。更多内容请访问：https://smsd75.github.io/RFDM_page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of causal video editing using natural language prompts, addressing the limitations of existing methods that require fixed-length inputs and high computational resources. The authors developed the Residual Flow Diffusion Model (RFDM), which adapts a 2D image-to-image diffusion model for variable-length video editing by conditioning edits on previous frame predictions and focusing on the residuals between frames. Experimental results demonstrate that RFDM outperforms traditional image-based methods and competes effectively with advanced spatiotemporal video models, while maintaining computational efficiency and scalability with respect to input video length.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高因果视频编辑的效率，利用自然语言提示，解决现有方法需要固定长度输入和高计算资源的局限性。作者提出了残差流扩散模型（RFDM），该模型通过基于先前预测来预测帧编辑，从而将二维图像到图像的扩散模型适应于可变长度的视频编辑，利用时间冗余。实验结果表明，RFDM在性能上超越了传统的基于图像的方法，并有效地与先进的时空模型竞争，同时保持计算效率，并在输入视频长度方面具有可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</div>
<div class="meta-line">Authors: Chao Zhou, Tianyi Wei, Yiling Chen, Wenbo Zhou, Nenghai Yu</div>
<div class="meta-line">First: 2026-02-06T16:39:10+00:00 · Latest: 2026-02-06T16:39:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06850v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend&#x27;&#x27; strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考多条件DiTs：通过位置对齐和关键词范围消除冗余注意力</div>
<div class="mono" style="margin-top:8px">现代文本到图像模型在基于提示的生成方面表现出色，但通常缺乏满足特定用户需求（如空间布局或主题外观）所需的细粒度控制。多条件控制解决了这一问题，但其在扩散变换器（DiTs）中的集成受到传统“连接并关注”策略的瓶颈，随着条件数量的增加，这种策略会导致二次计算和内存开销。我们的分析表明，这种跨模态交互在空间或语义上是冗余的。为此，我们提出了位置对齐和关键词范围注意力（PKA），这是一个高效的框架，旨在消除这些冗余。具体而言，位置对齐注意力（PAA）通过强制局部补丁对齐来线性化空间控制，而关键词范围注意力（KSA）通过语义感知掩蔽来修剪无关的主题驱动交互。为了促进高效学习，我们进一步引入了一种条件敏感性感知采样（CSAS）策略，该策略重新加权训练目标，关注关键去噪阶段，显著加快收敛并增强条件保真度。实证结果表明，PKA实现了10.0$\times$的推理加速和5.1$\times$的显存节省，为高保真多条件生成提供了可扩展且资源友好的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the fine-grained control in text-to-image models, particularly for specific user requirements, while addressing the inefficiencies of the conventional &#x27;concatenate-and-attend&#x27; strategy in Diffusion Transformers (DiTs). The authors propose a novel framework called Position-aligned and Keyword-scoped Attention (PKA), which includes Position-Aligned Attention (PAA) for localized patch alignment and Keyword-Scoped Attention (KSA) for semantic-aware masking to eliminate redundant interactions. Experimental results demonstrate that PKA achieves a 10.0× increase in inference speed and a 5.1× reduction in VRAM usage, making it a scalable and efficient solution for high-fidelity multi-conditioned generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本到图像模型的细粒度控制，特别是满足用户特定需求（如空间布局和主题外观），而传统的“连接并关注”策略在扩散变换器（DiTs）中存在效率低下的问题。作者提出了一种新的框架，称为位置对齐和关键词范围注意（PKA），通过位置对齐注意（PAA）实现局部补丁对齐，和关键词范围注意（KSA）通过语义感知掩蔽无关交互来消除冗余的跨模态交互。实验结果表明，PKA实现了10.0倍的推理速度提升和5.1倍的显存节省，使其成为高保真多条件生成的可扩展和高效解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models</div>
<div class="meta-line">Authors: Yuming Li, Qingyu Li, Chengyu Bai, Xiangyang Luo, Zeyue Xue, Wenyu Qin, Meng Wang, Yikai Wang, Shanghang Zhang</div>
<div class="meta-line">First: 2026-02-06T16:09:50+00:00 · Latest: 2026-02-06T16:09:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06825v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.
  To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.
  Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.
  By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AEGPO：用于扩散模型的自适应熵引导策略优化</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）在对齐扩散和流模型方面显示出前景，但策略优化方法如GRPO存在低效和静态的采样策略。这些方法对所有提示和去噪步骤采取统一处理，忽视了样本学习价值的显著变化以及关键探索时刻的动态特性。为了解决这个问题，我们对GRPO训练过程中的内部注意力动态进行了详细分析，并发现一个关键见解：注意力熵可以作为强大的双信号代理。首先，在不同样本之间，注意力熵的相对变化（ΔEntropy）反映了当前策略与基础策略之间的差异，作为样本学习价值的稳健指标。其次，在去噪过程中，绝对注意力熵的峰值（Entropy(t)）有效识别出高价值探索发生的关键时间步。基于这一观察，我们提出了自适应熵引导策略优化（AEGPO），这是一种新颖的双信号、双层次自适应优化策略。在全局层面，AEGPO使用ΔEntropy动态分配回滚预算，优先考虑学习价值更高的提示。在局部层面，它利用Entropy(t)的峰值在关键高分散时间步选择性引导探索，而不是在所有去噪步骤中均匀分布。通过将计算集中在最具信息的样本和最关键的时刻，AEGPO实现了更高效和有效的策略优化。在文本到图像生成任务上的实验表明，AEGPO显著加速了收敛，并实现了优于标准GRPO变体的对齐性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve policy optimization methods in reinforcement learning from human feedback (RLHF) for diffusion models, which currently suffer from inefficient sampling strategies. The authors analyze the internal attention dynamics during GRPO training and identify that attention entropy can serve as a dual-signal proxy to indicate sample learning value and critical exploration moments. They propose a novel method called Adaptive Entropy-Guided Policy Optimization (AEGPO), which dynamically allocates rollout budgets based on the relative change in attention entropy and selectively guides exploration at critical timesteps. Experimental results on text-to-image generation tasks show that AEGPO accelerates convergence and achieves better alignment performance compared to standard GRPO variants.</div>
<div class="mono" style="margin-top:8px">本研究解决了在使用人类反馈的强化学习（RLHF）对齐扩散模型时，像GRPO这样的策略优化方法的低效问题。作者分析了GRPO训练过程中的内部注意力动态，发现注意力熵可以作为双信号代理，指示样本学习价值并识别关键探索时刻。他们提出了自适应熵引导策略优化（AEGPO），该方法基于注意力熵的相对变化动态分配回滚预算，并在高分散时刻选择性引导探索。实验结果表明，AEGPO在文本到图像生成任务中显著加速了收敛，并改善了与标准GRPO变体相比的对齐性能。</div>
</details>
</div>
<div class="card">
<div class="title">RAIGen: Rare Attribute Identification in Text-to-Image Generative Models</div>
<div class="meta-line">Authors: Silpa Vadakkeeveetil Sreelatha, Dan Wang, Serge Belongie, Muhammad Awais, Anjan Dutta</div>
<div class="meta-line">First: 2026-02-06T15:54:41+00:00 · Latest: 2026-02-06T15:54:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06806v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06806v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAIGen：文本到图像生成模型中的稀有属性识别</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型实现了令人印象深刻的生成质量，但继承并放大了训练数据的偏见，扭曲了语义属性的覆盖。先前的工作通过两种方式解决这个问题。封闭集方法缓解预定义公平类别（例如性别、种族）中的偏见，假设社会显著的少数属性是已知的。开放集方法将任务框定为偏见识别，突出主导输出的多数属性。两者都忽视了一个互补任务：揭示在数据分布中代表性不足的稀有或少数特征（社会、文化或风格），但仍然在模型表示中编码。我们介绍了RAIGen，这是我们所知的第一个用于扩散模型中无监督稀有属性发现的框架。RAIGen利用马特ryoshka稀疏自编码器和一种新颖的少数群体度量，结合神经元激活频率与语义独特性，以识别可解释的神经元，其顶级激活图像揭示了代表性不足的属性。实验表明，RAIGen在Stable Diffusion中发现超出固定公平类别的属性，能够扩展到更大的模型如SDXL，支持跨架构的系统审计，并在生成过程中实现稀有属性的有针对性放大。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address biases in text-to-image generative models, which often overlook rare or minority attributes in their outputs. The authors introduce RAIGen, an unsupervised framework that utilizes Matryoshka Sparse Autoencoders and a novel minority metric to identify neurons associated with underrepresented attributes. Experimental results demonstrate that RAIGen effectively uncovers attributes beyond predefined fairness categories in models like Stable Diffusion and SDXL, facilitates systematic auditing across different architectures, and allows for the targeted amplification of rare attributes during image generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像生成模型中的偏见问题，这些模型通常忽视输出中的稀有或少数属性。作者提出了RAIGen，这是一个无监督框架，利用Matryoshka稀疏自编码器和一种新颖的少数属性度量来识别与扩散模型中未充分代表的属性相关的神经元。实验结果表明，RAIGen成功地发现了超出预定义公平类别的属性，能够有效扩展到更大的模型如SDXL，促进不同架构之间的系统审计，并允许在图像生成过程中有针对性地放大稀有属性。</div>
</details>
</div>
<div class="card">
<div class="title">AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</div>
<div class="meta-line">Authors: Fengpeng Li, Kemou Li, Qizhou Wang, Bo Han, Jiantao Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-06T15:27:42+00:00 · Latest: 2026-02-06T15:27:42+00:00</div>
<div class="meta-line">Comments: 30 pages,12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06771v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model&#x27;s overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AEGIS：对抗性目标引导的无保留数据鲁棒概念消除方法</div>
<div class="mono" style="margin-top:8px">概念消除有助于阻止扩散模型（DMs）生成有害内容；但当前方法面临鲁棒性与保留的权衡。鲁棒性意味着通过概念消除方法微调的模型能够抵抗被消除概念的重新激活，即使在语义相关的提示下。保留意味着无关概念被保留，以保持模型的整体效用。两者在实践中对概念消除至关重要，但同时解决这两个问题具有挑战性，因为现有工作通常在改善一个因素的同时牺牲另一个因素。以往的工作通常加强一个而削弱另一个，例如，将单个被消除的提示映射到固定的安全目标会留下可被提示攻击利用的类级残余，而以保留为导向的方案在对抗自适应对手时表现不佳。本文介绍了对抗性消除与梯度信息协同（AEGIS），这是一个无保留数据的框架，推动了鲁棒性和保留的双重进步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve concept erasure in diffusion models (DMs) to prevent the generation of harmful content while maintaining the model&#x27;s overall utility. The authors propose a novel framework called Adversarial Erasure with Gradient Informed Synergy (AEGIS), which addresses the simultaneous challenges of robustness and retention without relying on retention data. Experimental results demonstrate that AEGIS effectively enhances both the resistance to reactivation of erased concepts and the preservation of unrelated concepts, outperforming existing methods that typically compromise one aspect for the other.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善扩散模型（DMs）中的概念抹除，以防止生成有害内容，同时保持模型的整体效用。作者提出了一种名为对抗性抹除与梯度信息协同（AEGIS）的新框架，该框架在不依赖保留数据的情况下解决了稳健性和保留的双重挑战。实验结果表明，AEGIS有效增强了对抹除概念的再激活的抵抗力，同时保留了无关概念，优于通常在两者之间妥协的现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Anonymization Prompt Learning for Facial Privacy-Preserving Text-to-Image Generation</div>
<div class="meta-line">Authors: Liang Shi, Jie Zhang, Shiguang Shan</div>
<div class="meta-line">First: 2024-05-27T07:38:26+00:00 · Latest: 2026-02-06T15:13:03+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.16895v3">Abs</a> · <a href="https://arxiv.org/pdf/2405.16895v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models, such as Stable Diffusion, generate highly realistic images from text descriptions. However, the generation of certain content at such high quality raises concerns. A prominent issue is the accurate depiction of identifiable facial images, which could lead to malicious deepfake generation and privacy violations. In this paper, we propose Anonymization Prompt Learning (APL) to address this problem. Specifically, we train a learnable prompt prefix for text-to-image diffusion models, which forces the model to generate anonymized facial identities, even when prompted to produce images of specific individuals. Extensive quantitative and qualitative experiments demonstrate the successful anonymization performance of APL, which anonymizes any specific individuals without compromising the quality of non-identity-specific image generation. Furthermore, we reveal the plug-and-play property of the learned prompt prefix, enabling its effective application across different pretrained text-to-image models for transferrable privacy and security protection against the risks of deepfakes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面部隐私保护文本到图像生成的匿名化提示学习</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型，如稳定扩散，从文本描述生成高度逼真的图像。然而，以如此高的质量生成某些内容引发了担忧。一个突出的问题是准确描绘可识别的面部图像，这可能导致恶意深度伪造生成和隐私侵犯。本文提出了匿名化提示学习（APL）来解决这个问题。具体而言，我们为文本到图像扩散模型训练了一个可学习的提示前缀，强制模型生成匿名的面部身份，即使在提示生成特定个体的图像时也是如此。大量定量和定性实验表明APL成功的匿名化性能，能够在不影响非身份特定图像生成质量的情况下匿名化任何特定个体。此外，我们揭示了学习到的提示前缀的即插即用特性，使其能够有效应用于不同的预训练文本到图像模型，以实现可转移的隐私和安全保护，防范深度伪造的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address privacy concerns related to the generation of identifiable facial images in text-to-image diffusion models, which can lead to deepfake generation. The authors propose a method called Anonymization Prompt Learning (APL), which involves training a learnable prompt prefix that ensures the generation of anonymized facial identities, even when specific individuals are referenced. Experimental results show that APL effectively anonymizes identifiable individuals while maintaining the quality of non-identity-specific image generation, and it can be applied across various pretrained models for enhanced privacy and security against deepfake risks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到图像扩散模型中生成可识别面孔图像所带来的隐私问题，这可能导致深度伪造的生成。作者提出了一种名为匿名提示学习（APL）的方法，通过训练一个可学习的提示前缀，确保即使在提及特定个体时也能生成匿名的面孔身份。实验结果表明，APL有效地匿名化可识别个体，同时保持非身份特定图像生成的质量，并且展示了其即插即用的能力，可以应用于各种预训练模型，以增强对深度伪造风险的隐私保护。</div>
</details>
</div>
<div class="card">
<div class="title">F-GRPO: Don&#x27;t Let Your Policy Learn the Obvious and Forget the Rare</div>
<div class="meta-line">Authors: Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov, Viacheslav Sinii, Alexey Malakhov, Daniil Gavrilov</div>
<div class="meta-line">First: 2026-02-06T14:07:30+00:00 · Latest: 2026-02-06T14:07:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>F-GRPO：不要让你的策略学习显而易见的东西而忘记稀有的</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）通常基于组采样来估计优势并稳定策略更新。在实践中，由于计算限制，大组规模不可行，这使得学习偏向于已经可能的轨迹。较小的组往往错过稀有正确轨迹，同时仍包含混合奖励，将概率集中在常见解决方案上。我们推导了更新错过稀有正确模式的概率与组大小的关系，显示出非单调行为，并表征更新如何在正确集合内重新分配质量，揭示未采样的正确质量即使在总正确质量增长时也可能缩小。基于这一分析，我们提出了一种难度感知的优势缩放系数，灵感来自焦点损失，降低高成功提示的更新权重。这一轻量级修改可以直接集成到任何相对组的RLVR算法中，如GRPO、DAPO和CISPO。在Qwen2.5-7B的领域内和领域外基准测试中，我们的方法将pass@256从64.1提升到70.3（GRPO），69.3提升到72.5（DAPO），以及73.2提升到76.8（CISPO），同时保持或改善pass@1，而不增加组大小或计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of Reinforcement Learning with Verifiable Rewards (RLVR), particularly the bias introduced by group sampling that favors common trajectories while neglecting rare-correct ones. The authors analyze the probability of missing rare-correct modes based on group size and demonstrate how updates can inadvertently shrink unsampled-correct mass even as total correct mass increases. To counter this issue, they propose a difficulty-aware advantage scaling coefficient inspired by Focal loss, which down-weights updates on high-success prompts. Experimental results on Qwen2.5-7B show that their method significantly improves performance metrics across various benchmarks without increasing computational costs, enhancing pass rates for GRPO, DAPO, and CISPO algorithms while maintaining or improving pass@1 rates.</div>
<div class="mono" style="margin-top:8px">本研究解决了可验证奖励的强化学习（RLVR）中的局限性，特别是群体采样引入的偏差，使得常见轨迹受到青睐而忽视稀有正确轨迹。作者推导出一个概率函数，展示了群体大小如何影响错过稀有正确模式的可能性，以及更新如何可能无意中缩小未采样正确质量。为了解决这个问题，他们提出了一种难度感知的优势缩放系数，降低高成功提示的更新权重，可以集成到现有的RLVR算法中。对Qwen2.5-7B模型的实验结果显示，在各种基准测试中性能指标显著改善，提高了通过率，同时没有增加计算成本或群体大小。</div>
</details>
</div>
<div class="card">
<div class="title">SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers</div>
<div class="meta-line">Authors: Shentong Mo, Lanqing Li</div>
<div class="meta-line">First: 2026-02-06T13:50:13+00:00 · Latest: 2026-02-06T13:50:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06706v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SaDiT：通过潜在结构标记化和扩散变换器实现高效蛋白质主链设计</div>
<div class="mono" style="margin-top:8px">用于新型蛋白质主链设计的生成模型在创造新蛋白质结构方面取得了显著成功。然而，这些基于扩散的方法在大规模结构探索中仍然计算密集且速度较慢。尽管最近的努力如Proteina引入了流匹配以提高采样效率，但在蛋白质领域，标记化在结构压缩和加速方面的潜力仍然未被充分探索。在本研究中，我们提出了SaDiT，一个通过将SaProt标记化与扩散变换器（DiT）架构相结合来加速蛋白质主链生成的新框架。SaDiT利用离散潜在空间表示蛋白质几何形状，显著降低了生成过程的复杂性，同时保持理论上的SE(3)等价性。为了进一步提高效率，我们引入了一种IPA标记缓存机制，通过在迭代采样过程中重用计算的标记状态来优化不变点注意力（IPA）层。实验结果表明，SaDiT在计算速度和结构可行性方面均优于最先进的模型，包括RFDiffusion和Proteina。我们在无条件主链生成和折叠类别条件生成任务中评估了我们的模型，SaDiT在捕捉复杂拓扑特征和高设计性方面表现出优越的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of protein backbone design, which has been hindered by the computational intensity of existing diffusion-based models. The authors introduce SaDiT, a framework that combines SaProt Tokenization with a Diffusion Transformer architecture to streamline the generation process by utilizing a discrete latent space for protein geometry representation. Experimental results indicate that SaDiT surpasses current leading models, such as RFDiffusion and Proteina, in terms of both speed and the structural viability of generated proteins, demonstrating its effectiveness in unconditional and fold-class conditional generation tasks while capturing complex topological features with high designability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高蛋白质主链设计的效率，而现有的基于扩散的模型在计算上过于复杂。作者提出了SaDiT框架，将SaProt标记化与扩散变换器架构相结合，通过利用离散潜在空间来简化生成过程。实验结果表明，SaDiT在计算速度和结构可行性方面均优于当前领先模型，如RFDiffusion和Proteina，证明其在无条件和折叠类别条件主链生成任务中的有效性，同时能够捕捉复杂的拓扑特征并具有较高的设计性。</div>
</details>
</div>
<div class="card">
<div class="title">Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs</div>
<div class="meta-line">Authors: Ying Song, Balaji Palanisamy</div>
<div class="meta-line">First: 2026-02-06T13:37:24+00:00 · Latest: 2026-02-06T13:37:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06700v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Taipan：一种基于转移的多敏感属性推断攻击，无需查询，仅依赖公开发布的图</div>
<div class="mono" style="margin-top:8px">图结构数据支撑着现代应用的广泛领域。然而，复杂的图拓扑和同质模式可能通过使敏感信息泄露在局部邻域中传播，从而促进属性推断攻击（AIA）。现有的AIA主要假设对手可以通过重复模型查询来探测敏感属性。这种假设在现实世界中往往不切实际，因为严格的数据保护法规、昂贵的查询预算和更高的检测风险，尤其是在推断多个敏感属性时。更重要的是，这种以模型为中心的视角掩盖了一个普遍的盲点：仅由公开发布的图引起的内在多敏感信息泄露。为了利用这一未被探索的脆弱性，我们引入了一种新的攻击范式，并提出了Taipan，首个基于转移的无查询多敏感属性推断攻击框架（G-MSAIA）。Taipan集成了层次攻击知识路由，以捕捉复杂的属性间相关性，并采用提示引导的攻击原型精炼，以减轻负转移和性能下降。我们进一步提出了一个针对G-MSAIA的系统评估框架。在多种真实世界图数据集上的广泛实验表明，Taipan在同分布设置和异质相似及分布外设置中，尽管特征维度不匹配，始终实现了强大的攻击性能，并在严格的差分隐私保障下仍然有效。我们的研究结果强调了对更强大的多属性隐私保护图发布方法和数据共享实践的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the vulnerabilities in graph-structured data that allow for sensitive attribute inference attacks (AIAs) without the necessity of model queries, which are often impractical in real-world scenarios. The authors propose Taipan, a novel query-free transfer-based attack framework designed for multiple sensitive attribute inference attacks on graphs, utilizing Hierarchical Attack Knowledge Routing to understand inter-attribute correlations and Prompt-guided Attack Prototype Refinement to enhance performance. Experimental results on various real-world graph datasets reveal that Taipan demonstrates strong attack performance across different distribution settings and maintains effectiveness even under strict differential privacy conditions, highlighting the necessity for improved privacy-preserving methods in graph data sharing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图结构数据中敏感信息泄露的脆弱性，特别是在传统属性推断攻击依赖于重复模型查询的情况下，这在数据保护法规下往往不切实际。作者提出了Taipan，这是一种新颖的无查询转移基础的攻击框架，旨在进行图上的多敏感属性推断攻击。实验结果表明，Taipan有效捕捉属性间的相关性并减轻性能下降，在各种真实世界图数据集上实现了强大的攻击性能，即使在严格的差分隐私条件下也能保持有效，突显了改进图发布中的隐私保护方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts</div>
<div class="meta-line">Authors: Victor Armegioiu</div>
<div class="meta-line">First: 2026-02-06T13:21:52+00:00 · Latest: 2026-02-06T13:21:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete Grönwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于记忆的流匹配用于稳定的自回归偏微分方程展开</div>
<div class="mono" style="margin-top:8px">自回归生成偏微分方程求解器在一步预测中可以准确，但在长时间展开中会漂移，特别是在粗到细的范围内，每一步必须重新生成未解决的细尺度。这是扩散和流匹配生成器的范围：尽管它们的内部动态是马尔可夫的，但展开稳定性受每一步的条件法则误差控制。利用莫里-兹万齐 projection 形式，我们表明消除未解决变量会产生一个带有马尔可夫项、记忆项和正交强迫的精确解析演化，揭示了无记忆闭合的结构限制。基于此，我们引入基于记忆的扩散/流匹配，通过潜在特征将紧凑的在线状态注入去噪。通过分解，记忆为未解决尺度引入了结构化的条件尾先验，并减少了填充缺失频率所需的传输。我们证明了所得到的条件核的瓦瑟斯坦稳定性。然后，我们推导出离散的格伦瓦尔展开界限，将记忆近似与条件生成误差分开。在具有冲击和多尺度混合的可压缩流体实验中，显示出更高的准确性和显著更稳定的长时间展开，具有更好的细尺度谱和统计保真度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of drift in autoregressive generative PDE solvers during long rollouts, particularly in coarse-to-fine regimes where unresolved fine scales must be regenerated. The authors employ the Mori–Zwanzig projection formalism to demonstrate that eliminating unresolved variables leads to a more stable resolved evolution characterized by a Markov term, a memory term, and orthogonal forcing. Their proposed memory-conditioned diffusion/flow-matching method incorporates a compact online state into denoising, resulting in improved accuracy and stability in long-horizon rollouts, as evidenced by experiments on compressible flows with shocks and multiscale mixing, which show enhanced fine-scale spectral and statistical fidelity.</div>
<div class="mono" style="margin-top:8px">本研究解决了自回归生成PDE求解器在长时间滚动中漂移的问题，特别是在粗到细的范围内，需要重新生成未解析的细尺度。作者采用Mori-Zwanzig投影形式主义，证明消除未解析变量可以导致更稳定的演化，特征为马尔可夫项、记忆项和正交强迫项。他们提出的记忆条件扩散/流匹配方法将紧凑的在线状态引入去噪，结果在长时间滚动中提高了准确性和稳定性，实验表明在具有冲击和多尺度混合的可压缩流动中，细尺度保真度和未解析频率的运输需求均有所减少。</div>
</details>
</div>
<div class="card">
<div class="title">Adventures in Demand Analysis Using AI</div>
<div class="meta-line">Authors: Philipp Bach, Victor Chernozhukov, Sven Klaassen, Martin Spindler, Jan Teichert-Kluge, Suhas Vijaykumar</div>
<div class="meta-line">First: 2024-12-31T10:33:10+00:00 · Latest: 2026-02-06T11:46:31+00:00</div>
<div class="meta-line">Comments: 35 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.00382v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.00382v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on textit{Amazon.com}, we combine text descriptions, images, and tabular covariates to represent each product using transformer-based embedding models. These embeddings capture nuanced attributes, such as quality, branding, and visual characteristics, that traditional methods often struggle to summarize. Moreover, we fine-tune these embeddings for causal inference tasks. We show that the resulting embeddings substantially improve the predictive accuracy of sales ranks and prices and that they lead to more credible causal estimates of price elasticity. Notably, we uncover strong heterogeneity in price elasticity driven by these product-specific features. Our findings illustrate that AI-driven representations can enrich and modernize empirical demand analysis. The insights generated may also prove valuable for applied causal inference more broadly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用人工智能进行需求分析的冒险</div>
<div class="mono" style="margin-top:8px">本文通过整合来自人工智能（AI）的多模态产品表示，推动了实证需求分析。我们使用来自亚马逊网站的详细玩具车数据集，结合文本描述、图像和表格协变量，利用基于变换器的嵌入模型表示每个产品。这些嵌入捕捉了传统方法常常难以总结的细微属性，如质量、品牌和视觉特征。此外，我们对这些嵌入进行了因果推断任务的微调。我们展示了生成的嵌入显著提高了销售排名和价格的预测准确性，并导致了更可信的价格弹性因果估计。值得注意的是，我们发现这些产品特定特征驱动的价格弹性存在强烈异质性。我们的研究结果表明，基于AI的表示可以丰富和现代化实证需求分析。生成的见解也可能对更广泛的应用因果推断具有价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional demand analysis methods by incorporating multimodal product representations derived from artificial intelligence. The authors utilize a dataset of toy cars from Amazon.com, employing transformer-based embedding models to combine text descriptions, images, and tabular covariates for each product. The results demonstrate that these AI-generated embeddings significantly enhance the predictive accuracy of sales ranks and prices, while also providing more reliable causal estimates of price elasticity, revealing notable heterogeneity in price elasticity influenced by specific product features.</div>
<div class="mono" style="margin-top:8px">本文通过整合人工智能的多模态产品表示，解决了传统需求分析的局限性，旨在提高需求预测的准确性。作者利用亚马逊网站上的玩具车数据集，采用基于变换器的嵌入模型，将文本描述、图像和表格协变量整合到每个产品中。实验结果表明，这些人工智能生成的嵌入显著提高了销售排名和价格的预测准确性，同时也提供了更可靠的价格弹性因果估计，揭示了受特定产品特征影响的显著异质性。</div>
</details>
</div>
<div class="card">
<div class="title">DECEPTICON: How Dark Patterns Manipulate Web Agents</div>
<div class="meta-line">Authors: Phil Cuvin, Hao Zhu, Diyi Yang</div>
<div class="meta-line">First: 2025-12-28T11:55:20+00:00 · Latest: 2026-02-06T11:44:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22894v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22894v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DECEPTICON：黑暗模式如何操控网络代理</div>
<div class="mono" style="margin-top:8px">欺骗性的用户界面设计，广泛存在于网络上，通常被称为黑暗模式，操控用户执行与其目标不一致的行为。本文展示了黑暗模式在引导代理轨迹方面的高效性，给代理的鲁棒性带来了显著风险。为了量化这一风险，我们引入了DECEPTICON，一个用于单独测试黑暗模式的环境。DECEPTICON包含700个带有黑暗模式的网络导航任务——600个生成任务和100个真实任务，旨在测量遵循指令的成功率和黑暗模式的有效性。在最先进的代理中，我们发现黑暗模式成功地将代理轨迹引导向恶意结果，在超过70%的测试生成和真实任务中表现如此——而人类的平均值为31%。此外，我们发现黑暗模式的有效性与模型大小和测试时推理能力正相关，使得更大、更强大的模型更易受影响。针对对抗性攻击的主要对策，包括上下文提示和护栏模型，未能持续降低黑暗模式干预的成功率。我们的研究结果揭示了黑暗模式对网络代理构成潜在且未缓解的风险，强调了对抗操控设计的强大防御的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to investigate how deceptive user interface designs, known as dark patterns, manipulate web agents and pose risks to their robustness. The authors introduce DECEPTICON, an experimental environment that tests the effectiveness of individual dark patterns through 700 web navigation tasks, including both generated and real-world scenarios, to measure instruction-following success and dark pattern impact. The results indicate that dark patterns successfully direct agent trajectories towards harmful outcomes in over 70% of the tasks tested, significantly higher than the 31% average for humans, and show that larger models are more vulnerable to these manipulations, with existing countermeasures proving ineffective.</div>
<div class="mono" style="margin-top:8px">本研究探讨了用户界面设计中的黑暗模式对网络代理的影响，这些设计会操控代理，给其稳健性带来风险。作者引入了DECEPTICON，一个包含700个网页导航任务的实验环境，包括生成的任务和真实场景，以评估黑暗模式对代理性能的影响。结果表明，黑暗模式在超过70%的任务中成功误导代理，显著高于人类的31%平均水平，并且发现更大的模型更容易受到影响，而现有的对策并未有效减轻这些风险。</div>
</details>
</div>
<div class="card">
<div class="title">PromptSplit: Revealing Prompt-Level Disagreement in Generative Models</div>
<div class="meta-line">Authors: Mehdi Lotfian, Mohammad Jalali, Farzan Farnia</div>
<div class="meta-line">First: 2026-02-03T20:53:10+00:00 · Latest: 2026-02-06T11:34:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04009v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04009v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PromptSplit：揭示生成模型中的提示级别分歧</div>
<div class="mono" style="margin-top:8px">提示引导的生成AI模型在视觉和语言领域迅速扩展，从文本输入中生成逼真且多样的输出。这类模型的多样性不断增加，训练数据和架构各异，亟需有原则的方法来识别哪些类型的提示导致不同的模型行为。在本研究中，我们提出了PromptSplit，一个基于核的方法，用于检测和分析生成模型之间的提示依赖性分歧。对于每对比较的模型，PromptSplit通过形成提示和图像（或文本）特征的张量积嵌入，构建联合提示-输出表示，然后计算相应的核协方差矩阵。我们利用这些矩阵之间加权差异的特征空间来识别提示之间行为差异的主要方向。为了确保可扩展性，我们采用随机投影近似，将计算复杂度降低到$O(nr^2 + r^3)$，其中投影维度为$r$。我们进一步提供理论分析，表明该近似产生的特征结构估计与全维结果的期望偏差被界定为$O(1/r^2)$。在文本到图像、文本到文本和图像字幕设置中的实验表明，PromptSplit能够准确检测真实的行为差异并隔离出负责的提示，为检测生成模型之间的分歧提供了可解释的工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the need for methods that can identify which types of prompts lead to distinct behaviors in generative AI models, given their rapid expansion and diversity. The authors propose PromptSplit, a kernel-based framework that constructs joint prompt-output representations and computes kernel covariance matrices to analyze prompt-dependent disagreements between model pairs. Experimental results across various settings, including text-to-image and image-captioning, show that PromptSplit effectively detects behavioral differences and isolates the prompts responsible for these discrepancies, providing an interpretable tool for understanding generative model disagreements.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决识别不同提示如何影响生成AI模型行为的需求，这些模型在各个领域变得越来越普遍。作者提出了PromptSplit，这是一种基于核的方法，通过构建联合提示-输出表示并计算核协方差矩阵来分析生成模型之间的提示依赖性分歧。跨文本到图像、文本到文本和图像标题生成任务的实验结果表明，PromptSplit有效地检测到行为差异，并识别出导致这些差异的特定提示，为理解模型之间的分歧提供了有价值的工具。</div>
</details>
</div>
<div class="card">
<div class="title">DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters</div>
<div class="meta-line">Authors: Haoran Zhang, Haixuan Liu, Yong Liu, Yunzhong Qiu, Yuxuan Wang, Jianmin Wang, Mingsheng Long</div>
<div class="meta-line">First: 2026-02-06T10:48:13+00:00 · Latest: 2026-02-06T10:48:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06597v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06597v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiTS：多模态扩散变换器是时间序列预测器</div>
<div class="mono" style="margin-top:8px">尽管时间序列的生成建模促进了更强大和灵活的概率预测，但现有的生成时间序列模型并未很好地解决时间序列数据的多维特性。流行的扩散变换器（DiT）架构依赖于简单的条件控制和单流变换器主干，往往未能充分利用协变量感知预测中的跨变量依赖。受到将文本指导整合到视频生成中的多模态扩散变换器的启发，我们提出了时间序列扩散变换器（DiTS），这是一种通用架构，将内生和外生变量框架视为不同的模态。为了更好地捕捉变量间和变量内的依赖关系，我们设计了一个针对时间序列数据的双流变换器模块，包括一个用于沿时间维度自回归建模的时间注意力模块和一个用于跨变量建模的变量注意力模块。与常见的将2D标记网格展平为1D序列的方法不同，我们的设计利用了多变量依赖中固有的低秩特性，从而降低了计算成本。实验表明，DiTS在基准测试中实现了最先进的性能，无论未来外生变量观测的存在与否，展示了相较于传统确定性深度预测模型的独特生成预测优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing generative time series models in capturing the multi-dimensional properties of time series data. The authors propose a novel architecture called Diffusion Transformers for Time Series (DiTS), which utilizes a dual-stream Transformer block designed to handle both inter-variate and intra-variate dependencies effectively. Experimental results indicate that DiTS outperforms traditional deterministic deep forecasting models across various benchmarks, showcasing its superior generative forecasting capabilities even in the absence of future exogenous variate observations.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有生成时间序列模型在捕捉时间序列数据多维特性方面的局限性。作者提出了一种新颖的架构，称为时间序列扩散变换器（DiTS），该架构利用双流变换器模块，通过结合时间注意力模块和变量注意力模块，有效建模变量间和变量内的依赖关系。实验结果表明，DiTS在各种基准测试中超越了传统的确定性深度预测模型，展示了其在生成预测中的有效性，即使在缺乏未来外生变量观测的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew</div>
<div class="meta-line">Authors: Andy Rosenbaum, Assaf Siani, Ilan Kernerman</div>
<div class="meta-line">First: 2026-02-06T09:51:45+00:00 · Latest: 2026-02-06T09:51:45+00:00</div>
<div class="meta-line">Comments: Accepted to LoResLM at EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06546v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06546v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MTQE.en-he：英希机器翻译质量评估</div>
<div class="mono" style="margin-top:8px">我们发布了MTQE.en-he：据我们所知，这是第一个公开可用的英希机器翻译质量评估基准。MTQE.en-he包含来自WMT24++的959个英语段落，每个段落都配有希伯来语的机器翻译，以及由三位人类专家注释的翻译质量直接评估分数。我们对ChatGPT提示、TransQuest和CometKiwi进行了基准测试，结果表明，三种模型的集成优于最佳单一模型（CometKiwi）6.4个百分点的Pearson和5.6个百分点的Spearman。与TransQuest和CometKiwi的微调实验表明，完整模型更新对过拟合和分布崩溃敏感，而参数高效的方法（LoRA、BitFit和FTHead，即仅微调分类头）训练稳定，并带来2-3个百分点的提升。MTQE.en-he及我们的实验结果为未来对这一资源匮乏的语言对的研究提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of publicly available benchmarks for Machine Translation Quality Estimation specifically for English-Hebrew translations. The authors introduce MTQE.en-he, a dataset comprising 959 English segments paired with Hebrew translations and quality scores assessed by human experts. They benchmark three models—ChatGPT prompting, TransQuest, and CometKiwi—and find that an ensemble of these models surpasses the performance of the best individual model, CometKiwi, by 6.4 percentage points in Pearson correlation and 5.6 percentage points in Spearman correlation. Additionally, fine-tuning experiments indicate that while full-model updates can lead to overfitting, parameter-efficient methods like LoRA, BitFit, and FTHead provide stable training and yield improvements of 2-3 percentage points.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决英语-希伯来语语言对机器翻译质量评估缺乏公开基准的问题。作者推出了MTQE.en-he，包含959个英语段落及其对应的希伯来语翻译和由人类专家评估的质量评分。他们对三种模型（ChatGPT提示、TransQuest和CometKiwi）进行了基准测试，发现这三种模型的集成效果比最佳单一模型CometKiwi提高了6.4个百分点的Pearson相关性和5.6个百分点的Spearman相关性。此外，微调实验表明，尽管全模型更新可能导致过拟合，但像LoRA、BitFit和FTHead这样的参数高效方法能够实现稳定训练，并提高2-3个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">High-Precision Edge Detection via Task-Adaptive Texture Handling and Ideal-Prior Guidance</div>
<div class="meta-line">Authors: Hao Shu</div>
<div class="meta-line">First: 2024-07-29T13:24:55+00:00 · Latest: 2026-02-06T09:19:08+00:00</div>
<div class="meta-line">Comments: 30 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.19992v5">Abs</a> · <a href="https://arxiv.org/pdf/2407.19992v5">PDF</a> · <a href="https://github.com/Hao-B-Shu/SDPED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image edge detection (ED) requires specialized architectures, reliable supervision, and rigorous evaluation criteria to ensure accurate localization. In this work, we present a framework for high-precision ED that jointly addresses architectural design, data supervision, and evaluation consistency. We propose SDPED, a compact ED model built upon Cascaded Skipping Density Blocks (CSDB), motivated by a task-adaptive architectural transfer from image super-resolution. By re-engineering texture-oriented structures for ED, SDPED effectively differentiates textures from edges while preserving fine spatial precision. Extensive experiments on four benchmark datasets (BRIND, UDED, MDBD, and BIPED2) demonstrate consistent performance improvements, particularly in Average Precision (AP), with gains of up to 22.5% on MDBD and 11.8% on BIPED2. In addition, we introduce an ideal-prior guidance strategy that incorporates noiseless data into training by treating labels as noise-free samples, providing a practical means to mitigate the subjectivity and noise inherent in human annotations. To enable fair and resolution-independent evaluation, we further adopt a fixed-pixel criterion for assessing localization accuracy. Overall, this work offers a coherent solution for high-precision ED and provides insights applicable to precision-oriented modeling in low-level and soft-computing-based vision tasks. Codes can be found on https://github.com/Hao-B-Shu/SDPED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过任务自适应纹理处理和理想先验指导实现高精度边缘检测</div>
<div class="mono" style="margin-top:8px">图像边缘检测（ED）需要专门的架构、可靠的监督和严格的评估标准，以确保准确的定位。在这项工作中，我们提出了一个高精度ED框架，联合解决架构设计、数据监督和评估一致性。我们提出了SDPED，一个基于级联跳跃密度块（CSDB）的紧凑ED模型，受到图像超分辨率任务自适应架构转移的启发。通过重新设计面向纹理的结构，SDPED有效地区分纹理和边缘，同时保持细致的空间精度。在四个基准数据集（BRIND、UDED、MDBD和BIPED2）上的广泛实验表明，性能一致性提高，特别是在平均精度（AP）上，MDBD提高了22.5%，BIPED2提高了11.8%。此外，我们引入了一种理想先验指导策略，通过将标签视为无噪声样本，将无噪声数据纳入训练，提供了一种实用的方法来减轻人类注释中固有的主观性和噪声。为了实现公平和分辨率无关的评估，我们进一步采用了固定像素标准来评估定位精度。总体而言，这项工作为高精度ED提供了一个连贯的解决方案，并为低级和基于软计算的视觉任务中的精度导向建模提供了可应用的见解。代码可在https://github.com/Hao-B-Shu/SDPED找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the accuracy of image edge detection (ED) by addressing challenges in architectural design, data supervision, and evaluation consistency. The authors propose a compact edge detection model called SDPED, which utilizes Cascaded Skipping Density Blocks (CSDB) and adapts architectural elements from image super-resolution. Experimental results on four benchmark datasets reveal significant performance improvements, with Average Precision (AP) increases of up to 22.5% on MDBD and 11.8% on BIPED2, alongside the introduction of an ideal-prior guidance strategy that reduces noise in human annotations and a fixed-pixel criterion for consistent evaluation.</div>
<div class="mono" style="margin-top:8px">本研究通过提出一个整合架构设计、数据监督和评估一致性的框架，解决了图像边缘检测（ED）的挑战，以实现高精度结果。该方法名为SDPED，利用级联跳跃密度块（CSDB）从图像超分辨率中自适应转移架构原理，使得在保持空间精度的同时有效区分纹理和边缘。四个基准数据集上的实验结果显示出显著的性能提升，平均精度在MDBD上提高了22.5%，在BIPED2上提高了11.8%，同时引入了一种理想先验指导策略，减少了训练过程中人类注释的噪声，并采用固定像素标准来评估准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting the Shape Convention of Transformer Language Models</div>
<div class="meta-line">Authors: Feng-Ting Liao, Meng-Hsi Chen, Guan-Ting Yi, Da-shan Shiu</div>
<div class="meta-line">First: 2026-02-06T07:55:30+00:00 · Latest: 2026-02-06T07:55:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06471v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视变压器语言模型的形状约定</div>
<div class="mono" style="margin-top:8px">密集的变压器语言模型在架构形状上基本保持一致：每一层由一个注意力模块和一个前馈网络（FFN）组成，前者采用窄-宽-窄的多层感知器（MLP），在扩展比为2到4之间分配大部分参数。受到最近研究结果的启发，残差宽-窄-宽（沙漏形）MLP提供了更优的函数逼近能力，我们重新审视了变压器中长期以来的MLP形状约定，质疑窄-宽-窄设计的必要性。为此，我们开发了一种变压器变体，用更深的沙漏形FFN替代传统的FFN，该FFN由一系列通过残差路径连接的沙漏子MLP堆叠而成。我们认为，更深但更轻的沙漏FFN可以作为传统FFN的竞争替代品，并且通过使用更轻的沙漏FFN节省的参数可以更有效地利用，例如在固定预算下扩大模型的隐藏维度。我们通过不同模型规模的实证验证确认了这些观点：沙漏FFN在高达4亿参数的情况下优于传统FFN，并在更大规模（如10亿参数）时达到可比性能；在匹配预算的情况下，减少FFN和增加注意力参数的沙漏FFN变体在传统配置上显示出一致的改进。这些发现为近期的研究提供了新的视角，并促使我们重新思考窄-宽-窄MLP约定以及在高效和表现力现代语言模型中注意力与FFN之间的平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the potential for improved function approximation in Transformer language models through alternative MLP architectures. The authors propose a Transformer variant that replaces the traditional narrow-wide-narrow feed-forward network (FFN) with a deeper hourglass-shaped FFN, which consists of hourglass sub-MLPs connected by residual pathways. Experimental results demonstrate that hourglass FFNs outperform conventional FFNs in models up to 400M parameters and achieve comparable performance in larger models, indicating that a lighter hourglass FFN can effectively utilize parameters and improve the balance between attention and FFN in modern language models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于残差宽窄宽（hourglass）MLP相较于传统窄宽窄设计在Transformer语言模型中的潜在优势。作者提出了一种Transformer变体，采用更深的hourglass形状前馈网络（FFN）来替代传统FFN，旨在证明这种设计可以作为一种具有竞争力的替代方案，同时允许更有效的参数利用。实验结果表明，hourglass FFN在参数量达到4亿的模型中优于传统FFN，并在更大规模的模型中表现出可比的性能，这表明重新思考MLP形状约定可能会提高现代语言模型的效率和表现力。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Modeling via Drifting</div>
<div class="meta-line">Authors: Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</div>
<div class="meta-line">First: 2026-02-04T17:06:49+00:00 · Latest: 2026-02-06T07:18:33+00:00</div>
<div class="meta-line">Comments: Project page: https://lambertae.github.io/projects/drifting/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04770v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04770v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lambertae.github.io/projects/drifting/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>漂移生成建模</div>
<div class="mono" style="margin-top:8px">生成建模可以被表述为学习一个映射 f，使其推前分布与数据分布匹配。推前行为可以在推理时迭代进行，例如在扩散和流动模型中。在本文中，我们提出了一种新的范式，称为漂移模型，它在训练过程中演变推前分布，并自然地允许一步推理。我们引入了一个漂移场，控制样本运动，并在分布匹配时达到平衡。这导致了一个训练目标，使神经网络优化器能够演变分布。在实验中，我们的一步生成器在 256 x 256 分辨率下在 ImageNet 上取得了最先进的结果，潜在空间的 FID 为 1.54，像素空间的 FID 为 1.61。我们希望我们的工作为高质量的一步生成开辟新的机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve generative modeling by developing a method that allows for efficient one-step inference while maintaining high-quality output. The authors introduce a novel approach called Drifting Models, which involves evolving the pushforward distribution during training through a drifting field that governs sample movement until equilibrium is reached. Experimental results demonstrate that their one-step generator achieves state-of-the-art performance on ImageNet at a resolution of 256 x 256, with an FID score of 1.54 in latent space and 1.61 in pixel space, indicating significant advancements in generative modeling capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过开发一种方法来提高生成建模的效率，使其能够在保持高质量输出的同时实现高效的一步推断。作者提出了漂移模型，该模型在训练过程中通过漂移场演变推送分布，并在分布匹配时实现平衡。实验结果表明，他们的一步生成器在256 x 256分辨率的ImageNet上达到了最先进的性能，潜在空间的FID得分为1.54，像素空间的FID得分为1.61。</div>
</details>
</div>
<div class="card">
<div class="title">ChatUMM: Robust Context Tracking for Conversational Interleaved Generation</div>
<div class="meta-line">Authors: Wenxun Dai, Zhiyuan Zhao, Yule Zhong, Yiji Cheng, Jianwei Zhang, Linqing Wang, Shiyi Zhang, Yunlong Lin, Runze He, Fellix Song, Wayne Zhuang, Yong Liu, Haoji Zhang, Yansong Tang, Qinglin Lu, Chunyu Wang</div>
<div class="meta-line">First: 2026-02-06T07:11:50+00:00 · Latest: 2026-02-06T07:11:50+00:00</div>
<div class="meta-line">Comments: ChatUMM Project</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06442v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor&#x27;&#x27; turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ChatUMM：对话交错生成的稳健上下文跟踪</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）取得了显著进展，但仍受限于单轮交互范式，实际上作为独立请求的求解器，而非持续对话中的助手。为弥补这一差距，我们提出了ChatUMM。作为一个对话统一模型，它在稳健的上下文跟踪方面表现出色，以维持交错的多模态生成。ChatUMM的能力源于两个关键创新：一种交错多轮训练策略，将序列化的文本-图像流建模为连续的对话流，以及一个系统化的对话数据合成管道。该管道通过三个渐进阶段将多样化的标准单轮数据集转化为流畅的对话：构建基本的有状态对话，通过带有历史依赖查询重写的“干扰者”轮次强制执行长程依赖解析，以及合成自然交错的多模态响应。广泛的评估表明，ChatUMM在视觉理解和指令引导编辑基准测试中，在开源统一模型中实现了最先进的性能，同时在文本到图像生成中保持了竞争力的保真度。值得注意的是，ChatUMM在复杂的多轮场景中表现出卓越的稳健性，确保流畅、上下文感知的对话。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of unified multimodal models (UMMs) in continuous dialogue, as they currently operate mainly within a single-turn interaction framework. The authors introduce ChatUMM, which employs an interleaved multi-turn training strategy and a systematic conversational data synthesis pipeline to improve context tracking and facilitate interleaved multimodal generation. Experimental results indicate that ChatUMM outperforms existing open-source unified models in visual understanding and instruction-guided editing tasks, while also demonstrating strong performance in text-to-image generation and superior robustness in complex multi-turn interactions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强统一多模态模型（UMMs）在连续对话中的能力，因为它们目前主要在单轮交互框架内运作。作者提出了ChatUMM，采用交错多轮训练策略和系统的对话数据合成管道，以促进交错多模态生成的稳健上下文跟踪。实验结果表明，ChatUMM在视觉理解和指令引导编辑任务中优于现有的开源统一模型，同时在文本到图像生成方面也表现出色，尤其是在需要上下文意识的复杂多轮交互中。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters</div>
<div class="meta-line">Authors: Yuxiang Zhao, Yirong Yang, Yanqing Zhu, Yanfen Shen, Chiyu Wang, Zhining Gu, Pei Shi, Wei Guo, Mu Xu</div>
<div class="meta-line">First: 2026-02-06T06:52:23+00:00 · Latest: 2026-02-06T06:52:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06427v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06427v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥合室内外差距：以视觉为中心的指令引导的具身导航用于最后一米</div>
<div class="mono" style="margin-top:8px">具身导航在现实世界应用中具有重要前景，例如最后一公里配送。然而，大多数现有方法仅限于室内或室外环境，并且严重依赖于强假设，例如对精确坐标系统的访问。虽然当前的室外方法可以使用粗粒度定位引导代理到目标附近，但它们未能实现通过特定建筑入口的精细进入，严重限制了其在需要无缝室外到室内过渡的实际部署场景中的实用性。为了解决这一问题，我们引入了一项新任务：无外部先验的指令驱动的具身导航。该公式明确消除了对准确外部先验的依赖，要求代理仅根据指令引导的自我中心视觉观察进行导航。为了解决这一任务，我们提出了一个以视觉为中心的具身导航框架，利用基于图像的提示来驱动决策。此外，我们还提出了该任务的第一个开源数据集，具有将轨迹条件视频合成集成到数据生成过程中的管道。通过广泛的实验，我们证明了我们提出的方法在成功率和路径效率等关键指标上始终优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing navigation methods that are either indoor or outdoor focused and rely on precise coordinate systems, which restrict their practical applications in scenarios requiring seamless transitions between environments. The authors propose a vision-centric embodied navigation framework that enables agents to navigate using only egocentric visual observations and instruction-driven prompts, eliminating the need for accurate external priors. Experimental results show that this approach significantly outperforms state-of-the-art methods in terms of success rate and path efficiency, demonstrating its effectiveness for last-mile delivery applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有导航方法的局限性，这些方法要么专门针对室内或室外环境，要么依赖于精确的坐标系统，这限制了它们在需要无缝过渡的实际应用场景中的有效性。作者提出了一种以视觉为中心的具身导航框架，使代理能够仅通过自我中心的视觉观察和基于指令的提示进行导航，消除了对准确外部先验的需求。实验结果表明，该方法在成功率和路径效率等关键指标上优于最先进的方法，证明了其在最后一米导航任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</div>
<div class="meta-line">Authors: Yunze Tong, Mushui Liu, Canyu Zhao, Wanggui He, Shiyi Zhang, Hongwei Zhang, Peng Zhang, Jinlong Liu, Ju Huang, Jiamang Wang, Hao Jiang, Pipei Huang</div>
<div class="meta-line">First: 2026-02-06T06:37:10+00:00 · Latest: 2026-02-06T06:37:10+00:00</div>
<div class="meta-line">Comments: 18 pages, in submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06422v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06422v1">PDF</a> · <a href="https://github.com/YunzeTong/TurningPoint-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action&#x27;s &quot;pure&quot; effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过建模逐步和长期采样效应来缓解稀疏奖励的流基GRPO</div>
<div class="mono" style="margin-top:8px">在流匹配模型上部署GRPO已被证明对文本到图像生成有效。然而，现有范式通常将基于结果的奖励传播到所有前面的去噪步骤，而不区分每个步骤的局部效应。此外，当前的组内排名主要比较匹配时间步的轨迹，忽略了轨迹内的依赖关系，其中某些早期去噪动作可以通过延迟的隐式交互影响后续状态。我们提出了TurningPoint-GRPO（TP-GRPO），这是一个GRPO框架，缓解了逐步奖励稀疏性，并明确建模去噪轨迹中的长期效应。TP-GRPO有两个关键创新：（i）它用基于步骤的增量奖励替代基于结果的奖励，提供了一个密集的、关注步骤的学习信号，更好地隔离每个去噪动作的“纯”效应；（ii）它识别转折点——翻转局部奖励趋势的步骤，并使后续奖励演变与整体轨迹趋势一致——并为这些动作分配一个聚合的长期奖励，以捕捉它们的延迟影响。转折点仅通过增量奖励的符号变化来检测，使TP-GRPO高效且无超参数。大量实验还表明，TP-GRPO更有效地利用奖励信号，并持续改善生成。演示代码可在https://github.com/YunzeTong/TurningPoint-GRPO获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of sparse rewards in text-to-image generation using GRPO on Flow Matching models, which typically apply outcome-based rewards uniformly across denoising steps. The authors introduce TurningPoint-GRPO (TP-GRPO), a novel framework that replaces these outcome-based rewards with step-level incremental rewards to provide a more precise learning signal for each denoising action. Additionally, TP-GRPO identifies turning points in the reward trend to assign long-term rewards that reflect delayed impacts, leading to more effective reward signal utilization. Experimental results indicate that TP-GRPO consistently enhances generation performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了在使用GRPO进行文本到图像生成时稀疏奖励的问题，现有方法未能充分考虑各个去噪步骤的影响。作者提出了TurningPoint-GRPO（TP-GRPO），创新性地用步骤级增量奖励替代基于结果的奖励，并识别影响奖励趋势的转折点，从而在去噪过程中建模长期效果。实验结果表明，TP-GRPO有效利用奖励信号，与传统方法相比，生成质量持续改善。</div>
</details>
</div>
<div class="card">
<div class="title">T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Synthesis in Controllable Concept Art Generation</div>
<div class="meta-line">Authors: Zhenhong Sun, Yifu Wang, Yonhon Ng, Yongzhi Xu, Daoyi Dong, Hongdong Li, Pan Ji</div>
<div class="meta-line">First: 2024-12-18T04:01:32+00:00 · Latest: 2026-02-06T06:29:47+00:00</div>
<div class="meta-line">Comments: https://openreview.net/forum?id=lyn2BgKQ8F</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.13486v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.13486v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">2D concept art generation for 3D scenes is a crucial yet challenging task in computer graphics, as creating natural intuitive environments still demands extensive manual effort in concept design. While generative AI has simplified 2D concept design via text-to-image synthesis, it struggles with complex multi-instance scenes and offers limited support for structured terrain layout. In this paper, we propose a Training-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after reviewing the entire cross-attention mechanism. This scheme revitalizes the ControlNet model for detailed multi-instance generation via three key modules: Prompt Balance ensures keyword representation and minimizes the risk of missing critical instances; Characteristic Priority emphasizes sketch-based features by highlighting TopK indices in feature channels; and Dense Tuning refines contour details within instance-related regions of the attention map. Leveraging the controllability of T3-S2S, we also introduce a feature-sharing strategy with dual prompt sets to generate layer-aware isometric and terrain-view representations for the terrain layout. Experiments show that our sketch-to-scene workflow consistently produces multi-instance 2D scenes with details aligned with input prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T$^3$-S2S：无训练三元组调优用于可控概念艺术生成中的草图到场景合成</div>
<div class="mono" style="margin-top:8px">3D场景的2D概念艺术生成是计算机图形学中一项关键但具有挑战性的任务，因为创建自然直观的环境仍然需要在概念设计上进行大量手动努力。尽管生成性AI通过文本到图像合成简化了2D概念设计，但在复杂的多实例场景中仍然存在困难，并且对结构化地形布局的支持有限。本文在回顾整个交叉注意力机制后，提出了一种无训练的草图到场景生成的三元组调优（T3-S2S）。该方案通过三个关键模块重新激活了ControlNet模型以实现详细的多实例生成：提示平衡确保关键词表示并最小化遗漏关键实例的风险；特征优先强调基于草图的特征，通过突出特征通道中的TopK索引；密集调优在与实例相关的注意力图区域内细化轮廓细节。利用T3-S2S的可控性，我们还引入了一种特征共享策略，使用双提示集生成层感知等距和地形视图表示以进行地形布局。实验表明，我们的草图到场景工作流程始终生成与输入提示对齐的多实例2D场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in 2D concept art generation for 3D scenes, which often requires significant manual effort. The authors propose a Training-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation method that enhances the ControlNet model through three modules: Prompt Balance, Characteristic Priority, and Dense Tuning. Experimental results demonstrate that the T3-S2S method effectively generates detailed multi-instance 2D scenes that align well with input prompts, thereby improving the efficiency and quality of concept art generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成3D场景的2D概念艺术所面临的挑战，这通常需要大量的手动工作。作者提出了一种无训练的三元组调优方法（T3-S2S），通过三个模块增强ControlNet模型：提示平衡、特征优先和密集调优，这些模块共同改善了多实例生成和细节精炼。实验结果表明，T3-S2S工作流程有效地生成了与输入提示紧密对齐的多实例2D场景，展示了其在生成结构化地形布局和详细环境方面的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User&#x27;s Casual Sketches</div>
<div class="meta-line">Authors: Yongzhi Xu, Yonhon Ng, Yifu Wang, Inkyu Sa, Yunfei Duan, Zhenhong Sun, Yang Li, Pan Ji, Hongdong Li</div>
<div class="meta-line">First: 2024-08-08T16:27:37+00:00 · Latest: 2026-02-06T06:02:26+00:00</div>
<div class="meta-line">Comments: Project Page: https://xrvisionlabs.github.io/Sketch2Scene/ Code: https://github.com/Tencent/Triplet_Tuning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.04567v2">Abs</a> · <a href="https://arxiv.org/pdf/2408.04567v2">PDF</a> · <a href="https://github.com/Tencent/Triplet_Tuning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://xrvisionlabs.github.io/Sketch2Scene/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D Content Generation is at the heart of many computer graphics applications, including video gaming, film-making, virtual and augmented reality, etc. This paper proposes a novel deep-learning based approach for automatically generating interactive and playable 3D game scenes, all from the user&#x27;s casual prompts such as a hand-drawn sketch. Sketch-based input offers a natural, and convenient way to convey the user&#x27;s design intention in the content creation process. To circumvent the data-deficient challenge in learning (i.e. the lack of large training data of 3D scenes), our method leverages a pre-trained 2D denoising diffusion model to generate a 2D image of the scene as the conceptual guidance. In this process, we adopt the isometric projection mode to factor out unknown camera poses while obtaining the scene layout. From the generated isometric image, we use a pre-trained image understanding method to segment the image into meaningful parts, such as off-ground objects, trees, and buildings, and extract the 2D scene layout. These segments and layouts are subsequently fed into a procedural content generation (PCG) engine, such as a 3D video game engine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can be seamlessly integrated into a game development environment and is readily playable. Extensive tests demonstrate that our method can efficiently generate high-quality and interactive 3D game scenes with layouts that closely follow the user&#x27;s intention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sketch2Scene：从用户的随意草图自动生成互动3D游戏场景</div>
<div class="mono" style="margin-top:8px">3D内容生成是许多计算机图形应用的核心，包括视频游戏、电影制作、虚拟现实和增强现实等。本文提出了一种基于深度学习的新方法，能够自动生成互动和可玩的3D游戏场景，所有这些都基于用户的随意提示，如手绘草图。基于草图的输入为内容创作过程中的用户设计意图提供了一种自然且便捷的传达方式。为了克服学习中的数据不足挑战（即缺乏大规模3D场景训练数据），我们的方法利用预训练的2D去噪扩散模型生成场景的2D图像作为概念指导。在此过程中，我们采用等距投影模式来消除未知相机姿态，同时获取场景布局。从生成的等距图像中，我们使用预训练的图像理解方法将图像分割成有意义的部分，如离地物体、树木和建筑物，并提取2D场景布局。这些分段和布局随后被输入到程序化内容生成（PCG）引擎中，如Unity或Unreal等3D视频游戏引擎，以创建3D场景。生成的3D场景可以无缝集成到游戏开发环境中，并且可以直接玩。大量测试表明，我们的方法能够高效生成高质量和互动的3D游戏场景，其布局紧密跟随用户的意图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance 3D content generation for applications in gaming and virtual environments by allowing users to create scenes from casual sketches. The authors propose a deep-learning approach that utilizes a pre-trained 2D denoising diffusion model to generate a 2D image of the scene based on the user&#x27;s sketch, which serves as conceptual guidance. Experimental results indicate that the method effectively produces high-quality, interactive 3D game scenes that align closely with user intentions, demonstrating its potential for seamless integration into game development environments like Unity and Unreal.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过允许用户从简单的草图创建场景来增强游戏和虚拟环境中的3D内容生成。作者提出了一种深度学习方法，该方法利用预训练的2D去噪扩散模型，根据用户的草图生成场景的2D图像，作为概念指导。实验结果表明，该方法能够有效生成与用户意图紧密对齐的高质量互动3D游戏场景，展示了其在Unity和Unreal等游戏开发环境中无缝集成的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models</div>
<div class="meta-line">Authors: Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan</div>
<div class="meta-line">First: 2025-12-17T14:28:23+00:00 · Latest: 2026-02-06T05:23:08+00:00</div>
<div class="meta-line">Comments: 16 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22170v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22170v2">PDF</a> · <a href="https://github.com/lian700/SoliReward">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM&#x27;s score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark are available at https://github.com/lian700/SoliReward</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoliReward：减轻视频生成奖励模型对奖励黑客和注释噪声的敏感性</div>
<div class="mono" style="margin-top:8px">视频生成模型与人类偏好的后训练对齐是一个关键目标。为此过程开发有效的奖励模型（RMs）面临重大方法论障碍。目前的数据收集范式依赖于提示内的成对注释，存在标注噪声。同时，基于VLM的RMs的架构设计，特别是其输出机制，仍然未得到充分探索。此外，RM在后训练中容易受到奖励黑客的影响。为减轻这些限制，我们提出了SoliReward，一个系统化的视频RM训练框架。我们的框架首先通过单项二元注释获取高质量、成本效益高的数据，然后使用跨提示配对策略构建偏好对。在架构上，我们采用分层渐进查询注意机制来增强特征聚合。最后，我们引入了一种修改的BT损失，明确考虑胜平场景。这种方法对正样本的RM得分分布进行正则化，提供更细致的偏好信号，以减轻对少数高分样本的过度关注。我们的方法在评估物理合理性、主体变形和语义对齐的基准上得到了验证，显示出在直接RM评估指标和视频生成模型后训练的有效性方面的改进。代码和基准可在https://github.com/lian700/SoliReward获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of video generation models with human preferences, addressing challenges related to reward model (RM) training that include labeling noise and susceptibility to reward hacking. The authors propose SoliReward, a systematic framework that utilizes single-item binary annotations for data collection and employs a cross-prompt pairing strategy to construct preference pairs. Key experimental findings indicate that this approach enhances RM evaluation metrics and improves post-training efficacy on video generation models, as validated through benchmarks assessing physical plausibility, subject deformity, and semantic alignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善视频生成模型与人类偏好的对齐，解决在开发有效奖励模型（RM）时面临的标注噪声和奖励黑客攻击的挑战。作者提出了SoliReward，一个系统化框架，利用单项二元标注收集高质量数据，并采用跨提示配对策略构建偏好对。关键实验结果表明，这种方法结合了分层渐进查询注意机制和修改后的BT损失以改善分数分布，显著提高了RM评估指标，并增强了视频生成模型后训练的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260209_0325.html">20260209_0325</a>
<a href="archive/20260208_0323.html">20260208_0323</a>
<a href="archive/20260207_0339.html">20260207_0339</a>
<a href="archive/20260206_0339.html">20260206_0339</a>
<a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
