<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-02 03:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260202_0324</div>
    <div class="row"><div class="card">
<div class="title">One-step Latent-free Image Generation with Pixel Mean Flows</div>
<div class="meta-line">Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</div>
<div class="meta-line">First: 2026-01-29T18:59:56+00:00 · Latest: 2026-01-29T18:59:56+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &quot;pixel MeanFlow&quot; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步无潜变量图像生成与像素均流</div>
<div class="mono" style="margin-top:8px">现代基于扩散/流的图像生成模型通常具有两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无潜变量的一步扩散/流铺平了道路。在这项工作中，我们朝着这一目标迈出了进一步的步伐，提出了“像素均流”（pMF）。我们的核心指导方针是将网络输出空间和损失空间分别进行公式化。网络目标被设计为位于假定的低维图像流形上（即x预测），而损失则通过速度空间中的均流定义。我们引入了图像流形与平均速度场之间的简单变换。在实验中，pMF在256x256分辨率（2.22 FID）和512x512分辨率（2.48 FID）上实现了一步无潜变量生成的强大结果，填补了这一领域的一个关键缺失部分。我们希望我们的研究能进一步推动基于扩散/流的生成模型的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance image generation techniques by eliminating the reliance on multi-step sampling and latent spaces, which are common in modern diffusion and flow-based models. The authors propose a novel method called &quot;pixel MeanFlow&quot; (pMF), which separates the formulation of the network output space from the loss space, targeting a low-dimensional image manifold while defining the loss using MeanFlow in the velocity space. Experimental results demonstrate that pMF achieves competitive performance in one-step latent-free image generation on ImageNet, with FID scores of 2.22 at 256x256 resolution and 2.48 at 512x512 resolution, indicating significant progress in this area of generative modeling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过消除当前扩散和基于流的模型中常见的多步采样和潜在空间的需求，来推进图像生成技术。作者提出了一种新方法，称为“像素均值流”（pMF），该方法将网络输出空间与损失空间分开，针对低维图像流形进行预测，同时通过速度空间中的均值流定义损失。实验结果表明，pMF在ImageNet上实现了一步无潜在图像生成的竞争性能，在256x256分辨率下的FID得分为2.22，在512x512分辨率下为2.48，表明在生成建模领域取得了显著进展。</div>
</details>
</div>
<div class="card">
<div class="title">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</div>
<div class="meta-line">Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or</div>
<div class="meta-line">First: 2026-01-29T18:57:13+00:00 · Latest: 2026-01-29T18:57:13+00:00</div>
<div class="meta-line">Comments: Project webpage available at https://justdubit.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://justdubit.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JUST-DUB-IT：通过联合音视频扩散进行视频配音</div>
<div class="mono" style="margin-top:8px">音视频基础模型经过预训练，能够联合生成声音和视觉内容，最近展现出前所未有的多模态生成和编辑能力，为下游任务开辟了新的机会。在这些任务中，视频配音可以从这种先验中受益匪浅，但大多数现有解决方案仍依赖于复杂的特定任务管道，在现实环境中表现不佳。在本研究中，我们提出了一种单模型方法，通过轻量级LoRA调整基础音视频扩散模型，实现视频到视频的配音。LoRA使模型能够在生成翻译音频和同步面部动作的同时，基于输入音视频进行条件生成。为了训练这个LoRA，我们利用生成模型本身合成同一说话者的配对多语言视频。具体而言，我们在单个片段内生成带有语言切换的多语言视频，然后在每一半中修复面部和音频，以匹配另一半的语言。通过利用音视频模型丰富的生成先验，我们的方法在保持说话者身份和唇部同步的同时，仍然对复杂运动和现实动态具有鲁棒性。我们证明了我们的方法生成的配音视频在视觉保真度、唇部同步和鲁棒性方面优于现有的配音管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video dubbing by leveraging the capabilities of Audio-Visual Foundation Models, which can generate sound and visual content together. The authors propose a single-model approach that utilizes a lightweight LoRA to adapt a foundational audio-video diffusion model for video-to-video dubbing, allowing it to condition on input audio-video while generating translated audio and synchronized facial motion. Experimental results show that this method produces high-quality dubbed videos with enhanced visual fidelity, lip synchronization, and robustness against complex motion and real-world dynamics compared to traditional dubbing pipelines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用先进的音视频基础模型来改善视频配音，这些模型在多模态生成和编辑方面显示出潜力。作者提出了一种新颖的单模型方法，采用轻量级的LoRA来调整音视频扩散模型，以实现视频到视频的配音，使其能够在生成翻译音频和同步面部动作时对输入音视频进行条件处理。实验结果表明，该方法生成的配音视频在视觉保真度、唇同步性和对比传统配音管道的鲁棒性方面均有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</div>
<div class="meta-line">Authors: John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</div>
<div class="meta-line">First: 2026-01-29T18:49:27+00:00 · Latest: 2026-01-29T18:49:27+00:00</div>
<div class="meta-line">Comments: Project page: https://edit-yourself.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit-yourself.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EditYourself：基于扩散变换器的音频驱动的对话头视频生成与编辑</div>
<div class="mono" style="margin-top:8px">当前的生成视频模型在从文本和图像提示中生成新内容方面表现出色，但在编辑现有的预录视频时存在关键缺口，微小的脚本修改需要保持运动、时间一致性、说话者身份和准确的口型同步。我们介绍了EditYourself，这是一个基于DiT的音频驱动视频到视频（V2V）编辑框架，支持基于转录文本的对话头视频修改，包括无缝添加、删除和重新定时视觉内容。EditYourself在通用视频扩散模型的基础上，通过音频条件和区域感知的编辑专注训练扩展增强其V2V能力。这使得通过时空修补实现精确的口型同步和现有表演的时间一致性重构成为可能，包括在新添加的片段中合成逼真的人类运动，同时在长时间内保持视觉保真度和身份一致性。这项工作代表了生成视频模型作为专业视频后期制作实用工具的基础性步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current generative video models in editing pre-recorded videos, particularly in maintaining motion, temporal coherence, speaker identity, and lip synchronization during minor script alterations. The authors introduce EditYourself, a framework based on diffusion transformers that facilitates audio-driven video-to-video editing, allowing for transcript-based modifications of talking head videos. Key experimental findings demonstrate that EditYourself achieves precise lip synchronization and temporally coherent restructuring of performances through spatiotemporal inpainting, effectively synthesizing realistic human motion in newly added segments while preserving visual fidelity and identity consistency over extended durations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前生成视频模型在编辑现有预录视频时的局限性，特别是在进行小幅脚本修改时保持运动、时间一致性、说话者身份和口型同步。作者提出了EditYourself，这是一个基于扩散变换器的框架，能够实现音频驱动的视频到视频编辑，允许对谈话头视频进行基于文本的修改，包括添加、删除和重新定时口语内容。主要实验结果表明，EditYourself通过时空修补实现了精确的口型同步和表演的连贯重构，有效合成了逼真的人类运动，同时在较长时间内保持视觉保真度和身份一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Creative Image Generation with Diffusion Model</div>
<div class="meta-line">Authors: Kunpeng Song, Ahmed Elgammal</div>
<div class="meta-line">First: 2026-01-29T18:48:48+00:00 · Latest: 2026-01-29T18:48:48+00:00</div>
<div class="meta-line">Comments: Project page: https://creative-t2i.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22125v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22125v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://creative-t2i.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image&#x27;s existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的创意图像生成</div>
<div class="mono" style="margin-top:8px">创意图像生成已成为一个引人注目的研究领域，旨在生成新颖且高质量的图像，扩展想象的边界。在本研究中，我们提出了一种基于扩散模型的创意生成新框架，其中创意与图像在CLIP嵌入空间中存在的逆概率相关。与依赖于手动概念混合或排除子类别的先前方法不同，我们的方法计算生成图像的概率分布，并将其引导至低概率区域，以生成稀有、富有想象力且视觉上引人注目的输出。我们还引入了回拉机制，实现了高创意而不牺牲视觉保真度。在文本到图像的扩散模型上进行的广泛实验展示了我们创意生成框架的有效性和效率，展示了其生成独特、新颖和发人深省图像的能力。本研究为生成模型中的创意提供了新的视角，提供了一种原则性的方法来促进视觉内容合成中的创新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance creative image generation by producing novel and high-quality images that push the limits of imagination. The authors propose a new framework utilizing diffusion models, where creativity is linked to the inverse probability of an image&#x27;s existence in the CLIP embedding space, moving away from traditional methods that rely on manual concept blending. Key experimental findings indicate that their approach effectively generates unique and visually captivating images by directing the probability distribution of generated images towards low-probability regions, while also maintaining high visual fidelity through pullback mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过生成新颖和高质量的图像来推动创造性图像生成，拓展想象的边界。作者提出了一种利用扩散模型的新框架，其中创造性与图像在CLIP嵌入空间中存在的逆概率相关，并采用了一种将生成图像的概率分布引导至低概率区域的方法，以产生稀有且视觉引人注目的输出。实验结果表明，他们的方法有效地生成独特且发人深省的图像，同时保持高视觉保真度，从而为生成模型中的创造力提供了新的视角。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI</div>
<div class="meta-line">Authors: Haoyang Su, Jin-Yi Xiang, Shaohao Rui, Yifan Gao, Xingyu Chen, Tingxuan Yin, Shaoting Zhang, Xiaosong Wang, Lian-Ming Wu</div>
<div class="meta-line">First: 2025-08-26T17:23:43+00:00 · Latest: 2026-01-29T18:11:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.19325v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.19325v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis. We present PRISM (Prompt-guided Representation Integration for Survival Modeling), a self-supervised framework that integrates visual representations from non-contrast cardiac cine magnetic resonance imaging with structured electronic health records (EHRs) for survival analysis. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and modulates them using medically informed textual prompts to enable fine-grained risk prediction. Across four independent clinical cohorts, PRISM consistently surpasses classical survival prediction models and state-of-the-art (SOTA) deep learning baselines under internal and external validation. Further clinical findings demonstrate that the combined imaging and EHR representations derived from PRISM provide valuable insights into cardiac risk across diverse cohorts. Three distinct imaging signatures associated with elevated MACE risk are uncovered, including lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole. Prompt-guided attribution further identifies hypertension, diabetes, and smoking as dominant contributors among clinical and physiological EHR factors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：一个利用无监督视觉表示和文本提示进行可解释的心脏重大不良事件生存预测的框架</div>
<div class="mono" style="margin-top:8px">准确预测重大不良心脏事件（MACE）仍然是心血管预后中的一个核心挑战。我们提出了PRISM（生存建模的提示引导表示集成），这是一个自监督框架，结合了非对比心脏动态磁共振成像的视觉表示和结构化电子健康记录（EHR）进行生存分析。PRISM通过运动感知的多视图蒸馏提取时间同步的成像特征，并使用医学信息化的文本提示对其进行调节，以实现细粒度的风险预测。在四个独立的临床队列中，PRISM在内部和外部验证下始终超越经典生存预测模型和最先进的深度学习基线。进一步的临床发现表明，PRISM衍生的结合成像和EHR表示为不同队列的心脏风险提供了有价值的见解。发现与MACE风险升高相关的三种不同成像特征，包括侧壁不同步、下壁高敏感性和舒张期前壁升高焦点。提示引导的归因进一步识别出高血压、糖尿病和吸烟是临床和生理EHR因素中的主要贡献者。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for accurate prediction of major adverse cardiac events (MACE) in cardiovascular prognosis. The authors introduce PRISM, a self-supervised framework that combines visual representations from cardiac cine MRI with structured electronic health records for survival analysis. The key findings indicate that PRISM outperforms traditional survival prediction models and state-of-the-art deep learning approaches across multiple clinical cohorts, revealing three imaging signatures linked to increased MACE risk and identifying hypertension, diabetes, and smoking as significant contributors among EHR factors.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善重大不良心脏事件（MACE）的预测，这是心血管预后中的一个重大挑战。作者开发了PRISM，这是一个自监督框架，将心脏动态MRI的视觉表示与结构化电子健康记录结合用于生存分析。实验结果表明，PRISM在四个临床队列中优于传统生存预测模型和最先进的深度学习方法，揭示了与MACE风险增加相关的三种影像特征，并从电子健康记录数据中识别出高血压、糖尿病和吸烟作为关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation</div>
<div class="meta-line">Authors: Li Zhang, Pengtao Xie</div>
<div class="meta-line">First: 2026-01-29T17:58:55+00:00 · Latest: 2026-01-29T17:58:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22061v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22061v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BLO-Inst：基于双层优化的YOLO与SAM对齐以实现鲁棒实例分割</div>
<div class="mono" style="margin-top:8px">Segment Anything Model通过其零样本能力彻底改变了图像分割，但其对手动提示的依赖阻碍了完全自动化的部署。虽然将目标检测器集成作为提示生成器提供了自动化的途径，但现有管道存在两个基本限制：目标不匹配，即为几何定位优化的检测器与SAM所需的最佳提示上下文不对应，以及标准联合训练中的对齐过拟合，即检测器仅仅记忆训练样本的特定提示调整，而不是学习可泛化的策略。为了解决这一问题，我们提出了BLO-Inst，一个通过双层优化对齐检测和分割目标的统一框架。我们将对齐形式化为一个在不相交数据划分上的嵌套优化问题。在下层，SAM被微调以最大化给定当前检测提案的分割保真度（在子集$D_1$上）。在上层，检测器被更新以生成明确最小化微调SAM在另一个子集$D_2$上的验证损失的边界框。这有效地将检测器转变为一个分割感知的提示生成器，优化边界框不仅仅是为了定位精度，而是为了下游掩膜质量。大量实验表明，BLO-Inst实现了卓越的性能，在一般和生物医学领域的任务中超越了标准基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the automation of the Segment Anything Model (SAM) for image segmentation, which currently relies on manual prompts that limit its deployment. The authors propose BLO-Inst, a bi-level optimization framework that aligns the objectives of object detection and segmentation by formulating the problem as a nested optimization task over separate data splits. Experimental results show that BLO-Inst significantly improves performance, surpassing standard baselines in both general and biomedical segmentation tasks by transforming the detector into a segmentation-aware prompt generator that optimizes bounding boxes for both localization accuracy and mask quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要实现Segment Anything Model（SAM）在图像分割中的完全自动化部署，而这一点目前受到其依赖手动提示的限制。为了解决这个问题，作者提出了BLO-Inst，这是一种基于双层优化的框架，旨在对齐目标检测和分割的目标。该方法涉及一个嵌套优化问题，其中SAM在一个数据子集上进行微调以增强分割精度，而目标检测器在另一个子集上进行更新以最小化验证损失。实验结果表明，BLO-Inst在一般和生物医学分割任务中显著优于标准基线。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models</div>
<div class="meta-line">Authors: Longxuan Yu, Yu Fu, Shaorong Zhang, Hui Liu, Mukund Varma T, Greg Ver Steeg, Yue Dong</div>
<div class="meta-line">First: 2026-01-29T17:40:58+00:00 · Latest: 2026-01-29T17:40:58+00:00</div>
<div class="meta-line">Comments: 18 pages, 13 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22035v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term &quot;order robustness&quot;. Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越顺序思维：当输出顺序不再反映扩散语言模型中的推理顺序时</div>
<div class="mono" style="margin-top:8px">自回归（AR）语言模型强制执行固定的从左到右的生成顺序，当所需的输出结构与自然推理发生冲突时，这造成了根本性的限制（例如，由于展示或结构约束，先生成答案再生成解释）。在这种情况下，AR模型必须在生成中间推理之前就承诺答案，这一严格的约束迫使其过早承诺。掩蔽扩散语言模型（MDLMs）通过并行迭代地细化所有标记，提供了一种将计算顺序与输出结构解耦的方法。我们在GSM8K、Math500和我们引入的具有可控难度和顺序级评估的基准ReasonOrderQA上验证了这一能力。当提示要求在推理之前给出答案时，AR模型与标准的思维链顺序相比表现出较大的准确性差距（相对下降高达67%），而MDLMs保持稳定（相对下降不超过14%），我们称之为“顺序鲁棒性”。通过ReasonOrderQA，我们提供证据表明，MDLMs通过在扩散过程中比复杂标记（例如最终答案）更早稳定简单标记（例如推理步骤）来实现顺序鲁棒性，从而使推理标记在答案承诺之前稳定。最后，我们识别出这种优势减弱的失败条件，概述了实现顺序鲁棒性所需的限制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of autoregressive (AR) language models, which enforce a fixed left-to-right generation order that can conflict with natural reasoning processes. The authors propose masked diffusion language models (MDLMs) as a solution, allowing for parallel refinement of tokens and decoupling computation order from output structure. Experimental results on benchmarks such as GSM8K, Math500, and a new benchmark called ReasonOrderQA demonstrate that while AR models show significant accuracy drops (up to 67%) when required to produce answers before reasoning, MDLMs maintain stability with only a 14% drop, indicating their &#x27;order robustness&#x27; by stabilizing simpler reasoning tokens before complex answers during the diffusion process.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自回归（AR）语言模型的局限性，这些模型强制执行固定的从左到右的输出顺序，这可能与自然推理过程相冲突。研究引入了掩蔽扩散语言模型（MDLMs），允许并行的标记细化，从而将计算顺序与输出结构解耦。对GSM8K、Math500和新引入的ReasonOrderQA等基准的实验结果表明，当要求在推理之前生成答案时，AR模型的准确率下降显著（高达67%），而MDLMs保持稳定，最大下降仅为14%，展示了所谓的“顺序鲁棒性”特性，通过在过程中较早稳定简单标记而非复杂标记来实现。</div>
</details>
</div>
<div class="card">
<div class="title">Align &amp; Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representation Alignment</div>
<div class="meta-line">Authors: Loukas Sfountouris, Giannis Daras, Paris Giampouras</div>
<div class="meta-line">First: 2025-11-21T00:37:04+00:00 · Latest: 2026-01-29T17:34:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16870v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16870v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a DINOv2 visual encoder, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we empirically show that aligning model representations of approximate target features can substantially enhance reconstruction quality and perceptual realism. We provide theoretical results showing (a) that REPA regularization can be viewed as a variational approach for minimizing a divergence measure in the DINOv2 embedding space, and (b) how under certain regularity assumptions REPA updates steer the latent diffusion states toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by We integrate REPA into multiple state-of-the-art inverse problem solvers, and provide extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirming that our method consistently improves reconstruction quality, while also providing efficiency gains reducing the number of required discretization steps.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐与反转：通过表示对齐解决扩散和基于流的模型的逆问题</div>
<div class="mono" style="margin-top:8px">最近的研究表明，强制扩散或基于流的生成模型的内部表示与预训练自监督编码器的表示之间的对齐可以提供强大的归纳偏置，从而改善收敛性和样本质量。在本研究中，我们将这一思想扩展到逆问题，其中预训练生成模型被用作先验。我们建议在扩散或基于流的模型与DINOv2视觉编码器之间应用表示对齐（REPA），以指导推理时的重建过程。尽管在逆问题中无法获得真实信号，但我们通过实验证明，对齐近似目标特征的模型表示可以显著提高重建质量和感知真实感。我们提供理论结果表明（a）REPA正则化可以被视为在DINOv2嵌入空间中最小化散度度量的变分方法，以及（b）在某些正则性假设下，REPA更新如何引导潜在扩散状态朝向干净图像。这些结果为REPA在提高感知保真度中的作用提供了见解。最后，我们通过将REPA集成到多个最先进的逆问题求解器中，展示了我们方法的普遍性，并在超分辨率、框内填充、高斯去模糊和运动去模糊等任务上进行了广泛实验，确认我们的方法始终提高重建质量，同时在减少所需离散化步骤的数量上也提供了效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to improve reconstruction quality in inverse problems using generative models as priors. The authors propose a method called representation alignment (REPA), which aligns the internal representations of diffusion or flow-based models with those of a pretrained DINOv2 visual encoder during the reconstruction process. Experimental results demonstrate that this alignment significantly enhances reconstruction quality and perceptual realism across various tasks, including super-resolution and Gaussian deblurring, while also improving efficiency by reducing the number of required discretization steps.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高使用生成模型解决逆问题时的收敛性和样本质量。作者提出了一种称为表示对齐（REPA）的方法，该方法在重建过程中将扩散或基于流的模型的内部表示与预训练的DINOv2视觉编码器的表示对齐。实验结果表明，这种对齐显著提高了在超分辨率和去模糊等任务中的重建质量和感知真实感，同时通过减少所需的离散化步骤数量提高了效率。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering</div>
<div class="meta-line">Authors: Dongxuan Zhu, Ly Tran Ho Khanh, Andy Yat-Ming Cheung, Man-Chung Yue, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T17:17:04+00:00 · Latest: 2026-01-29T17:17:04+00:00</div>
<div class="meta-line">Comments: 34 pages, 2 figures. Accepted for publication at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22010v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过推理时的斯蒂费尔激活引导探索多样化生成路径</div>
<div class="mono" style="margin-top:8px">语言模型通常默认使用一组狭窄的高概率输出，使其生成路径同质化并容易出现模式崩溃。基于采样的策略注入随机性，但仍然难以保证多个并发生成运行的多样性。我们通过引入STARS（基于斯蒂费尔的多样化推理激活引导），一种无训练、推理时的干预方法，将激活引导转变为探索引擎。在每个标记处，STARS收集并发生成运行的隐藏激活，并在斯蒂费尔流形上联合优化多个附加引导方向。STARS最大化引导激活的几何体积，同时斯蒂费尔流形引入了引导干预的正交性。这种公式明确促进了并发生成运行的发散激活向量，并隐式促进了发散生成轨迹。该流形优化公式可以使用具有收敛保证的黎曼梯度下降算法解决，但该算法对于实时推理来说耗时过长。为了保证低延迟，我们进一步设计了一种轻量级的一步更新，采用激进的封闭形式步长。对于测试用例生成和科学发现基准，STARS始终优于标准采样方法，实现了更大的多样性而不牺牲定性性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of limited diversity in outputs generated by language models, which often lead to mode collapse. The authors propose a novel method called STARS (Stiefel-based Activation Steering for Diverse Reasoning), which operates during inference to enhance activation steering by optimizing multiple steering directions on the Stiefel manifold. Experimental results demonstrate that STARS significantly improves output diversity in test case generation and scientific discovery benchmarks compared to standard sampling methods, achieving this without compromising the quality of the generated content.</div>
<div class="mono" style="margin-top:8px">该研究解决了语言模型倾向于产生同质输出的问题，这可能导致模式崩溃。作者提出了STARS，这是一种无训练、推理时的方法，通过增强激活引导来促进多样化的生成路径。实验结果表明，STARS在生成多样化输出方面显著优于传统采样方法，同时在各种测试用例生成和科学发现基准上保持高质量表现。</div>
</details>
</div>
<div class="card">
<div class="title">Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models</div>
<div class="meta-line">Authors: Pranav Mahajan, Ihor Kendiukhov, Syed Hussain, Lydia Nottingham</div>
<div class="meta-line">First: 2026-01-29T16:51:43+00:00 · Latest: 2026-01-29T16:51:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21975v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21975v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman&#x27;s rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意差距：引导协议如何影响语言模型中的陈述-揭示偏好差距</div>
<div class="mono" style="margin-top:8px">最近的研究发现语言模型（LMs）中存在陈述-揭示（SvR）偏好差距：模型所支持的价值观与其在上下文中做出的选择之间的不匹配。现有评估严重依赖于二元强制选择提示，这将真实偏好与引导协议的伪影纠缠在一起。我们系统地研究了引导协议如何影响24个LMs中的SvR相关性。在陈述偏好引导中允许中立和弃权使我们能够排除弱信号，显著提高自愿陈述偏好与强制选择揭示偏好之间的斯皮尔曼等级相关性（$ρ$）。然而，进一步在揭示偏好中允许弃权会导致$ρ$接近零或负值，因为中立率较高。最后，我们发现，在揭示偏好引导中使用陈述偏好进行系统提示引导并不能可靠地改善AIRiskDilemmas上的SvR相关性。总的来说，我们的结果表明SvR相关性高度依赖于协议，偏好引导需要考虑不确定偏好的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the stated-revealed (SvR) preference gap in language models, which highlights the discrepancy between the values models express and their contextual choices. The authors systematically analyze the impact of different elicitation protocols on SvR correlation across 24 language models. They find that allowing neutrality and abstention in stated preference elicitation significantly enhances the Spearman&#x27;s rank correlation between stated and revealed preferences, while permitting abstention in revealed preferences leads to a near-zero correlation due to high neutrality rates. Additionally, using stated preferences to steer system prompts during revealed preference elicitation does not consistently improve SvR correlation, indicating that preference elicitation methods must account for indeterminate preferences to be effective.</div>
<div class="mono" style="margin-top:8px">本研究探讨了语言模型中的陈述-揭示（SvR）偏好差距，研究动机在于模型表达的价值与其在上下文中做出的选择之间的差异。该研究系统分析了24个语言模型，以评估不同引导协议如何影响SvR相关性。主要发现表明，在陈述偏好引导中允许中立和弃权显著提高了陈述偏好与揭示偏好之间的斯皮尔曼等级相关性，而在揭示偏好中允许弃权则因高中立率导致相关性接近零或为负。此外，在揭示偏好引导中使用陈述偏好来引导系统提示并未始终改善SvR相关性，突显了偏好引导方法的协议依赖性。</div>
</details>
</div>
<div class="card">
<div class="title">Optimistic Transfer under Task Shift via Bellman Alignment</div>
<div class="meta-line">Authors: Jinhang Chai, Enpei Zhang, Elynn Chen, Yujun Yan</div>
<div class="meta-line">First: 2026-01-29T16:16:24+00:00 · Latest: 2026-01-29T16:16:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21924v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过贝尔曼对齐实现任务转移的乐观转移</div>
<div class="mono" style="margin-top:8px">我们研究了在情节马尔可夫决策过程中的在线转移强化学习（RL），在目标任务学习期间可以利用相关源任务的经验。一个基本困难是，任务相似性通常是根据奖励或转移来定义的，而在线RL算法则基于贝尔曼回归目标。因此，简单地重用源贝尔曼更新会引入系统性偏差并使遗憾保证失效。
我们确定一步贝尔曼对齐是在线RL中转移的正确抽象，并提出了重加权目标（RWT），这是一种操作级别的修正，通过测度变化重新定位延续值并补偿转移不匹配。RWT将任务不匹配减少到固定的一步修正，并使源数据的统计上合理重用成为可能。
这种对齐产生了一个两阶段的RWT $Q$-学习框架，将方差减少与偏差修正分开。在RKHS函数逼近下，我们建立了遗憾界限，这些界限与任务转移的复杂性相关，而不是目标MDP。实证结果在表格和神经网络设置中均显示出相对于单任务学习和简单池化的一致改进，突显了贝尔曼对齐作为在线RL的模型无关转移原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of online transfer reinforcement learning (RL) in episodic Markov decision processes, where leveraging experience from related source tasks can enhance learning on a target task. The authors propose a method called re-weighted targeting (RWT), which aligns one-step Bellman updates to correct for biases introduced by task mismatches. Experimental results show that the two-stage RWT Q-learning framework significantly improves performance over traditional single-task learning and naive pooling, demonstrating the effectiveness of Bellman alignment as a transfer principle in online RL settings.</div>
<div class="mono" style="margin-top:8px">本研究解决了在情节马尔可夫决策过程中在线迁移强化学习（RL）的挑战，其中利用相关源任务的经验可以增强对目标任务的学习。作者提出了一种称为重加权目标（RWT）的方法，通过对齐一步贝尔曼目标来纠正因天真重用源贝尔曼更新而引入的系统性偏差。实验结果表明，两阶段RWT Q学习框架在传统单任务学习和天真池化方法上显著提高了性能，确立了贝尔曼对齐作为在线RL设置中的一种稳健迁移原则。</div>
</details>
</div>
<div class="card">
<div class="title">FreeFuse: Multi-Subject LoRA Fusion via Adaptive Token-Level Routing at Test Time</div>
<div class="meta-line">Authors: Yaoli Liu, Yao-Xiang Ding, Kun Zhou</div>
<div class="meta-line">First: 2025-10-27T16:54:08+00:00 · Latest: 2026-01-29T16:14:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23515v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23515v2">PDF</a> · <a href="https://github.com/yaoliliu/FreeFuse">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes FreeFuse, a training-free framework for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to prior studies that focus on retraining LoRA to alleviate feature conflicts, our analysis reveals that simply spatially confining the subject LoRA&#x27;s output to its target region and preventing other LoRAs from directly intruding into this area is sufficient for effective mitigation. Accordingly, we implement Adaptive Token-Level Routing during the inference phase. We introduce FreeFuseAttn, a mechanism that exploits the flow matching model&#x27;s intrinsic semantic alignment to dynamically match subject-specific tokens to their corresponding spatial regions at early denoising timesteps, thereby bypassing the need for external segmentors. FreeFuse distinguishes itself through high practicality: it necessitates no additional training, model modifications, or user-defined masks spatial conditions. Users need only provide subject activation words to achieve seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both identity preservation and compositional fidelity. Our code is available at https://github.com/yaoliliu/FreeFuse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeFuse：通过自适应令牌级路由在测试时进行多主题LoRA融合</div>
<div class="mono" style="margin-top:8px">本文提出了FreeFuse，一个无训练的框架，通过自动融合多个主题LoRA实现多主题文本到图像生成。与以往研究集中于重新训练LoRA以缓解特征冲突不同，我们的分析表明，仅仅将主题LoRA的输出空间限制在其目标区域，并防止其他LoRA直接侵入该区域，足以有效缓解。因此，我们在推理阶段实现了自适应令牌级路由。我们引入了FreeFuseAttn，一种利用流匹配模型内在语义对齐的机制，在早期去噪时间步动态匹配主题特定令牌与其对应的空间区域，从而绕过对外部分割器的需求。FreeFuse以高实用性为特点：它不需要额外的训练、模型修改或用户定义的掩码空间条件。用户只需提供主题激活词即可实现与标准工作流程的无缝集成。大量实验验证了FreeFuse在身份保留和组合保真度方面优于现有方法。我们的代码可在https://github.com/yaoliliu/FreeFuse获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve multi-subject text-to-image generation without the need for retraining models, addressing the issue of feature conflicts in existing methods. The authors propose FreeFuse, a training-free framework that utilizes Adaptive Token-Level Routing during inference to automatically fuse multiple subject LoRAs by spatially confining their outputs. Experimental results demonstrate that FreeFuse significantly outperforms previous approaches in terms of identity preservation and compositional fidelity, achieving effective integration with minimal user input.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善多主体文本到图像生成，而无需重新训练模型，解决现有方法中的特征冲突问题。作者提出了FreeFuse，这是一种无训练的框架，通过在推理过程中使用自适应令牌级路由，自动融合多个主体LoRA，将其输出空间限制在指定区域。实验结果表明，FreeFuse在身份保留和组合保真度方面显著优于以前的方法，实现了在标准工作流程中有效集成，且所需用户输入最小。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</div>
<div class="meta-line">Authors: Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</div>
<div class="meta-line">First: 2026-01-29T16:14:07+00:00 · Latest: 2026-01-29T16:14:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21922v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视频扩散模型的零-shot视频修复与增强</div>
<div class="mono" style="margin-top:8px">尽管基于扩散的零-shot图像修复与增强方法取得了巨大成功，但将其应用于视频修复或增强会导致严重的时间闪烁。本文提出了第一个框架，利用快速发展的视频扩散模型来辅助基于图像的方法，以保持零-shot视频修复与增强的时间一致性。我们提出了同源潜变量融合、异源潜变量融合和基于COT的融合比例策略，以利用同源和异源的文本到视频扩散模型来补充图像方法。此外，我们提出了时间增强后处理，利用图像到视频的扩散模型进一步提高时间一致性。我们的方法是无训练的，可以应用于任何基于扩散的图像修复与增强方法。实验结果证明了所提方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of severe temporal flickering in video restoration and enhancement when using diffusion-based zero-shot image methods. The authors propose a novel framework that leverages video diffusion models to enhance temporal consistency in these processes. Key experimental findings indicate that their approach, which includes homologous and heterogeneous latents fusion along with a COT-based fusion ratio strategy, significantly improves the performance of existing image-based methods without requiring training, demonstrating its effectiveness in achieving superior results in video restoration and enhancement.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在应用扩散基础的零-shot图像方法进行视频修复和增强时出现的严重时间闪烁问题。作者提出了一种新颖的框架，利用视频扩散模型通过整合同源和异源潜在融合技术以及基于COT的融合比例策略来增强视频修复的时间一致性。实验结果表明，所提出的方法在时间一致性方面显著优于现有方法，证明了其在零-shot视频修复和增强中的有效性，无需训练。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</div>
<div class="meta-line">Authors: Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</div>
<div class="meta-line">First: 2026-01-29T15:49:31+00:00 · Latest: 2026-01-29T15:49:31+00:00</div>
<div class="meta-line">Comments: 24 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21892v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过流匹配的流形投影改进无分类器引导</div>
<div class="mono" style="margin-top:8px">无分类器引导（CFG）是一种广泛用于扩散和基于流的模型中的可控生成技术。尽管其在经验上取得了成功，但CFG依赖于一种通常对引导尺度敏感的启发式线性外推。在本研究中，我们通过优化的视角提供了对CFG的原则性解释。我们证明流匹配中的速度场对应于一系列平滑距离函数的梯度，这引导潜变量朝向缩放的目标图像集。这个视角揭示了标准CFG公式是该梯度的近似，其中预测差距，即条件输出和无条件输出之间的差异，决定了引导的敏感性。利用这一见解，我们将CFG采样重新表述为具有流形约束的同伦优化。该公式需要一个流形投影步骤，我们在采样过程中通过增量梯度下降方案实现。为了提高计算效率和稳定性，我们进一步通过安德森加速增强这一迭代过程，而无需额外的模型评估。我们提出的方法是无训练的，并持续提高生成的保真度、提示对齐和对引导尺度的鲁棒性。我们在多个基准上验证了它们的有效性，显示出在大型模型如DiT-XL-2-256、Flux和Stable Diffusion 3.5上显著的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance classifier-free guidance (CFG) in diffusion and flow-based models, which is often limited by its heuristic linear extrapolation and sensitivity to guidance scale. The authors provide a principled interpretation of CFG through optimization, showing that the velocity field in flow matching corresponds to the gradient of smoothed distance functions that guide latent variables. They reformulate CFG sampling as a homotopy optimization with a manifold constraint, implementing a manifold projection step via incremental gradient descent and improving efficiency with Anderson Acceleration. The methods proposed are training-free and lead to significant improvements in generation fidelity, prompt alignment, and robustness across various benchmarks, particularly for large-scale models like DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高扩散和基于流的模型中的无分类器引导（CFG），由于启发式线性外推的敏感性，CFG常常面临指导尺度的问题。作者通过优化提供了对CFG的原则性解释，表明流匹配中的速度场对应于平滑距离函数的梯度，这些梯度引导潜在变量朝向目标图像。研究者将CFG采样重新表述为具有流形约束的同伦优化，通过增量梯度下降实现流形投影，并利用安德森加速提高效率。所提出的方法无需训练，并在多个基准测试中显著提高了生成的保真度、提示对齐和鲁棒性，包括在大型模型如DiT-XL-2-256、Flux和Stable Diffusion 3.5上的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents</div>
<div class="meta-line">Authors: Taewon Kang</div>
<div class="meta-line">First: 2026-01-29T15:28:48+00:00 · Latest: 2026-01-29T15:28:48+00:00</div>
<div class="meta-line">Comments: 47 pages, 36 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21857v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于轨迹引导的扩散方法用于多层文档中的前景保留背景生成</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于扩散的框架，用于文档中心的背景生成，通过潜在空间设计实现前景保留和多页风格一致性，而不是依赖显式约束。我们的方法将扩散重新解释为通过结构化潜在空间的随机轨迹演变，而不是抑制扩散更新或应用掩蔽启发式。通过塑造初始噪声及其几何对齐，背景生成自然避免指定的前景区域，使可读内容保持完整，无需辅助机制。为了解决跨页风格漂移的长期问题，我们将风格控制与文本条件解耦，并引入缓存的风格方向作为潜在空间中的持久向量。一旦选择，这些方向约束扩散轨迹到共享的风格子空间，确保跨页和编辑迭代的一致外观。这种公式化消除了对重复提示式风格规范的需求，为多页生成提供了更稳定的基础。我们的框架允许几何和物理解释，其中扩散路径在由优选方向塑造的潜在流形上演变，前景区域很少被穿越，这是轨迹初始化的结果，而不是显式排除。所提出的方法无需训练，兼容现有的扩散骨干，并在复杂文档中生成视觉一致、前景保留的结果。通过将扩散重新框定为潜在空间中的轨迹设计，我们提供了一种一致且结构化的生成建模的原则性方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating backgrounds for multi-layer documents while preserving foreground content and maintaining stylistic consistency across pages. The authors propose a diffusion-based framework that reinterprets diffusion as the evolution of stochastic trajectories in a structured latent space, avoiding the need for explicit constraints or masking techniques. Experimental results demonstrate that the method effectively preserves readable content and achieves consistent stylistic appearance across multiple pages without requiring repeated style prompts, thus providing a stable and coherent solution for complex document generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在多层文档中有效生成背景，同时保持前景内容和跨页的风格一致性。作者提出了一种基于扩散的框架，将扩散解释为在结构化潜在空间中随机轨迹的演变，避免了显式约束，并允许自然背景生成而不干扰可读内容。关键实验结果表明，该方法成功保持了前景的完整性和跨页的风格一致性，无需重复的风格提示，从而产生了与现有扩散模型兼容的视觉一致输出。</div>
</details>
</div>
<div class="card">
<div class="title">Redefining Neural Operators in $d+1$ Dimensions for Embedding Evolution</div>
<div class="meta-line">Authors: Haoze Song, Zhihao Li, Xiaobo Zhang, Zecheng Gan, Zhilu Lai, Wei Wang</div>
<div class="meta-line">First: 2025-05-17T00:15:00+00:00 · Latest: 2026-01-29T15:18:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.11766v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.11766v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Operators (NOs) have emerged as powerful tools for learning mappings between function spaces. Among them, the kernel integral operator has been widely used in universally approximating architectures. Following the original formulation, most advancements focus on designing better parameterizations for the kernel over the original physical domain (with $d$ spatial dimensions, $d\in{1,2,3,\ldots}$). In contrast, embedding evolution remains largely unexplored, which often drives models toward brute-force embedding lengthening to improve approximation, but at the cost of substantially increased computation.
  In this paper, we introduce an auxiliary dimension that explicitly models embedding evolution in operator form, thereby redefining the NO framework in $d+1$ dimensions (the original $d$ dimensions plus one auxiliary dimension). Under this formulation, we develop a Schrödingerised Kernel Neural Operator (SKNO), which leverages Fourier-based operators to model the $d+1$ dimensional evolution. Across more than ten increasingly challenging benchmarks, ranging from the 1D heat equation to the highly nonlinear 3D Rayleigh-Taylor instability, SKNO consistently outperforms other baselines. We further validate its resolution invariance under mixed-resolution training and super-resolution inference, and evaluate zero-shot generalization to unseen temporal regimes. In addition, we present a broader set of design choices for the lifting and recovery operators, demonstrating their impact on SKNO&#x27;s predictive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在 $d+1$ 维中重新定义神经算子以嵌入演化</div>
<div class="mono" style="margin-top:8px">神经算子（NOs）已成为学习函数空间之间映射的强大工具。其中，核积分算子在通用逼近架构中得到了广泛应用。根据原始公式，大多数进展集中在为原始物理域（具有 $d$ 个空间维度，$d\in{1,2,3,\ldots}$）设计更好的参数化。相比之下，嵌入演化仍然在很大程度上未被探索，这通常驱使模型通过强行延长嵌入长度来提高逼近，但代价是计算量显著增加。
  在本文中，我们引入一个辅助维度，明确以算子形式建模嵌入演化，从而在 $d+1$ 维中重新定义 NO 框架（原始 $d$ 维加一个辅助维度）。在这种公式下，我们开发了一个施罗丁根化核神经算子（SKNO），利用基于傅里叶的算子来建模 $d+1$ 维的演化。在超过十个日益具有挑战性的基准测试中，从一维热方程到高度非线性的三维瑞利-泰勒不稳定性，SKNO 始终优于其他基线。我们进一步验证了其在混合分辨率训练和超分辨率推理下的分辨率不变性，并评估了对未见时间范围的零样本泛化。此外，我们还提出了一组更广泛的提升和恢复算子的设计选择，展示了它们对 SKNO 预测性能的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Neural Operators (NOs) in learning mappings between function spaces, particularly by addressing the largely unexplored area of embedding evolution. The authors introduce a new framework that incorporates an auxiliary dimension to redefine NOs in $d+1$ dimensions, leading to the development of a Schrödingerised Kernel Neural Operator (SKNO) that utilizes Fourier-based operators for modeling this evolution. Experimental results show that SKNO significantly outperforms existing baselines across more than ten challenging benchmarks, including the 1D heat equation and the 3D Rayleigh-Taylor instability, while also demonstrating resolution invariance and effective zero-shot generalization to new temporal regimes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高神经算子（NOs）在学习函数空间映射中的效率，特别是在嵌入演化方面，这一领域尚未得到充分关注。作者提出了一种新方法，通过引入一个辅助维度，将NO框架重新定义为$d+1$维，从而开发出一种利用傅里叶基础算子的薛定谔核神经算子（SKNO）来建模这种演化。实验结果表明，SKNO在超过十个基准测试中优于现有基线，包括一维热方程和三维瑞利-泰勒不稳定性，同时还展示了其分辨率不变性和零-shot泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</div>
<div class="meta-line">Authors: Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe</div>
<div class="meta-line">First: 2026-01-29T14:41:01+00:00 · Latest: 2026-01-29T14:41:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model&#x27;s output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识向量削弱：大规模视觉语言模型的高效无训练遗忘</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）因其强大的多模态能力而被广泛采用，但也引发了隐私泄露和有害内容生成等严重问题。机器无学习已成为去除训练模型中特定数据影响的有前景的解决方案。然而，现有方法主要依赖于基于梯度的优化，导致大规模LVLMs的计算成本高昂。为了解决这一限制，我们提出了知识向量削弱（KVW），这是一种无训练的无学习方法，直接干预完整模型而无需梯度计算。KVW识别在遗忘集上模型输出生成过程中激活的知识向量，并逐步削弱其贡献，从而防止模型利用不良知识。在MLLMU和CLEAR基准上的实验表明，KVW在显著提高计算效率的同时，实现了稳定的遗忘-保留权衡，优于基于梯度和LoRA的无学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address privacy concerns and harmful content generation associated with Large Vision-Language Models (LVLMs), which necessitate effective unlearning methods. The authors propose a novel approach called Knowledge Vector Weakening (KVW), which is a training-free unlearning method that modifies the model directly without relying on gradient computations. Experimental results on the MLLMU and CLEAR benchmarks show that KVW effectively balances the forget-retain trade-off while significantly enhancing computational efficiency compared to traditional gradient-based and LoRA-based unlearning methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决与大型视觉-语言模型（LVLMs）相关的隐私问题和有害内容生成，这需要有效的遗忘方法。作者提出了一种新方法，称为知识向量削弱（KVW），该方法无需基于梯度的优化，从而降低了计算成本。在MLLMU和CLEAR基准上的实验结果表明，KVW成功地保持了稳定的遗忘-保留权衡，同时显著提高了计算效率，相较于传统的基于梯度和LoRA的遗忘方法。</div>
</details>
</div>
<div class="card">
<div class="title">Pushing the Limits of Distillation-Based Class-Incremental Learning via Lightweight Plugins</div>
<div class="meta-line">Authors: Zhiming Xu, Baile Xu, Jian Zhao, Furao Shen, Suorong Yang</div>
<div class="meta-line">First: 2025-12-03T07:57:48+00:00 · Latest: 2026-01-29T14:39:17+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03537v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03537v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing replay and distillation-based class-incremental learning (CIL) methods are effective at retaining past knowledge but are still constrained by the stability-plasticity dilemma. Since their resulting models are learned over a sequence of incremental tasks, they encode rich representations and can be regarded as pre-trained bases. Building on this view, we propose a plug-in extension paradigm termed Deployment of LoRA Components (DLC) to enhance them. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model&#x27;s deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable content in software, DLC serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4\% of the parameters of a standard ResNet-18, our DLC model achieves a significant 8\% improvement in accuracy, demonstrating exceptional efficiency. Under a fixed memory budget, methods equipped with DLC surpass state-of-the-art expansion-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过轻量级插件推动基于蒸馏的增量学习的极限</div>
<div class="mono" style="margin-top:8px">现有的基于重放和蒸馏的增量学习（CIL）方法在保留过去知识方面有效，但仍受到稳定性-可塑性困境的限制。由于其生成的模型是在一系列增量任务上学习的，因此它们编码了丰富的表示，可以视为预训练基础。在此基础上，我们提出了一种称为LoRA组件部署（DLC）的插件扩展范式，以增强它们。对于每个任务，我们使用低秩适应（LoRA）将任务特定的残差注入基础模型的深层。在推理过程中，带有任务特定残差的表示被聚合以生成分类预测。为了减轻非目标LoRA插件的干扰，我们引入了一个轻量级加权单元。该单元学习为不同的LoRA调优表示分配重要性分数。像软件中的可下载内容一样，DLC作为一种即插即用的增强，能够有效扩展基础方法。值得注意的是，在大规模的ImageNet-100上，我们的DLC模型仅使用标准ResNet-18的4\%参数，便实现了8\%的显著准确率提升，展现出卓越的效率。在固定内存预算下，配备DLC的方法超越了最先进的基于扩展的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the stability-plasticity dilemma in class-incremental learning (CIL) methods, which struggle to retain past knowledge while adapting to new tasks. The authors propose a novel plug-in extension paradigm called Deployment of LoRA Components (DLC), which utilizes Low-Rank Adaptation (LoRA) to inject task-specific residuals into the deep layers of a pre-trained model for each incremental task. Experimental results on the large-scale ImageNet-100 dataset show that the DLC model, using only 4% of the parameters of a standard ResNet-18, achieves an 8% improvement in accuracy compared to existing methods, demonstrating its efficiency and effectiveness in enhancing CIL performance under a fixed memory budget.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决类增量学习（CIL）方法中的稳定性-可塑性困境，这些方法在适应新任务的同时难以保留过去的知识。作者提出了一种新的插件扩展范式，称为LoRA组件的部署（DLC），该方法利用低秩适应（LoRA）在每个增量任务中将特定于任务的残差注入到基础模型的深层中。实验结果表明，DLC模型在大规模ImageNet-100数据集上仅使用标准ResNet-18的4%参数，便实现了8%的准确率提升，证明了其在固定内存预算下增强CIL性能的效率和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">LoRA and Privacy: When Random Projections Help (and When They Don&#x27;t)</div>
<div class="meta-line">Authors: Yaxi Hu, Johanna Düngler, Bernhard Schölkopf, Amartya Sanyal</div>
<div class="meta-line">First: 2026-01-29T13:43:37+00:00 · Latest: 2026-01-29T13:43:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21719v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $&gt; 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoRA与隐私：随机投影何时有效（何时无效）</div>
<div class="mono" style="margin-top:8px">我们介绍了（Wishart）投影机制，一种形式为 $S \mapsto M f(S)$ 的随机映射，其中 $M \sim W_d(1/r I_d, r)$，并研究其差分隐私特性。对于向量值查询 $f$，我们证明了在没有任何加性噪声的情况下的非渐近DP保证，表明仅Wishart随机性就足够。然而，对于矩阵值查询，我们建立了一个尖锐的负结果：在无噪声的情况下，该机制不是DP，我们通过实施近乎完美的成员推断攻击（AUC $&gt; 0.99$）展示了其脆弱性。然后，我们分析了一个带噪声的变体，并证明了由于随机性和低秩投影导致的隐私增强，在大秩和小秩情况下均可实现，提供比单独加性噪声更强的隐私保证。最后，我们表明LoRA风格的更新是矩阵值机制的一个实例，这意味着尽管LoRA内置随机性，但并不固有地私密，而低秩微调在相同噪声水平下可能比完全微调更私密。初步实验表明，更严格的会计能够在实践中实现更低的噪声和更高的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the differential privacy properties of the Wishart projection mechanism, motivated by the need to understand how random projections can enhance privacy in data queries. The authors prove non-asymptotic differential privacy guarantees for vector-valued queries without requiring additive noise, indicating that Wishart randomness can be sufficient for privacy. However, they find that for matrix-valued queries, the mechanism fails to provide differential privacy in a noise-free context, as demonstrated by a highly effective membership inference attack. The study further explores a noisy variant of the mechanism, showing that it can achieve stronger privacy guarantees through randomness and low-rank projection, and concludes that while LoRA-style updates are not inherently private, low-rank fine-tuning can offer better privacy compared to full fine-tuning under the same noise conditions, with preliminary experiments indicating improved accuracy with tighter accounting.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Wishart投影机制的差分隐私特性，旨在理解随机映射在数据分析中的隐私影响。作者证明了在没有附加噪声的情况下，向量值查询的非渐近差分隐私保证，表明Wishart随机性足以提供隐私。然而，他们发现对于矩阵值查询，该机制在无噪声设置下无法提供差分隐私，且通过近乎完美的成员推断攻击证明了其脆弱性。他们还分析了一个带噪声的变体，显示通过随机性和低秩投影实现隐私增强，提供比附加噪声更强的隐私保证，并得出结论，尽管LoRA风格的更新并非固有隐私，但在某些条件下可以比完全微调提供更好的隐私，初步实验表明更紧密的会计能够提高准确性。</div>
</details>
</div>
<div class="card">
<div class="title">SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models</div>
<div class="meta-line">Authors: Nan Lin, Yanbo Wang, Jacco Heres, Peter Palensky, Pedro P. Vergara</div>
<div class="meta-line">First: 2026-01-29T13:35:39+00:00 · Latest: 2026-01-29T13:35:39+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SmartMeterFM：使用流匹配模型统一智能表数据生成任务</div>
<div class="mono" style="margin-top:8px">智能表数据是规划和运营配电网络的基础。不幸的是，由于隐私法规，这些数据并不总是可用。同时，收集的数据可能因传感器或传输故障而损坏，或者可能没有足够的分辨率用于下游任务。为了解决这些问题，提出了广泛的生成任务，包括合成数据生成、缺失数据插补和超分辨率。尽管机器学习模型在这些任务上取得了成功，但需要为每个任务设计和训练专用模型，导致冗余和低效。本文通过认识到流匹配模型的强大建模能力，提出了一种新方法，用单一模型统一多样的智能表数据生成任务，该模型经过条件生成训练。所提出的流匹配模型被训练以生成具有挑战性的高维时间序列数据，特别是以15分钟分辨率的每月智能表数据。通过将不同的生成任务视为部分数据观测的不同形式并将其注入生成过程，我们用单一模型统一了插补和超分辨率等任务，消除了重新训练的需要。我们模型生成的数据不仅与给定观测一致，而且保持现实性，表现出比插值和其他专门针对这些任务的机器学习基线更好的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges associated with the availability and quality of smart meter data, which is crucial for the effective planning and operation of distribution networks. The authors propose a novel approach using flow matching models to unify various generative tasks such as synthetic data generation, missing data imputation, and super-resolution into a single model trained for conditional generation. Experimental results demonstrate that the model effectively generates high-dimensional time series data, specifically monthly smart meter data at a 15-minute resolution, achieving better performance than traditional interpolation methods and other machine learning baselines while maintaining consistency with the provided observations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决智能电表数据的可用性和质量问题，这对配电网络的有效规划和运营至关重要。作者提出了一种新方法，利用流匹配模型将合成数据生成、缺失数据插补和超分辨率等多种生成任务统一为一个训练用于条件生成的单一模型。实验结果表明，所提出的模型有效生成高维时间序列数据，特别是以15分钟分辨率的每月智能电表数据，其性能优于传统插值方法和其他专门针对这些任务的机器学习基线，同时保持与给定观测值的一致性。</div>
</details>
</div>
<div class="card">
<div class="title">TS-PEFT: Unveiling Token-Level Redundancy in Parameter-Efficient Fine-Tuning</div>
<div class="meta-line">Authors: Dabiao Ma, Ziming Dai, Zhimin Xin, Shu Wang, Jian Yang, Haojun Fei</div>
<div class="meta-line">First: 2025-11-20T08:41:20+00:00 · Latest: 2026-01-29T12:19:42+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16147v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16147v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Parameter-Efficient Fine-Tuning (PEFT) methods typically operate under an implicit assumption: Once a target module is selected, every token passing through it contributes equally to the downstream task and requires a parameter update. In this paper, we challenge this convention by revealing a pervasive token-level redundancy in the fine-tuning of large models (LMs). We propose TS-PEFT, a theoretical framework utilizing proximal optimization that acts as a dynamic probe to identify token-level redundancy during the fine-tuning process. Extensive experiments demonstrate that indiscriminately updating all tokens is not only computationally superfluous but often introduces optimization noise. Surprisingly, by discarding 30%-70% of token updates, TS-PEFT consistently matches or exceeds the performance of dense baselines such as LoRA, DoRA. Our in-depth analysis shows that the learned token-level sparsity is a superior indicator of module importance compared to traditional weight criteria, providing a novel data-driven perspective on the intrinsic adaptation mechanism of LMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TS-PEFT：揭示参数高效微调中的令牌级冗余</div>
<div class="mono" style="margin-top:8px">当前的参数高效微调（PEFT）方法通常在一个隐含假设下运作：一旦选择了目标模块，经过它的每个令牌对下游任务的贡献是相等的，并且都需要参数更新。本文挑战了这一常规，揭示了在大型模型（LMs）微调中普遍存在的令牌级冗余。我们提出了TS-PEFT，一个利用近端优化的理论框架，作为动态探针在微调过程中识别令牌级冗余。大量实验表明，毫无区别地更新所有令牌不仅在计算上是多余的，而且往往会引入优化噪声。令人惊讶的是，通过丢弃30%-70%的令牌更新，TS-PEFT始终能够匹配或超越LoRA、DoRA等密集基线的性能。我们的深入分析表明，学习到的令牌级稀疏性是模块重要性的优越指标，相较于传统的权重标准，提供了对LMs内在适应机制的新数据驱动视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to challenge the conventional assumption in Parameter-Efficient Fine-Tuning (PEFT) that all tokens contribute equally to the downstream task and require updates. The authors propose TS-PEFT, a theoretical framework that employs proximal optimization to identify token-level redundancy during the fine-tuning of large models. Experimental results reveal that by eliminating 30%-70% of token updates, TS-PEFT can achieve performance that matches or surpasses dense baselines like LoRA and DoRA, indicating that learned token-level sparsity is a more effective measure of module importance than traditional weight criteria.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于挑战参数高效微调（PEFT）中普遍存在的假设，即所有令牌对下游任务的贡献相等并需要参数更新。作者提出了TS-PEFT，这是一种利用近端优化的理论框架，用于在大型模型的微调过程中识别令牌级冗余。实验结果表明，通过丢弃30%-70%的令牌更新，TS-PEFT能够始终匹配或超越像LoRA和DoRA这样的密集基线，表明学习到的令牌级稀疏性比传统权重标准更能有效指示模块的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration</div>
<div class="meta-line">Authors: Luwei Tu, Jiawei Wu, Xing Luo, Zhi Jin</div>
<div class="meta-line">First: 2026-01-29T12:02:42+00:00 · Latest: 2026-01-29T12:02:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21592v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21592v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations. Existing methods are often constrained by coarse-grained control mechanisms or fixed mapping schedules, yielding suboptimal adaptation. To address this, we propose an Uncertainty-Aware Diffusion Bridge Model (UDBM), which innovatively reformulates AiOIR as a stochastic transport problem steered by pixel-wise uncertainty. By introducing a relaxed diffusion bridge formulation which replaces the strict terminal constraint with a relaxed constraint, we model the uncertainty of degradations while theoretically resolving the drift singularity inherent in standard diffusion bridges. Furthermore, we devise a dual modulation strategy: the noise schedule aligns diverse degradations into a shared high-entropy latent space, while the path schedule adaptively regulates the transport trajectory motivated by the viscous dynamics of entropy regularization. By effectively rectifying the transport geometry and dynamics, UDBM achieves state-of-the-art performance across diverse restoration tasks within a single inference step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一异构退化：面向全图像恢复的考虑不确定性的扩散桥模型</div>
<div class="mono" style="margin-top:8px">全图像恢复（AiOIR）面临着调和异构退化之间冲突优化目标的根本挑战。现有方法通常受到粗粒度控制机制或固定映射计划的限制，导致适应性不佳。为此，我们提出了一种考虑不确定性的扩散桥模型（UDBM），创新性地将AiOIR重新表述为由像素级不确定性驱动的随机传输问题。通过引入一种放松的扩散桥表述，替代严格的终端约束，我们在理论上解决了标准扩散桥固有的漂移奇点，同时建模退化的不确定性。此外，我们设计了一种双重调制策略：噪声调度将多样化的退化对齐到共享的高熵潜在空间，而路径调度则根据熵正则化的粘性动力学自适应调节传输轨迹。通过有效地修正传输几何和动力学，UDBM在单次推理步骤中实现了多样恢复任务的最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenges of All-in-One Image Restoration (AiOIR), which struggles with conflicting optimization objectives due to heterogeneous degradations. The authors propose an Uncertainty-Aware Diffusion Bridge Model (UDBM) that reformulates AiOIR as a stochastic transport problem guided by pixel-wise uncertainty. The key experimental findings demonstrate that UDBM achieves state-of-the-art performance across various restoration tasks in a single inference step by effectively managing the transport geometry and dynamics through a dual modulation strategy that aligns diverse degradations into a shared latent space.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在各种图像退化类型中，由于优化目标冲突而导致的全能图像修复（AiOIR）面临的挑战。作者提出了一种不确定性感知扩散桥模型（UDBM），将AiOIR重新表述为一个由像素级不确定性引导的随机传输问题。关键实验结果表明，UDBM通过使用放松的扩散桥和双重调制策略的创新方法，在单次推理步骤中实现了多种修复任务的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</div>
<div class="meta-line">Authors: Sangoh Lee, Sangwoo Mo, Wook-Shin Han</div>
<div class="meta-line">First: 2025-12-23T03:13:39+00:00 · Latest: 2026-01-29T12:02:16+00:00</div>
<div class="meta-line">Comments: Project page with videos and code: https://vap-project.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20014v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20014v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vap-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as &quot;bring my cup,&quot; where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带上我的杯子！通过视觉注意提示个性化视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作（VLA）模型对通用指令具有良好的泛化能力，但在处理个性化命令（如“带上我的杯子”）时却面临挑战，因为机器人必须在视觉上相似的物体中识别并操作一个特定实例。我们研究了操控个人物品的场景，在该场景中，VLA必须仅通过少量参考图像识别和控制在训练期间未见过的用户特定物体。为了解决这一挑战，我们提出了视觉注意提示（VAP），这是一种简单而有效的无训练感知适配器，赋予冻结的VLA自上而下的选择性注意。VAP将参考图像视为非参数视觉记忆，通过开放词汇检测和基于嵌入的匹配将个人物体定位于场景中，然后通过突出物体和重写指令将这一定位注入为视觉提示。我们构建了两个模拟基准，个性化-SIMPLER和个性化-VLABench，以及一个真实世界的桌面基准，以评估多个机器人和任务中的个性化操控。实验表明，VAP在成功率和正确物体操控方面始终优于通用策略和基于标记学习的基线，帮助弥合语义理解与实例级控制之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of Vision-Language-Action (VLA) models in executing personalized commands, particularly in scenarios where a robot must identify and manipulate a specific object among visually similar ones. The authors introduce Visual Attentive Prompting (VAP), a training-free perceptual adapter that enhances frozen VLAs with selective attention by using reference images as a non-parametric visual memory. Experimental results demonstrate that VAP significantly outperforms generic policies and token-learning baselines in terms of success rates and accurate object manipulation across various benchmarks, thereby improving the interaction between semantic understanding and instance-level control.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视觉语言行动（VLA）模型在执行个性化指令时的表现，特别是在机器人必须在视觉上相似的物体中识别和操作特定物体的场景中。作者提出了视觉注意提示（VAP），这是一种无训练的感知适配器，通过将参考图像视为非参数视觉记忆来增强冻结的VLA的选择性注意力。实验结果表明，VAP在多个基准测试中在成功率和正确物体操作方面显著优于通用策略和基于标记的学习基线，表明其在弥合语义理解与实例级控制之间的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</div>
<div class="meta-line">Authors: Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</div>
<div class="meta-line">First: 2025-10-23T17:42:14+00:00 · Latest: 2026-01-29T11:37:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20766v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20766v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://noamissachar.github.io/DyPE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism&#x27;s quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model&#x27;s positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyPE：超高分辨率扩散的动态位置外推</div>
<div class="mono" style="margin-top:8px">扩散变换器模型能够生成具有显著保真度和细节的图像，但由于自注意力机制在图像标记数量上的平方扩展，训练它们在超高分辨率下仍然非常昂贵。本文介绍了动态位置外推（DyPE），一种新颖的无训练方法，使预训练的扩散变换器能够在远超其训练数据的分辨率下合成图像，而无需额外的采样成本。DyPE利用扩散过程固有的频谱进展，其中低频结构早期收敛，而高频结构需要更多步骤来解析。具体而言，DyPE在每个扩散步骤动态调整模型的位置信息编码，使其频谱与生成过程的当前阶段相匹配。这种方法使我们能够生成超出训练分辨率的图像，例如，使用FLUX生成1600万像素的图像。在多个基准测试中，DyPE始终提高性能，并在超高分辨率图像生成中实现了最先进的保真度，且在更高分辨率下增益更加明显。项目页面可访问 https://noamissachar.github.io/DyPE/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs associated with training diffusion transformer models at ultra-high resolutions due to the quadratic scaling of the self-attention mechanism. The authors propose a novel method called Dynamic Position Extrapolation (DyPE), which allows pre-trained diffusion transformers to generate images at resolutions significantly higher than those used during training without incurring additional sampling costs. Experimental results demonstrate that DyPE enables the generation of images with resolutions up to 16 million pixels and consistently improves performance across multiple benchmarks, achieving state-of-the-art fidelity in ultra-high-resolution image generation, particularly at higher resolutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决训练扩散变换器模型以生成超高分辨率图像所需的高计算成本。作者提出了一种名为动态位置外推（DyPE）的新方法，使得预训练的扩散变换器能够在不增加采样成本的情况下合成远高于训练分辨率的图像。实验结果表明，DyPE在多个基准测试中有效提升了性能，在超高分辨率图像生成中达到了最先进的保真度，特别是在使用FLUX数据集时，分辨率高达1600万像素时的改进尤为显著。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training</div>
<div class="meta-line">Authors: Brown Ebouky, Ajad Chhatkuli, Cristiano Malossi, Christoph Studer, Roy Assaf, Andrea Bartezzaghi</div>
<div class="meta-line">First: 2025-09-22T14:11:02+00:00 · Latest: 2026-01-29T10:21:21+00:00</div>
<div class="meta-line">Comments: 23 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17816v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17816v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While prior work has investigated parameter-efficient adaptation methods like adapters, LoRA, and prompt tuning, primarily targeting downstream finetuning, extending the SSL pre-training itself in a continual manner to new domains under limited data remains largely underexplored, especially for downstream dense prediction tasks like semantic segmentation. In this work, we address the challenge of adapting vision foundation models to low-data target domains through continual self-supervised pre-training, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream semantic segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过持续自监督预训练增强语义分割</div>
<div class="mono" style="margin-top:8px">自监督学习（SSL）已成为通过利用大规模无标签数据集训练基础模型的核心范式，通常产生具有强大泛化能力的表示。这些模型通常在通用数据集（如ImageNet）上进行预训练，然后通过微调适应各种下游任务。尽管之前的工作已经研究了如适配器、LoRA和提示调优等参数高效的适应方法，主要针对下游微调，但在有限数据下以持续方式扩展SSL预训练到新领域仍然很少被探索，尤其是针对像语义分割这样的下游密集预测任务。在本研究中，我们解决了通过持续自监督预训练将视觉基础模型适应于低数据目标领域的挑战，特别针对下游语义分割。我们提出了GLARE（全局局部和区域强化），这是一种新颖的持续自监督预训练任务，旨在增强下游语义分割性能。GLARE引入了补丁级增强以鼓励局部一致性，并结合了利用数据空间语义的区域一致性约束。为了高效的持续预训练，我们用现有SSL模型的权重初始化视觉变换器（ViTs），并仅更新轻量级适配器模块，特别是UniAdapter，同时保持其余主干不变。在不同领域的多个语义分割基准上的实验表明，GLARE在计算和参数开销最小的情况下，始终提高下游性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of semantic segmentation in low-data target domains through continual self-supervised pre-training, an area that has been underexplored. The authors propose a novel method called GLARE, which incorporates patch-level augmentations and a regional consistency constraint to improve local and spatial semantics in the data. Experimental results across various semantic segmentation benchmarks indicate that GLARE significantly enhances downstream performance while maintaining low computational and parameter overhead.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过持续的自监督预训练，改善视觉基础模型在低数据目标领域的语义分割适应性，这是一个尚未被充分探索的领域。作者提出了一种名为GLARE的新方法，该方法引入了基于补丁的增强和区域一致性约束，以增强预训练过程中的局部和空间语义一致性。多个语义分割基准的实验结果表明，GLARE显著提高了下游性能，同时保持了较低的计算和参数开销。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Next-Brain-Token Prediction for MEG</div>
<div class="meta-line">Authors: Richard Csaky</div>
<div class="meta-line">First: 2026-01-28T00:00:23+00:00 · Latest: 2026-01-29T10:17:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20138v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20138v2">PDF</a> · <a href="https://github.com/ricsinaruto/brain-gen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展MEG的下一个脑部标记预测</div>
<div class="mono" style="margin-top:8px">我们提出了一种大型自回归模型，用于源空间MEG，能够在数据集和扫描仪之间扩展下一个标记的预测：处理超过500小时的语料库和三个最大MEG数据集中的数千个会话。修改后的SEANet风格向量量化器将多通道MEG减少为扁平化的标记流，我们从头开始训练Qwen2.5-VL主干，以预测下一个脑部标记，并递归生成多达一分钟上下文的MEG分钟。为了评估长时间生成，我们引入了任务匹配测试：（i）通过生成的漂移与真实滑动窗口的时间分布相比，评估流形稳定性，以及（ii）通过正确上下文与提示交换控制，使用神经生理学基础的度量集评估条件特异性。我们在CamCAN和Omega上进行训练，并在保留的MOUS上运行所有分析，建立跨数据集的泛化。在各项指标中，生成结果在长时间滚动中保持相对稳定，并且比交换控制更接近正确的延续。代码可在：https://github.com/ricsinaruto/brain-gen获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance next-token prediction in source-space magnetoencephalography (MEG) by scaling it to handle extensive datasets and long contexts. The authors employed a modified SEANet-style vector-quantizer to convert multichannel MEG data into a token stream, training a Qwen2.5-VL backbone from scratch to predict the next brain token and generate extended MEG sequences. The experimental results demonstrate that the generated outputs maintain stability over long durations and show improved accuracy in predicting brain activity compared to control conditions, with successful cross-dataset generalization established through evaluations on held-out data.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过扩展数据集和长上下文的处理能力，增强源空间脑磁图（MEG）中的下一个标记预测。作者开发了一个大型自回归模型，利用修改过的SEANet风格向量量化器将多通道MEG数据转换为标记流，从头开始训练Qwen2.5-VL骨干网，以预测后续脑标记并生成扩展的MEG序列。实验结果表明，该模型在不同数据集上实现了稳定的长时间生成，生成的输出与实际脑活动的对齐程度优于控制条件，从而表明有效的跨数据集泛化和改进的预测性能。</div>
</details>
</div>
<div class="card">
<div class="title">BIR-Adapter: A parameter-efficient diffusion adapter for blind image restoration</div>
<div class="meta-line">Authors: Cem Eteke, Alexander Griessel, Wolfgang Kellerer, Eckehard Steinbach</div>
<div class="meta-line">First: 2025-09-08T17:22:18+00:00 · Latest: 2026-01-29T09:06:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.06904v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.06904v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the BIR-Adapter, a parameter-efficient diffusion adapter for blind image restoration. Diffusion-based restoration methods have demonstrated promising performance in addressing this fundamental problem in computer vision, typically relying on auxiliary feature extractors or extensive fine-tuning of pre-trained models. Motivated by the observation that large-scale pretrained diffusion models can retain informative representations under common image degradations, BIR-Adapter introduces a parameter-efficient, plug-and-play attention mechanism that substantially reduces the number of trained parameters. To further improve reliability, we propose a sampling guidance mechanism that mitigates hallucinations during the restoration process. Experiments on synthetic and real-world degradations demonstrate that BIR-Adapter achieves competitive, and in several settings superior, performance compared to state-of-the-art methods while requiring up to 36x fewer trained parameters. Moreover, the adapter-based design enables seamless integration into existing models. We validate this generality by extending a super-resolution-only diffusion model to handle additional unknown degradations, highlighting the adaptability of our approach for broader image restoration tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BIR-Adapter：一种用于盲图像恢复的参数高效扩散适配器</div>
<div class="mono" style="margin-top:8px">我们介绍了BIR-Adapter，一种用于盲图像恢复的参数高效扩散适配器。基于扩散的恢复方法在解决计算机视觉中的这一基本问题上表现出良好的性能，通常依赖于辅助特征提取器或对预训练模型的广泛微调。基于观察到的大规模预训练扩散模型在常见图像退化下能够保留信息表示，BIR-Adapter引入了一种参数高效的即插即用注意力机制，显著减少了训练参数的数量。为了进一步提高可靠性，我们提出了一种采样引导机制，以减轻恢复过程中的幻觉。对合成和真实世界退化的实验表明，BIR-Adapter在多个设置中与最先进的方法相比，取得了具有竞争力的，甚至在某些情况下更优的性能，同时所需的训练参数减少了多达36倍。此外，基于适配器的设计使其能够无缝集成到现有模型中。我们通过扩展仅用于超分辨率的扩散模型以处理额外未知退化来验证这一通用性，突显了我们方法在更广泛图像恢复任务中的适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for efficient methods in blind image restoration, a critical challenge in computer vision. The authors introduce the BIR-Adapter, a parameter-efficient diffusion adapter that employs a plug-and-play attention mechanism to significantly reduce the number of trained parameters while maintaining performance. Experimental results show that BIR-Adapter achieves competitive and often superior results compared to state-of-the-art methods on both synthetic and real-world image degradations, all while requiring up to 36 times fewer trained parameters and demonstrating adaptability for various image restoration tasks.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要有效的盲图像恢复方法，能够利用大规模预训练的扩散模型，同时最小化参数使用。作者提出了BIR-Adapter，这是一种参数高效的扩散适配器，采用即插即用的注意机制和采样引导策略，以增强恢复的可靠性并减少幻觉。实验结果表明，BIR-Adapter在合成和真实图像退化方面的性能与最先进的方法相当，且所需训练参数减少了多达36倍，展示了其在各种图像恢复任务中的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</div>
<div class="meta-line">Authors: Qian Wan, Ziao Xu, Luona Wei, Xiaoxuan Shen, Jianwen Sun</div>
<div class="meta-line">First: 2026-01-29T08:56:45+00:00 · Latest: 2026-01-29T08:56:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过难度感知强化学习减轻大型推理模型的过度思考</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过模仿人类的深度思维行为实现明确的思维链扩展，在复杂任务场景中表现出色。然而，深度思维模式在处理简单任务时往往导致不必要的冗长推理和资源低效。这种过度思考现象可能源于后训练阶段奖励函数引发的生成偏好。现有研究尝试从提示设计或模型训练的角度减轻过度思考，但通常低估了任务难度意识的重要性，这使得LRMs难以有效分配推理资源。本文提出了一种基于强化学习的LRM训练框架——难度感知策略优化（DiPO）。DiPO鼓励LRM自发建模任务复杂性，并将其整合到强化学习框架中，以调整后训练引入的生成偏好。我们提出了一种基于模型自我推理的难度建模方法，显著减少了对人工标注的依赖，并形式化任务复杂性。我们进一步开发了一种难度信号增强的奖励函数，该函数在考虑推理性能和输出格式的同时，对冗长推理施加惩罚。实验结果表明，DiPO使模型能够自发调整推理开销，显著减少冗余标记，同时不因思维压缩而失去性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of overthinking in Large Reasoning Models (LRMs), which can lead to inefficient reasoning and excessive resource use during simple tasks. The authors propose a novel training framework called Difficulty-aware Policy Optimization (DiPO), which utilizes reinforcement learning to help LRMs better understand task complexity and adjust their reasoning accordingly. Experimental results demonstrate that DiPO effectively reduces unnecessary reasoning tokens while maintaining performance, indicating that the model can optimize its inference overhead through an enhanced reward function that penalizes lengthy reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型推理模型（LRMs）中的过度思考问题，这在处理简单任务时可能导致低效的推理过程。作者提出了一种新颖的训练框架，称为困难感知策略优化（DiPO），该框架利用强化学习帮助LRMs更好地理解和适应任务复杂性。实验结果表明，DiPO有效减少了不必要的推理，通过最小化冗余标记来优化推理开销，同时保持性能，从而使模型能够根据任务难度进行调整。</div>
</details>
</div>
<div class="card">
<div class="title">Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification</div>
<div class="meta-line">Authors: Kailash A. Hambarde, Hugo Proença</div>
<div class="meta-line">First: 2026-01-29T08:41:42+00:00 · Latest: 2026-01-29T08:41:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21405v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21405v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aerial-ground person re-identification (AG-ReID) is fundamentally challenged by extreme viewpoint and distance discrepancies between aerial and ground cameras, which induce severe geometric distortions and invalidate the assumption of a shared similarity space across views. Existing methods primarily rely on geometry-aware feature learning or appearance-conditioned prompting, while implicitly assuming that the geometry-invariant dot-product similarity used in attention mechanisms remains reliable under large viewpoint and scale variations. We argue that this assumption does not hold. Extreme camera geometry systematically distorts the query-key similarity space and degrades attention-based matching, even when feature representations are partially aligned.
  To address this issue, we introduce Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that explicitly rectifies the similarity space by conditioning query-key interactions on camera geometry. Rather than modifying feature representations or the attention formulation itself, GIQT adapts the similarity computation to compensate for dominant geometry-induced anisotropic distortions. Building on this local similarity rectification, we further incorporate a geometry-conditioned prompt generation mechanism that provides global, view-adaptive representation priors derived directly from camera geometry.
  Experiments on four aerial-ground person re-identification benchmarks demonstrate that the proposed framework consistently improves robustness under extreme and previously unseen geometric conditions, while introducing minimal computational overhead compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>纠正几何引起的相似性失真以实现真实世界的空地人重识别</div>
<div class="mono" style="margin-top:8px">空地人重识别（AG-ReID）在空中和地面摄像机之间的极端视角和距离差异面临根本挑战，这导致严重的几何失真，并使得跨视角共享相似性空间的假设失效。现有方法主要依赖于几何感知特征学习或外观条件提示，同时隐含假设在大视角和尺度变化下，注意机制中使用的几何不变点积相似性仍然可靠。我们认为这一假设并不成立。极端的摄像机几何系统性地扭曲了查询-关键相似性空间，并降低了基于注意力的匹配效果，即使特征表示部分对齐。为了解决这个问题，我们引入了几何引起的查询-关键变换（GIQT），这是一个轻量级低秩模块，通过基于摄像机几何条件化查询-关键交互，明确纠正相似性空间。GIQT并不修改特征表示或注意力公式本身，而是调整相似性计算，以补偿主导的几何引起的各向异性失真。在此局部相似性纠正的基础上，我们进一步结合了几何条件的提示生成机制，提供直接源自摄像机几何的全局、视角自适应表示先验。在四个空地人重识别基准上的实验表明，所提出的框架在极端和以前未见的几何条件下始终提高了鲁棒性，同时与最先进的方法相比引入了最小的计算开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in aerial-ground person re-identification (AG-ReID) caused by significant viewpoint and distance discrepancies between aerial and ground cameras, which lead to geometric distortions that compromise similarity assessments. The authors propose a method called Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that adjusts the similarity space by conditioning query-key interactions based on camera geometry, rather than altering feature representations or attention mechanisms. Experimental results across four AG-ReID benchmarks show that GIQT enhances robustness against extreme geometric variations while maintaining low computational overhead compared to existing state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由于空中和地面相机之间显著的视角和距离差异而导致的几何失真问题，这对空中-地面人员重识别（AG-ReID）造成了挑战，影响了相似性评估。作者提出了一种新方法，称为几何诱导查询-键变换（GIQT），这是一种轻量级模块，通过根据相机几何调整查询-键交互来修正相似性空间，而不是改变特征表示或注意力机制。四个AG-ReID基准的实验结果表明，GIQT在极端几何变化下提高了鲁棒性，同时与现有的最先进方法相比，计算开销保持在较低水平。</div>
</details>
</div>
<div class="card">
<div class="title">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</div>
<div class="meta-line">Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:15:00+00:00 · Latest: 2026-01-29T08:38:27+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026, URL: https://github.com/AMAP-ML/SpatialGenEval</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20354v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20354v2">PDF</a> · <a href="https://github.com/AMAP-ML/SpatialGenEval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一切归位：文本到图像模型的空间智能基准测试</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在生成高保真图像方面取得了显著成功，但在处理复杂空间关系（例如空间感知、推理或交互）时常常失败。这些关键方面在当前基准测试中大多被忽视，原因在于其短小或信息稀疏的提示设计。本文介绍了SpatialGenEval，这是一个旨在系统评估T2I模型空间智能的新基准，涵盖两个关键方面：（1）SpatialGenEval涉及25个真实场景中的1,230个长且信息密集的提示。每个提示整合了10个空间子领域和相应的10对多项选择问答，涵盖从物体位置和布局到遮挡和因果关系。我们对21个最先进模型的广泛评估表明，高阶空间推理仍然是主要瓶颈。（2）为了证明我们信息密集设计的实用性超越简单评估，我们还构建了SpatialT2I数据集。该数据集包含15,400对文本-图像对，重写提示以确保图像一致性，同时保持信息密度。在当前基础模型（即Stable Diffusion-XL、Uniworld-V1、OmniGen2）上的微调结果显示出一致的性能提升（+4.2%、+5.7%、+4.4%）和更真实的空间关系效果，突显了实现T2I模型空间智能的数据中心范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of text-to-image (T2I) models in understanding complex spatial relationships, which are often neglected in existing benchmarks. The authors introduce SpatialGenEval, a benchmark featuring 1,230 detailed prompts across 25 real-world scenes, each incorporating multiple spatial sub-domains and corresponding questions to evaluate spatial reasoning. Their evaluation of 21 leading models shows that higher-order spatial reasoning is a significant challenge, while fine-tuning on the newly created SpatialT2I dataset, which includes 15,400 text-image pairs, results in consistent performance improvements and enhanced realism in spatial relations, indicating the effectiveness of a data-centric approach to enhance spatial intelligence in T2I models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像（T2I）模型在处理复杂空间关系方面的不足，这在现有基准中常常被忽视。作者提出了SpatialGenEval，这是一个新的基准，通过在25个真实场景中使用1,230个长且信息密集的提示来评估T2I模型的空间智能，每个提示都包含多个空间子领域和相应的问题-答案对。对21个最先进模型的评估显示，高阶空间推理仍然是一个重大挑战，而在新构建的SpatialT2I数据集上进行微调，包含15,400对文本-图像对，导致模型性能的一致提升和更现实的空间关系。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
