<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-01 11:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251101_1119</div>
    <div class="row"><div class="card">
<div class="title">ResMatching: Noise-Resilient Computational Super-Resolution via Guided   Conditional Flow Matching</div>
<div class="meta-line">Authors: Anirban Ray, Vera Galinova, Florian Jug</div>
<div class="meta-line">First: 2025-10-30T15:29:20+00:00 · Latest: 2025-10-30T15:29:20+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26601v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResMatching：通过引导条件流匹配实现抗噪声计算超分辨率</div>
<div class="mono" style="margin-top:8px">荧光显微镜中的计算超分辨率（CSR）尽管是一个病态问题，但有着悠久的历史。CSR的核心在于寻找一个先验，用于推断在图像生成显微镜下从未成像的微观图像中的频率。随着更好的数据驱动机器学习技术的出现，可以学习到更强的先验，从而使CSR能够获得更好的结果。在这里，我们提出了ResMatching，一种新颖的CSR方法，利用引导条件流匹配来学习改进的数据先验。我们在BioSR数据集上的4种不同生物结构上评估ResMatching，并将其结果与7个基线进行比较。ResMatching始终取得竞争力的结果，在所有情况下都展示了数据保真度与感知现实之间的最佳权衡。我们观察到，使用ResMatching的CSR在强先验难以学习的情况下特别有效，例如当给定的低分辨率图像包含大量噪声时。此外，我们还展示了ResMatching可以用于从隐式学习的后验分布中采样，并且该分布在所有测试用例中都经过校准，使我们的方法能够提供逐像素的数据不确定性项，指导未来用户拒绝不确定的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Computational Super-Resolution (CSR) in fluorescence microscopy, which is challenged by the ill-posed nature of the problem and the need for effective data-priors. The authors introduce ResMatching, a novel CSR method that employs guided conditional flow matching to learn enhanced data-priors. Experimental evaluations on four diverse biological structures from the BioSR dataset show that ResMatching consistently outperforms seven baseline methods, achieving the best balance between data fidelity and perceptual realism, particularly in scenarios with high noise levels in low-resolution images, while also providing a calibrated pixel-wise data-uncertainty term for better prediction reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善荧光显微镜中的计算超分辨率（CSR），该问题因其不适定性和对更好数据驱动先验的需求而面临挑战。作者提出了ResMatching，这是一种新颖的CSR方法，采用引导条件流匹配来学习增强的数据先验。实验结果表明，ResMatching在BioSR数据集中四种生物结构上优于七种基线方法，实现了数据保真度和感知现实性之间的最佳平衡，尤其是在低分辨率图像中噪声较大的情况下，同时还提供了经过校准的逐像素数据不确定性项，以指导未来的预测。</div>
</details>
</div>
<div class="card">
<div class="title">LinearSR: Unlocking Linear Attention for Stable and Efficient Image   Super-Resolution</div>
<div class="meta-line">Authors: Xiaohui Li, Shaobin Zhuang, Shuo Cao, Yang Yang, Yuandong Pu, Qi Qin, Siqi Luo, Bin Fu, Yihao Liu</div>
<div class="meta-line">First: 2025-10-09T19:41:51+00:00 · Latest: 2025-10-30T14:46:21+00:00</div>
<div class="meta-line">Comments: 19 pages, 9 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08771v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.08771v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models for Image Super-Resolution (SR) are increasingly powerful,
yet their reliance on self-attention&#x27;s quadratic complexity (O(N^2)) creates a
major computational bottleneck. Linear Attention offers an O(N) solution, but
its promise for photorealistic SR has remained largely untapped, historically
hindered by a cascade of interrelated and previously unsolved challenges. This
paper introduces LinearSR, a holistic framework that, for the first time,
systematically overcomes these critical hurdles. Specifically, we resolve a
fundamental, training instability that causes catastrophic model divergence
using our novel &quot;knee point&quot;-based Early-Stopping Guided Fine-tuning (ESGF)
strategy. Furthermore, we mitigate the classic perception-distortion trade-off
with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we
establish an effective and lightweight guidance paradigm, TAG, derived from our
&quot;precision-over-volume&quot; principle. Our resulting LinearSR model simultaneously
delivers state-of-the-art perceptual quality with exceptional efficiency. Its
core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its
overall multi-step inference time remains highly competitive. This work
provides the first robust methodology for applying Linear Attention in the
photorealistic SR domain, establishing a foundational paradigm for future
research in efficient generative super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinearSR：解锁线性注意力以实现稳定高效的图像超分辨率</div>
<div class="mono" style="margin-top:8px">图像超分辨率（SR）的生成模型越来越强大，但它们对自注意力的二次复杂性（O(N^2)）的依赖造成了主要的计算瓶颈。线性注意力提供了O(N)的解决方案，但其在照片级真实感SR中的潜力仍未得到充分利用，历史上受到一系列相互关联且未解决的挑战的阻碍。本文介绍了LinearSR，这是一个整体框架，首次系统性地克服了这些关键障碍。具体而言，我们通过新颖的基于“拐点”的早停引导微调（ESGF）策略解决了导致灾难性模型发散的基本训练不稳定性。此外，我们通过专门的基于信噪比的专家混合（MoE）架构缓解了经典的感知失真权衡。最后，我们建立了一个有效且轻量的引导范式TAG，源自我们的“精度优于体积”原则。我们得到的LinearSR模型同时提供了卓越的感知质量和出色的效率。其核心扩散前向传播（1-NFE）实现了SOTA级别的速度，而其整体多步推理时间仍然具有高度竞争力。这项工作为在照片级真实感SR领域应用线性注意力提供了首个稳健的方法论，为未来高效生成超分辨率的研究奠定了基础范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational bottleneck caused by the quadratic complexity of self-attention in generative models for image super-resolution (SR). The authors introduce LinearSR, a framework that employs a novel Early-Stopping Guided Fine-tuning strategy to resolve training instability and a SNR-based Mixture of Experts architecture to tackle the perception-distortion trade-off. The experimental results demonstrate that LinearSR achieves state-of-the-art perceptual quality and efficiency, with a core diffusion forward pass that offers competitive speed and an overall multi-step inference time that remains highly efficient, marking a significant advancement in applying Linear Attention for photorealistic SR.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成模型在图像超分辨率（SR）中自注意力的平方复杂度所带来的计算瓶颈。作者提出了LinearSR框架，利用线性注意力实现线性复杂度，同时克服了历史上在真实感SR中面临的挑战。主要实验结果包括成功实施了一种新颖的基于早停的引导微调策略以稳定训练，采用基于信噪比的专家混合架构来平衡感知与失真，并提出了一种轻量级的引导范式以提高效率，从而在图像超分辨率任务中实现了最先进的感知质量和具有竞争力的推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and   High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?</div>
<div class="meta-line">Authors: Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-30T10:46:28+00:00 · Latest: 2025-10-30T10:46:28+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures. Includes supplementary material. Under review as
  a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26339v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLYPH-SR：我们能否通过VLM引导的潜在扩散模型实现高质量图像超分辨率和高保真文本恢复？</div>
<div class="mono" style="margin-top:8px">图像超分辨率（SR）是许多视觉系统的基础——从监控和自主驾驶到文档分析和零售分析——因为恢复高频细节，特别是场景文本，使得下游感知更可靠。场景文本，即嵌入自然图像中的文本，如标志、产品标签和店面，通常携带最具可操作性的信息；当字符模糊或幻觉时，即使图像的其余部分看起来清晰，光学字符识别（OCR）和后续决策也会失败。然而，以往的SR研究往往调优于失真（PSNR/SSIM）或学习的感知度量（LIPIS、MANIQA、CLIP-IQA、MUSIQ），这些度量对字符级错误的敏感性较低。此外，解决文本SR的研究通常集中在简化的基准测试上，孤立字符，忽视了复杂自然场景中文本的挑战。因此，场景文本实际上被视为通用纹理。为了使SR在实际部署中有效，因此必须明确优化文本可读性和感知质量。我们提出GLYPH-SR，一个视觉-语言引导的扩散框架，旨在共同实现这两个目标。GLYPH-SR利用由OCR数据引导的文本SR融合控制网络（TS-ControlNet），以及一个在文本和场景中心指导之间交替的乒乓调度器。为了实现针对性的文本恢复，我们在一个合成语料库上训练这些组件，同时保持主要SR分支不变。在SVT、SCUT-CTW1500和CUTE80的x4和x8上，GLYPH-SR在保持竞争力的MANIQA、CLIP-IQA和MUSIQ的同时，将OCR F1提高了多达+15.18个百分点（SVT x8，OpenOCR）。GLYPH-SR旨在同时满足这两个目标——高可读性和高视觉真实感——提供看起来正确且读取正确的SR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance image super-resolution (SR) specifically for scene-text recovery, which is crucial for various applications such as surveillance and document analysis. The authors propose GLYPH-SR, a vision-language-guided diffusion framework that integrates a Text-SR Fusion ControlNet (TS-ControlNet) driven by optical character recognition (OCR) data, along with a ping-pong scheduling method for alternating guidance between text and scene elements. Experimental results demonstrate that GLYPH-SR significantly improves OCR F1 scores by up to 15.18 percentage points compared to baseline methods while also achieving competitive performance in perceptual quality metrics like MANIQA, CLIP-IQA, and MUSIQ.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像超分辨率（SR），以更好地恢复高频细节，特别是场景文本，这对于监控和文档分析等多种应用至关重要。作者提出了GLYPH-SR，这是一种受视觉语言指导的扩散框架，采用了基于光学字符识别（OCR）数据的文本超分辨率融合控制网络（TS-ControlNet），并使用乒乓调度器进行交替指导。实验结果表明，GLYPH-SR在OCR F1分数上比基线方法提高了最多15.18个百分点，同时在感知质量指标上保持了竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">DOVE: Efficient One-Step Diffusion Model for Real-World Video   Super-Resolution</div>
<div class="meta-line">Authors: Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo, Yulun Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T05:16:45+00:00 · Latest: 2025-10-30T06:40:44+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025. Code is available at:
  https://github.com/zhengchen1999/DOVE</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16239v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16239v2">PDF</a> · <a href="https://github.com/zhengchen1999/DOVE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have demonstrated promising performance in real-world video
super-resolution (VSR). However, the dozens of sampling steps they require,
make inference extremely slow. Sampling acceleration techniques, particularly
single-step, provide a potential solution. Nonetheless, achieving one step in
VSR remains challenging, due to the high training overhead on video data and
stringent fidelity demands. To tackle the above issues, we propose DOVE, an
efficient one-step diffusion model for real-world VSR. DOVE is obtained by
fine-tuning a pretrained video diffusion model (i.e., CogVideoX). To
effectively train DOVE, we introduce the latent-pixel training strategy. The
strategy employs a two-stage scheme to gradually adapt the model to the video
super-resolution task. Meanwhile, we design a video processing pipeline to
construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning
on this dataset further enhances the restoration capability of DOVE. Extensive
experiments show that DOVE exhibits comparable or superior performance to
multi-step diffusion-based VSR methods. It also offers outstanding inference
efficiency, achieving up to a 28$\times$ speed-up over existing methods such as
MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DOVE：高效的一步扩散模型用于真实世界视频超分辨率</div>
<div class="mono" style="margin-top:8px">扩散模型在真实世界视频超分辨率（VSR）中表现出良好的性能。然而，它们所需的数十个采样步骤使得推理极其缓慢。采样加速技术，特别是单步采样，提供了潜在的解决方案。然而，由于视频数据的高训练开销和严格的保真度要求，在VSR中实现一步仍然具有挑战性。为了解决上述问题，我们提出了DOVE，一种高效的一步扩散模型用于真实世界VSR。DOVE是通过微调预训练的视频扩散模型（即CogVideoX）获得的。为了有效训练DOVE，我们引入了潜在像素训练策略。该策略采用两阶段方案逐步使模型适应视频超分辨率任务。同时，我们设计了一个视频处理管道，以构建一个针对VSR量身定制的高质量数据集，称为HQ-VSR。在该数据集上的微调进一步增强了DOVE的恢复能力。大量实验表明，DOVE在性能上与基于多步扩散的VSR方法相当或更优。它还提供了出色的推理效率，相较于现有方法如MGLD-VSR，速度提升可达28倍。代码可在：https://github.com/zhengchen1999/DOVE获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the slow inference times associated with diffusion models in real-world video super-resolution (VSR), which typically require numerous sampling steps. The authors propose DOVE, an efficient one-step diffusion model, which is developed by fine-tuning a pretrained video diffusion model, CogVideoX, and introducing a latent-pixel training strategy that employs a two-stage scheme for effective adaptation to the VSR task. Experimental results demonstrate that DOVE achieves comparable or superior performance to multi-step diffusion methods while significantly improving inference efficiency, with a speed-up of up to 28 times over existing techniques like MGLD-VSR.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高使用扩散模型的视频超分辨率（VSR）的效率，而扩散模型通常需要多个采样步骤，导致推理速度缓慢。作者提出了DOVE，这是一种高效的一步扩散模型，通过微调预训练的视频扩散模型CogVideoX开发而成。他们引入了一种潜像素训练策略和专门的视频处理管道，以创建高质量的VSR数据集，结果表明DOVE在性能上与多步骤方法相当或更优，同时在推理速度上比现有技术提高了多达28倍。</div>
</details>
</div>
<div class="card">
<div class="title">BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and   Enhanced Motion Compensation</div>
<div class="meta-line">Authors: Wei Shang, Wanying Zhang, Shuhang Gu, Pengfei Zhu, Qinghua Hu, Dongwei Ren</div>
<div class="meta-line">First: 2025-10-30T05:08:45+00:00 · Latest: 2025-10-30T05:08:45+00:00</div>
<div class="meta-line">Comments: 13 pages, 10 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26149v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26149v1">PDF</a> · <a href="https://github.com/shangwei5/BasicAVSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BasicAVSR：通过图像先验和增强运动补偿实现任意尺度视频超分辨率</div>
<div class="mono" style="margin-top:8px">任意尺度视频超分辨率（AVSR）旨在提高视频帧的分辨率，可能在不同的缩放因子下，这带来了空间细节再现、时间一致性和计算复杂性等多个挑战。本文提出了一个强基线BasicAVSR，通过整合四个关键组件：1）从图像拉普拉斯金字塔生成的自适应多尺度频率先验，2）一个流引导传播单元，用于聚合相邻帧的时空信息，3）一个二阶运动补偿单元，以更准确地对齐相邻帧的空间，4）一个超上采样单元，以生成尺度感知和内容无关的上采样核。为了满足多样化的应用需求，我们实例化了三种传播变体：（i）用于严格在线推理的单向RNN单元，（ii）具有有限前瞻的单向RNN单元，允许小的输出延迟，以及（iii）为离线任务设计的双向RNN单元，其中计算资源的限制较小。实验结果证明了我们模型在这些不同场景中的有效性和适应性。通过广泛的实验，我们表明BasicAVSR在超分辨率质量、泛化能力和推理速度方面显著优于现有方法。我们的工作不仅推动了AVSR的最新进展，还将其核心组件扩展到多个框架以适应多样化场景。代码可在https://github.com/shangwei5/BasicAVSR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of arbitrary-scale video super-resolution (AVSR), which involves enhancing video frame resolution while maintaining spatial detail, temporal consistency, and managing computational complexity. The authors propose a baseline model called BasicAVSR that incorporates adaptive multi-scale frequency priors, a flow-guided propagation unit, a second-order motion compensation unit, and a hyper-upsampling unit. Experimental results indicate that BasicAVSR significantly outperforms existing methods in super-resolution quality, generalization ability, and inference speed across various application scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决任意尺度视频超分辨率（AVSR）中的挑战，包括增强空间细节、保持时间一致性和管理计算复杂性。作者提出了一种名为BasicAVSR的基线模型，该模型集成了自适应多尺度频率先验、流引导传播单元、二阶运动补偿单元和超高采样单元。实验结果表明，BasicAVSR在超分辨率质量、泛化能力和推理速度方面显著优于现有方法，适用于各种应用场景。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Image Restoration and Super-Resolution using Physics-Informed   Synthetic Data for Scanning Tunneling Microscopy</div>
<div class="meta-line">Authors: Nikola L. Kolev, Tommaso Rodani, Neil J. Curson, Taylor J. Z. Stock, Alberto Cazzaniga</div>
<div class="meta-line">First: 2025-10-29T19:50:34+00:00 · Latest: 2025-10-29T19:50:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25921v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25921v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理信息合成数据的扫描隧道显微镜生成图像修复与超分辨率</div>
<div class="mono" style="margin-top:8px">扫描隧道显微镜（STM）能够实现原子分辨率成像和原子操控，但其应用常常受到探针退化和慢速串行数据采集的限制。制造过程增加了复杂性，因为探针通常会承受高电压，这可能改变其顶端形状，需要进行调理。在此，我们提出了一种机器学习（ML）方法用于图像修复和超分辨率，以缓解这两种挑战。使用仅36幅Si(001):H的原始实验图像的数据集，我们证明了基于物理信息的合成数据生成管道可以用于训练多种最先进的流匹配和扩散模型。通过CLIP最大均值差异（CMMD）分数和结构相似性等指标进行的定量评估表明，我们的模型能够有效恢复图像，并通过准确重建稀疏采样数据中的图像，实现图像采集时间减少两到四倍。我们的框架有潜力显著提高STM实验的通量，通过减少探针调理程序的频率和提高现有高速STM系统的帧率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of scanning tunneling microscopy (STM) caused by tip degradation and slow data acquisition, which hinder its effectiveness in atomic-resolution imaging. The authors propose a machine learning approach that utilizes a physics-informed synthetic data generation pipeline to train advanced flow-matching and diffusion models, based on a limited dataset of 36 pristine experimental images. The experimental results indicate that the models can effectively restore images and achieve a two- to fourfold reduction in image acquisition time by reconstructing images from sparsely sampled data, thereby enhancing STM experimental throughput and reducing the need for frequent tip-conditioning procedures.</div>
<div class="mono" style="margin-top:8px">该研究解决了扫描隧道显微镜（STM）因探针退化和数据采集缓慢而导致的局限性，这些问题妨碍了其在原子分辨率成像中的有效性。作者提出了一种机器学习方法，利用物理信息合成数据生成管道，基于仅有的36幅Si(001):H的原始实验图像训练先进的流匹配和扩散模型。实验结果表明，这些模型能够有效恢复图像，并通过从稀疏采样数据中准确重建图像，实现图像采集时间减少两到四倍，从而提高STM实验的通量，并减少频繁进行探针调理的需求。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Temporal Consistency and Fidelity at Inference-time in   Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</div>
<div class="meta-line">Authors: Nasrin Rahimi, A. Murat Tekalp</div>
<div class="meta-line">First: 2025-10-29T11:40:06+00:00 · Latest: 2025-10-29T11:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25420v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as powerful priors for single-image
restoration, but their application to zero-shot video restoration suffers from
temporal inconsistencies due to the stochastic nature of sampling and
complexity of incorporating explicit temporal modeling. In this work, we
address the challenge of improving temporal coherence in video restoration
using zero-shot image-based diffusion models without retraining or modifying
their architecture. We propose two complementary inference-time strategies: (1)
Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired
perceptual straightening hypothesis, which steers the diffusion denoising
process towards smoother temporal evolution by incorporating a curvature
penalty in a perceptual space to improve temporal perceptual scores, such as
Fr\&#x27;echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path
Ensemble Sampling (MPES), which aims at reducing stochastic variation by
ensembling multiple diffusion trajectories to improve fidelity (distortion)
scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these
training-free techniques provide a practical path toward temporally stable
high-fidelity perceptual video restoration using large pretrained diffusion
models. We performed extensive experiments over multiple datasets and
degradation types, systematically evaluating each strategy to understand their
strengths and limitations. Our results show that while PSG enhances temporal
naturalness, particularly in case of temporal blur, MPES consistently improves
fidelity and spatio-temporal perception--distortion trade-off across all tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过零-shot基于图像的扩散模型在推理时改善感知视频恢复的时间一致性和保真度</div>
<div class="mono" style="margin-top:8px">扩散模型已成为单图像恢复的强大先验，但其在零-shot视频恢复中的应用由于采样的随机性和显式时间建模的复杂性而受到时间不一致性的影响。在本研究中，我们解决了使用零-shot基于图像的扩散模型在视频恢复中改善时间一致性的挑战，而无需重新训练或修改其架构。我们提出了两种互补的推理时策略：(1) 基于神经科学启发的感知整直假设的感知整直引导（PSG），通过在感知空间中引入曲率惩罚，指导扩散去噪过程朝向更平滑的时间演变，以提高时间感知分数，如Fréchet视频距离（FVD）和感知整直性；(2) 多路径集成采样（MPES），旨在通过集成多个扩散轨迹来减少随机变化，以提高保真度（失真）分数，如PSNR和SSIM，而不牺牲清晰度。这些无训练的技术为使用大型预训练扩散模型实现时间稳定的高保真感知视频恢复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估每种策略以了解其优缺点。我们的结果表明，尽管PSG在时间自然性方面有所增强，特别是在时间模糊的情况下，MPES在所有任务中始终改善保真度和时空感知-失真权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance temporal consistency and fidelity in zero-shot video restoration using diffusion models, which typically face challenges due to their stochastic sampling nature. The authors propose two inference-time strategies: Perceptual Straightening Guidance (PSG), which utilizes a curvature penalty to improve temporal perceptual scores, and Multi-Path Ensemble Sampling (MPES), which reduces stochastic variation by combining multiple diffusion trajectories. Experimental results demonstrate that PSG effectively enhances temporal naturalness, especially in cases of temporal blur, while MPES consistently improves fidelity and the trade-off between perception and distortion across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高零-shot视频恢复中扩散模型的时间一致性，这些模型通常由于其随机采样特性而面临挑战。作者提出了两种推理时策略：感知直线引导（PSG），通过施加曲率惩罚来改善时间感知分数，以及多路径集成采样（MPES），通过结合多个扩散路径来减少随机变化以增强保真度。实验结果表明，PSG在时间模糊的情况下改善了时间自然性，而MPES在各种任务中始终增强了保真度和感知与失真之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired   Monocentric Design</div>
<div class="meta-line">Authors: Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang</div>
<div class="meta-line">First: 2025-10-29T09:27:38+00:00 · Latest: 2025-10-29T09:27:38+00:00</div>
<div class="meta-line">Comments: The source code will be publicly available at
  https://github.com/ZongxiYu-ZJU/BMI</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25314v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25314v1">PDF</a> · <a href="https://github.com/ZongxiYu-ZJU/BMI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清晰而深刻的观察：一种基于生物启发的单心设计的RGBD成像方法</div>
<div class="mono" style="margin-top:8px">实现高保真、紧凑的RGBD成像面临双重挑战：传统紧凑光学在整个景深范围内难以保持RGB清晰度，而仅依赖软件的单目深度估计（MDE）是一个不适定问题，依赖于不可靠的语义先验。虽然具有衍射光学元件（DOE）的深度光学可以编码深度，但它们在制造复杂性和色差方面引入了权衡，妨碍了简单性。为了解决这个问题，我们首先引入了一种新颖的生物启发的全球面单心透镜，围绕它构建了仿生单心成像（BMI）框架，这是一个整体协同设计。该光学设计自然地将深度编码到其深度变化的点扩散函数（PSF）中，而无需复杂的衍射或自由形状元件。我们建立了一个严格的基于物理的前向模型，通过精确模拟光学退化过程生成合成数据集。该模拟管道与一个双头多尺度重建网络共同设计，该网络采用共享编码器从单个编码捕获中联合恢复高保真的全聚焦（AiF）图像和精确的深度图。大量实验验证了所提框架的最先进性能。在深度估计中，该方法达到了0.026的绝对相对误差（Abs Rel）和0.130的均方根误差（RMSE），显著优于领先的软件方法和其他深度光学系统。在图像恢复方面，该系统达到了0.960的结构相似性指数（SSIM）和0.082的感知LPIPS分数，从而确认了图像保真度和深度准确性之间的优越平衡。本研究表明，生物启发的全球面光学与联合重建算法的结合构成了解决高性能紧凑RGBD成像内在挑战的有效策略。源代码将公开发布在https://github.com/ZongxiYu-ZJU/BMI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to overcome the limitations of conventional compact optics and software-only Monocular Depth Estimation (MDE) in achieving high-fidelity RGBD imaging. The authors introduce a bio-inspired all-spherical monocentric lens and develop the Bionic Monocentric Imaging (BMI) framework, which integrates optical design with a dual-head, multi-scale reconstruction network. Experimental results demonstrate that the proposed method achieves superior performance in depth estimation with an Absolute Relative Error (Abs Rel) of 0.026 and RMSE of 0.130, as well as image restoration with an SSIM of 0.960 and a perceptual LPIPS score of 0.082, indicating a successful balance between image fidelity and depth accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服传统紧凑光学和仅依赖软件的单目深度估计（MDE）在实现高保真RGBD成像方面的局限性。作者提出了一种受生物启发的全球面单光心透镜，并开发了生物单光心成像（BMI）框架，将光学设计与双头多尺度重建网络相结合。实验结果表明，所提出的方法显著优于现有技术，在深度估计中实现了0.026的绝对相对误差和0.130的均方根误差，同时在图像恢复中获得了0.960的结构相似性指数（SSIM）和0.082的感知LPIPS分数，表明在图像保真度和深度准确性之间取得了成功的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt   State Space Model</div>
<div class="meta-line">Authors: Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao</div>
<div class="meta-line">First: 2025-04-24T16:46:32+00:00 · Latest: 2025-10-29T07:04:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.17732v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.17732v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-One image restoration aims to address multiple image degradation
problems using a single model, offering a more practical and versatile solution
compared to designing dedicated models for each degradation type. Existing
approaches typically rely on Degradation-specific models or coarse-grained
degradation prompts to guide image restoration. However, they lack fine-grained
modeling of degradation information and face limitations in balancing
multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a
novel All-in-One image restoration framework that introduces a fine-grained
degradation extractor and a Degradation-Aware Prompt State Space Model
(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured
by the extractor as dynamic prompts, which are then incorporated into the state
space modeling process. This enhances the model&#x27;s adaptability to diverse
degradation types, while a complementary High-Frequency Enhancement Block (HEB)
recovers local high-frequency details. Extensive experiments on a mixed dataset
containing seven degradation types show that DPMambaIR achieves the best
performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These
results highlight the potential and superiority of DPMambaIR as a unified
solution for All-in-One image restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPMambaIR：通过降解感知提示的全能图像恢复状态空间模型</div>
<div class="mono" style="margin-top:8px">全能图像恢复旨在通过单一模型解决多种图像降解问题，提供比为每种降解类型设计专用模型更实用和多功能的解决方案。现有方法通常依赖于特定降解模型或粗粒度降解提示来指导图像恢复。然而，它们缺乏对降解信息的细粒度建模，并在平衡多任务冲突方面面临限制。为克服这些限制，我们提出了DPMambaIR，这是一种新颖的全能图像恢复框架，引入了细粒度降解提取器和降解感知提示状态空间模型（DP-SSM）。DP-SSM利用提取器捕获的细粒度降解特征作为动态提示，然后将其纳入状态空间建模过程。这增强了模型对多种降解类型的适应性，同时一个补充的高频增强模块（HEB）恢复局部高频细节。在包含七种降解类型的混合数据集上的广泛实验表明，DPMambaIR在PSNR和SSIM中分别达到了27.69dB和0.893的最佳性能。这些结果突显了DPMambaIR作为全能图像恢复统一解决方案的潜力和优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a versatile solution for All-in-One image restoration that can effectively handle multiple types of image degradation without the need for separate models. The authors propose DPMambaIR, which incorporates a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM) to dynamically adapt to various degradation types. Experimental results demonstrate that DPMambaIR outperforms existing methods, achieving a PSNR of 27.69dB and an SSIM of 0.893 across a mixed dataset with seven degradation types, indicating its effectiveness as a unified restoration framework.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一种多功能的图像恢复解决方案，能够有效处理多种类型的图像退化，而无需单独的模型。作者提出了DPMambaIR，该模型结合了细粒度退化提取器和退化感知提示状态空间模型（DP-SSM），以改善退化信息的建模。对包含七种退化类型的混合数据集的实验结果表明，DPMambaIR的性能优于现有方法，PSNR达到27.69dB，SSIM为0.893，表明其作为统一恢复框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Aware Super-Resolution via Distance-Adaptive Variational   Formulation</div>
<div class="meta-line">Authors: Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu</div>
<div class="meta-line">First: 2025-09-06T15:35:37+00:00 · Latest: 2025-10-29T04:32:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05746v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.05746v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于距离自适应变分公式的深度感知超分辨率</div>
<div class="mono" style="margin-top:8px">单幅图像超分辨率传统上假设空间不变的退化模型，但现实世界的成像系统表现出复杂的距离依赖效应，包括大气散射、景深变化和透视失真。这一基本限制要求采用空间自适应重建策略，明确结合几何场景理解以实现最佳性能。我们提出了一个严格的变分框架，将超分辨率表征为空间变化的逆问题，将退化算子表述为具有距离依赖谱特性的伪微分算子，从而能够对不同深度范围的重建极限进行理论分析。我们的神经架构通过级联残差块和深度条件卷积核实现离散梯度流动态，确保收敛到理论能量泛函的平稳点，同时结合学习的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的谱约束防止了远场区域的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。对五个基准数据集的全面评估表明，性能达到最先进水平，在KITTI户外场景中，在2倍和4倍缩放下分别实现36.89/0.9516和30.54/0.8721的PSNR/SSIM，分别比现有方法提高0.44dB和0.36dB。该工作建立了第一个理论基础的距离自适应超分辨率框架，并在深度变化场景中展示了显著改进，同时在传统基准上保持竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional single image super-resolution methods that rely on spatially-invariant degradation models, which do not account for real-world complexities such as atmospheric scattering and depth variations. The authors propose a variational framework that treats super-resolution as a spatially-varying inverse problem, utilizing a pseudodifferential operator to analyze reconstruction limits across different depths. Experimental results show that their method achieves state-of-the-art performance on five benchmark datasets, with notable improvements in PSNR and SSIM metrics, surpassing existing techniques by 0.44dB and 0.36dB at various scales on KITTI outdoor scenes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统单幅图像超分辨率方法的局限性，这些方法通常依赖于空间不变的降解模型，而未考虑大气散射和深度变化等现实世界的复杂性。作者提出了一种变分框架，将超分辨率视为一个空间变化的逆问题，利用具有距离依赖特性的伪微分算子。实验结果表明，他们的方法在五个基准数据集上实现了最先进的性能，特别是在深度变化场景中，PSNR和SSIM指标显著提高，超越了现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
