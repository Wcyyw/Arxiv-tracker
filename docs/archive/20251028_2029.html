<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-28 20:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251028_2029</div>
    <div class="row"><div class="card">
<div class="title">Alita-G: Self-Evolving Generative Agent for Agent Generation</div>
<div class="meta-line">Authors: Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2025-10-27T17:59:14+00:00 · Latest: 2025-10-27T17:59:14+00:00</div>
<div class="meta-line">Comments: 15 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23601v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool&#x27;s descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity&#x27;s Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alita-G: 自我进化的生成代理用于代理生成</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在具备记忆、工具和反馈的代理框架下表现更佳。自我进化代理的出现超越了这一点，但目前的工作主要将适应性限制在提示重写或失败重试。因此，我们提出了ALITA-G，一个自我进化框架，通过系统地生成、抽象和策划模型上下文协议（MCP）工具，将通用代理转变为领域专家。在该框架中，通用代理执行一套策划的目标领域任务，并从成功轨迹中合成候选MCP。这些候选MCP随后被抽象为参数化原语，并整合到MCP盒中。在推理时，ALITA-G借助每个工具的描述和用例执行检索增强的MCP选择，然后执行配备MCP执行器的代理。在多个基准GAIA、PathVQA和人类最后考试中，ALITA-G取得了显著的提升，同时降低了计算成本。在GAIA验证中，它达到了83.03%的pass@1和89.09%的pass@3，建立了新的最先进结果，同时相对于强基线代理减少了每个示例的平均标记约15%。因此，ALITA-G提供了一条从通用能力到可重用领域特定能力的原则性路径，提高了复杂推理任务的准确性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of large language models (LLMs) by developing self-evolving agents that can adapt beyond simple prompt rewriting and failure retries. The authors introduce ALITA-G, a framework that enables a general-purpose agent to evolve into a domain expert by generating, abstracting, and curating Model Context Protocol (MCP) tools through systematic execution of target-domain tasks. Experimental results demonstrate that ALITA-G achieves significant improvements across benchmarks such as GAIA, PathVQA, and Humanity&#x27;s Last Exam, with a state-of-the-art pass rate of 83.03% on GAIA validation while also reducing computation costs by approximately 15% compared to a strong baseline agent.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将大型语言模型（LLM）演变为能够自适应的代理，从而提高其在特定领域的表现。作者提出了ALITA-G，这是一种自我进化框架，使通用代理能够通过系统生成、抽象和策划模型上下文协议（MCP）工具，成为领域专家。实验结果表明，ALITA-G在GAIA等基准测试中显著提高了性能，在pass@1和pass@3的通过率分别达到了83.03%和89.09%，同时与强基线代理相比，计算成本降低了约15%，因此在该领域建立了新的最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</div>
<div class="meta-line">Authors: Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-11T20:13:59+00:00 · Latest: 2025-10-27T17:51:21+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight Paper at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15963v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.15963v2">PDF</a> · <a href="https://github.com/video-fm/LASER">Code1</a> · <a href="https://github.com/video-fm/ESCA">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, existing MLLMs do not reliably
capture fine-grained links between low-level visual features and high-level
textual semantics, leading to weak grounding and inaccurate perception. To
overcome this challenge, we propose ESCA, a framework that contextualizes
embodied agents by grounding their perception in spatial-temporal scene graphs.
At its core is SGCLIP, a novel, open-domain, promptable foundation model for
generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+
open-domain videos using a neurosymbolic pipeline that aligns automatically
generated captions with scene graphs produced by the model itself, eliminating
the need for human-labeled annotations. We demonstrate that SGCLIP excels in
both prompt-based inference and task-specific fine-tuning, achieving
state-of-the-art results on scene graph generation and action localization
benchmarks. ESCA with SGCLIP improves perception for embodied agents based on
both open-source and commercial MLLMs, achieving state of-the-art performance
across two embodied environments. Notably, ESCA significantly reduces agent
perception errors and enables open-source models to surpass proprietary
baselines. We release the source code for SGCLIP model training at
https://github.com/video-fm/LASER and for the embodied agent at
https://github.com/video-fm/ESCA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESCA：通过场景图生成为具身智能体提供背景</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）正在快速发展成为通用的具身智能体。然而，现有的MLLMs并未可靠地捕捉低级视觉特征与高级文本语义之间的细粒度联系，导致基础不稳和感知不准确。为了解决这一挑战，我们提出了ESCA，一个通过将感知基于时空场景图进行基础的具身智能体的框架。其核心是SGCLIP，一个新颖的开放域、可提示的基础模型，用于生成基于CLIP的场景图。SGCLIP在87K+开放域视频上进行训练，使用神经符号管道自动对齐生成的字幕与模型自身生成的场景图，消除了对人工标注的需求。我们证明SGCLIP在基于提示的推理和任务特定的微调方面表现出色，在场景图生成和动作定位基准上取得了最先进的结果。基于开源和商业MLLMs的ESCA与SGCLIP结合，提高了具身智能体的感知能力，在两个具身环境中实现了最先进的性能。值得注意的是，ESCA显著减少了智能体的感知错误，并使开源模型超越专有基线。我们在https://github.com/video-fm/LASER发布了SGCLIP模型训练的源代码，并在https://github.com/video-fm/ESCA发布了具身智能体的源代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing multi-modal large language models (MLLMs) in accurately linking low-level visual features with high-level textual semantics, which results in weak grounding and perception issues for embodied agents. The authors propose a framework called ESCA, which utilizes a novel foundation model named SGCLIP for generating scene graphs, trained on over 87,000 open-domain videos through a neurosymbolic pipeline that aligns automatically generated captions with scene graphs. Experimental results show that SGCLIP achieves state-of-the-art performance in scene graph generation and action localization, significantly improving the perception of embodied agents and reducing perception errors, allowing open-source models to outperform proprietary ones in various environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改善低级视觉特征与高级文本语义之间的联系，增强具身智能体的基础，当前的多模态大型语言模型在这方面存在困难。作者提出了ESCA框架，利用SGCLIP这一新型基础模型生成场景图，该模型在超过87,000个开放域视频上训练，通过神经符号管道将自动生成的字幕与场景图对齐。实验结果表明，SGCLIP在场景图生成和动作定位方面达到了最先进的性能，显著减少了具身智能体的感知错误，并使开源模型在两个不同的具身环境中超越了专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language   Models via Selective Layer-Wise Model Merging</div>
<div class="meta-line">Authors: Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Holger Boche</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-03-21T15:44:09+00:00 · Latest: 2025-10-27T17:40:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.17239v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.17239v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning large language models (LLMs) is a common practice to adapt
generalist models to specialized domains. However, recent studies show that
fine-tuning can erode safety alignment, causing LLMs to respond to harmful or
unethical prompts. Many methods to realign safety have been proposed, but often
introduce custom algorithms that are difficult to implement or compromise task
utility. In this work, we propose SafeMERGE, a lightweight, post-fine-tuning
framework that preserves safety while maintaining downstream performance.
SafeMERGE selectively merges fine-tuned with safety-aligned model layers only
when they deviate from safe behavior, measured by a cosine similarity
criterion. Across three LLMs and two tasks, SafeMERGE consistently reduces
harmful outputs compared to other defenses, with negligible or even positive
impact on utility. Our results demonstrate that selective layer-wise merging
offers an effective safeguard against the inadvertent loss of safety during
fine-tuning, establishing SafeMERGE as a simple post-fine-tuning defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeMERGE：通过选择性层级模型合并保持微调大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）是将通用模型适应于专业领域的常见做法。然而，最近的研究表明，微调可能会侵蚀安全对齐，导致LLMs对有害或不道德的提示做出反应。虽然提出了许多重新对齐安全的方法，但通常会引入难以实施的自定义算法，或妨碍任务效用。在本研究中，我们提出了SafeMERGE，这是一种轻量级的后微调框架，能够在保持下游性能的同时保护安全。SafeMERGE仅在微调与安全对齐的模型层偏离安全行为时，选择性地合并这些层，合并依据余弦相似度标准进行。在三个LLM和两个任务中，SafeMERGE始终能有效减少有害输出，相较于其他防御方法，其对效用的影响微乎其微，甚至是正面的。我们的结果表明，选择性层级合并为微调过程中意外丧失安全性提供了有效的保护，确立了SafeMERGE作为一种简单的后微调防御。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of safety alignment erosion in fine-tuned large language models (LLMs), which can lead to harmful or unethical responses. The authors propose a method called SafeMERGE, a lightweight framework that selectively merges layers from fine-tuned models with safety-aligned layers based on a cosine similarity criterion. Experimental results across three LLMs and two tasks show that SafeMERGE significantly reduces harmful outputs compared to other methods, while maintaining or even improving task utility.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在微调大型语言模型（LLMs）过程中安全对齐的侵蚀问题，这可能导致有害或不道德的响应。作者提出了一种名为SafeMERGE的方法，这是一种轻量级框架，根据余弦相似性标准选择性地将微调模型的层与安全对齐的层合并。三种LLM和两项任务的实验结果表明，SafeMERGE在有效减少有害输出的同时，保持或甚至改善了任务效用，相较于现有方法表现更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image   Synthesis</div>
<div class="meta-line">Authors: Boming Miao, Chunxiao Li, Xiaoxiao Wang, Andi Zhang, Rui Sun, Zizhe Wang, Yao Zhu</div>
<div class="meta-line">First: 2024-11-25T15:40:47+00:00 · Latest: 2025-10-27T17:31:22+00:00</div>
<div class="meta-line">Comments: Updated author formatting; no substantive changes</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2411.16503v2">Abs</a> · <a href="http://arxiv.org/pdf/2411.16503v2">PDF</a> · <a href="https://github.com/Bomingmiao/NoiseDiffusion">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have achieved impressive success in generating
photorealistic images, but challenges remain in ensuring precise semantic
alignment with input prompts. Optimizing the initial noisy latent offers a more
efficient alternative to modifying model architectures or prompt engineering
for improving semantic alignment. A latest approach, InitNo, refines the
initial noisy latent by leveraging attention maps; however, these maps capture
only limited information, and the effectiveness of InitNo is highly dependent
on the initial starting point, as it tends to converge on a local optimum near
this point. To this end, this paper proposes leveraging the language
comprehension capabilities of large vision-language models (LVLMs) to guide the
optimization of the initial noisy latent, and introduces the Noise Diffusion
process, which updates the noisy latent to generate semantically faithful
images while preserving distribution consistency. Furthermore, we provide a
theoretical analysis of the condition under which the update improves semantic
faithfulness. Experimental results demonstrate the effectiveness and
adaptability of our framework, consistently enhancing semantic alignment across
various diffusion models. The code is available at
https://github.com/Bomingmiao/NoiseDiffusion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声扩散在文本到图像合成中增强语义忠实度</div>
<div class="mono" style="margin-top:8px">扩散模型在生成逼真图像方面取得了显著成功，但在确保与输入提示的精确语义对齐方面仍然面临挑战。优化初始噪声潜变量提供了一种比修改模型架构或提示工程更有效的替代方案，以改善语义对齐。最新的方法InitNo通过利用注意力图来精炼初始噪声潜变量；然而，这些图仅捕捉有限的信息，InitNo的有效性高度依赖于初始起点，因为它往往会收敛到该点附近的局部最优解。为此，本文提出利用大型视觉-语言模型（LVLM）的语言理解能力来指导初始噪声潜变量的优化，并引入噪声扩散过程，该过程更新噪声潜变量以生成语义忠实的图像，同时保持分布一致性。此外，我们提供了更新改善语义忠实度的条件的理论分析。实验结果证明了我们框架的有效性和适应性，在各种扩散模型中持续增强语义对齐。代码可在https://github.com/Bomingmiao/NoiseDiffusion获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in achieving precise semantic alignment in text-to-image synthesis using diffusion models. The authors propose a method called Noise Diffusion, which utilizes the language comprehension capabilities of large vision-language models to optimize the initial noisy latent, improving semantic faithfulness without altering model architectures or relying solely on prompt engineering. Experimental results indicate that this approach consistently enhances semantic alignment across various diffusion models, demonstrating its effectiveness and adaptability in generating semantically faithful images while maintaining distribution consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用扩散模型在文本到图像合成中实现精确语义对齐的挑战。作者提出了一种名为噪声扩散的新方法，该方法利用大型视觉-语言模型的语言理解能力来优化初始噪声潜变量，提高语义忠实度，而无需改变模型架构或仅依赖提示工程。实验结果表明，该方法在各种扩散模型中一致地增强了语义对齐，证明了其在生成语义忠实图像时的有效性和适应性，同时保持了分布一致性。</div>
</details>
</div>
<div class="card">
<div class="title">FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time</div>
<div class="meta-line">Authors: Yaoli Liu, Yao-Xiang Ding, Kun Zhou</div>
<div class="meta-line">First: 2025-10-27T16:54:08+00:00 · Latest: 2025-10-27T16:54:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23515v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://future-item.github.io/FreeFuse/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes FreeFuse, a novel training-free approach for
multi-subject text-to-image generation through automatic fusion of multiple
subject LoRAs. In contrast to existing methods that either focus on
pre-inference LoRA weight merging or rely on segmentation models and complex
techniques like noise blending to isolate LoRA outputs, our key insight is that
context-aware dynamic subject masks can be automatically derived from
cross-attention layer weights. Mathematical analysis shows that directly
applying these masks to LoRA outputs during inference well approximates the
case where the subject LoRA is integrated into the diffusion model and used
individually for the masked region. FreeFuse demonstrates superior practicality
and efficiency as it requires no additional training, no modification to LoRAs,
no auxiliary models, and no user-defined prompt templates or region
specifications. Alternatively, it only requires users to provide the LoRA
activation words for seamless integration into standard workflows. Extensive
experiments validate that FreeFuse outperforms existing approaches in both
generation quality and usability under the multi-subject generation tasks. The
project page is at https://future-item.github.io/FreeFuse/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeFuse：通过测试时自动掩码实现多主题LoRA融合</div>
<div class="mono" style="margin-top:8px">本文提出了FreeFuse，一种新颖的无训练方法，通过自动融合多个主题LoRA实现多主题文本到图像生成。与现有方法（这些方法要么专注于推理前的LoRA权重合并，要么依赖于分割模型和复杂技术如噪声混合来隔离LoRA输出）不同，我们的关键见解是可以从交叉注意力层权重中自动推导出上下文感知的动态主题掩码。数学分析表明，在推理过程中将这些掩码直接应用于LoRA输出，可以很好地近似将主题LoRA集成到扩散模型中并单独用于掩码区域的情况。FreeFuse展示了卓越的实用性和效率，因为它不需要额外的训练，不需要修改LoRA，不需要辅助模型，也不需要用户定义的提示模板或区域规范。相反，它只需要用户提供LoRA激活词，以便无缝集成到标准工作流程中。大量实验验证了FreeFuse在多主题生成任务中在生成质量和可用性方面优于现有方法。项目页面为 https://future-item.github.io/FreeFuse/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve multi-subject text-to-image generation without the need for additional training or complex techniques. The authors propose FreeFuse, a training-free method that utilizes context-aware dynamic subject masks derived from cross-attention layer weights to automatically fuse multiple subject LoRAs during inference. Experimental results demonstrate that FreeFuse significantly enhances generation quality and usability compared to existing methods, making it a more practical solution for multi-subject generation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善多主体文本到图像生成，而无需额外的训练或复杂的技术。作者提出了FreeFuse，这是一种无训练的方法，利用从交叉注意力层权重中推导出的上下文感知动态主体掩码，在推理过程中自动融合多个主体LoRA。实验结果表明，FreeFuse在生成质量和可用性方面显著优于现有方法，证明了其在多主体生成任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Large Language Models for Stance Detection on Financial   Targets from SEC Filing Reports and Earnings Call Transcripts</div>
<div class="meta-line">Authors: Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea</div>
<div class="meta-line">First: 2025-10-27T16:03:20+00:00 · Latest: 2025-10-27T16:03:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23464v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23464v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs&#x27; performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型在金融目标的立场检测中的应用：来自SEC文件报告和财报电话会议记录</div>
<div class="mono" style="margin-top:8px">来自美国证券交易委员会（SEC）文件报告和季度财报电话会议记录（ECTs）的金融叙述对投资者、审计师和监管机构非常重要。然而，它们的长度、金融术语和细微的语言使得细粒度分析变得困难。以往在金融领域的情感分析需要大量昂贵的标注数据集，使得对特定金融目标的句子级立场分析具有挑战性。在本研究中，我们引入了一个专注于三个核心金融指标（债务、每股收益（EPS）和销售额）的句子级语料库，用于立场检测。这些句子来自Form 10-K年度报告和ECTs，并使用先进的ChatGPT-o3-pro模型在严格的人类验证下进行立场标注（积极、消极、中立）。利用该语料库，我们对现代大型语言模型（LLMs）进行了系统评估，采用零-shot、少-shot和思维链（CoT）提示策略。我们的结果表明，少-shot与CoT提示的组合表现优于监督基线，LLMs在SEC和ECT数据集上的表现各异。我们的研究结果突显了在金融领域利用LLMs进行目标特定立场分析的实际可行性，而无需大量标注数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of analyzing financial narratives from SEC filing reports and earnings call transcripts, which are often lengthy and filled with complex jargon, making stance detection difficult. The authors introduce a new sentence-level corpus for stance detection focused on key financial metrics such as debt, earnings per share, and sales, using the ChatGPT-o3-pro model for labeling under human validation. The study evaluates various large language models using zero-shot, few-shot, and Chain-of-Thought prompting strategies, finding that few-shot with CoT prompting outperforms supervised baselines, with performance varying across different datasets, demonstrating the potential of LLMs for stance detection in finance without extensive labeled data requirements.</div>
<div class="mono" style="margin-top:8px">本研究解决了分析美国证券交易委员会（SEC）备案报告和季度财报电话会议记录中的金融叙述的挑战，这些叙述通常冗长且充满复杂术语，使得立场检测变得困难。作者引入了一个新的句子级语料库，专注于关键财务指标的立场检测，将句子标记为积极、消极或中立，使用ChatGPT-o3-pro模型并经过人工验证。该研究评估了多种大型语言模型，采用零样本、少样本和思维链提示策略，发现少样本与思维链提示的表现优于监督基线，同时不同数据集的表现存在差异，展示了在金融领域中利用大型语言模型进行立场检测的潜力，而无需大量标记数据。</div>
</details>
</div>
<div class="card">
<div class="title">Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal   Queries</div>
<div class="meta-line">Authors: Pengfei Cai, Yan Song, Qing Gu, Nan Jiang, Haoyu Song, Ian McLoughlin</div>
<div class="meta-line">Venue: MM 2025</div>
<div class="meta-line">First: 2025-07-22T08:24:01+00:00 · Latest: 2025-10-27T15:55:48+00:00</div>
<div class="meta-line">Comments: Accepted by MM 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.16343v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.16343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cai525.github.io/Transformer4SED/demo_page/DASM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most existing sound event detection~(SED) algorithms operate under a
closed-set assumption, restricting their detection capabilities to predefined
classes. While recent efforts have explored language-driven zero-shot SED by
exploiting audio-language models, their performance is still far from
satisfactory due to the lack of fine-grained alignment and cross-modal feature
fusion. In this work, we propose the Detect Any Sound Model (DASM), a
query-based framework for open-vocabulary SED guided by multi-modal queries.
DASM formulates SED as a frame-level retrieval task, where audio features are
matched against query vectors derived from text or audio prompts. To support
this formulation, DASM introduces a dual-stream decoder that explicitly
decouples event recognition and temporal localization: a cross-modality event
decoder performs query-feature fusion and determines the presence of sound
events at the clip-level, while a context network models temporal dependencies
for frame-level localization. Additionally, an inference-time attention masking
strategy is proposed to leverage semantic relations between base and novel
classes, substantially enhancing generalization to novel classes. Experiments
on the AudioSet Strong dataset demonstrate that DASM effectively balances
localization accuracy with generalization to novel classes, outperforming
CLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in
the closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot
evaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the
supervised CRNN baseline. The project page is available at
https://cai525.github.io/Transformer4SED/demo_page/DASM/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检测任何声音：基于多模态查询的开放词汇声音事件检测</div>
<div class="mono" style="margin-top:8px">大多数现有的声音事件检测（SED）算法在封闭集假设下运行，限制了它们对预定义类别的检测能力。尽管最近的研究通过利用音频-语言模型探索了基于语言的零-shot SED，但由于缺乏细粒度对齐和跨模态特征融合，其性能仍然远未令人满意。在本研究中，我们提出了检测任何声音模型（DASM），这是一个基于查询的开放词汇SED框架，由多模态查询引导。DASM将SED公式化为帧级检索任务，其中音频特征与来自文本或音频提示的查询向量进行匹配。为了支持这一公式，DASM引入了一个双流解码器，明确解耦事件识别和时间定位：跨模态事件解码器执行查询-特征融合并确定声音事件在片段级别的存在，而上下文网络则建模帧级定位的时间依赖性。此外，提出了一种推理时注意力掩蔽策略，以利用基础类和新类之间的语义关系，显著增强对新类的泛化能力。在AudioSet Strong数据集上的实验表明，DASM有效平衡了定位准确性与对新类的泛化能力，在开放词汇设置中超越了基于CLAP的方法（+7.8 PSDS）和封闭集设置中的基线（+6.9 PSDS）。此外，在DESED的跨数据集零-shot评估中，DASM获得了42.2的PSDS1分数，甚至超过了监督的CRNN基线。项目页面可访问https://cai525.github.io/Transformer4SED/demo_page/DASM/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve sound event detection (SED) capabilities beyond predefined classes, addressing limitations in existing algorithms that operate under a closed-set assumption. The authors propose the Detect Any Sound Model (DASM), which utilizes a query-based framework for open-vocabulary SED by treating it as a frame-level retrieval task, where audio features are matched with query vectors from text or audio prompts. Experimental results on the AudioSet Strong dataset show that DASM significantly enhances localization accuracy and generalization to novel classes, outperforming CLAP-based methods by 7.8 PSDS and a closed-set baseline by 6.9 PSDS, while achieving a PSDS1 score of 42.2 in cross-dataset zero-shot evaluation on DESED, surpassing the supervised CRNN baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高声音事件检测（SED）的能力，超越预定义类别，解决现有算法在封闭集假设下的局限性。作者提出了检测任何声音模型（DASM），这是一种基于查询的框架，将SED形式化为帧级检索任务，利用双流解码器将事件识别与时间定位分开。对AudioSet Strong数据集的实验结果表明，DASM显著提高了定位准确性和对新类别的泛化能力，超越了基于CLAP的方法，并在跨数据集零样本评估中获得42.2的PSDS1分数，超过了监督的CRNN基线。</div>
</details>
</div>
<div class="card">
<div class="title">Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</div>
<div class="meta-line">Authors: Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-05-26T16:53:47+00:00 · Latest: 2025-10-27T14:42:01+00:00</div>
<div class="meta-line">Comments: Published as a main conference paper at EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23799v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.23799v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are prone to hallucinations and sensitiveto
prompt perturbations, often resulting in inconsistent or unreliablegenerated
text. Different methods have been proposed to mitigate suchhallucinations and
fragility, one of which is to measure theconsistency of LLM responses -- the
model&#x27;s confidence in the responseor likelihood of generating a similar
response when resampled. Inprevious work, measuring LLM response consistency
often relied oncalculating the probability of a response appearing within a
pool of resampledresponses, analyzing internal states, or evaluating logits of
resopnses.However, it was not clear how well theseapproaches approximated
users&#x27; perceptions of consistency of LLMresponses. To find out, we performed a
user study ($n=2,976$)demonstrating that current methods for measuring LLM
responseconsistency typically do not align well with humans&#x27; perceptions of
LLMconsistency. We propose a logit-based ensemble method for estimatingLLM
consistency and show that our method matches the performance of
thebest-performing existing metric in estimating human ratings of
LLMconsistency. Our results suggest that methods for estimating LLMconsistency
without human evaluation are sufficiently imperfect towarrant broader use of
evaluation with human input; this would avoidmisjudging the adequacy of models
because of the imperfections ofautomated consistency metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>估计大型语言模型一致性：用户基线与替代指标</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）容易出现幻觉，并对提示扰动敏感，常常导致生成的不一致或不可靠的文本。为减轻这种幻觉和脆弱性，提出了不同的方法，其中之一是测量LLM响应的一致性——模型对响应的信心或在重新采样时生成相似响应的可能性。在之前的研究中，测量LLM响应一致性通常依赖于计算响应在重新采样响应池中出现的概率、分析内部状态或评估响应的logits。然而，这些方法与用户对LLM响应一致性的感知之间的契合度并不明确。为了找出答案，我们进行了用户研究（$n=2,976$），证明当前测量LLM响应一致性的方法通常与人类对LLM一致性的感知不太一致。我们提出了一种基于logit的集成方法来估计LLM一致性，并显示我们的方法在估计人类对LLM一致性的评分时与表现最佳的现有指标的性能相匹配。我们的结果表明，未经过人类评估的LLM一致性估计方法存在足够的缺陷，值得更广泛地使用人类输入的评估；这将避免因自动一致性指标的缺陷而错误判断模型的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inconsistency and unreliability of large language models (LLMs), which often produce hallucinations and are sensitive to prompt changes. The authors conducted a user study with 2,976 participants to evaluate how well existing methods for measuring LLM response consistency align with human perceptions. The findings indicate that these methods generally do not reflect users&#x27; views accurately, and the authors propose a logit-based ensemble method that performs comparably to the best existing metrics in estimating human ratings of LLM consistency, highlighting the need for human evaluation in assessing model adequacy due to the limitations of automated metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLM）的一致性和可靠性问题，这些模型经常产生幻觉，并对提示变化敏感。作者进行了一个包含2976名参与者的用户研究，以评估现有的LLM响应一致性测量方法与人类感知之间的匹配程度。研究结果表明，这些方法通常与用户对一致性的看法不太相关，因此提出了一种基于logit的集成方法，该方法在预测人类对LLM一致性的评分方面表现与现有最佳指标相当，强调了在准确评估模型性能时需要进行人类评估。</div>
</details>
</div>
<div class="card">
<div class="title">An Efficient Remote Sensing Super Resolution Method Exploring Diffusion   Priors and Multi-Modal Constraints for Crop Type Mapping</div>
<div class="meta-line">Authors: Songxi Yang, Tang Sui, Qunying Huang</div>
<div class="meta-line">First: 2025-10-27T14:34:52+00:00 · Latest: 2025-10-27T14:34:52+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23382v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super resolution offers a way to harness medium even lowresolution but
historically valuable remote sensing image archives. Generative models,
especially diffusion models, have recently been applied to remote sensing super
resolution (RSSR), yet several challenges exist. First, diffusion models are
effective but require expensive training from scratch resources and have slow
inference speeds. Second, current methods have limited utilization of auxiliary
information as real-world constraints to reconstruct scientifically realistic
images. Finally, most current methods lack evaluation on downstream tasks. In
this study, we present a efficient LSSR framework for RSSR, supported by a new
multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built
on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention
with auxiliary knowledge (Digital Elevation Model, land cover, month) and
Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier
NDVI loss to balance spatial details and spectral fidelity. Extensive
experiments demonstrate that LSSR significantly improves crop boundary
delineation and recovery, achieving state-of-the-art performance with Peak
Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)
and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while
maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers
effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,
yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:
0.85). These results highlight the potential of RSSR to advance precision
agriculture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的遥感超分辨率方法：探索扩散先验和多模态约束用于作物类型映射</div>
<div class="mono" style="margin-top:8px">超分辨率提供了一种利用中等甚至低分辨率但历史上有价值的遥感图像档案的方法。生成模型，尤其是扩散模型，最近被应用于遥感超分辨率（RSSR），但仍存在几个挑战。首先，扩散模型有效但需要昂贵的从零开始的训练资源，并且推理速度较慢。其次，当前方法对辅助信息的利用有限，无法作为现实世界约束来重建科学上真实的图像。最后，大多数当前方法缺乏对下游任务的评估。在本研究中，我们提出了一种高效的LSSR框架用于RSSR，支持一个新的多模态数据集，包含配对的30米Landsat 8和10米Sentinel 2影像。基于冻结的预训练稳定扩散，LSSR将跨模态注意力与辅助知识（数字高程模型、土地覆盖、月份）和合成孔径雷达指导相结合，通过适配器和定制的傅里叶NDVI损失来平衡空间细节和光谱保真度。大量实验表明，LSSR显著改善了作物边界的描绘和恢复，达到最先进的性能，峰值信噪比/结构相似性指数分别为32.63/0.84（RGB）和23.99/0.78（IR），NDVI均方误差最低（0.042），同时保持高效推理（0.39秒/图像）。此外，LSSR有效转移到NASA协调的Landsat和Sentinel（HLS）超分辨率，产生比Sentinel-2更可靠的作物分类（F1: 0.86，Sentinel-2: F1: 0.85）。这些结果突显了RSSR在推动精准农业方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the utility of low-resolution remote sensing images for crop type mapping, addressing challenges in existing super resolution methods. The authors propose an efficient LSSR framework that leverages a new multimodal dataset of paired Landsat 8 and Sentinel 2 imagery, utilizing a pretrained Stable Diffusion model integrated with crossmodal attention and auxiliary knowledge. Experimental results show that LSSR significantly improves crop boundary delineation and recovery, achieving state-of-the-art performance metrics and efficient inference times, while also demonstrating effective transferability to NASA&#x27;s HLS super resolution for improved crop classification.</div>
<div class="mono" style="margin-top:8px">本研究解决了遥感超分辨率（RSSR）中的挑战，特别是训练扩散模型所需的高资源需求和其缓慢的推理速度，以及有限的辅助信息用于真实图像重建。作者提出了一种高效的LSSR框架，利用一组新的配对Landsat 8和Sentinel 2影像的多模态数据集，结合跨模态注意力、辅助知识和合成孔径雷达指导。实验结果表明，LSSR显著提高了作物边界的划分和恢复，达到了最先进的性能指标和高效的推理时间，同时在NASA的HLS超分辨率中也表现出有效的可转移性，改善了作物分类。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning as an Adaptive Defense for Safety</div>
<div class="meta-line">Authors: Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar</div>
<div class="meta-line">First: 2025-07-01T17:20:04+00:00 · Latest: 2025-10-27T14:28:11+00:00</div>
<div class="meta-line">Comments: 44 pages, 10 Figures, 7 Tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.00971v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.00971v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a ``lightweight&#x27;&#x27; warmstart SFT stage, (2) a mix of
harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as
too many refusals, and (3) a reward function to prevent degeneration of
reasoning capabilities during training. Models trained with TARS exhibit
adaptive behaviors by spending more compute on ambiguous queries, leading to
better safety-refusal trade-offs. They also internally learn to better
distinguish between safe and unsafe prompts and attain greater robustness to
both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our
work provides an effective, open recipe for training LLMs against jailbreaks
and harmful requests by reasoning per prompt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>作为安全适应性防御的推理</div>
<div class="mono" style="margin-top:8px">适应性分配测试时计算的推理方法提高了大型语言模型在易于验证的领域（如数学和代码）上的表现。在这项工作中，我们研究如何利用这种方法训练在安全漏洞方面表现出一定鲁棒性的模型，并展示这样做可以带来好处。我们构建了一种名为$\textit{TARS}$（安全性训练适应性推理器）的方案，这是一种强化学习（RL）方法，训练模型通过思维链迹和一个平衡安全性与任务完成的奖励信号来推理安全性。为了构建TARS，我们确定了三个关键设计选择：（1）一个“轻量级”热启动SFT阶段，（2）混合有害、无害和模糊提示以防止过多拒绝等捷径行为，以及（3）一个奖励函数以防止训练过程中推理能力的退化。使用TARS训练的模型通过在模糊查询上花费更多计算展现出适应性行为，从而实现更好的安全拒绝权衡。它们还在内部学习更好地区分安全和不安全的提示，并在白盒（例如GCG）和黑盒攻击（例如PAIR）方面获得更大的鲁棒性。总体而言，我们的工作提供了一种有效的、开放的方案，用于通过逐个提示推理来训练大型语言模型以抵御越狱和有害请求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the robustness of large language models (LLMs) against safety vulnerabilities by employing adaptive reasoning methods. The authors propose a reinforcement learning approach called TARS (Training Adaptive Reasoners for Safety), which trains models to reason about safety through chain-of-thought traces and a reward signal that balances safety with task completion. Experimental results indicate that models trained with TARS demonstrate adaptive behaviors, allocating more computational resources to ambiguous queries, improving safety-refusal trade-offs, and achieving greater robustness against both white-box and black-box attacks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过采用自适应推理方法来增强大型语言模型（LLM）对安全漏洞的鲁棒性，这些方法在简单领域中已取得了性能提升。作者提出了一种名为TARS（安全自适应推理训练）的强化学习框架，该框架通过思维链迹和一种平衡安全性与任务完成的奖励系统来训练模型进行安全推理。实验结果表明，使用TARS训练的模型表现出更好的自适应行为，在模糊查询上分配更多计算资源，从而实现更好的安全拒绝权衡，并增强了对白盒和黑盒攻击的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrapping Referring Multi-Object Tracking</div>
<div class="meta-line">Authors: Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong</div>
<div class="meta-line">First: 2024-06-07T16:02:10+00:00 · Latest: 2025-10-27T14:22:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.05039v2">Abs</a> · <a href="http://arxiv.org/pdf/2406.05039v2">PDF</a> · <a href="https://github.com/zyn213/TempRMOT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Referring understanding is a fundamental task that bridges natural language
and visual content by localizing objects described in free-form expressions.
However, existing works are constrained by limited language expressiveness,
lacking the capacity to model object dynamics in spatial numbers and temporal
states. To address these limitations, we introduce a new and general referring
understanding task, termed referring multi-object tracking (RMOT). Its core
idea is to employ a language expression as a semantic cue to guide the
prediction of multi-object tracking, comprehensively accounting for variations
in object quantity and temporal semantics. Along with RMOT, we introduce a RMOT
benchmark named Refer-KITTI-V2, featuring scalable and diverse language
expressions. To efficiently generate high-quality annotations covering object
dynamics with minimal manual effort, we propose a semi-automatic labeling
pipeline that formulates a total of 9,758 language prompts. In addition, we
propose TempRMOT, an elegant end-to-end Transformer-based framework for RMOT.
At its core is a query-driven Temporal Enhancement Module that represents each
object as a Transformer query, enabling long-term spatial-temporal interactions
with other objects and past frames to efficiently refine these queries.
TempRMOT achieves state-of-the-art performance on both Refer-KITTI and
Refer-KITTI-V2, demonstrating the effectiveness of our approach. The source
code and dataset is available at https://github.com/zyn213/TempRMOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>引导多目标跟踪</div>
<div class="mono" style="margin-top:8px">引导理解是一个基本任务，通过定位在自由表达中描述的对象，桥接自然语言和视觉内容。然而，现有工作受到有限语言表达能力的限制，缺乏建模空间数量和时间状态中对象动态的能力。为了解决这些限制，我们引入了一种新的通用引导理解任务，称为引导多目标跟踪（RMOT）。其核心思想是使用语言表达作为语义线索，引导多目标跟踪的预测，全面考虑对象数量和时间语义的变化。与RMOT一起，我们引入了一个名为Refer-KITTI-V2的RMOT基准，具有可扩展和多样化的语言表达。为了高效生成覆盖对象动态的高质量注释，最小化人工努力，我们提出了一种半自动标注流程，制定了总共9,758个语言提示。此外，我们提出了TempRMOT，一个优雅的端到端基于Transformer的RMOT框架。其核心是一个查询驱动的时间增强模块，将每个对象表示为一个Transformer查询，使其能够与其他对象和过去帧进行长期的时空交互，以高效地优化这些查询。TempRMOT在Refer-KITTI和Refer-KITTI-V2上实现了最先进的性能，证明了我们方法的有效性。源代码和数据集可在https://github.com/zyn213/TempRMOT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance referring understanding, which connects natural language with visual content, particularly in localizing objects described by free-form expressions. The authors introduce a new task called referring multi-object tracking (RMOT) that utilizes language expressions as semantic cues to improve multi-object tracking by addressing the limitations of existing methods in modeling object dynamics. They develop a semi-automatic labeling pipeline to create a benchmark dataset, Refer-KITTI-V2, with 9,758 language prompts, and propose an end-to-end Transformer-based framework called TempRMOT, which incorporates a query-driven Temporal Enhancement Module for efficient long-term interactions. Experimental results show that TempRMOT achieves state-of-the-art performance on both Refer-KITTI and Refer-KITTI-V2, validating the effectiveness of their approach.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在建模物体动态和语言表达能力方面的局限性来增强指代理解。作者提出了一项新任务，称为指代多目标跟踪（RMOT），利用语言表达作为多目标跟踪预测的语义线索。他们还提出了一种半自动标注管道，生成了9,758个语言提示，用于新的基准Refer-KITTI-V2，并提出了一种名为TempRMOT的端到端Transformer框架。实验结果表明，TempRMOT在Refer-KITTI和Refer-KITTI-V2数据集上实现了最先进的性能，验证了他们方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Prompting is not Enough: Exploring Knowledge Integration and   Controllable Generation</div>
<div class="meta-line">Authors: Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen</div>
<div class="meta-line">First: 2025-05-26T08:18:33+00:00 · Latest: 2025-10-27T14:08:24+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19660v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.19660v3">PDF</a> · <a href="https://github.com/USTC-StarTeam/GenKI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model&#x27;s ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示不足：探索知识整合与可控生成</div>
<div class="mono" style="margin-top:8px">开放领域问答（OpenQA）是自然语言处理（NLP）的基石，主要集中在从非结构化文本数据中提取答案。随着大语言模型（LLMs）的快速发展，基于LLM的OpenQA方法相比传统方法受益于由大量参数驱动的新兴理解和回答能力。然而，这些方法面临两个关键挑战：如何有效地将知识整合到LLM中，以及如何根据不同任务情况自适应地生成特定答案格式的结果。为了解决这些挑战，我们提出了一个名为GenKI的新框架，旨在通过同时探索知识整合和可控生成来提高OpenQA性能。具体而言，我们首先训练一个密集段落检索模型，从给定的知识库中检索相关知识。随后，我们引入一个新颖的知识整合模型，在微调过程中将检索到的知识融入指令中，以增强模型。此外，为了在LLM中实现可控生成，我们利用某个微调的LLM和基于文本一致性的集成，确保所有连贯性、流畅性和答案格式的保证。最后，在TriviaQA、MSMARCO和CMRC2018数据集上进行的大量实验，展示了GenKI的有效性，并与最先进的基线进行了比较。此外，消融研究揭示了检索知识的频率与模型准确回忆知识的能力之间的线性关系。我们的GenKI代码可在https://github.com/USTC-StarTeam/GenKI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance open-domain question answering (OpenQA) by addressing the challenges of knowledge integration and controllable generation in large language models (LLMs). The authors propose a novel framework called GenKI, which involves training a dense passage retrieval model to extract relevant knowledge from a knowledge base, followed by a knowledge integration model that incorporates this knowledge into the fine-tuning process. Experimental results on datasets such as TriviaQA, MSMARCO, and CMRC2018 show that GenKI outperforms state-of-the-art baselines, and ablation studies reveal a linear relationship between the frequency of retrieved knowledge and the model&#x27;s accuracy in recalling information.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决大语言模型（LLMs）中有效知识整合和可控生成的挑战来增强开放域问答（OpenQA）。作者提出了一种名为GenKI的新框架，该框架包括训练一个密集段落检索模型，从知识库中提取相关知识，然后通过知识整合模型将这些知识纳入微调过程。对TriviaQA、MSMARCO和CMRC2018数据集的实验结果表明，GenKI的表现优于最先进的基线，消融研究揭示了检索知识的频率与模型准确回忆信息之间的线性关系。</div>
</details>
</div>
<div class="card">
<div class="title">Autoregressive Styled Text Image Generation, but Make it Reliable</div>
<div class="meta-line">Authors: Carmine Zaccagnino, Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara</div>
<div class="meta-line">First: 2025-10-27T11:54:23+00:00 · Latest: 2025-10-27T11:54:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23240v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating faithful and readable styled text images (especially for Styled
Handwritten Text generation - HTG) is an open problem with several possible
applications across graphic design, document understanding, and image editing.
A lot of research effort in this task is dedicated to developing strategies
that reproduce the stylistic characteristics of a given writer, with promising
results in terms of style fidelity and generalization achieved by the recently
proposed Autoregressive Transformer paradigm for HTG. However, this method
requires additional inputs, lacks a proper stop mechanism, and might end up in
repetition loops, generating visual artifacts. In this work, we rethink the
autoregressive formulation by framing HTG as a multimodal prompt-conditioned
generation task, and tackle the content controllability issues by introducing
special textual input tokens for better alignment with the visual ones.
Moreover, we devise a Classifier-Free-Guidance-based strategy for our
autoregressive model. Through extensive experimental validation, we demonstrate
that our approach, dubbed Eruku, compared to previous solutions requires fewer
inputs, generalizes better to unseen styles, and follows more faithfully the
textual prompt, improving content adherence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归风格文本图像生成，但要可靠</div>
<div class="mono" style="margin-top:8px">生成忠实且可读的风格文本图像（特别是风格手写文本生成 - HTG）是一个开放问题，具有多个可能的应用，包括图形设计、文档理解和图像编辑。该任务的许多研究工作致力于开发再现特定作者风格特征的策略，最近提出的自回归变换器范式在风格保真度和泛化方面取得了令人鼓舞的结果。然而，该方法需要额外的输入，缺乏适当的停止机制，可能导致重复循环，生成视觉伪影。在本研究中，我们通过将HTG框架重新思考为多模态提示条件生成任务，重新审视自回归公式，并通过引入特殊文本输入标记来解决内容可控性问题，以更好地与视觉输入对齐。此外，我们为我们的自回归模型设计了一种无分类器引导策略。通过广泛的实验验证，我们证明了我们的方法Eruku与之前的解决方案相比，所需输入更少，对未见风格的泛化更好，并更忠实地遵循文本提示，提高了内容一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of styled text images, particularly handwritten text, which has applications in graphic design and document understanding. The authors propose a novel approach by rethinking the autoregressive formulation and framing handwritten text generation as a multimodal prompt-conditioned task, addressing issues of content controllability through the introduction of special textual input tokens. Experimental results show that their method, named Eruku, requires fewer inputs, generalizes better to unseen styles, and adheres more closely to textual prompts compared to previous solutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成真实且可读的风格化文本图像，特别是风格化手写文本生成中的挑战，该领域在图形设计和文档理解等多个应用中具有重要意义。作者提出了一种新方法，通过重新思考自回归公式，将任务框架设定为多模态提示条件生成，并引入特殊的文本输入标记以增强与视觉元素的对齐。实验结果表明，他们的方法Eruku相比于之前的解决方案，所需输入更少，对未见风格的泛化能力更强，并且更忠实于文本提示，从而改善了内容可控性并减少了视觉伪影。</div>
</details>
</div>
<div class="card">
<div class="title">A Guardrail for Safety Preservation: When Safety-Sensitive Subspace   Meets Harmful-Resistant Null-Space</div>
<div class="meta-line">Authors: Bingjie Zhang, Yibo Yang, Zhe Ren, Dandan Guo, Jindong Gu, Philip Torr, Bernard Ghanem</div>
<div class="meta-line">First: 2025-10-16T04:57:53+00:00 · Latest: 2025-10-27T11:11:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14301v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.14301v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全保护的护栏：当安全敏感子空间遇到抗有害空域</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在多种任务中取得了显著成功，但在适应过程中其安全对齐仍然脆弱。即使在良性数据上进行微调或使用低秩适配，预训练的安全行为也容易退化，导致微调模型产生有害响应。为了解决这一挑战，我们提出了GuardSpace，一个用于在微调过程中保持安全对齐的护栏框架，包含两个关键组件：安全敏感子空间和抗有害空域。首先，我们使用协方差预处理的奇异值分解明确地将预训练权重分解为与安全相关和与安全无关的组件，并从与安全无关的组件初始化低秩适配器，同时冻结与安全相关的组件以保持其相关的安全机制。其次，我们构建了一个空域投影器，限制适配器更新不改变对有害提示的安全输出，从而保持原有的拒绝行为。在多个下游任务上对各种预训练模型的实验表明，GuardSpace在性能上优于现有方法。值得注意的是，对于在GSM8K上微调的Llama-2-7B-Chat，GuardSpace的表现超过了最先进的方法AsFT，将平均有害评分从14.4%降低到3.6%，同时将准确率从26.0%提高到28.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the safety alignment of large language models (LLMs) during fine-tuning, as existing methods often lead to degraded safety behaviors and harmful responses. The authors propose a framework called GuardSpace, which consists of a safety-sensitive subspace and a harmful-resistant null space, utilizing covariance-preconditioned singular value decomposition to separate pre-trained weights into safety-relevant and safety-irrelevant components. Experimental results show that GuardSpace significantly improves safety preservation, as demonstrated by fine-tuning Llama-2-7B-Chat on GSM8K, where it reduced the average harmful score from 14.4% to 3.6% and increased accuracy from 26.0% to 28.0% compared to the state-of-the-art method AsFT.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型语言模型（LLMs）在适应过程中的安全对齐，因为现有方法往往导致安全行为下降和有害响应。作者提出了GuardSpace框架，该框架由安全敏感子空间和抗有害空子空间组成，采用协方差预处理奇异值分解对预训练权重进行分解，并在保留安全相关组件的同时初始化低秩适配器。实验结果表明，GuardSpace显著优于现有方法，以Llama-2-7B-Chat在GSM8K上的微调为例，其平均有害评分从14.4%降低至3.6%，准确率从26.0%提高至28.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Vision-LLMs in Surveillance Video</div>
<div class="meta-line">Authors: Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-27T10:27:02+00:00 · Latest: 2025-10-27T10:27:02+00:00</div>
<div class="meta-line">Comments: Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,
  Language, and Embodied AI</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23190v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23190v1">PDF</a> · <a href="https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread use of cameras in our society has created an overwhelming
amount of video data, far exceeding the capacity for human monitoring. This
presents a critical challenge for public safety and security, as the timely
detection of anomalous or criminal events is crucial for effective response and
prevention. The ability for an embodied agent to recognize unexpected events is
fundamentally tied to its capacity for spatial reasoning. This paper
investigates the spatial reasoning of vision-language models (VLMs) by framing
anomalous action recognition as a zero-shot, language-grounded task, addressing
the embodied perception challenge of interpreting dynamic 3D scenes from sparse
2D video. Specifically, we investigate whether small, pre-trained vision--LLMs
can act as spatially-grounded, zero-shot anomaly detectors by converting video
into text descriptions and scoring labels via textual entailment. We evaluate
four open models on UCF-Crime and RWF-2000 under prompting and
privacy-preserving conditions. Few-shot exemplars can improve accuracy for some
models, but may increase false positives, and privacy filters -- especially
full-body GAN transforms -- introduce inconsistencies that degrade accuracy.
These results chart where current vision--LLMs succeed (simple, spatially
salient events) and where they falter (noisy spatial cues, identity
obfuscation). Looking forward, we outline concrete paths to strengthen spatial
grounding without task-specific training: structure-aware prompts, lightweight
spatial memory across clips, scene-graph or 3D-pose priors during description,
and privacy methods that preserve action-relevant geometry. This positions
zero-shot, language-grounded pipelines as adaptable building blocks for
embodied, real-world video understanding. Our implementation for evaluating
VLMs is publicly available at:
https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>监控视频中视觉-LLMs的评估</div>
<div class="mono" style="margin-top:8px">我们社会中摄像头的广泛使用产生了大量视频数据，远超人类监控的能力。这对公共安全和安保提出了重大挑战，因为及时检测异常或犯罪事件对于有效响应和预防至关重要。具身代理识别意外事件的能力与其空间推理能力密切相关。本文通过将异常行为识别框定为零-shot、语言基础任务，研究视觉-语言模型（VLMs）的空间推理，解决从稀疏2D视频中解释动态3D场景的具身感知挑战。具体而言，我们研究小型预训练视觉-LLMs是否可以通过将视频转换为文本描述并通过文本蕴涵评分标签，作为空间基础的零-shot异常检测器。我们在UCF-Crime和RWF-2000上评估四个开放模型，采用提示和隐私保护条件。少量示例可以提高某些模型的准确性，但可能增加假阳性，而隐私过滤器——尤其是全身GAN变换——引入的不一致性会降低准确性。这些结果描绘了当前视觉-LLMs成功的地方（简单、空间显著的事件）和失败的地方（嘈杂的空间线索、身份模糊）。展望未来，我们概述了在没有特定任务训练的情况下加强空间基础的具体路径：结构感知提示、跨片段的轻量级空间记忆、描述期间的场景图或3D姿态先验，以及保留与动作相关几何的隐私方法。这使得零-shot、语言基础的管道成为具身、现实世界视频理解的可适应构建块。我们用于评估VLMs的实现已公开可用：
https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of surveillance cameras has led to an overwhelming volume of video data, posing challenges for timely detection of anomalous events critical for public safety. This study explores the spatial reasoning capabilities of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, where video is converted into text descriptions for scoring labels through textual entailment. The evaluation of four open models on UCF-Crime and RWF-2000 reveals that while few-shot exemplars can enhance accuracy for some models, they may also raise false positives, and privacy filters can introduce inconsistencies that negatively impact accuracy, highlighting the strengths and weaknesses of current VLMs in recognizing spatially salient events versus noisy cues.</div>
<div class="mono" style="margin-top:8px">监控摄像头的普及导致了监控大量视频数据以确保公共安全的重大挑战。本研究通过将异常行为识别视为零-shot、基于语言的任务，探讨了视觉语言模型（VLMs）的空间推理能力，这涉及从有限的二维视频中解释动态三维场景。对四个开放模型在UCF-Crime和RWF-2000上的评估显示，虽然少量示例可以提高某些模型的准确性，但也可能导致更多的假阳性，而隐私保护方法可能引入不一致性，从而降低准确性。研究结果表明，当前的VLMs在简单的、空间显著的事件上表现良好，但在处理嘈杂的空间线索和身份模糊时存在困难，这为在不进行特定任务训练的情况下改善空间基础提供了未来方向。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications
from healthcare to financial advice, safety evaluation struggles to keep pace.
Current benchmarks focus on single-turn interactions with generic policies,
failing to capture the conversational dynamics of real-world usage and the
application-specific harms that emerge in context. Such potential oversights
can lead to harms that go unnoticed in standard safety benchmarks and other
current evaluation methodologies. To address these needs for robust AI safety
evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated
modular framework designed for customized and dynamic harm evaluations. SAGE
employs prompted adversarial agents with diverse personalities based on the Big
Five model, enabling system-aware multi-turn conversations that adapt to target
applications and harm policies. We evaluate seven state-of-the-art LLMs across
three applications and harm policies. Multi-turn experiments show that harm
increases with conversation length, model behavior varies significantly when
exposed to different user personalities and scenarios, and some models minimize
harm via high refusal rates that reduce usefulness. We also demonstrate policy
sensitivity within a harm category where tightening a child-focused sexual
policy substantially increases measured defects across applications. These
results motivate adaptive, policy-aware, and context-specific testing for safer
real-world deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid deployment of Large Language Models (LLMs) across various applications has highlighted the inadequacies of current safety evaluation benchmarks, which often overlook the complexities of real-world interactions and context-specific harms. To address this gap, the authors developed SAGE (Safety AI Generic Evaluation), an automated modular framework that utilizes prompted adversarial agents based on the Big Five personality model to facilitate dynamic multi-turn conversations tailored to specific applications and harm policies. Experimental results from evaluating seven state-of-the-art LLMs reveal that harm tends to increase with longer conversations, model responses vary significantly with different user personalities, and some models exhibit high refusal rates that may reduce their overall usefulness, emphasizing the need for adaptive and context-aware safety testing in real-world applications.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中的快速部署引发了对其安全评估的关注，而现有评估往往未能考虑现实世界的对话动态和特定应用的危害。为了解决这些问题，作者提出了SAGE（安全人工智能通用评估），这是一个自动化模块化框架，利用基于五大人格特质的对抗代理进行系统感知的多轮对话，针对特定应用和危害政策进行定制。实验结果表明，随着对话长度的增加，危害也会增加，模型行为在不同用户个性下显著变化，某些模型通过高拒绝率来减少危害，但这降低了其有效性，强调了在现实场景中对LLMs进行适应性和上下文特定的安全测试的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Identity-Preserving Text-to-Video Generation Guided by Simple yet   Effective Spatial-Temporal Decoupled Representations</div>
<div class="meta-line">Authors: Yuji Wang, Moran Li, Xiaobin Hu, Ran Yi, Jiangning Zhang, Han Feng, Weijian Cao, Yabiao Wang, Chengjie Wang, Lizhuang Ma</div>
<div class="meta-line">First: 2025-07-07T06:54:44+00:00 · Latest: 2025-10-27T09:09:50+00:00</div>
<div class="meta-line">Comments: ACM Multimedia 2025; code URL: https://github.com/rain152/IPVG</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04705v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.04705v3">PDF</a> · <a href="https://github.com/rain152/IPVG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identity-preserving text-to-video (IPT2V) generation, which aims to create
high-fidelity videos with consistent human identity, has become crucial for
downstream applications. However, current end-to-end frameworks suffer a
critical spatial-temporal trade-off: optimizing for spatially coherent layouts
of key elements (e.g., character identity preservation) often compromises
instruction-compliant temporal smoothness, while prioritizing dynamic realism
risks disrupting the spatial coherence of visual structures. To tackle this
issue, we propose a simple yet effective spatial-temporal decoupled framework
that decomposes representations into spatial features for layouts and temporal
features for motion dynamics. Specifically, our paper proposes a semantic
prompt optimization mechanism and stage-wise decoupled generation paradigm. The
former module decouples the prompt into spatial and temporal components.
Aligned with the subsequent stage-wise decoupled approach, the spatial prompts
guide the text-to-image (T2I) stage to generate coherent spatial features,
while the temporal prompts direct the sequential image-to-video (I2V) stage to
ensure motion consistency. Experimental results validate that our approach
achieves excellent spatiotemporal consistency, demonstrating outstanding
performance in identity preservation, text relevance, and video quality. By
leveraging this simple yet robust mechanism, our algorithm secures the
runner-up position in 2025 ACM MultiMedia Challenge. Our code is available at
https://github.com/rain152/IPVG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于简单有效的时空解耦表示的身份保留文本到视频生成</div>
<div class="mono" style="margin-top:8px">身份保留文本到视频（IPT2V）生成旨在创建具有一致人类身份的高保真视频，已成为下游应用的关键。然而，当前的端到端框架面临着关键的时空权衡：优化关键元素（例如角色身份保留）的空间一致布局往往会妥协指令合规的时间平滑性，而优先考虑动态真实感则可能破坏视觉结构的空间一致性。为了解决这个问题，我们提出了一种简单而有效的时空解耦框架，将表示分解为用于布局的空间特征和用于运动动态的时间特征。具体而言，我们的论文提出了一种语义提示优化机制和阶段性解耦生成范式。前者将提示解耦为空间和时间组件。与后续的阶段性解耦方法相一致，空间提示引导文本到图像（T2I）阶段生成一致的空间特征，而时间提示则指导顺序图像到视频（I2V）阶段以确保运动一致性。实验结果验证了我们的方法在时空一致性方面表现出色，在身份保留、文本相关性和视频质量方面表现优异。通过利用这一简单而强大的机制，我们的算法在2025年ACM多媒体挑战赛中获得亚军。我们的代码可在https://github.com/rain152/IPVG获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in identity-preserving text-to-video (IPT2V) generation, particularly the spatial-temporal trade-off that affects video quality and coherence. The authors propose a spatial-temporal decoupled framework that separates spatial features for layout from temporal features for motion dynamics, utilizing a semantic prompt optimization mechanism and a stage-wise decoupled generation paradigm. Experimental results demonstrate that this approach achieves significant improvements in spatiotemporal consistency, identity preservation, text relevance, and overall video quality, earning the runner-up position in the 2025 ACM MultiMedia Challenge.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善身份保留的文本到视频（IPT2V）生成，这对于需要高保真度且人类身份一致的视频应用至关重要。作者提出了一种空间-时间解耦框架，将空间特征用于布局一致性与时间特征用于运动动态分开，利用语义提示优化机制和阶段性解耦生成范式。实验结果表明，该方法在时空一致性方面表现优越，在身份保留、文本相关性和视频质量方面均表现出色，最终在2025年ACM多媒体挑战赛中获得亚军。</div>
</details>
</div>
<div class="card">
<div class="title">First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM   Reasoning via Unsupervised Post-Training</div>
<div class="meta-line">Authors: Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-28T15:11:16+00:00 · Latest: 2025-10-27T09:06:32+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.22453v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.22453v2">PDF</a> · <a href="https://github.com/waltonfuture/MM-UPT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL), which require expensive and manually annotated multi-modal
data--an ultimately unsustainable resource. This limitation has motivated a
growing interest in unsupervised paradigms as a third stage of post-training
after SFT and RL. While recent efforts have explored this direction, their
methods are complex and difficult to iterate. To address this, we propose
MM-UPT, a simple yet effective framework for unsupervised post-training of
MLLMs, enabling continual self-improvement without any external supervision.
The training method of MM-UPT builds upon GRPO, replacing traditional reward
signals with a self-rewarding mechanism based on majority voting over multiple
sampled responses. Our experiments demonstrate that such training method
effectively improves the reasoning ability of Qwen2.5-VL-7B (e.g.,
66.3\%$\rightarrow$72.9\% on MathVista, 62.9\%$\rightarrow$68.7\% on We-Math),
using standard dataset without ground truth labels. To further explore
scalability, we extend our framework to a data self-generation setting,
designing two strategies that prompt the MLLM to synthesize new training
samples on its own. Additional experiments show that combining these synthetic
data with the unsupervised training method can also boost performance,
highlighting a promising approach for scalable self-improvement. Overall,
MM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a
critical third step after initial SFT and RL in the absence of external
supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>首次SFT，第二次RL，第三次UPT：通过无监督后训练持续改进多模态LLM推理</div>
<div class="mono" style="margin-top:8px">在后训练阶段，提高多模态大型语言模型（MLLMs）通常依赖于监督微调（SFT）或强化学习（RL），这需要昂贵且手动标注的多模态数据——这是一种最终不可持续的资源。这一限制促使人们对无监督范式作为SFT和RL之后的第三阶段产生越来越大的兴趣。尽管最近的努力探索了这个方向，但它们的方法复杂且难以迭代。为了解决这个问题，我们提出了MM-UPT，一个简单而有效的无监督后训练框架，能够在没有任何外部监督的情况下实现持续自我改进。MM-UPT的训练方法基于GRPO，使用基于多个采样响应的多数投票的自我奖励机制替代传统的奖励信号。我们的实验表明，这种训练方法有效提高了Qwen2.5-VL-7B的推理能力（例如，MathVista上从66.3\%提升到72.9\%，We-Math上从62.9\%提升到68.7\%），使用标准数据集而没有真实标签。为了进一步探索可扩展性，我们将框架扩展到数据自生成设置，设计了两种策略，促使MLLM自行合成新的训练样本。额外的实验表明，将这些合成数据与无监督训练方法结合也可以提升性能，突显出一种可扩展自我改进的有前景的方法。总体而言，MM-UPT为MLLM的自主增强提供了一种新范式，作为在缺乏外部监督的情况下，初始SFT和RL之后的关键第三步。我们的代码可在https://github.com/waltonfuture/MM-UPT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of supervised fine-tuning and reinforcement learning in improving Multi-modal Large Language Models (MLLMs), which require costly and manually annotated data. The authors propose a framework called MM-UPT for unsupervised post-training that allows for continual self-improvement without external supervision, utilizing a self-rewarding mechanism based on majority voting over multiple sampled responses. Experimental results demonstrate that this method significantly enhances the reasoning capabilities of the Qwen2.5-VL-7B model, with notable performance improvements on datasets like MathVista and We-Math, and further experiments indicate that combining synthetic data generated by the model with the unsupervised training can further boost performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在后期训练阶段提高多模态大型语言模型（MLLM）时，监督微调和强化学习的局限性，这些方法依赖于昂贵的标注数据。作者提出了一种名为MM-UPT的无监督后期训练框架，允许在没有外部监督的情况下进行持续自我改进，利用基于多个采样响应的多数投票的自我奖励机制。实验结果表明，该方法显著提高了Qwen2.5-VL-7B模型的推理能力，在MathVista和We-Math等数据集上取得了显著改善，进一步实验表明，将模型生成的合成数据与无监督训练方法结合可以进一步提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning</div>
<div class="meta-line">Authors: Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-22T17:59:40+00:00 · Latest: 2025-10-27T08:58:16+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Camera Ready. Project Page:
  https://polyu-chenlab.github.io/unipixel/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.18094v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.18094v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://polyu-chenlab.github.io/unipixel/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniPixel：用于像素级视觉推理的统一对象引用与分割</div>
<div class="mono" style="margin-top:8px">最近，大型多模态模型（LMMs）的进展展示了它们作为通用多模态助手的显著成功，特别关注整体图像和视频语言理解。相反，较少关注于扩展细粒度像素级理解能力，模型需要实现视觉信号与语言语义之间的像素级对齐。一些先前的研究已将LMMs应用于相关任务，如区域级字幕和引用表达分割。然而，这些模型仅限于独立执行引用或分割任务，未能将这些细粒度感知能力整合到视觉推理中。为了解决这一问题，我们提出了UniPixel，一个能够灵活理解视觉提示输入并生成基于掩码的响应的大型多模态模型。我们的模型通过无缝集成像素级感知与一般视觉理解能力而脱颖而出。具体而言，UniPixel处理视觉提示并按需生成相关掩码，并在推理过程中基于这些中间指针执行后续推理，从而实现细粒度的像素级推理。我们的方法在10个基准测试中得到了验证，涵盖了包括像素级引用/分割和图像/视频中的对象中心理解在内的多样化任务。还设计了一项新颖的PixelQA任务，联合要求引用、分割和问答，以验证我们方法的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance fine-grained pixel-level understanding in large multi-modal models (LMMs), which have primarily focused on holistic image and video language understanding. The authors propose UniPixel, a model that integrates pixel-level perception with general visual understanding, allowing it to process visual prompts and generate mask-grounded responses while performing reasoning based on these inputs. Experimental results demonstrate the effectiveness of UniPixel across 10 benchmarks, showcasing its capabilities in pixel-level referring, segmentation, and a novel PixelQA task that combines referring, segmentation, and question answering.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于提高大规模多模态模型在细粒度像素级理解方面的能力，而这些模型主要集中于整体图像和视频语言理解。作者提出了UniPixel模型，该模型将像素级感知与一般视觉理解相结合，使其能够处理视觉提示并生成基于掩码的响应，同时基于这些输出进行推理。实验结果表明，该模型在10个基准测试中表现出色，展示了其在像素级引用、分割以及一个结合引用、分割和问答的新PixelQA任务中的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Residual Diffusion Bridge Model for Image Restoration</div>
<div class="meta-line">Authors: Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</div>
<div class="meta-line">First: 2025-10-27T08:35:49+00:00 · Latest: 2025-10-27T08:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23116v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23116v1">PDF</a> · <a href="https://github.com/MiliLab/RDBM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion bridge models establish probabilistic paths between arbitrary
paired distributions and exhibit great potential for universal image
restoration. Most existing methods merely treat them as simple variants of
stochastic interpolants, lacking a unified analytical perspective. Besides,
they indiscriminately reconstruct images through global noise injection and
removal, inevitably distorting undegraded regions due to imperfect
reconstruction. To address these challenges, we propose the Residual Diffusion
Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic
differential equations of generalized diffusion bridge and derive the
analytical formulas of its forward and reverse processes. Crucially, we
leverage the residuals from given distributions to modulate the noise injection
and removal, enabling adaptive restoration of degraded regions while preserving
intact others. Moreover, we unravel the fundamental mathematical essence of
existing bridge models, all of which are special cases of RDBM and empirically
demonstrate the optimality of our proposed models. Extensive experiments are
conducted to demonstrate the state-of-the-art performance of our method both
qualitatively and quantitatively across diverse image restoration tasks. Code
is publicly available at https://github.com/MiliLab/RDBM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于图像恢复的残差扩散桥模型</div>
<div class="mono" style="margin-top:8px">扩散桥模型在任意配对分布之间建立概率路径，展现出在通用图像恢复中的巨大潜力。现有大多数方法仅将其视为随机插值的简单变体，缺乏统一的分析视角。此外，它们通过全局噪声注入和去除不加区分地重建图像，因不完美的重建不可避免地扭曲未退化区域。为了解决这些挑战，我们提出了残差扩散桥模型（RDBM）。具体而言，我们从理论上重新表述广义扩散桥的随机微分方程，并推导出其正向和反向过程的解析公式。关键是，我们利用给定分布的残差来调节噪声的注入和去除，实现对退化区域的自适应恢复，同时保留完整的其他区域。此外，我们揭示了现有桥模型的基本数学本质，所有这些模型都是RDBM的特例，并实证证明了我们提出模型的最优性。我们进行了广泛的实验，以定性和定量的方式展示我们方法在多种图像恢复任务中的最先进性能。代码可在 https://github.com/MiliLab/RDBM 上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image restoration techniques that currently suffer from distortion in undegraded regions due to global noise injection and removal. The authors propose the Residual Diffusion Bridge Model (RDBM), which reformulates stochastic differential equations of generalized diffusion bridges and derives analytical formulas for both forward and reverse processes. Experimental results show that RDBM effectively adapts noise modulation based on residuals from given distributions, leading to superior performance in restoring degraded regions while preserving intact areas, thus demonstrating its state-of-the-art capabilities across various image restoration tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善图像恢复技术，这些技术常因噪声处理不足而扭曲未退化区域。作者提出了残差扩散桥模型（RDBM），该模型重新公式化了广义扩散桥的随机微分方程，并推导出其过程的解析公式。实验结果表明，RDBM能够根据给定分布的残差有效地调整噪声注入和去除，从而在各种任务中实现优于现有方法的恢复性能。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Video Generation with Human Feedback</div>
<div class="meta-line">Authors: Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, Wanli Ouyang</div>
<div class="meta-line">First: 2025-01-23T18:55:41+00:00 · Latest: 2025-10-27T08:22:57+00:00</div>
<div class="meta-line">Comments: https://github.com/KwaiVGI/VideoAlign</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.13918v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.13918v2">PDF</a> · <a href="https://github.com/KwaiVGI/VideoAlign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation has achieved significant advances through rectified flow
techniques, but issues like unsmooth motion and misalignment between videos and
prompts persist. In this work, we develop a systematic pipeline that harnesses
human feedback to mitigate these problems and refine the video generation
model. Specifically, we begin by constructing a large-scale human preference
dataset focused on modern video generation models, incorporating pairwise
annotations across multi-dimensions. We then introduce VideoReward, a
multi-dimensional video reward model, and examine how annotations and various
design choices impact its rewarding efficacy. From a unified reinforcement
learning perspective aimed at maximizing reward with KL regularization, we
introduce three alignment algorithms for flow-based models. These include two
training-time strategies: direct preference optimization for flow (Flow-DPO)
and reward weighted regression for flow (Flow-RWR), and an inference-time
technique, Flow-NRG, which applies reward guidance directly to noisy videos.
Experimental results indicate that VideoReward significantly outperforms
existing reward models, and Flow-DPO demonstrates superior performance compared
to both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG
lets users assign custom weights to multiple objectives during inference,
meeting personalized video quality needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过人类反馈改善视频生成</div>
<div class="mono" style="margin-top:8px">视频生成通过修正流技术取得了显著进展，但仍存在运动不平滑和视频与提示之间的错位等问题。在这项工作中，我们开发了一个系统化的流程，利用人类反馈来缓解这些问题并优化视频生成模型。具体而言，我们首先构建了一个大型人类偏好数据集，专注于现代视频生成模型，包含多维度的成对注释。然后，我们引入了VideoReward，一个多维视频奖励模型，并研究了注释和各种设计选择如何影响其奖励效果。从旨在最大化奖励的统一强化学习视角出发，结合KL正则化，我们为基于流的模型引入了三种对齐算法。这包括两种训练时策略：流的直接偏好优化（Flow-DPO）和流的奖励加权回归（Flow-RWR），以及一种推理时技术Flow-NRG，直接将奖励指导应用于噪声视频。实验结果表明，VideoReward显著优于现有奖励模型，而Flow-DPO的表现优于Flow-RWR和监督微调方法。此外，Flow-NRG允许用户在推理过程中为多个目标分配自定义权重，以满足个性化的视频质量需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the persistent issues of unsmooth motion and misalignment in video generation despite advancements in rectified flow techniques. The authors developed a systematic pipeline that utilizes human feedback to enhance video generation models, starting with the creation of a large-scale human preference dataset that includes pairwise annotations. They introduced VideoReward, a multi-dimensional reward model, and explored the impact of various design choices on its effectiveness. The study presents three alignment algorithms for flow-based models, with experimental results showing that VideoReward significantly outperforms existing models, and Flow-DPO achieves better performance than Flow-RWR and supervised fine-tuning methods, while Flow-NRG allows for personalized adjustments during inference.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决尽管在修正流技术上取得了进展，但视频生成中仍然存在不平滑运动和视频与提示之间不对齐的问题。作者开发了一个系统化的流程，利用人类反馈来增强视频生成模型，首先创建了一个大规模的人类偏好数据集，其中包含成对注释。他们引入了VideoReward，一个多维奖励模型，并通过各种设计选择和对齐算法（包括用于训练的Flow-DPO和Flow-RWR，以及用于推理的Flow-NRG）探讨了其有效性。实验结果表明，VideoReward显著优于现有模型，Flow-DPO的表现优于Flow-RWR和监督微调，而Flow-NRG则允许在视频生成过程中进行个性化调整。</div>
</details>
</div>
<div class="card">
<div class="title">T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting</div>
<div class="meta-line">Authors: Yifei Qian, Zhongliang Guo, Bowen Deng, Chun Tong Lei, Shuai Zhao, Chun Pong Lau, Xiaopeng Hong, Michael P. Pound</div>
<div class="meta-line">First: 2025-02-28T01:09:18+00:00 · Latest: 2025-10-27T07:31:06+00:00</div>
<div class="meta-line">Comments: Accepted by CVPR2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.20625v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.20625v3">PDF</a> · <a href="https://github.com/cha15yq/T2ICount">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot object counting aims to count instances of arbitrary object
categories specified by text descriptions. Existing methods typically rely on
vision-language models like CLIP, but often exhibit limited sensitivity to text
prompts. We present T2ICount, a diffusion-based framework that leverages rich
prior knowledge and fine-grained visual understanding from pretrained diffusion
models. While one-step denoising ensures efficiency, it leads to weakened text
sensitivity. To address this challenge, we propose a Hierarchical Semantic
Correction Module that progressively refines text-image feature alignment, and
a Representational Regional Coherence Loss that provides reliable supervision
signals by leveraging the cross-attention maps extracted from the denosing
U-Net. Furthermore, we observe that current benchmarks mainly focus on majority
objects in images, potentially masking models&#x27; text sensitivity. To address
this, we contribute a challenging re-annotated subset of FSC147 for better
evaluation of text-guided counting ability. Extensive experiments demonstrate
that our method achieves superior performance across different benchmarks. Code
is available at https://github.com/cha15yq/T2ICount.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T2ICount：增强零样本计数的跨模态理解</div>
<div class="mono" style="margin-top:8px">零样本物体计数旨在计数由文本描述指定的任意物体类别的实例。现有方法通常依赖于视觉语言模型，如CLIP，但往往对文本提示的敏感性有限。我们提出了T2ICount，一个基于扩散的框架，利用预训练扩散模型的丰富先验知识和细粒度视觉理解。虽然一步去噪确保了效率，但导致文本敏感性减弱。为了解决这个挑战，我们提出了一个分层语义校正模块，逐步细化文本-图像特征对齐，以及一个表征区域一致性损失，通过利用从去噪U-Net提取的交叉注意力图提供可靠的监督信号。此外，我们观察到当前基准主要关注图像中的主要物体，可能掩盖模型的文本敏感性。为了解决这个问题，我们贡献了一个具有挑战性的重新标注的FSC147子集，以更好地评估文本引导的计数能力。大量实验表明，我们的方法在不同基准上实现了卓越的性能。代码可在https://github.com/cha15yq/T2ICount获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve zero-shot object counting, which struggles with sensitivity to text prompts in existing vision-language models. The authors introduce T2ICount, a diffusion-based framework that enhances visual understanding by utilizing pretrained diffusion models and incorporates a Hierarchical Semantic Correction Module for better text-image feature alignment. Experimental results show that T2ICount outperforms existing methods across various benchmarks, highlighting its effectiveness in addressing the limitations of current approaches in text-guided counting tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善零-shot物体计数，现有的视觉语言模型在文本提示的敏感性方面存在不足。作者提出了T2ICount，这是一种基于扩散的框架，利用预训练的扩散模型来增强视觉理解，并结合分层语义校正模块来精细化文本-图像特征对齐。实验结果表明，T2ICount在多个基准测试中优于现有方法，特别是在通过提供重新标注的FSC147子集来更好地评估文本引导计数能力后，解决了当前评估数据集的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Reconstruction Alignment Improves Unified Multimodal Models</div>
<div class="meta-line">Authors: Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang</div>
<div class="meta-line">First: 2025-09-08T23:59:32+00:00 · Latest: 2025-10-27T07:29:43+00:00</div>
<div class="meta-line">Comments: 34 pages, 28 figures and 11 tables; Update ablation study</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.07295v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.07295v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense &quot;text prompts,&quot; providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重建对齐改善统一多模态模型</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在单一架构中统一视觉理解和生成。然而，传统训练依赖于图像-文本对（或序列），其标题通常稀疏，缺乏细粒度的视觉细节——即使它们使用数百个词来描述简单图像。我们引入了重建对齐（RecA），这是一种资源高效的后训练方法，利用视觉理解编码器嵌入作为密集的“文本提示”，在没有标题的情况下提供丰富的监督。具体而言，RecA将UMM条件化于其自身的视觉理解嵌入，并优化其重建输入图像，使用自监督重建损失，从而重新对齐理解和生成。尽管其简单性，RecA具有广泛的适用性：在自回归、掩蔽自回归和基于扩散的UMMs中，它始终提高生成和编辑的保真度。仅需27个GPU小时，使用RecA的后训练显著提高了GenEval（0.73$\rightarrow$0.90）和DPGBench（80.93$\rightarrow$88.15）上的图像生成性能，同时也提升了编辑基准（ImgEdit 3.38$\rightarrow$3.75，GEdit 6.94$\rightarrow$7.25）。值得注意的是，RecA超越了更大规模的开源模型，并广泛适用于多种UMM架构，确立了其作为UMMs高效且通用的后训练对齐策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of unified multimodal models (UMMs), which often suffer from sparse and inadequate image-text pair training data. The authors propose a novel post-training method called Reconstruction Alignment (RecA), which utilizes visual understanding encoder embeddings as dense supervision instead of relying on captions. Experimental results demonstrate that RecA significantly improves image generation and editing fidelity across various UMM architectures, achieving notable performance increases on benchmarks such as GenEval and DPGBench with minimal computational resources, thus establishing RecA as an effective alignment strategy for UMMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型（UMMs）的性能，这些模型整合了视觉理解和生成，但通常依赖于缺乏详细视觉信息的稀疏图像-文本对。作者提出了一种新的后训练方法，称为重建对齐（RecA），该方法利用视觉理解编码器嵌入作为密集的“文本提示”，在不需要标题的情况下提供丰富的监督。实验结果表明，RecA显著提高了各种UMM架构的图像生成性能，在GenEval和DPGBench等指标上取得了显著提升，同时增强了编辑能力，从而确立了RecA作为UMMs有效且资源高效的对齐策略。</div>
</details>
</div>
<div class="card">
<div class="title">Think before Recommendation: Autonomous Reasoning-enhanced Recommender</div>
<div class="meta-line">Authors: Xiaoyu Kong, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Jiancan Wu, Xiang Wang</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-27T07:26:32+00:00 · Latest: 2025-10-27T07:26:32+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 poster</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23077v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23077v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model&#x27;s
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) &quot;Think-before-Recommendation&quot; prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推荐前思考：增强自主推理的推荐系统</div>
<div class="mono" style="margin-top:8px">推荐系统的核心任务是从历史用户-项目交互中学习用户偏好。随着大语言模型（LLMs）的快速发展，最近的研究探索了利用LLMs的推理能力来增强评分预测任务。然而，现有的基于蒸馏的方法存在教师模型推荐能力不足、监督成本高且静态、推理能力转移表面化等局限性。为了解决这些问题，本文提出了RecZero，一种基于强化学习（RL）的推荐范式，摒弃了传统的多模型和多阶段蒸馏方法。相反，RecZero通过纯RL训练单个LLM，自主发展评分预测的推理能力。RecZero由两个关键组件组成：（1）“推荐前思考”提示构建，采用结构化推理模板指导模型逐步分析用户兴趣、项目特征和用户-项目兼容性；（2）基于规则的奖励建模，采用群体相对策略优化（GRPO）计算推理轨迹的奖励并优化LLM。此外，本文探索了一种混合范式RecOne，将监督微调与RL结合，使用冷启动推理样本初始化模型，并进一步通过RL优化。实验结果表明，RecZero和RecOne在多个基准数据集上显著优于现有基线方法，验证了RL范式在实现增强自主推理推荐系统方面的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of recommender systems by leveraging the reasoning capabilities of large language models (LLMs) while addressing limitations of existing distillation-based methods. The authors propose RecZero, a reinforcement learning (RL)-based recommendation paradigm that trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction, incorporating a structured reasoning template and rule-based reward modeling. Experimental results show that both RecZero and a hybrid approach, RecOne, significantly outperform existing baseline methods across multiple benchmark datasets, demonstrating the effectiveness of the RL paradigm in enhancing recommender systems with autonomous reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用大型语言模型（LLMs）的推理能力来改善推荐系统，以实现更好的评分预测。作者提出了RecZero，这是一种基于强化学习的方法，摒弃了传统的多模型和多阶段蒸馏方法，而是训练一个单一的LLM自主发展推理能力。实验结果表明，RecZero和一种名为RecOne的混合模型在多个基准数据集上显著优于现有的基线方法，证明了所提出的强化学习范式在增强推荐系统方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Editable Noise Map Inversion: Encoding Target-image into Noise For   High-Fidelity Image Manipulation</div>
<div class="meta-line">Authors: Mingyu Kang, Yong Suk Choi</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-09-30T04:44:53+00:00 · Latest: 2025-10-27T06:34:13+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25776v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.25776v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models have achieved remarkable success in generating
high-quality and diverse images. Building on these advancements, diffusion
models have also demonstrated exceptional performance in text-guided image
editing. A key strategy for effective image editing involves inverting the
source image into editable noise maps associated with the target image.
However, previous inversion methods face challenges in adhering closely to the
target text prompt. The limitation arises because inverted noise maps, while
enabling faithful reconstruction of the source image, restrict the flexibility
needed for desired edits. To overcome this issue, we propose Editable Noise Map
Inversion (ENM Inversion), a novel inversion technique that searches for
optimal noise maps to ensure both content preservation and editability. We
analyze the properties of noise maps for enhanced editability. Based on this
analysis, our method introduces an editable noise refinement that aligns with
the desired edits by minimizing the difference between the reconstructed and
edited noise maps. Extensive experiments demonstrate that ENM Inversion
outperforms existing approaches across a wide range of image editing tasks in
both preservation and edit fidelity with target prompts. Our approach can also
be easily applied to video editing, enabling temporal consistency and content
manipulation across frames.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可编辑噪声图逆转：将目标图像编码为噪声以实现高保真图像处理</div>
<div class="mono" style="margin-top:8px">文本到图像的扩散模型在生成高质量和多样化图像方面取得了显著成功。在这些进展的基础上，扩散模型在文本引导的图像编辑中也表现出色。有效图像编辑的关键策略涉及将源图像逆转为与目标图像相关的可编辑噪声图。然而，以前的逆转方法在紧密遵循目标文本提示方面面临挑战。这一限制的原因在于，虽然逆转的噪声图能够忠实重建源图像，但限制了所需编辑所需的灵活性。为了解决这个问题，我们提出了可编辑噪声图逆转（ENM逆转），这是一种新颖的逆转技术，旨在搜索最佳噪声图，以确保内容保留和可编辑性。我们分析了噪声图的特性，以增强可编辑性。基于这一分析，我们的方法引入了一种可编辑的噪声精炼，通过最小化重建和编辑噪声图之间的差异来与所需编辑对齐。大量实验表明，ENM逆转在广泛的图像编辑任务中在保留和编辑保真度方面优于现有方法。我们的方法也可以轻松应用于视频编辑，实现帧间的时间一致性和内容处理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the effectiveness of text-guided image editing by addressing the limitations of previous inversion methods that struggle to align closely with target text prompts. The authors propose a novel technique called Editable Noise Map Inversion (ENM Inversion), which optimizes noise maps to balance content preservation and editability. Experimental results show that ENM Inversion significantly outperforms existing methods in various image editing tasks, achieving better preservation and fidelity to target prompts, and it is also applicable to video editing for maintaining temporal consistency and content manipulation across frames.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本到图像扩散模型中的图像编辑能力，现有的反演方法在与目标文本提示的紧密对齐方面存在局限性。作者提出了一种新技术，称为可编辑噪声图反演（ENM反演），该技术优化噪声图以增强内容保留和可编辑性。实验结果表明，ENM反演在多种图像编辑任务中显著优于以前的方法，能够更好地符合目标提示，同时也适用于视频编辑，以保持时间一致性。</div>
</details>
</div>
<div class="card">
<div class="title">FaithLM: Towards Faithful Explanations for Large Language Models</div>
<div class="meta-line">Authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, Vladimir Braverman, Xia Hu</div>
<div class="meta-line">First: 2024-02-07T09:09:14+00:00 · Latest: 2025-10-27T06:19:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.04678v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.04678v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly produce natural language
explanations, yet these explanations often lack faithfulness, and they do not
reliably reflect the evidence the model uses to decide. We introduce FaithLM, a
model-agnostic framework that evaluates and improves the faithfulness of LLM
explanations without token masking or task-specific heuristics. FaithLM
formalizes explanation faithfulness as an intervention property: a faithful
explanation should yield a prediction shift when its content is contradicted.
Theoretical analysis shows that the resulting contrary-hint score is a sound
and discriminative estimator of faithfulness. Building on this principle,
FaithLM iteratively refines both the elicitation prompt and the explanation to
maximize the measured score. Experiments on three multi-domain datasets and
multiple LLM backbones demonstrate that FaithLM consistently increases
faithfulness and produces explanations more aligned with human rationales than
strong self-explanation baselines. These findings highlight that
intervention-based evaluation, coupled with iterative optimization, provides a
principled route toward faithful and reliable LLM explanations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FaithLM：面向大型语言模型的可信解释</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地生成自然语言解释，但这些解释往往缺乏可信性，无法可靠地反映模型用于决策的证据。我们提出了FaithLM，这是一个与模型无关的框架，评估和改善LLM解释的可信性，而无需进行标记掩蔽或特定任务的启发式方法。FaithLM将解释的可信性形式化为一种干预属性：一个可信的解释在其内容被反驳时应产生预测偏移。理论分析表明，得到的反向提示分数是可信性的有效和可区分的估计器。基于这一原则，FaithLM迭代地优化引导提示和解释，以最大化测得的分数。在三个多领域数据集和多个LLM骨干网络上的实验表明，FaithLM始终提高可信性，并生成与人类推理更一致的解释，超越强大的自我解释基线。这些发现强调，基于干预的评估结合迭代优化，为实现可信和可靠的LLM解释提供了一条原则性路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of faithfulness in explanations generated by large language models (LLMs), which often do not accurately reflect the evidence used in decision-making. The authors introduce FaithLM, a model-agnostic framework that evaluates and enhances the faithfulness of LLM explanations without relying on token masking or specific heuristics. Experimental results across three multi-domain datasets show that FaithLM significantly improves the faithfulness of explanations, aligning them more closely with human rationales compared to existing self-explanation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型语言模型（LLMs）生成的解释缺乏可信度的问题，因为这些解释通常无法准确反映模型使用的证据。作者提出了FaithLM，这是一种模型无关的框架，可以在不依赖于标记掩蔽或特定启发式方法的情况下评估和增强LLM解释的可信度。三种多领域数据集的实验结果表明，FaithLM显著提高了解释的可信度，使其与人类推理更加一致，相较于强自我解释基线，展示了基于干预的评估和迭代优化在实现可靠LLM解释中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance,   Multi-Relation Text-to-Image Benchmark</div>
<div class="meta-line">Authors: Huixuan Zhang, Xiaojun Wan</div>
<div class="meta-line">First: 2025-10-27T05:32:50+00:00 · Latest: 2025-10-27T05:32:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23020v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image models are known to struggle with generating images that
perfectly align with textual prompts. Several previous studies have focused on
evaluating image-text alignment in text-to-image generation. However, these
evaluations either address overly simple scenarios, especially overlooking the
difficulty of prompts with multiple different instances belonging to the same
category, or they introduce metrics that do not correlate well with human
evaluation. In this study, we introduce M$^3$T2IBench, a large-scale,
multi-category, multi-instance, multi-relation along with an
object-detection-based evaluation metric, $AlignScore$, which aligns closely
with human evaluation. Our findings reveal that current open-source
text-to-image models perform poorly on this challenging benchmark.
Additionally, we propose the Revise-Then-Enforce approach to enhance image-text
alignment. This training-free post-editing method demonstrates improvements in
image-text alignment across a broad range of diffusion models. \footnote{Our
code and data has been released in supplementary material and will be made
publicly available after the paper is accepted.}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M$^{3}$T2IBench：一个大规模多类别、多实例、多关系的文本到图像基准</div>
<div class="mono" style="margin-top:8px">文本到图像模型在生成与文本提示完美对齐的图像方面存在困难。之前的几项研究集中于评估文本到图像生成中的图像-文本对齐。然而，这些评估要么处理过于简单的场景，尤其忽视了同一类别中多个不同实例的提示的难度，要么引入与人类评估相关性不佳的指标。在本研究中，我们引入了M$^3$T2IBench，一个大规模的多类别、多实例、多关系的基准，以及一个基于目标检测的评估指标$AlignScore$，该指标与人类评估紧密对齐。我们的研究结果表明，目前的开源文本到图像模型在这一具有挑战性的基准上表现不佳。此外，我们提出了Revise-Then-Enforce方法来增强图像-文本对齐。这种无训练的后期编辑方法在广泛的扩散模型中显示出图像-文本对齐的改善。\footnote{我们的代码和数据已在补充材料中发布，并将在论文被接受后公开。}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing text-to-image models in generating images that accurately reflect complex textual prompts, particularly those involving multiple instances from the same category. The authors introduce M$^3$T2IBench, a comprehensive benchmark that evaluates text-to-image generation across multiple categories, instances, and relations, utilizing a new evaluation metric called AlignScore that aligns closely with human assessments. Experimental results indicate that current open-source text-to-image models perform inadequately on this benchmark, and the proposed Revise-Then-Enforce method shows promise in improving image-text alignment across various diffusion models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有文本到图像模型在生成准确反映复杂文本提示方面的局限性，特别是涉及同一类别中多个实例的情况。作者引入了M$^3$T2IBench，这是一个全面的基准，评估多类别和多实例的文本到图像生成，采用了一种新的评估指标AlignScore，该指标与人类评估高度相关。实验结果表明，当前的开源文本到图像模型在该基准上表现不佳，而提出的Revise-Then-Enforce方法在无需额外训练的情况下显示出改善图像与文本对齐的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Score-informed Neural Operator for Enhancing Ordering-based Causal   Discovery</div>
<div class="meta-line">Authors: Jiyeon Kang, Songseong Kim, Chanhui Lee, Doyeong Hwang, Joanie Hayoun Chung, Yunkyung Ko, Sumin Lee, Sungwoong Kim, Sungbin Lim</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-08-18T06:25:41+00:00 · Latest: 2025-10-27T05:20:40+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025. 36 pages, 18 figures, 12 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.12650v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.12650v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. In this paper, we aim to improve the approximation of the
Hessian diagonal of the log-densities, thereby enhancing the performance of
ordering-based causal discovery algorithms. Existing approaches that rely on
Stein gradient estimators are computationally expensive and memory-intensive,
while diffusion-model-based methods remain unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO&#x27;s probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于评分的神经算子用于增强基于排序的因果发现</div>
<div class="mono" style="margin-top:8px">基于排序的因果发现方法识别因果图的拓扑顺序，为组合搜索方法提供可扩展的替代方案。在加性噪声模型（ANM）假设下，最近基于评分匹配的因果排序方法需要准确估计对数密度的海森对角线。本文旨在改善对数密度海森对角线的近似，从而增强基于排序的因果发现算法的性能。现有依赖于斯坦梯度估计器的方法计算成本高且内存密集，而基于扩散模型的方法由于评分模型的二阶导数而不稳定。为了解决这些问题，我们提出了评分信息神经算子（SciNO），这是一种在平滑函数空间中设计的概率生成模型，旨在稳定地近似海森对角线并在评分建模过程中保留结构信息。实证结果表明，与DiffAN相比，SciNO在合成图上减少了42.7%的顺序发散，在真实数据集上平均减少了31.5%，同时保持了内存效率和可扩展性。此外，我们提出了一种用于因果推理的概率控制算法，该算法与自回归模型集成SciNO的概率估计与自回归模型先验，能够实现可靠的数据驱动因果排序，受语义信息的启发。因此，所提出的方法增强了大型语言模型的因果推理能力，无需额外的微调或提示工程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of ordering-based causal discovery algorithms, which are essential for identifying causal relationships in data. The authors propose a new method called Score-informed Neural Operator (SciNO), a probabilistic generative model designed to stably approximate the Hessian diagonal of log-densities while preserving structural information. Experimental results demonstrate that SciNO significantly reduces order divergence by 42.7% on synthetic graphs and 31.5% on real-world datasets compared to existing methods, while also maintaining memory efficiency and scalability, thereby enhancing causal reasoning capabilities in large language models without requiring additional fine-tuning or prompt engineering.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有基于排序的因果发现方法的局限性，这些方法在加性噪声模型假设下需要准确估计对数密度的海森矩阵对角线。作者提出了得分信息神经算子（SciNO），这是一种有效近似海森矩阵对角线的概率生成模型，同时保留结构信息，从而改善因果发现算法的性能。实验结果表明，与之前的方法DiffAN相比，SciNO在合成图上平均减少了42.7%的排序偏差，在真实世界数据集上减少了31.5%，同时保持了内存效率和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">DOS: Directional Object Separation in Text Embeddings for Multi-Object   Image Generation</div>
<div class="meta-line">Authors: Dongnam Byun, Jungwon Park, Jumgmin Ko, Changin Choi, Wonjong Rhee</div>
<div class="meta-line">First: 2025-10-16T07:17:23+00:00 · Latest: 2025-10-27T05:18:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14376v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.14376v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DOS：文本嵌入中的方向性对象分离用于多对象图像生成</div>
<div class="mono" style="margin-top:8px">最近在文本到图像（T2I）生成模型方面的进展，显著提高了与文本提示对齐的高质量图像生成。然而，这些模型在处理涉及多个对象的提示时仍然存在困难，常常导致对象忽视或对象混合。通过广泛的研究，我们识别出四种问题场景：相似形状、相似纹理、不同背景偏差和多个对象，其中对象间关系经常导致这些失败。基于对CLIP嵌入的两个关键观察，我们提出了DOS（方向性对象分离），这是一种在将三种类型的CLIP文本嵌入传递给文本到图像模型之前进行修改的方法。实验结果表明，DOS始终提高了多对象图像生成的成功率，并减少了对象混合。在人类评估中，DOS显著优于四种竞争方法，在四个基准测试中获得了26.24%-43.04%的更多投票。这些结果突显了DOS作为改善多对象图像生成的实用有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by text-to-image generative models in accurately generating images that involve multiple objects, which often leads to issues such as object neglect and mixing. The authors propose a novel method called DOS (Directional Object Separation) that modifies three types of CLIP text embeddings prior to their use in text-to-image models. Experimental findings demonstrate that DOS significantly enhances the success rate of multi-object image generation and minimizes object mixing, with human evaluations indicating that DOS outperforms four competing methods by receiving 26.24%-43.04% more votes across four benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于文本到图像生成模型在处理涉及多个对象的提示时面临的挑战，这常常导致对象忽视和混合等问题。作者提出了一种新方法，称为DOS（方向性对象分离），该方法在将三种类型的CLIP文本嵌入用于文本到图像模型之前进行修改。实验结果表明，DOS显著提高了多对象图像生成的成功率，并减少了对象混合，人工评估显示，DOS在四个基准测试中比四种竞争方法获得了26.24%-43.04%的更多投票。</div>
</details>
</div>
<div class="card">
<div class="title">From Prompt Optimization to Multi-Dimensional Credibility Evaluation:   Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports</div>
<div class="meta-line">Authors: Qiuli Wang, Xiaoming Li, Jie Chen, Yongxu Liu, Xingpeng Zhang, Chen Liu, Wei Chen</div>
<div class="meta-line">First: 2025-10-27T04:57:20+00:00 · Latest: 2025-10-27T04:57:20+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23008v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从提示优化到多维可信度评估：提升中国LLM生成的肝脏MRI报告的可信性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在从影像发现生成诊断结论方面表现出色，从而支持放射学报告、培训教育和质量控制。然而，如何在不同临床环境中优化提示设计的系统性指导仍然未被充分探索。此外，评估LLM生成的放射学报告可信性的全面和标准化框架尚未建立。本研究旨在通过引入多维可信度评估（MDCA）框架并提供机构特定的提示优化指导，来提升LLM生成的肝脏MRI报告的可信性。所提框架应用于评估和比较几种先进LLM的性能，包括Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3和ByteDance-Seed-OSS-36B-Instruct，使用SiliconFlow平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for improved trustworthiness in liver MRI reports generated by large language models (LLMs), as existing methods for prompt optimization and credibility assessment are insufficient. The researchers developed a Multi-Dimensional Credibility Assessment (MDCA) framework and provided guidelines for optimizing prompts tailored to specific clinical contexts. The framework was utilized to evaluate and compare the performance of several advanced LLMs, revealing significant differences in their ability to generate reliable radiology reports.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在临床环境中优化大型语言模型（LLMs）提示设计的系统指导需求，特别是在生成肝脏MRI报告方面。作者提出了一个多维可信度评估（MDCA）框架，以增强这些报告的可信度，并提供特定机构的提示优化策略。实验结果展示了MDCA框架在评估和比较几种先进LLMs性能中的应用，揭示了它们在从影像发现中生成诊断结论方面的可靠性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
