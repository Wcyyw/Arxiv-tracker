<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-08 03:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260108_0323</div>
    <div class="row"><div class="card">
<div class="title">LTX-2: Efficient Joint Audio-Visual Foundation Model</div>
<div class="meta-line">Authors: Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</div>
<div class="meta-line">First: 2026-01-06T18:24:41+00:00 · Latest: 2026-01-06T18:24:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LTX-2：高效的联合音视频基础模型</div>
<div class="mono" style="margin-top:8px">最近的文本到视频扩散模型能够生成引人注目的视频序列，但它们仍然是无声的——缺少音频提供的语义、情感和氛围线索。我们介绍LTX-2，这是一个开源基础模型，能够以统一的方式生成高质量、时间同步的视听内容。LTX-2由一个不对称的双流变换器组成，具有140亿参数的视频流和50亿参数的音频流，通过具有时间位置嵌入的双向音视频交叉注意层和用于共享时间步条件的跨模态AdaLN相结合。该架构使得统一视听模型的高效训练和推理成为可能，同时为视频生成分配更多的容量而不是音频生成。我们采用多语言文本编码器以更广泛地理解提示，并引入了一种模态感知的无分类器引导（modality-CFG）机制，以改善视听对齐和可控性。除了生成语音，LTX-2还生成丰富、连贯的音轨，跟随每个场景的角色、环境、风格和情感——配有自然的背景和音效元素。在我们的评估中，该模型在开源系统中实现了最先进的视听质量和提示遵循，同时以其计算成本和推理时间的极小部分提供了与专有模型相当的结果。所有模型权重和代码均已公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides.</div>
<div class="mono" style="margin-top:8px">该研究的动机是通过引入音频来增强文本到视频的扩散模型，从而为生成的视频内容增加语义和情感深度。作者开发了LTX-2，一个开源基础模型，采用不对称双流变换器架构，具有14B参数的视频流和5B参数的音频流，通过先进的交叉注意机制进行集成。实验结果表明，LTX-2在视听质量和提示遵循方面达到了最先进的水平，超越了现有的开源系统，并在性能上与专有模型相匹配，同时在计算成本和推理时间上更为高效。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-01-06T17:52:02+00:00</div>
<div class="meta-line">Comments: Preprint. Under review at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评引导强化遗忘</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型中的机器遗忘旨在去除目标概念，同时保持整体效用。先前的扩散遗忘方法通常依赖于监督权重编辑或全局惩罚；强化学习（RL）方法虽然灵活，但通常优化稀疏的轨迹末端奖励，导致高方差更新和弱信用分配。我们提出了一种通用的RL框架，用于扩散遗忘，将去噪视为一个序列决策过程，并引入具有噪声步奖励的时间步感知批评者。具体而言，我们在噪声潜变量上训练基于CLIP的奖励预测器，并使用其每步信号计算反向扩散核的策略梯度更新的优势估计。我们的算法易于实现，支持离线策略重用，并可与标准文本到图像骨干网络结合。在多个概念上，该方法在保持图像质量和良性提示保真度的同时，实现了比强基线更好或可比的遗忘；消融实验表明（i）每步批评者和（ii）噪声条件奖励是稳定性和有效性的关键。我们发布了代码和评估脚本，以促进可重复性和未来基于RL的扩散遗忘研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance machine unlearning in text-to-image diffusion models by effectively removing targeted concepts while maintaining overall utility. The authors propose a general reinforcement learning framework that treats the denoising process as a sequential decision-making task, incorporating a timestep-aware critic and noisy-step rewards to improve learning stability. Experimental results demonstrate that their method achieves comparable or superior concept forgetting compared to strong baselines while preserving image quality and prompt fidelity, with key findings indicating that per-step critics and noisy-conditioned rewards are crucial for the method&#x27;s effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善文本到图像扩散模型中的机器遗忘，通过有效去除目标概念同时保持整体效用。作者提出了一种强化学习框架，将去噪过程视为一个顺序决策任务，结合了时间步感知的评论者和噪声步奖励以增强性能。实验结果表明，该方法在概念遗忘方面与现有强基线相比具有可比或更优的效果，同时保持图像质量和对良性提示的忠实度，关键发现表明每步评论者和噪声条件奖励对稳定性和有效性至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays</div>
<div class="meta-line">Authors: Dylan Saeed, Ramtin Gharleghi, Susann Beier, Sonit Singh</div>
<div class="meta-line">First: 2025-11-14T09:11:41+00:00 · Latest: 2026-01-06T17:32:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures. Under review for MIDL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11093v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11093v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coronary artery calcification (CAC) is a strong predictor of cardiovascular events, with CT-based Agatston scoring widely regarded as the clinical gold standard. However, CT is costly and impractical for large-scale screening, while chest X-rays (CXRs) are inexpensive but lack reliable ground truth labels, constraining deep learning development. Digitally reconstructed radiographs (DRRs) offer a scalable alternative by projecting CT volumes into CXR-like images while inheriting precise labels. In this work, we provide the first systematic evaluation of DRRs as a surrogate training domain for CAC detection. Using 667 CT scans from the COCA dataset, we generate synthetic DRRs and assess model capacity, super-resolution fidelity enhancement, preprocessing, and training strategies. Lightweight CNNs trained from scratch outperform large pretrained networks; pairing super-resolution with contrast enhancement yields significant gains; and curriculum learning stabilises training under weak supervision. Our best configuration achieves a mean AUC of 0.754, comparable to or exceeding prior CXR-based studies. These results establish DRRs as a scalable, label-rich foundation for CAC detection, while laying the foundation for future transfer learning and domain adaptation to real CXRs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于机器学习的冠状动脉钙化检测：使用合成胸部X光片</div>
<div class="mono" style="margin-top:8px">冠状动脉钙化（CAC）是心血管事件的重要预测指标，基于CT的Agatston评分被广泛认为是临床金标准。然而，CT成本高且不适合大规模筛查，而胸部X光片（CXR）成本低但缺乏可靠的真实标签，限制了深度学习的发展。数字重建放射图（DRR）通过将CT体积投影为类似CXR的图像，同时继承精确标签，提供了一种可扩展的替代方案。在这项工作中，我们首次系统评估了DRR作为CAC检测的替代训练领域。使用来自COCA数据集的667个CT扫描，我们生成合成DRR并评估模型能力、超分辨率保真度增强、预处理和训练策略。从头开始训练的轻量级CNN优于大型预训练网络；将超分辨率与对比度增强相结合可获得显著提升；而课程学习在弱监督下稳定训练。我们最佳配置的平均AUC达到0.754， comparable to or exceeding prior CXR-based studies。这些结果确立了DRR作为CAC检测的可扩展、标签丰富的基础，同时为未来的迁移学习和真实CXR的领域适应奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to find a cost-effective method for detecting coronary artery calcification (CAC), which is a significant predictor of cardiovascular events, using chest X-rays (CXRs) instead of the expensive CT scans that are currently the gold standard. The authors employed digitally reconstructed radiographs (DRRs) generated from CT scans to create a scalable training domain for machine learning models. The experimental results demonstrated that lightweight convolutional neural networks (CNNs) trained from scratch outperformed larger pretrained models, and the combination of super-resolution with contrast enhancement led to substantial improvements, achieving a mean AUC of 0.754, which is comparable to or better than previous studies using CXRs. This work highlights the potential of DRRs as a rich and scalable resource for CAC detection, paving the way for future advancements in transfer learning and domain adaptation to real CXRs.</div>
<div class="mono" style="margin-top:8px">本研究的动机是寻找一种有效且可扩展的方法，通过廉价的胸部X光片（CXR）而非昂贵的CT扫描来检测冠状动脉钙化（CAC），后者是心血管事件的重要预测指标。作者利用从CT扫描生成的数字重建放射图（DRR）创建了一个用于机器学习模型的训练领域，旨在检测CAC。研究发现，从头开始训练的轻量级卷积神经网络（CNN）优于大型预训练模型，结合超分辨率与对比度增强显著提高了性能，最佳模型的平均AUC达到0.754，表明DRR可以作为CAC检测的可行替代方案，并支持未来在真实CXR上进行迁移学习和领域适应的进展。</div>
</details>
</div>
<div class="card">
<div class="title">DIP: Dynamic In-Context Planner For Diffusion Language Models</div>
<div class="meta-line">Authors: Yang Li, Han Meng, Chenan Wang, Haipeng Chen</div>
<div class="meta-line">First: 2026-01-06T17:24:16+00:00 · Latest: 2026-01-06T17:24:16+00:00</div>
<div class="meta-line">Comments: 4 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03199v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \textbf{D}ynamic \textbf{I}n-Context \textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\times$ inference speedup over standard inference and 1.17$\times$ over KV cache-enhanced inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DIP：扩散语言模型的动态上下文规划器</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）在上下文示例的自然语言任务中展现出强大的潜力。然而，由于双向注意机制，随着上下文长度的增加，DLMs的计算成本显著增加。本研究通过一个关键发现解决了这个问题：与自回归语言模型（ARLMs）中的顺序生成不同，DLMs中的扩散生成范式允许在生成过程中\textit{高效动态调整上下文}。基于这一见解，我们提出了\textbf{D}ynamic \textbf{I}n-Context \textbf{P}lanner（DIP），这是一种上下文优化方法，在生成过程中动态选择和插入上下文示例，而不是在提示中一次性提供所有示例。结果表明，DIP在保持生成质量的同时，实现了比标准推理快12.9$\times$，比KV缓存增强推理快1.17$\times$的推理加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of diffusion language models (DLMs) in handling in-context examples, which traditionally incur high computational costs due to their bidirectional attention mechanism. The authors introduce the Dynamic In-Context Planner (DIP), a method that optimizes context by dynamically selecting and inserting examples during generation, rather than using a fixed set of examples. Experimental results demonstrate that DIP maintains the quality of generated outputs while achieving up to 12.9 times faster inference compared to standard methods and 1.17 times faster than KV cache-enhanced inference.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高扩散语言模型（DLMs）在处理自然语言任务时的效率，特别是解决由于其双向注意机制导致的上下文长度增加而带来的高计算成本。作者提出了动态上下文规划器（DIP），一种通过在生成过程中动态选择和插入相关示例来优化上下文的方法，而不是事先使用固定的示例集合。实验结果表明，DIP在保持生成输出质量的同时，较标准方法实现了高达12.9倍的推理速度提升，相较于KV缓存增强推理也提高了1.17倍。</div>
</details>
</div>
<div class="card">
<div class="title">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</div>
<div class="meta-line">Authors: Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</div>
<div class="meta-line">First: 2026-01-06T17:15:50+00:00 · Latest: 2026-01-06T17:15:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03193v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniCorn：通过自生成监督实现自我改进的统一多模态模型</div>
<div class="mono" style="margin-top:8px">尽管统一多模态模型（UMMs）在跨模态理解方面取得了显著成功，但它们在利用内部知识进行高质量生成方面仍存在显著差距。我们将这种差距形式化为传导性失语症，这是一种现象，模型能够准确解释多模态输入，但难以将这种理解转化为真实且可控的合成。为了解决这个问题，我们提出了UniCorn，一个简单而优雅的自我改进框架，消除了对外部数据或教师监督的需求。通过将单个UMM划分为三个协作角色：提议者、解决者和评判者，UniCorn通过自我对弈生成高质量的交互，并采用认知模式重构将潜在理解提炼为明确的生成信号。为了验证多模态一致性的恢复，我们引入了UniCycle，这是一个基于文本到图像再到文本重构循环的循环一致性基准。大量实验表明，UniCorn在六个通用图像生成基准上相较于基础模型实现了全面且显著的改进。值得注意的是，它在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle上达到了SOTA性能，同时在WISE上进一步实现了+5.0的显著提升，在OneIG上实现了+6.5的提升。这些结果突显了我们的方法显著增强了T2I生成，同时保持了强大的理解能力，展示了完全自我监督精炼在统一多模态智能中的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve Unified Multimodal Models (UMMs) in generating high-quality outputs from multimodal inputs, addressing the issue known as Conduction Aphasia, where models can interpret inputs but fail in synthesis. The authors propose UniCorn, a self-improvement framework that utilizes a collaborative approach by dividing a UMM into three roles: Proposer, Solver, and Judge, enabling self-play and cognitive pattern reconstruction without external supervision. Experimental results show that UniCorn significantly enhances performance across six image generation benchmarks, achieving state-of-the-art results on several tasks and demonstrating its effectiveness in improving text-to-image generation while preserving comprehension.</div>
<div class="mono" style="margin-top:8px">本研究解决了统一多模态模型（UMMs）在生成高质量输出方面的挑战，尽管它们在跨模态理解方面表现出色，这一差距被称为传导失语症。为了解决这个问题，作者提出了UniCorn，这是一种自我改进框架，无需外部数据或监督，通过将UMM分为三个角色：提议者、解决者和评判者，通过自我对弈和认知模式重构进行交互。实验结果表明，UniCorn在六个图像生成基准测试中显著提高了性能，在多个指标上实现了最先进的结果，包括TIIF、DPG和CompBench，同时在WISE和OneIG上也有所提升，表明其在改进多模态生成能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Automatic Prompt Engineering with No Task Cues and No Tuning</div>
<div class="meta-line">Authors: Faisal Chowdhury, Nandana Mihindukulasooriya, Niharika S D&#x27;Souza, Horst Samulowitz, Neeru Gupta, Tomasz Hanusiak, Michal Kapitonow</div>
<div class="meta-line">Venue: The IEEE International Conference on Data Mining (ICDM) 2025 : Demo Track</div>
<div class="meta-line">First: 2026-01-06T16:04:45+00:00 · Latest: 2026-01-06T16:04:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03130v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无任务提示和无调优的自动提示工程</div>
<div class="mono" style="margin-top:8px">本文提出了一种自动提示工程系统，其设计和应用都简单得多，但效果与现有方法一样有效。它不需要调优，也不需要关于任务的明确线索。我们在数据库表中的神秘列名扩展（CNE）任务上评估了我们的方法，该任务对表格数据的搜索、访问和理解至关重要，但现有工作非常少。我们在英语和德语的两个语言数据集上进行了评估。这是首次报告自动提示工程在CNE任务中的应用。根据我们所知，这也是首次在英语以外的语言中应用自动提示工程的工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to simplify the process of automatic prompt engineering, which traditionally requires tuning and task-specific cues. The authors developed a system that operates without these requirements and evaluated its effectiveness on the cryptic column name expansion (CNE) task in database tables, which is essential for enhancing tabular data search and understanding. The experimental results demonstrated that their approach is as effective as existing methods, marking the first application of automatic prompt engineering for the CNE task in both English and German datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是简化自动提示工程的过程，传统方法通常需要调优和特定任务线索。作者开发了一个无需这些要求的系统，并在数据库表中的隐晦列名扩展（CNE）任务上评估其有效性，该任务对于改善表格数据搜索和理解至关重要。实验结果表明，他们的方法与现有方法同样有效，这标志着自动提示工程首次应用于英语和德语数据集中的CNE任务。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Thinker: A General Reasoning Modular Core for Image Generation</div>
<div class="meta-line">Authors: Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</div>
<div class="meta-line">First: 2026-01-06T15:59:33+00:00 · Latest: 2026-01-06T15:59:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一思考者：用于图像生成的通用推理模块核心</div>
<div class="mono" style="margin-top:8px">尽管高保真图像合成取得了显著进展，生成模型在逻辑密集型指令跟随方面仍然存在困难，暴露出持续的推理与执行差距。同时，封闭源系统（例如Nano Banana）展示了强大的推理驱动图像生成，突显出与当前开源模型之间的显著差距。我们认为，缩小这一差距不仅需要更好的视觉生成器，还需要可执行的推理：将高层意图分解为有根据的、可验证的计划，直接引导生成过程。为此，我们提出了统一思考者，一种任务无关的推理架构，用于通用图像生成，设计为一个统一的规划核心，可以插入多种生成器和工作流程。统一思考者将专用思考者与图像生成器解耦，使推理的模块化升级成为可能，而无需重新训练整个生成模型。我们进一步引入了一个两阶段的训练范式：首先为思考者构建一个结构化的规划接口，然后应用强化学习将其策略基于像素级反馈进行落地，鼓励优化视觉正确性的计划，而非文本合理性。大量关于文本到图像生成和图像编辑的实验表明，统一思考者显著提高了图像推理和生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the reasoning-execution gap in generative models for image synthesis, particularly in logic-intensive instruction following. The authors introduce Unified Thinker, a modular reasoning architecture that separates reasoning from image generation, allowing for flexible integration with various generators. Experimental results demonstrate that Unified Thinker enhances both image reasoning and generation quality significantly through a two-stage training approach that combines structured planning and reinforcement learning based on pixel-level feedback.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成模型在图像合成中逻辑密集型指令执行的推理与执行差距。作者提出了Unified Thinker，这是一种模块化的推理架构，能够将推理与图像生成分离，从而允许灵活的升级而无需重新训练整个模型。实验结果表明，Unified Thinker在文本到图像生成和图像编辑等任务中显著提高了图像推理和生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">DiT-JSCC: Rethinking Deep JSCC with Diffusion Transformers and Semantic Representations</div>
<div class="meta-line">Authors: Kailin Tan, Jincheng Dai, Sixian Wang, Guo Lu, Shuo Shao, Kai Niu, Wenjun Zhang, Ping Zhang</div>
<div class="meta-line">First: 2026-01-06T15:42:45+00:00 · Latest: 2026-01-06T15:42:45+00:00</div>
<div class="meta-line">Comments: 14pages, 14figures, 2tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative joint source-channel coding (GJSCC) has emerged as a new Deep JSCC paradigm for achieving high-fidelity and robust image transmission under extreme wireless channel conditions, such as ultra-low bandwidth and low signal-to-noise ratio. Recent studies commonly adopt diffusion models as generative decoders, but they frequently produce visually realistic results with limited semantic consistency. This limitation stems from a fundamental mismatch between reconstruction-oriented JSCC encoders and generative decoders, as the former lack explicit semantic discriminability and fail to provide reliable conditional cues. In this paper, we propose DiT-JSCC, a novel GJSCC backbone that can jointly learn a semantics-prioritized representation encoder and a diffusion transformer (DiT) based generative decoder, our open-source project aims to promote the future research in GJSCC. Specifically, we design a semantics-detail dual-branch encoder that aligns naturally with a coarse-to-fine conditional DiT decoder, prioritizing semantic consistency under extreme channel conditions. Moreover, a training-free adaptive bandwidth allocation strategy inspired by Kolmogorov complexity is introduced to further improve the transmission efficiency, thereby indeed redefining the notion of information value in the era of generative decoding. Extensive experiments demonstrate that DiT-JSCC consistently outperforms existing JSCC methods in both semantic consistency and visual quality, particularly in extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiT-JSCC：用扩散变换器和语义表示重新思考深度JSCC</div>
<div class="mono" style="margin-top:8px">生成联合源信道编码（GJSCC）已成为一种新的深度JSCC范式，旨在在极端无线信道条件下实现高保真和鲁棒的图像传输，例如超低带宽和低信噪比。最近的研究通常采用扩散模型作为生成解码器，但它们经常产生视觉上逼真的结果，但语义一致性有限。这一限制源于重建导向的JSCC编码器与生成解码器之间的根本不匹配，因为前者缺乏明确的语义可区分性，未能提供可靠的条件线索。本文提出了DiT-JSCC，一种新颖的GJSCC骨干网络，可以共同学习一个以语义为优先的表示编码器和一个基于扩散变换器（DiT）的生成解码器，我们的开源项目旨在促进GJSCC的未来研究。具体而言，我们设计了一个语义-细节双分支编码器，自然与粗到细的条件DiT解码器对齐，在极端信道条件下优先考虑语义一致性。此外，引入了一种受Kolmogorov复杂性启发的无训练自适应带宽分配策略，以进一步提高传输效率，从而确实重新定义了生成解码时代的信息价值概念。大量实验表明，DiT-JSCC在语义一致性和视觉质量方面始终优于现有的JSCC方法，特别是在极端条件下。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of generative joint source-channel coding (GJSCC) for robust image transmission in challenging wireless conditions, where existing methods struggle with semantic consistency. The authors propose DiT-JSCC, which integrates a semantics-prioritized representation encoder with a diffusion transformer-based generative decoder, featuring a dual-branch encoder that aligns with a conditional DiT decoder to improve semantic fidelity. Experimental results indicate that DiT-JSCC significantly outperforms traditional JSCC methods in terms of both semantic consistency and visual quality, especially under extreme conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高生成联合源信道编码（GJSCC）在极端无线条件下的图像传输性能，而现有方法在语义一致性方面存在不足。作者提出了DiT-JSCC，该方法将语义优先的表示编码器与基于扩散变换器的生成解码器相结合，采用双分支架构以改善编码和解码过程之间的对齐。实验结果表明，DiT-JSCC在极端带宽和信噪比场景下，在语义一致性和视觉质量方面显著优于传统的JSCC方法。</div>
</details>
</div>
<div class="card">
<div class="title">SAGOnline: Segment Any Gaussians Online</div>
<div class="meta-line">Authors: Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li</div>
<div class="meta-line">First: 2025-08-11T17:38:50+00:00 · Latest: 2026-01-06T14:58:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08219v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.08219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D Gaussian Splatting has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Existing segmentation approaches typically rely on high-dimensional feature lifting, which causes costly optimization, implicit semantics, and task-specific constraints. We present \textbf{Segment Any Gaussians Online (SAGOnline)}, a unified, zero-shot framework that achieves real-time, cross-view consistent segmentation without scene-specific training. SAGOnline decouples the monolithic segmentation problem into lightweight sub-tasks. By integrating video foundation models (e.g., SAM 2), we first generate temporally consistent 2D masks across rendered views. Crucially, instead of learning continuous feature fields, we introduce a \textbf{Rasterization-aware Geometric Consensus} mechanism that leverages the traceability of the Gaussian rasterization pipeline. This allows us to deterministically map 2D predictions to explicit, discrete 3D primitive labels in real-time. This discrete representation eliminates the memory and computational burden of feature distillation, enabling instant inference. Extensive evaluations on NVOS and SPIn-NeRF benchmarks demonstrate that SAGOnline achieves state-of-the-art accuracy (92.7\% and 95.2\% mIoU) while operating at the fastest speed at 27 ms per frame. By providing a flexible interface for diverse foundation models, our framework supports instant prompt, instance, and semantic segmentation, paving the way for interactive 3D understanding in AR/VR and robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGOnline：在线分割任意高斯</div>
<div class="mono" style="margin-top:8px">3D高斯点云已成为显式3D场景表示的强大范式，但实现高效且一致的3D分割仍然具有挑战性。现有的分割方法通常依赖于高维特征提升，这导致了昂贵的优化、隐式语义和特定任务的约束。我们提出了\textbf{在线分割任意高斯（SAGOnline）}，这是一个统一的零样本框架，能够在没有场景特定训练的情况下实现实时、跨视图一致的分割。SAGOnline将单一的分割问题解耦为轻量级子任务。通过整合视频基础模型（例如，SAM 2），我们首先生成跨渲染视图的时间一致的2D掩码。关键是，我们引入了一种\textbf{光栅化感知几何共识}机制，利用高斯光栅化管道的可追溯性。这使我们能够在实时中确定性地将2D预测映射到显式、离散的3D原始标签。这种离散表示消除了特征蒸馏的内存和计算负担，实现了即时推理。在NVOS和SPIn-NeRF基准上的广泛评估表明，SAGOnline在以每帧27毫秒的最快速度运行时，达到了最先进的准确率（92.7\%和95.2\% mIoU）。通过为多样的基础模型提供灵活的接口，我们的框架支持即时提示、实例和语义分割，为AR/VR和机器人中的交互式3D理解铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to address the challenges of efficient and consistent 3D segmentation in the context of 3D Gaussian Splatting, which often suffers from high-dimensional feature lifting and task-specific constraints. The authors introduce Segment Any Gaussians Online (SAGOnline), a zero-shot framework that performs real-time, cross-view consistent segmentation without the need for scene-specific training by breaking down the segmentation problem into manageable sub-tasks. Key experimental results show that SAGOnline achieves state-of-the-art accuracy with 92.7% and 95.2% mean Intersection over Union (mIoU) on the NVOS and SPIn-NeRF benchmarks, respectively, while maintaining a rapid processing speed of 27 milliseconds per frame.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决3D高斯点云分割中高效且一致的3D分割问题，该问题通常受到高维特征提升和特定任务约束的影响。作者提出了一种名为在线分割任意高斯(SAGOnline)的新框架，该框架将分割问题分解为可管理的子任务，并利用视频基础模型生成时间一致的2D掩膜。关键实验结果表明，SAGOnline在NVOS和SPIn-NeRF基准测试中分别达到了92.7%和95.2%的平均交并比(mIoU)的最先进准确率，同时保持每帧27毫秒的快速处理速度，从而实现实时的跨视图一致分割，无需特定场景训练。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion</div>
<div class="meta-line">Authors: Han Zhang, Yanwei Wang, Fang Li, Hongjun Wang</div>
<div class="meta-line">First: 2026-01-06T14:28:21+00:00 · Latest: 2026-01-06T14:28:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态模糊特征融合的运动模糊鲁棒小麦害虫损伤检测</div>
<div class="mono" style="margin-top:8px">相机抖动造成的运动模糊产生了显著降低边缘侧物体检测的鬼影伪影。现有方法要么将模糊视为噪声并失去区分性结构，要么应用全图像恢复，增加延迟并限制在资源受限设备上的部署。我们提出了DFRCP，即动态模糊鲁棒卷积金字塔，作为YOLOv11的插件升级，以实现模糊鲁棒检测。DFRCP通过结合大规模和中等规模特征，同时保留原生表示，增强了YOLOv11特征金字塔，并引入动态鲁棒切换单元，自适应地注入模糊特征，以增强抖动下的全局感知。模糊特征通过旋转和非线性插值多尺度特征合成，然后通过透明卷积合并，学习原始和模糊线索之间的内容自适应权衡。我们进一步开发了一个CUDA并行旋转和插值内核，避免边界溢出，并实现超过400倍的加速，使设计在边缘部署中变得实用。我们在一个约3500张图像的私有小麦害虫损伤数据集上进行成对监督训练，使用两种模糊模式将数据增强三倍，均匀图像宽度运动模糊和边界框限制的旋转模糊。在模糊测试集上，带有DFRCP的YOLOv11的准确率比仅有适度训练时间开销的YOLOv11基线高出约10.4个百分点，减少了数据收集后手动过滤的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of motion blur in object detection, which can significantly impair the identification of wheat pest damage. The authors propose a Dynamic Fuzzy Robust Convolutional Pyramid (DFRCP) as an enhancement to YOLOv11, which integrates large and medium scale features while maintaining original representations and introducing Dynamic Robust Switch units for improved global perception. Experimental results demonstrate that YOLOv11 with DFRCP achieves approximately 10.4% higher accuracy on blurred test sets compared to the baseline YOLOv11, with only a slight increase in training time, thus minimizing the need for manual data filtering.</div>
<div class="mono" style="margin-top:8px">本研究解决了运动模糊对物体检测的挑战，这种模糊会显著影响小麦害虫损害的识别。作者提出了一种动态模糊鲁棒卷积金字塔（DFRCP），作为YOLOv11的增强版，集成了大规模和中等规模特征，同时保持原始表示，并采用动态鲁棒开关单元来引入模糊特征，以改善运动模糊下的检测。实验结果表明，带有DFRCP的YOLOv11在模糊测试集上的准确率比基线YOLOv11高出约10.4%，且训练时间仅略有增加，从而减少了数据收集后手动过滤的需求。</div>
</details>
</div>
<div class="card">
<div class="title">SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation</div>
<div class="meta-line">Authors: Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</div>
<div class="meta-line">First: 2025-10-11T07:12:44+00:00 · Latest: 2026-01-06T13:04:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10069v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10069v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head/face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncLipMAE：用于音视频说话人面部表征的对比掩码预训练</div>
<div class="mono" style="margin-top:8px">我们介绍了SyncLipMAE，这是一种自监督的说话面部视频预训练框架，从未标记的音视频流中学习同步感知和可转移的面部动态。我们的方法将掩码视觉建模与跨模态对比对齐相结合，并采用三个逐帧提示标记，明确编码说话面部帧的基本因素——身份、声乐运动（与语音同步的面部动态）和环境运动（与音频无关的运动，如眨眼和头部姿态）。对比目标使用时间对齐的声乐运动和音频标记作为正样本，使用不对齐的对作为负样本，将两种模态驱动到共享嵌入空间，并实现标记级音视频流同步。预训练后，对齐的音频标记与视觉提示标记（身份、声乐运动、环境运动）形成一个统一的接口，适用于四个不同的下游设置：（i）音视频流同步；（ii）面部情感和头/面部动作识别；（iii）视觉语音识别；（iv）视觉配音，我们在单一模型中实现了不可区分的音频或视频驱动控制。在需要不同能力的四个任务家族中，SyncLipMAE实现了最先进的结果，强调了同步感知、分解自监督预训练的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SyncLipMAE is to develop a self-supervised pretraining framework that effectively learns synchronization-aware facial dynamics from unlabeled audio-visual streams for talking-face video representation. The method combines masked visual modeling with cross-modal contrastive alignment, utilizing prompt tokens to encode identity, vocal motion, and ambient motion. The key experimental findings demonstrate that SyncLipMAE achieves state-of-the-art results across four diverse downstream tasks, including audio-visual stream synchronization and facial emotion recognition, highlighting the framework&#x27;s effectiveness in creating a unified interface for various applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个自监督的预训练框架，用于从未标记的音视频数据中有效学习说话面部视频的面部动态。该方法称为SyncLipMAE，结合了掩蔽视觉建模和跨模态对比对齐，利用表示身份、语音运动和环境运动的提示标记来促进音频和视觉流之间的同步。实验结果表明，SyncLipMAE在包括音视频流同步和面部情感识别在内的四个不同任务中达到了最先进的性能，突显了其同步感知的自监督学习方法的优势。</div>
</details>
</div>
<div class="card">
<div class="title">LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing</div>
<div class="meta-line">Authors: Wingwa Fu, Takayuki Okatani</div>
<div class="meta-line">First: 2026-01-06T12:57:05+00:00 · Latest: 2026-01-06T12:57:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02987v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02987v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAMS-Edit：使用调度器的潜在和注意力混合以改善基于扩散的图像和风格编辑中的内容保留</div>
<div class="mono" style="margin-top:8px">使用扩散模型的文本到图像编辑面临在内容保留与编辑应用之间平衡以及处理真实图像编辑的挑战。为了解决这些问题，我们提出了LAMS-Edit，在编辑图像生成过程中利用反演过程中的中间状态——这是真实图像编辑中的一个关键步骤。具体而言，两个过程中的潜在表示和注意力图在每一步通过加权插值结合，受调度器控制。这种技术，称为使用调度器的潜在和注意力混合（LAMS），与Prompt-to-Prompt（P2P）集成形成LAMS-Edit——一个可扩展的框架，支持使用区域掩码进行精确编辑，并通过LoRA实现风格转移。大量实验表明，LAMS-Edit有效地平衡了内容保留和编辑应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve text-to-image editing using diffusion models, specifically addressing the challenges of content preservation and real-image editing. The authors propose a method called LAMS-Edit, which utilizes intermediate states from the inversion process to enhance the generation of edited images by combining latent representations and attention maps through weighted interpolation controlled by a scheduler. Experimental results show that LAMS-Edit successfully balances content preservation with the application of edits, demonstrating its effectiveness in precise editing and style transfer capabilities.</div>
<div class="mono" style="margin-top:8px">该研究解决了使用扩散模型进行文本到图像编辑时内容保留与编辑应用之间的挑战。作者提出了LAMS-Edit，该方法在图像生成过程中利用反演过程中的中间状态，通过加权插值结合潜在表示和注意力图，并由调度器控制。实验结果表明，LAMS-Edit成功地平衡了内容保留与编辑应用，提高了使用区域掩码的编辑精度，并通过LoRA促进了风格转移。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</div>
<div class="meta-line">Authors: Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</div>
<div class="meta-line">First: 2026-01-06T11:00:17+00:00 · Latest: 2026-01-06T11:00:17+00:00</div>
<div class="meta-line">Comments: Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02918v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ethanliang99.github.io/ZOOMIQA-Projectpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Zoom-IQA：具有可靠区域感知推理的图像质量评估</div>
<div class="mono" style="margin-top:8px">图像质量评估（IQA）是计算机视觉中的一个长期问题。以往的方法通常侧重于预测数值评分而没有解释，或提供缺乏精确评分的低级描述。最近，基于推理的视觉语言模型（VLMs）在IQA中显示出强大的潜力，能够联合生成质量描述和评分。然而，我们注意到现有的基于VLM的IQA方法往往表现出不可靠的推理，因为它们在整合视觉和文本线索方面能力有限。在本研究中，我们引入了Zoom-IQA，一个基于VLM的IQA模型，明确模拟关键的认知行为：不确定性意识、区域推理和迭代精炼。具体而言，我们提出了一个两阶段的训练流程：1）在我们的Grounded-Rationale-IQA（GR-IQA）数据集上进行监督微调（SFT），以教会模型将其评估基于关键区域；2）进行强化学习（RL）以动态探索策略，主要通过我们的KL-Coverage正则化器稳定，以防止推理和评分多样性崩溃，并通过渐进重采样策略来减轻注释偏差。大量实验表明，Zoom-IQA在鲁棒性、可解释性和泛化能力上都有所提升。对下游任务（如图像修复）的应用进一步证明了Zoom-IQA的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing image quality assessment (IQA) methods, which often lack reliable reasoning and precise scoring. The authors propose Zoom-IQA, a vision language model (VLM)-based approach that incorporates cognitive behaviors such as uncertainty awareness and region reasoning through a two-stage training process. Experimental results indicate that Zoom-IQA demonstrates enhanced robustness, explainability, and generalization in IQA tasks, and its effectiveness is further validated through applications in downstream tasks like image restoration.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有图像质量评估（IQA）方法的局限性，这些方法通常缺乏可靠的推理和精确的评分。作者提出了Zoom-IQA，这是一种基于视觉语言模型（VLM）的方法，通过两阶段的训练过程，结合了不确定性意识和区域推理等认知行为。实验结果表明，Zoom-IQA在稳健性、可解释性和泛化能力方面优于以往的方法，其在图像修复等下游任务中的应用进一步验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">IPA: An Information-Reconstructive Input Projection Framework for Efficient Foundation Model Adaptation</div>
<div class="meta-line">Authors: Yuan Yin, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Matthieu Cord</div>
<div class="meta-line">First: 2025-09-04T17:10:01+00:00 · Latest: 2026-01-06T09:17:16+00:00</div>
<div class="meta-line">Comments: Accepted to TMLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04398v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.04398v3">PDF</a> · <a href="https://github.com/valeoai/peft-ipa">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA&#x27;s down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly aims to reconstruct the original input within a reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen. Code available at https://github.com/valeoai/peft-ipa .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IPA：一种信息重构输入投影框架，用于高效的基础模型适应</div>
<div class="mono" style="margin-top:8px">参数高效微调（PEFT）方法，如LoRA，通过将低秩更新注入预训练权重来降低适应成本。然而，LoRA的下投影是随机初始化且与数据无关，丢弃了潜在有用的信息。先前的分析表明，这种投影在训练过程中变化不大，而上投影承载了大部分适应，使得随机输入压缩成为性能瓶颈。我们提出了IPA，一种特征感知投影框架，明确旨在在减少的隐藏空间中重构原始输入。在线性情况下，我们用近似主成分的算法实例化IPA，实现了高效的投影器预训练，几乎没有推理开销。在语言和视觉基准测试中，IPA始终优于LoRA和DoRA，在常识推理上平均提高1.5分，在VTAB-1k上提高2.3分，同时在投影冻结时以大约一半的可训练参数匹配完整的LoRA性能。代码可在https://github.com/valeoai/peft-ipa获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing parameter-efficient fine-tuning methods like LoRA, which suffer from a performance bottleneck due to their randomly initialized and data-agnostic down-projection. The authors propose the Information-Reconstructive Input Projection (IPA) framework, which focuses on reconstructing the original input in a reduced hidden space using algorithms that approximate top principal components. Experimental results demonstrate that IPA outperforms LoRA and DoRA across various language and vision benchmarks, achieving an average accuracy improvement of 1.5 points on commonsense reasoning tasks and 2.3 points on VTAB-1k, while maintaining comparable performance to full LoRA with approximately half the trainable parameters when the projection is frozen.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有参数高效微调方法（如LoRA）的局限性，这些方法的随机和与数据无关的下投影可能忽视有价值的信息。作者提出了IPA，这是一种特征感知投影框架，旨在在减少的隐藏空间中重建原始输入，利用近似主成分的算法进行高效的投影器预训练。实验结果表明，IPA在各种语言和视觉基准测试中优于LoRA和DoRA，在常识推理上平均提高了1.5个百分点，在VTAB-1k上提高了2.3个百分点，同时在冻结投影时以大约一半的可训练参数保持与完整LoRA相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection</div>
<div class="meta-line">Authors: Yuetong Li, Qing Zhang, Yilin Zhao, Gongyang Li, Zeming Liu</div>
<div class="meta-line">First: 2026-01-06T09:04:23+00:00 · Latest: 2026-01-06T09:04:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02831v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02831v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&quot; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DGA-Net：通过深度提示和图锚引导增强SAM以进行伪装物体检测</div>
<div class="mono" style="margin-top:8px">为了充分利用伪装物体检测（COD）中的深度线索，我们提出了DGA-Net，这是一种通过新颖的“深度提示”范式调整Segment Anything Model（SAM）的专用框架。与主要依赖稀疏提示（例如点或框）的现有方法不同，我们的方法引入了一种构建和传播密集深度提示的整体机制。具体而言，我们提出了一个跨模态图增强（CGE）模块，该模块在异构图中合成RGB语义和深度几何，以形成统一的引导信号。此外，我们设计了一个锚引导细化（AGR）模块。为了抵消特征层次中固有的信息衰减，AGR锻造了一个全局锚点，并建立了直接的非局部路径，以从深层到浅层传播这一引导，确保精确和一致的分割。定量和定性实验结果表明，我们提出的DGA-Net优于最先进的COD方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance camouflaged object detection (COD) by effectively utilizing depth cues. The authors introduce DGA-Net, a framework that modifies the Segment Anything Model (SAM) using a novel depth prompting approach, which differs from traditional methods that depend on sparse prompts. The method includes a Cross-modal Graph Enhancement module that integrates RGB semantics with depth information within a heterogeneous graph, and an Anchor-Guided Refinement module that maintains information integrity across feature hierarchies. Experimental results indicate that DGA-Net significantly outperforms existing state-of-the-art COD techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过有效利用深度线索来增强伪装物体检测（COD）。作者提出了DGA-Net，这是一个通过新颖的深度提示方法修改Segment Anything Model (SAM)的框架，该方法不同于传统的使用稀疏提示的技术。该方法包括一个跨模态图增强模块，将RGB语义与深度信息结合，以及一个锚引导细化模块，保持特征层次中的信息完整性。实验结果表明，DGA-Net显著优于现有的最先进COD技术。</div>
</details>
</div>
<div class="card">
<div class="title">How Many Images Does It Take? Estimating Imitation Thresholds in Text-to-Image Models</div>
<div class="meta-line">Authors: Sahil Verma, Royi Rassin, Arnav Das, Gantavya Bhatt, Preethi Seshadri, Chirag Shah, Jeff Bilmes, Hannaneh Hajishirzi, Yanai Elazar</div>
<div class="meta-line">Venue: NeurIPS 2024</div>
<div class="meta-line">First: 2024-10-19T06:28:14+00:00 · Latest: 2026-01-06T08:26:13+00:00</div>
<div class="meta-line">Comments: Accepted at TMLR 2025, ATTRIB, RegML, and SafeGenAI workshops at NeurIPS 2024 and NLLP Workshop 2024. https://openreview.net/forum?id=x0qJo7SPhs</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15002v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.15002v2">PDF</a> · <a href="https://github.com/vsahil/MIMETIC-2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://how-many-van-goghs-does-it-take.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image models are trained using large datasets of image-text pairs collected from the internet. These datasets often include copyrighted and private images. Training models on such datasets enables them to generate images that might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we estimate the point at which a model was trained on enough instances of a concept to be able to imitate it -- the imitation threshold. We posit this question as a new problem and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training these models from scratch. We experiment with two domains -- human faces and art styles, and evaluate four text-to-image models that were trained on three pretraining datasets. We estimate the imitation threshold of these models to be in the range of 200-700 images, depending on the domain and the model. The imitation threshold provides an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. Website: https://how-many-van-goghs-does-it-take.github.io/. Code: https://github.com/vsahil/MIMETIC-2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>需要多少图像？估计文本到图像模型的模仿阈值</div>
<div class="mono" style="margin-top:8px">文本到图像模型使用从互联网收集的大量图像-文本对数据集进行训练。这些数据集通常包含受版权保护和私人图像。在这些数据集上训练模型使其能够生成可能违反版权法和个人隐私的图像。这种现象被称为模仿——生成与其训练图像具有可识别相似性的内容的图像。在这项工作中，我们估计模型在足够实例的概念上进行训练以能够模仿的点——模仿阈值。我们将这个问题视为一个新问题，并提出一种有效的方法来估计模仿阈值，而无需承担从头开始训练这些模型的巨大成本。我们在两个领域进行实验——人脸和艺术风格，并评估在三个预训练数据集上训练的四个文本到图像模型。我们估计这些模型的模仿阈值在200-700图像的范围内，具体取决于领域和模型。模仿阈值为版权侵犯索赔提供了实证基础，并作为旨在遵守版权和隐私法的文本到图像模型开发者的指导原则。网站：https://how-many-van-goghs-does-it-take.github.io/。代码：https://github.com/vsahil/MIMETIC-2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the issue of copyright violation and privacy concerns in text-to-image models, which are trained on large datasets that may include copyrighted content. The authors propose a method to estimate the imitation threshold, defined as the minimum number of training images required for a model to generate content that resembles its training data. By experimenting with human faces and art styles across four text-to-image models trained on three different datasets, they find that the imitation threshold ranges from 200 to 700 images, varying by domain and model. This threshold serves as a practical guideline for developers to navigate copyright and privacy regulations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决因使用受版权保护和私人图像训练文本到图像模型而引发的版权侵犯和隐私问题。作者提出了一种方法来估计模仿阈值，即模型看到足够多的概念实例以进行模仿的点，而无需进行大量的重新训练。通过在人脸和艺术风格两个领域的实验，他们发现所评估模型的模仿阈值在200到700幅图像之间，具体取决于领域和模型，为理解版权影响提供了基础，并指导模型开发者遵守法律标准。</div>
</details>
</div>
<div class="card">
<div class="title">HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion</div>
<div class="meta-line">Authors: Jiahang Li, Peng Yun, Yang Xu, Ye Zhang, Mingjian Sun, Qijun Chen, Ilin Alexander, Rui Fan</div>
<div class="meta-line">First: 2024-04-04T15:31:11+00:00 · Latest: 2026-01-06T08:10:08+00:00</div>
<div class="meta-line">Comments: 16 pages, 4 figures. Accepted to the Biomimetic Intelligence and Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.03527v3">Abs</a> · <a href="https://arxiv.org/pdf/2404.03527v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data-fusion networks have shown significant promise for RGB-thermal scene parsing. However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities. Recent progress in vision foundation models (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features. However, this potential has yet to be fully leveraged in the domain. In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing. Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a convolutional neural network. This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner. Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing. Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets. We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAPNet：通过混合、非对称和渐进的异构特征融合实现优越的RGB-热场景解析</div>
<div class="mono" style="margin-top:8px">数据融合网络在RGB-热场景解析中展现出显著的潜力。然而，现有研究大多依赖对称双重编码器进行异构特征提取和融合，未能充分关注RGB和热模态之间的固有差异。最近在通过自监督在大量未标记数据上训练的视觉基础模型（VFM）方面的进展证明了其提取信息性、通用特征的能力。然而，这一潜力在该领域尚未得到充分利用。在本研究中，我们朝着这一新研究领域迈出了一步，探索了一种可行的策略，以充分利用VFM特征进行RGB-热场景解析。具体而言，我们深入研究RGB和热模态的独特特性，从而设计了一种混合、非对称编码器，结合了VFM和卷积神经网络。这一设计允许更有效地提取互补的异构特征，随后以双路径、渐进的方式进行融合。此外，我们引入了一项辅助任务，以进一步丰富融合特征的局部语义，从而提高RGB-热场景解析的整体性能。我们提出的HAPNet配备了所有这些组件，表现出优于所有其他最先进的RGB-热场景解析网络的性能，在三个广泛使用的公共RGB-热场景解析数据集上取得了最高排名。我们相信这一新范式为未来数据融合场景解析方法的发展开辟了新的机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing data-fusion networks for RGB-thermal scene parsing, which often utilize symmetric duplex encoders and overlook the differences between RGB and thermal modalities. The authors propose HAPNet, a hybrid, asymmetric encoder that combines a vision foundation model with a convolutional neural network to enhance the extraction of heterogeneous features. Experimental results show that HAPNet outperforms state-of-the-art networks on three public RGB-thermal scene parsing datasets, indicating its effectiveness in improving scene parsing performance through innovative feature fusion techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有对称双重编码器的局限性来增强RGB-热场景解析，这些编码器未能充分考虑RGB和热模态之间的差异。作者提出了HAPNet，这是一种混合的非对称编码器，结合了视觉基础模型（VFM）和卷积神经网络，以有效提取和融合异构特征，采用双路径渐进方式。实验结果表明，HAPNet在三种广泛使用的公共数据集上超越了最先进的RGB-热场景解析网络，显示其在推动数据融合场景解析方法方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">DreamStyle: A Unified Framework for Video Stylization</div>
<div class="meta-line">Authors: Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He</div>
<div class="meta-line">First: 2026-01-06T07:42:12+00:00 · Latest: 2026-01-06T07:42:12+00:00</div>
<div class="meta-line">Comments: Github Page: https://lemonsky1995.github.io/dreamstyle/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lemonsky1995.github.io/dreamstyle/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamStyle：视频风格化的统一框架</div>
<div class="mono" style="margin-top:8px">视频风格化是视频生成模型的重要下游任务，但尚未得到深入探索。其输入风格条件通常包括文本、风格图像和风格化的第一帧。每种条件都有其特定优势：文本更灵活，风格图像提供更准确的视觉锚点，而风格化的第一帧使长视频风格化成为可能。然而，现有方法大多局限于单一类型的风格条件，限制了其应用范围。此外，缺乏高质量的数据集导致风格不一致和时间闪烁。为了解决这些限制，我们提出了DreamStyle，一个统一的视频风格化框架，支持（1）文本引导、（2）风格图像引导和（3）第一帧引导的视频风格化，并配备精心设计的数据整理管道以获取高质量的配对视频数据。DreamStyle基于一个基础的图像到视频（I2V）模型，并使用低秩适应（LoRA）进行训练，采用特定于令牌的上升矩阵，减少不同条件令牌之间的混淆。定性和定量评估均表明，DreamStyle在所有三种视频风格化任务中表现出色，并在风格一致性和视频质量上优于竞争对手。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video stylization, a crucial aspect of video generation models, which has not been extensively studied. The authors propose DreamStyle, a unified framework that integrates multiple style conditions—text, style images, and stylized first frames—addressing the limitations of existing methods that typically focus on a single condition. The framework utilizes a vanilla Image-to-Video model and employs Low-Rank Adaptation for training, resulting in improved style consistency and video quality, as demonstrated through both qualitative and quantitative evaluations, where it outperforms existing competitors in all three stylization tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强视频风格化，这是视频生成模型的重要方面，但现有方法主要集中于单一风格条件，限制了其应用范围。作者提出了DreamStyle，一个统一的框架，支持文本、风格图像和首帧指导的视频风格化，同时实施数据策划管道以确保高质量的配对视频数据。实验结果表明，DreamStyle在所有三种风格化任务中表现出色，在风格一致性和视频质量方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning</div>
<div class="meta-line">Authors: Jiahe Song, Chuang Wang, Bowen Jiang, Yinfan Wang, Hao Zheng, Xingjian Wei, Chengjin Liu, Rui Nie, Junyuan Gao, Jiaxing Sun, Yubin Wang, Lijun Wu, Zhenhua Huang, Jiang Wu, Qian Yu, Conghui He</div>
<div class="meta-line">First: 2025-11-04T09:08:44+00:00 · Latest: 2026-01-06T02:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02384v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02384v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision Language Models (LVLMs) handle naturally. We introduce a strategy termed BBox and Index as Visual Prompt (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-15k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RxnCaption：将反应图解析重新表述为视觉提示引导的字幕生成</div>
<div class="mono" style="margin-top:8px">大规模化学反应数据集对化学领域的人工智能研究至关重要。然而，现有的化学反应数据通常以图像形式存在于论文中，使其无法被机器读取，也无法用于训练机器学习模型。为应对这一挑战，我们提出了RxnCaption框架，用于化学反应图解析（RxnDP）任务。我们的框架将传统的坐标预测驱动的解析过程重新表述为图像字幕生成问题，这一问题被大型视觉语言模型（LVLMs）自然处理。我们引入了一种称为BBox和Index作为视觉提示（BIVP）的策略，该策略利用我们最先进的分子检测器MolYOLO，直接在输入图像上预绘制分子边界框和索引。这将下游解析转变为自然语言描述问题。大量实验表明，BIVP策略显著提高了结构提取质量，同时简化了模型设计。我们进一步构建了RxnCaption-15k数据集，其规模比之前的真实世界文献基准大一个数量级，并在四种布局原型上具有平衡的测试子集。实验表明，RxnCaption-VL在多个指标上达到了最先进的性能。我们相信我们的方法、数据集和模型将推动化学文献中的结构信息提取，并催化化学领域更广泛的人工智能应用。我们将在GitHub上发布数据、模型和代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of extracting chemical reaction data from images in academic papers, which are not machine-readable and hinder the training of machine learning models. The authors propose the RxnCaption framework, which reformulates chemical Reaction Diagram Parsing into an image captioning problem using Large Vision Language Models (LVLMs). They introduce the BBox and Index as Visual Prompt (BIVP) strategy, utilizing the MolYOLO molecular detector to create bounding boxes and indices on input images, significantly enhancing structural extraction quality while simplifying model design. Extensive experiments demonstrate that the RxnCaption-VL achieves state-of-the-art performance across multiple metrics, supported by the creation of the RxnCaption-15k dataset, which is significantly larger than previous benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决从学术论文中的图像中提取化学反应数据的挑战，这些图像不可机器读取，阻碍了化学领域AI的进步。作者提出了RxnCaption框架，将化学反应图解析重新构建为图像描述任务，利用大型视觉语言模型。研究中引入了一种新策略，称为BBox和Index作为视觉提示（BIVP），利用分子检测器通过在图像上预绘制分子边界框来增强解析。实验结果表明，BIVP策略显著提高了结构提取的质量并简化了模型设计，RxnCaption-VL在多个指标上实现了最先进的性能，同时创建了一个大型数据集RxnCaption-15k，以支持进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning</div>
<div class="meta-line">Authors: Bo Jiang, Weijun Zhao, Beibei Wang, Jin Tang</div>
<div class="meta-line">First: 2026-01-06T02:22:04+00:00 · Latest: 2026-01-06T02:22:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node&#x27;s all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当提示遇上脉冲：通过脉冲图提示学习进行图稀疏提示</div>
<div class="mono" style="margin-top:8px">图提示特征（GPF）学习已广泛应用于将预训练的GNN模型适应于下游任务。GPF首先引入一些提示原子，然后使用提示原子的线性组合为每个图节点学习最佳提示向量。然而，现有的GPF通常在节点的所有特征维度上进行提示，这显然是冗余的，并且对节点特征噪声敏感。为了解决这个问题，本文首次提出通过利用脉冲神经元机制学习稀疏图提示，称为脉冲图提示特征（SpikingGPF）。我们的方法的动机是观察到脉冲神经元可以进行低成本的信息处理并产生稀疏输出，这自然适合我们的图稀疏提示任务。具体而言，SpikingGPF有两个主要方面。首先，它通过利用脉冲神经元架构为每个节点学习稀疏提示向量，从而在选择性节点特征上进行提示。这产生了更紧凑和轻量的提示设计，同时提高了对节点噪声的鲁棒性。其次，SpikingGPF引入了一种基于稀疏表示理论的新型提示表示学习模型，即将每个节点提示表示为提示原子的稀疏组合。这鼓励了更紧凑的表示，并促进了高效计算。在多个基准上的广泛实验证明了SpikingGPF的有效性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing Graph Prompt Feature (GPF) learning methods, which often process all feature dimensions of graph nodes, leading to redundancy and sensitivity to noise. The authors propose a novel approach called Spiking Graph Prompt Feature (SpikingGPF), which utilizes a spiking neuron mechanism to learn sparse prompt vectors for each node, allowing for selective feature prompting. Experimental results on various benchmarks show that SpikingGPF achieves a more compact and robust prompting design, significantly improving performance against node feature noise while facilitating efficient computation through sparse representation.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有的图提示特征（GPF）学习方法的局限性，这些方法通常处理图节点的所有特征维度，导致冗余和对噪声的敏感性。作者提出了一种新方法，称为脉冲图提示特征（SpikingGPF），利用脉冲神经元机制为每个节点学习稀疏提示向量，从而允许对节点特征进行选择性提示。实验结果表明，SpikingGPF不仅提供了更紧凑和轻量的提示设计，还增强了对节点噪声的鲁棒性，在多个基准测试中证明了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation</div>
<div class="meta-line">Authors: Yongmin Yoo, Qiongkai Xu, Longbing Cao</div>
<div class="meta-line">First: 2025-05-25T22:28:27+00:00 · Latest: 2026-01-05T22:59:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19347v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.19347v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Patent similarity evaluation plays a critical role in intellectual property analysis. However, existing methods often overlook the intricate structure of patent documents, which integrate technical specifications, legal boundaries, and application contexts. We introduce PatentMind, a novel framework for patent similarity assessment based on a Multi-Aspect Reasoning Graph (MARG). PatentMind decomposes patents into their three dimensions of technical features, application domains, and claim scopes, then dimension-specific similarity scores are calculated over the MARG. These scores are dynamically weighted through a context-aware reasoning process, which integrates contextual signals to emulate expert-level judgment. To support evaluation, we construct a human-annotated benchmark PatentSimBench, comprising 500 patent pairs. Experimental results demonstrate that the PatentMind-generated scores show a strong correlation ($r=0.938$) with expert annotations, significantly outperforming embedding-based models, patent-specific models, and advanced prompt engineering methods. Beyond computational linguistics, our framework provides a structured and semantically grounded foundation for real-world decision-making, particularly for tasks such as infringement risk assessment, underscoring its broader impact on both patent analytics and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatentMind：一种用于专利相似性评估的多方面推理图</div>
<div class="mono" style="margin-top:8px">专利相似性评估在知识产权分析中发挥着关键作用。然而，现有方法往往忽视专利文档的复杂结构，这些文档整合了技术规范、法律边界和应用背景。我们介绍了PatentMind，这是一种基于多方面推理图（MARG）的新型专利相似性评估框架。PatentMind将专利分解为技术特征、应用领域和权利要求范围三个维度，然后在MARG上计算特定维度的相似性得分。这些得分通过上下文感知推理过程动态加权，整合上下文信号以模拟专家级判断。为了支持评估，我们构建了一个人工标注的基准数据集PatentSimBench，包含500对专利。实验结果表明，PatentMind生成的得分与专家标注之间具有强相关性（$r=0.938$），显著优于基于嵌入的模型、专利特定模型和先进的提示工程方法。除了计算语言学，我们的框架为现实世界的决策提供了结构化和语义基础，特别适用于侵权风险评估等任务，突显了其在专利分析和评估中的更广泛影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve patent similarity evaluation, which is essential for intellectual property analysis but often inadequately addressed by existing methods that fail to consider the complex structure of patent documents. The authors propose PatentMind, a framework utilizing a Multi-Aspect Reasoning Graph (MARG) that breaks down patents into three dimensions: technical features, application domains, and claim scopes, calculating dimension-specific similarity scores that are dynamically weighted through a context-aware reasoning process. Experimental results indicate that the similarity scores generated by PatentMind correlate strongly with expert annotations (r=0.938) and significantly outperform traditional embedding-based models, patent-specific models, and advanced prompt engineering methods, highlighting its potential for real-world applications in patent analytics and evaluation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善专利相似性评估，这对知识产权分析至关重要，但现有方法往往未能充分捕捉专利文档的复杂结构。作者提出了PatentMind，一个利用多方面推理图（MARG）的框架，将专利分解为三个维度：技术特征、应用领域和权利要求范围，并计算维度特定的相似性分数，这些分数通过上下文感知推理过程进行调整。实验结果表明，PatentMind生成的相似性分数与专家注释之间的相关性很强（r=0.938），并显著优于传统的嵌入模型、专利特定模型和先进的提示工程方法，突显了其在专利分析和评估中的潜在现实应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Topological Perspectives on Optimal Multimodal Embedding Spaces</div>
<div class="meta-line">Authors: Abdul Aziz A. B, A. B Abdul Rahim</div>
<div class="meta-line">First: 2024-05-29T08:28:23+00:00 · Latest: 2026-01-05T22:57:40+00:00</div>
<div class="meta-line">Comments: This manuscript contains substantive technical inaccuracies and an incomplete treatment of the stated topic. Subsequent developments and a reassessment of the problem indicate that the scope and framing of the work do not adequately reflect the current state of research, and the analysis is therefore incomplete and outdated</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.18867v2">Abs</a> · <a href="https://arxiv.org/pdf/2405.18867v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent strides in multimodal model development have ignited a paradigm shift in the realm of text-to-image generation. Among these advancements, CLIP stands out as a remarkable achievement which is a sophisticated autoencoder adept at encoding both textual and visual information within a unified latent space. This paper delves into a comparative analysis between CLIP and its recent counterpart, CLOOB. To unravel the intricate distinctions within the embedding spaces crafted by these models, we employ topological data analysis. Our approach encompasses a comprehensive examination of the modality gap drivers, the clustering structures existing across both high and low dimensions, and the pivotal role that dimension collapse plays in shaping their respective embedding spaces. Empirical experiments substantiate the implications of our analyses on downstream performance across various contextual scenarios. Through this investigation, we aim to shed light on the nuanced intricacies that underlie the comparative efficacy of CLIP and CLOOB, offering insights into their respective strengths and weaknesses, and providing a foundation for further refinement and advancement in multimodal model research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于最优多模态嵌入空间的拓扑视角</div>
<div class="mono" style="margin-top:8px">最近多模态模型的发展取得了重大进展，点燃了文本到图像生成领域的范式转变。在这些进展中，CLIP作为一种复杂的自编码器，能够在统一的潜在空间中编码文本和视觉信息，脱颖而出。本文深入分析了CLIP与其最近的对手CLOOB之间的比较。为了揭示这些模型所构建的嵌入空间中的复杂差异，我们采用了拓扑数据分析。我们的方法包括对模态差距驱动因素的全面检查、在高维和低维中存在的聚类结构，以及维度崩溃在塑造各自嵌入空间中的关键作用的研究。实证实验证实了我们分析对各种上下文场景下下游性能的影响。通过这项研究，我们旨在阐明CLIP和CLOOB之间比较有效性的细微复杂性，提供对它们各自优缺点的见解，并为多模态模型研究的进一步完善和发展奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the distinctions in embedding spaces between the multimodal models CLIP and CLOOB, which have emerged from recent advancements in text-to-image generation. The authors employ topological data analysis to investigate the modality gap drivers, clustering structures, and the impact of dimension collapse on the embedding spaces of both models. The experimental findings reveal significant differences in performance across various contexts, highlighting the strengths and weaknesses of CLIP and CLOOB, and providing insights for future improvements in multimodal model development.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索多模态模型的进展，特别是聚焦于CLIP和CLOOB在文本到图像生成中的比较。作者利用拓扑数据分析来研究这些模型嵌入空间的差异，考察模态差距驱动因素和不同维度下的聚类结构等因素。关键实验结果揭示了维度崩溃如何影响嵌入空间，并展示了这些差异对模型在各种上下文中的性能影响，最终为未来多模态模型的发展提供了对其优缺点的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Annealed Langevin Posterior Sampling (ALPS): A Rapid Algorithm for Image Restoration with Multiscale Energy Models</div>
<div class="meta-line">Authors: Jyothi Rikhab Chand, Mathews Jacob</div>
<div class="meta-line">First: 2026-01-05T22:53:23+00:00 · Latest: 2026-01-05T22:53:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02594v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02594v1">PDF</a> · <a href="https://github.com/JyoChand/ALPS}{ALPS">Code1</a> · <a href="https://github.com/JyoChand/ALPS">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving inverse problems in imaging requires models that support efficient inference, uncertainty quantification, and principled probabilistic reasoning. Energy-Based Models (EBMs), with their interpretable energy landscapes and compositional structure, are well-suited for this task but have historically suffered from high computational costs and training instability. To overcome the historical shortcomings of EBMs, we introduce a fast distillation strategy to transfer the strengths of pre-trained diffusion models into multi-scale EBMs. These distilled EBMs enable efficient sampling and preserve the interpretability and compositionality inherent to potential-based frameworks. Leveraging EBM compositionality, we propose Annealed Langevin Posterior Sampling (ALPS) algorithm for Maximum-A-Posteriori (MAP), Minimum Mean Square Error (MMSE), and uncertainty estimates for inverse problems in imaging. Unlike diffusion models that use complex guidance strategies for latent variables, we perform annealing on static posterior distributions that are well-defined and composable. Experiments on image inpainting and MRI reconstruction demonstrate that our method matches or surpasses diffusion-based baselines in both accuracy and efficiency, while also supporting MAP recovery. Overall, our framework offers a scalable and principled solution for inverse problems in imaging, with potential for practical deployment in scientific and clinical settings. ALPS code is available at the GitHub repository \href{https://github.com/JyoChand/ALPS}{ALPS}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>退火朗之万后验采样（ALPS）：一种用于多尺度能量模型图像恢复的快速算法</div>
<div class="mono" style="margin-top:8px">解决成像中的逆问题需要支持高效推理、不确定性量化和原则性概率推理的模型。基于能量的模型（EBMs）因其可解释的能量景观和组合结构，非常适合此任务，但历史上受到高计算成本和训练不稳定性的困扰。为克服EBMs的历史缺陷，我们引入了一种快速蒸馏策略，将预训练扩散模型的优势转移到多尺度EBMs中。这些蒸馏的EBMs能够高效采样，并保留潜在基础框架固有的可解释性和组合性。利用EBM的组合性，我们提出了用于成像逆问题的最大后验（MAP）、最小均方误差（MMSE）和不确定性估计的退火朗之万后验采样（ALPS）算法。与使用复杂引导策略的扩散模型不同，我们对静态后验分布进行退火，这些分布是明确定义且可组合的。在图像修复和MRI重建的实验中，我们的方法在准确性和效率上与基于扩散的基线相匹配或超越，同时也支持MAP恢复。总体而言，我们的框架为成像中的逆问题提供了一种可扩展且原则性的解决方案，具有在科学和临床环境中实际部署的潜力。ALPS代码可在GitHub仓库\href{https://github.com/JyoChand/ALPS}{ALPS}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of efficient inference and uncertainty quantification in imaging inverse problems using Energy-Based Models (EBMs), which have been hindered by high computational costs and training instability. The authors introduce a fast distillation strategy that integrates pre-trained diffusion models into multi-scale EBMs, leading to the development of the Annealed Langevin Posterior Sampling (ALPS) algorithm. Experimental results on image inpainting and MRI reconstruction show that ALPS achieves comparable or superior accuracy and efficiency compared to diffusion-based methods while also providing Maximum-A-Posteriori recovery, thus presenting a scalable solution for practical applications in scientific and clinical imaging.</div>
<div class="mono" style="margin-top:8px">本研究解决了成像中的逆问题所面临的挑战，这些问题需要高效的推理和不确定性量化。作者提出了一种名为退火朗之万后验采样（ALPS）的新算法，该算法利用快速蒸馏策略将预训练扩散模型的优势整合到多尺度能量模型（EBM）中。对图像修复和MRI重建的实验结果表明，ALPS在准确性和效率上与基于扩散的方法相当或更优，同时还支持最大后验恢复，从而为成像的实际应用提供了一种可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models</div>
<div class="meta-line">Authors: Samuel Lavoie, Michael Noukhovitch, Aaron Courville</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-07-16T15:12:17+00:00 · Latest: 2026-01-05T21:20:00+00:00</div>
<div class="meta-line">Comments: Published at NeurIPS, 22 pages, 7 tables, 12 figures, code and models available</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12318v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.12318v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We argue that diffusion models&#x27; success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高保真、高效能扩散模型的组合离散潜在编码</div>
<div class="mono" style="margin-top:8px">我们认为，扩散模型在建模复杂分布方面的成功主要来自于其输入条件。本文从理想表示应提高样本保真度、易于生成且具有组合性以允许生成训练外样本的角度，研究了用于条件扩散模型的表示。我们引入了离散潜在编码（DLC），这是一种基于自监督学习目标训练的简单嵌入生成的图像表示。DLC是离散标记的序列，而不是标准的连续图像嵌入。它们易于生成，其组合性使得能够在训练分布之外采样新图像。使用DLC训练的扩散模型提高了生成保真度，确立了在ImageNet上无条件图像生成的新最先进水平。此外，我们展示了组合DLC如何使图像生成器以多种方式连贯地结合图像的语义，生成训练外样本。最后，我们展示了DLC如何通过利用大规模预训练语言模型来实现文本到图像的生成。我们有效地微调了一个文本扩散语言模型，以生成DLC，从而产生训练分布之外的新样本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the fidelity and compositionality of diffusion models used for image generation. The authors introduce a novel representation called Discrete Latent Code (DLC), which is derived from Simplicial Embeddings and utilizes a self-supervised learning objective, allowing for easy generation of discrete token sequences. The experimental results demonstrate that diffusion models trained with DLCs achieve improved generation fidelity, setting a new state-of-the-art for unconditional image generation on ImageNet, and enabling the generation of out-of-distribution samples that combine image semantics in innovative ways, as well as facilitating text-to-image generation through fine-tuning a text diffusion language model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高用于图像生成的扩散模型的保真度和组合性。作者引入了一种新颖的表示方法，称为离散潜在编码（DLC），它由通过自监督学习目标训练的简单嵌入派生的离散标记序列组成。主要发现表明，使用DLC的扩散模型在图像生成保真度方面取得了改进，在ImageNet上设定了无条件图像生成的新状态，并通过组合DLC实现了生成分布外样本的能力，同时通过微调文本扩散语言模型促进了文本到图像的生成。</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-05T18:56:34+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：一个统一的视觉生成器，具有交错的全模态上下文</div>
<div class="mono" style="margin-top:8px">我们提出了VINO，一个统一的视觉生成器，在一个框架内执行图像和视频的生成与编辑。VINO不依赖于特定任务的模型或每种模态的独立模块，而是使用一个共享的扩散骨干网络，基于文本、图像和视频进行条件化，从而在一个模型下实现广泛的视觉创作和编辑任务。具体而言，VINO将视觉-语言模型（VLM）与多模态扩散变换器（MMDiT）结合，其中多模态输入被编码为交错的条件令牌，然后用于指导扩散过程。该设计支持多参考基础、长格式指令跟随，以及在静态和动态内容中保持一致的身份，同时避免特定模态的架构组件。为了训练这样一个统一的系统，我们引入了一个多阶段训练管道，逐步将视频生成基础模型扩展为一个统一的多任务生成器，能够处理图像和视频的输入和输出。在多样的生成和编辑基准测试中，VINO展示了强大的视觉质量、忠实的指令跟随、改进的参考和属性保留，以及更可控的多身份编辑。我们的结果突显了可扩展统一视觉生成的实际路径，以及交错的上下文计算作为通用视觉创作基础的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to create a unified visual generator capable of handling both image and video generation and editing tasks within a single framework. The authors developed VINO, which employs a shared diffusion backbone that integrates multimodal inputs—text, images, and videos—using interleaved conditioning tokens to guide the diffusion process. Experimental results show that VINO achieves high visual quality, effectively follows complex instructions, preserves references and attributes, and allows for controllable edits across multiple identities, demonstrating its potential for scalable unified visual generation.</div>
<div class="mono" style="margin-top:8px">VINO的开发动机是创建一个统一的视觉生成器，能够在单一框架内处理图像和视频的生成与编辑，从而消除对特定任务模型的需求。该方法结合了视觉语言模型和多模态扩散变换器，将多模态输入编码为交错的条件标记，以引导扩散过程。主要实验结果表明，VINO在各种生成和编辑基准测试中实现了强大的视觉质量、准确的指令跟随、改进的参考和属性保留，以及对多身份编辑的增强控制。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?</div>
<div class="meta-line">Authors: Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le</div>
<div class="meta-line">Venue: NeurIPS 2024</div>
<div class="meta-line">First: 2025-12-26T07:40:11+00:00 · Latest: 2026-01-05T18:55:51+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, 4 tables; NeurIPS 2024 format</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21907v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialBench：代理能否分析真实世界的空间生物数据？</div>
<div class="mono" style="margin-top:8px">空间转录组学检测的规模和复杂性迅速增加，使得计算分析成为生物发现的主要瓶颈。尽管前沿AI代理在软件工程和一般数据分析方面有了显著改善，但尚不清楚它们是否能够从混乱的真实世界空间数据集中提取生物学见解。我们介绍了SpatialBench，这是一个基于来自五种空间技术和七个任务类别的实际空间分析工作流程衍生的146个可验证问题的基准。每个问题提供了分析步骤之前的实验数据快照和一个确定性的评分器，用于评估关键生物结果的恢复。关于前沿模型的基准数据表明，基础模型的准确性仍然较低（各模型系列之间为20-38%），并且存在强烈的模型-任务和模型-平台交互。设计的工具对性能有很大的实证影响，这表明工具、提示、控制流程和执行环境应作为一流对象进行评估和改进。SpatialBench既是一个测量工具，也是一个诊断视角，用于开发能够真实、透明和可重复地与真实空间数据集交互的代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational challenges posed by the increasing scale and complexity of spatial transcriptomics assays in biological discovery. The authors introduce SpatialBench, a benchmark comprising 146 verifiable problems derived from real-world spatial analysis workflows across various technologies and task categories. Experimental results reveal that the accuracy of frontier AI models remains low, ranging from 20-38%, with significant interactions between model-task and model-platform, highlighting the need for improved design in tools and methodologies for better performance in analyzing spatial datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是空间转录组检测的复杂性不断增加，这导致生物发现中的计算分析成为瓶颈。作者介绍了SpatialBench，这是一个基于各种技术和任务类别的真实空间分析工作流程的146个可验证问题的基准。实验结果显示，前沿AI模型在解决这些问题上的准确率较低，范围为20-38%，并强调了模型任务和模型平台之间的显著交互，表明工具和提示的设计对性能有重要影响，未来发展中应优先考虑。</div>
</details>
</div>
<div class="card">
<div class="title">DARC: Drum accompaniment generation with fine-grained rhythm control</div>
<div class="meta-line">Authors: Trey Brosnan</div>
<div class="meta-line">First: 2026-01-05T18:55:43+00:00 · Latest: 2026-01-05T18:55:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARC：具有细粒度节奏控制的鼓伴奏生成</div>
<div class="mono" style="margin-top:8px">在音乐创作中，快速原型制作对于探索和完善想法至关重要，但现有的生成工具在用户需要结构控制和风格灵活性时往往不够理想。之前的干声到干声生成方法可以基于其他音乐干声进行条件生成，但对节奏的控制有限，而音色转移方法允许用户指定特定的节奏，但无法基于音乐上下文进行条件生成。我们介绍了DARC，一种生成鼓伴奏模型，它同时基于其他干声的音乐上下文和明确的节奏提示（如打击乐或敲击轨道）进行条件生成。通过参数高效的微调，我们增强了STAGE，一个最先进的鼓干声生成器，提供细粒度的节奏控制，同时保持音乐上下文的意识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve generative tools for music creation by providing both structural control and stylistic flexibility, which are often lacking in existing models. The authors introduce DARC, a drum accompaniment generation model that combines musical context from other stems with explicit rhythm prompts, utilizing parameter-efficient fine-tuning to enhance the STAGE drum stem generator. Experimental results demonstrate that DARC effectively allows users to generate drum accompaniments with precise rhythm control while maintaining coherence with the musical context.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过提供结构控制和风格灵活性来改善音乐创作的生成工具，而现有模型往往缺乏这些功能。作者提出了DARC，这是一种生成鼓伴奏模型，通过参数高效的微调增强了STAGE这一领先的鼓音轨生成器，使其能够基于音乐上下文和明确的节奏提示（如打击乐或敲击轨道）进行条件生成。关键实验结果表明，DARC成功实现了细粒度的节奏控制，同时保持了对音乐上下文的意识，解决了以往鼓伴奏生成方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation</div>
<div class="meta-line">Authors: Salim Khazem</div>
<div class="meta-line">First: 2026-01-05T17:03:45+00:00 · Latest: 2026-01-05T17:03:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02273v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02273v1">PDF</a> · <a href="https://github.com/salimkhazem/Seglab.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TopoLoRA-SAM：面向拓扑的基础分割器参数高效适应薄结构和跨域二元语义分割</div>
<div class="mono" style="margin-top:8px">基础分割模型如Segment Anything Model (SAM)通过大规模预训练展现出强大的零样本泛化能力，但将其适应于特定领域的语义分割仍然具有挑战性，特别是对于薄结构（例如视网膜血管）和噪声模态（例如SAR影像）。全面微调计算成本高且存在灾难性遗忘的风险。我们提出了\textbf{TopoLoRA-SAM}，一种面向拓扑的参数高效二元语义分割适应框架。TopoLoRA-SAM将低秩适应（LoRA）注入冻结的ViT编码器，并结合轻量级空间卷积适配器和可选的通过可微分clDice的拓扑感知监督。我们在五个基准上评估了我们的方法，涵盖视网膜血管分割（DRIVE、STARE、CHASE\_DB1）、息肉分割（Kvasir-SEG）和SAR海洋/陆地分割（SL-SSDD），并与U-Net、DeepLabV3+、SegFormer和Mask2Former进行比较。TopoLoRA-SAM在数据集上实现了最佳的视网膜平均Dice和最佳的整体平均Dice，同时仅训练了\textbf{5.2\%}的模型参数（$\sim$4.9M）。在具有挑战性的CHASE\_DB1数据集上，我们的方法显著提高了分割准确性和鲁棒性，证明了面向拓扑的参数高效适应可以匹配或超越完全微调的专业模型。代码可在：https://github.com/salimkhazem/Seglab.git</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting foundation segmentation models like the Segment Anything Model (SAM) for domain-specific semantic segmentation, particularly for thin structures and noisy modalities, where full fine-tuning is computationally expensive and prone to catastrophic forgetting. The authors propose TopoLoRA-SAM, a topology-aware and parameter-efficient adaptation framework that integrates Low-Rank Adaptation into a frozen ViT encoder, supplemented by a lightweight spatial convolutional adapter and optional topology-aware supervision. Experimental results demonstrate that TopoLoRA-SAM outperforms existing models, achieving the best average Dice scores on multiple benchmarks, including retinal vessel and polyp segmentation, while training only 5.2% of model parameters, indicating its effectiveness in enhancing segmentation accuracy and robustness without extensive fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决将基础分割模型适应于薄结构和噪声模态的挑战，同时避免全量微调带来的高计算成本和灾难性遗忘风险。作者提出了TopoLoRA-SAM，这是一种拓扑感知和参数高效的适应框架，集成了低秩适应到冻结的ViT编码器中，并辅以轻量级空间卷积适配器和可选的拓扑感知监督。实验结果表明，TopoLoRA-SAM在五个基准测试中优于现有模型，如U-Net和DeepLabV3+，在视网膜血管分割和整体数据集的平均Dice分数上均取得最佳成绩，同时仅训练5.2%的模型参数，表明其在提高分割准确性和鲁棒性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</div>
<div class="meta-line">Authors: Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang</div>
<div class="meta-line">First: 2026-01-05T16:51:45+00:00 · Latest: 2026-01-05T16:51:45+00:00</div>
<div class="meta-line">Comments: Page: https://wrk226.github.io/DiffProxy.html, Code: https://github.com/wrk226/DiffProxy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02267v1">PDF</a> · <a href="https://github.com/wrk226/DiffProxy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://wrk226.github.io/DiffProxy.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models&#x27; training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffProxy：通过扩散生成的密集代理进行多视角人类网格恢复</div>
<div class="mono" style="margin-top:8px">从多视角图像恢复人类网格面临一个基本挑战：现实世界数据集包含不完美的真实标注，导致模型训练偏差，而具有精确监督的合成数据则存在领域差距。本文提出了DiffProxy，一个生成多视角一致的人类代理以进行网格恢复的新框架。DiffProxy的核心在于利用基于扩散的生成先验来弥合合成训练与现实世界泛化之间的差距。其关键创新包括：（1）用于生成多视角一致、像素对齐的人类代理的多条件机制；（2）一个手部细化模块，结合灵活的视觉提示以增强局部细节；（3）一种不确定性感知的测试时缩放方法，在优化过程中提高对挑战性案例的鲁棒性。这些设计确保网格恢复过程有效利用精确的合成真实数据和基于扩散的管道的生成优势。DiffProxy完全在合成数据上训练，在五个现实世界基准上实现了最先进的性能，特别是在遮挡和部分视图的挑战场景中展现出强大的零样本泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in human mesh recovery from multi-view images, particularly the issues arising from imperfect ground-truth annotations in real-world datasets and the domain gap present in synthetic data. The authors propose DiffProxy, a framework that utilizes diffusion-based generative priors to create multi-view consistent human proxies for mesh recovery. Key experimental results indicate that DiffProxy, trained solely on synthetic data, achieves state-of-the-art performance on five real-world benchmarks, demonstrating effective zero-shot generalization, especially in complex scenarios involving occlusions and partial views.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多视角图像中的人类网格恢复面临的挑战，特别是来自真实世界数据集中不完美的真实标注和合成数据中的领域差距问题。作者提出了DiffProxy，一个利用扩散生成先验生成多视角一致的人类代理以进行网格恢复的框架。关键实验结果表明，DiffProxy仅在合成数据上训练，在五个真实世界基准测试中实现了最先进的性能，尤其在涉及遮挡和部分视图的困难场景中表现出显著的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion</div>
<div class="meta-line">Authors: Binglei Li, Mengping Yang, Zhiyu Tan, Junping Zhang, Hao Li</div>
<div class="meta-line">First: 2026-01-05T15:32:53+00:00 · Latest: 2026-01-05T15:32:53+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block&#x27;s functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解开MMDiT模块：无训练分析与文本条件扩散的增强</div>
<div class="mono" style="margin-top:8px">基于变换器的扩散模型，特别是由多模态扩散变换器（MMDiT）驱动的模型，如FLUX和Qwen Image，最近取得了突破性进展，为文本到图像的生成和编辑带来了激动人心的体验。为了理解基于MMDiT模型的内部机制，现有方法尝试分析特定组件（如位置编码和注意力层）的影响。然而，如何不同模块及其与文本条件的交互对合成过程的贡献仍然难以全面理解。本文首先开发了一个系统化的流程，通过移除、禁用和增强相应模块的文本隐状态，全面调查每个模块的功能。我们的分析揭示了：1）语义信息出现在早期模块，细节在后期模块中呈现；2）移除特定模块通常比禁用文本条件的干扰小；3）在选择性模块中增强文本条件可以改善语义属性。基于这些观察，我们进一步提出了新颖的无训练策略，以改善文本对齐、精确编辑和加速。大量实验表明，我们的方法在各种基准测试中表现优于其他方法，并在文本到图像生成、图像编辑和推理加速方面保持灵活性。我们的方法将T2I-Combench++从56.92%提高到63.00%，将GenEval从66.42%提高到71.63%，而不牺牲合成质量。这些结果推动了对MMDiT模型的理解，并为进一步改进提供了宝贵的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to gain a comprehensive understanding of the internal mechanisms of Multimodal Diffusion Transformers (MMDiT) in text-to-image generation and editing. The authors developed a systematic pipeline to analyze the functionality of different blocks by removing, disabling, and enhancing textual hidden states. The key findings indicate that semantic information is present in earlier blocks while finer details are rendered in later ones, that removing specific blocks is less disruptive than disabling text conditions, and that enhancing textual conditions in selective blocks improves semantic attributes. The proposed training-free strategies demonstrated significant improvements in text alignment and editing precision, achieving notable performance gains in T2I-Combench++ and GenEval metrics without compromising synthesis quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于全面理解用于文本到图像生成和编辑的多模态扩散变换器（MMDiT）的内部机制。作者开发了一个系统化的流程，通过操控文本隐藏状态来分析不同模块的功能，揭示了语义信息出现在早期模块，而细节则在后期模块中呈现。主要发现包括，移除特定模块的干扰通常小于禁用文本条件，并且在选择性模块中增强文本条件可以改善语义属性，从而提出了新颖的无训练策略，增强文本对齐和编辑，导致在各种基准测试中显著提高性能，而不牺牲合成质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
