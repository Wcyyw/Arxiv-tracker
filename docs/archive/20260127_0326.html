<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 03:26</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0326</div>
    <div class="row"><div class="card">
<div class="title">AnyView: Synthesizing Any Novel View in Dynamic Scenes</div>
<div class="meta-line">Authors: Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad, Igor Vasiljevic, Swati Gupta, Fangzhou Cheng, Sergey Zakharov, Vitor Campagnolo Guizilini</div>
<div class="meta-line">First: 2026-01-23T18:59:58+00:00 · Latest: 2026-01-23T18:59:58+00:00</div>
<div class="meta-line">Comments: Project webpage: https://tri-ml.github.io/AnyView/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16982v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16982v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tri-ml.github.io/AnyView/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyView：在动态场景中合成任意新视图</div>
<div class="mono" style="margin-top:8px">现代生成视频模型在生成令人信服的高质量输出方面表现出色，但在高度动态的现实环境中难以保持多视图和时空一致性。在这项工作中，我们介绍了\textbf{AnyView}，一种基于扩散的视频生成框架，用于\emph{动态视图合成}，具有最小的归纳偏见或几何假设。我们利用多种数据源，结合不同级别的监督，包括单目（2D）、多视图静态（3D）和多视图动态（4D）数据集，训练出一种通用的时空隐式表示，能够从任意相机位置和轨迹生成零样本新视频。我们在标准基准上评估了AnyView，显示出与当前最先进技术的竞争结果，并提出了\textbf{AnyViewBench}，这是一个针对多样化现实场景中\emph{极端}动态视图合成的新挑战基准。在这种更戏剧化的设置中，我们发现大多数基线的性能显著下降，因为它们需要视点之间有显著重叠，而AnyView在从\emph{任何}视点提示时仍能保持生成真实、合理且时空一致的视频的能力。结果、数据、代码和模型可在以下网址查看：https://tri-ml.github.io/AnyView/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of modern generative video models in maintaining multi-view and spatiotemporal consistency in dynamic environments. The authors introduce AnyView, a diffusion-based video generation framework that synthesizes novel views without relying on strong geometric assumptions. Experimental results demonstrate that AnyView outperforms existing methods on standard benchmarks and shows robust performance in extreme dynamic view synthesis scenarios, where traditional models struggle due to viewpoint overlap requirements.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现代生成视频模型在动态环境中保持多视角和时空一致性方面的局限性。作者提出了AnyView，这是一种基于扩散的视频生成框架，能够在最小几何假设下合成新视角，利用包括单目、多视角静态和多视角动态数据集在内的多种数据源。实验结果表明，AnyView在标准基准测试中表现出竞争力，并在极端动态视角合成场景中表现优异，能够从任何视角生成真实且一致的视频，而其他模型在视角重叠要求下则表现不佳。</div>
</details>
</div>
<div class="card">
<div class="title">SyncLight: Controllable and Consistent Multi-View Relighting</div>
<div class="meta-line">Authors: David Serrano-Lozano, Anand Bhattad, Luis Herranz, Jean-François Lalonde, Javier Vazquez-Corral</div>
<div class="meta-line">First: 2026-01-23T18:59:57+00:00 · Latest: 2026-01-23T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: http://sync-light.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16981v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16981v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="http://sync-light.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncLight：可控且一致的多视角重光照</div>
<div class="mono" style="margin-top:8px">我们提出了SyncLight，这是首个能够在多个未校准视角下实现一致的参数化重光照的方法。尽管单视角重光照已经取得了显著进展，但现有的生成方法在保持多摄像机广播、立体电影和虚拟制作所需的严格光照一致性方面仍然面临挑战。SyncLight通过在场景的多视角捕捉中实现对光强和颜色的精确控制，解决了这一问题，且以单一参考编辑为条件。我们的方法利用了一个使用潜在桥匹配公式训练的多视角扩散变换器，在单次推理步骤中实现整个图像集的高保真重光照。为了促进训练，我们引入了一个大规模混合数据集，包含多样的合成环境——从现有来源和新设计场景中策划而来——以及在校准照明下的高保真真实多视角捕捉。令人惊讶的是，尽管仅在图像对上进行训练，SyncLight仍能零-shot推广到任意数量的视点，有效地在所有视角间传播光照变化，而无需摄像机位置信息。SyncLight为多视角捕捉系统提供了实用的重光照工作流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of maintaining consistent lighting across multiple uncalibrated views in applications such as multi-camera broadcasts and virtual production. The authors introduce SyncLight, a method that allows for parametric relighting by leveraging a multi-view diffusion transformer trained with a latent bridge matching formulation. Key experimental findings demonstrate that SyncLight achieves high-fidelity relighting across an entire image set in a single inference step and generalizes effectively to an arbitrary number of viewpoints, propagating lighting changes without the need for camera pose information, thus facilitating practical workflows for multi-view capture systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有生成方法在多个未校准视图中保持一致照明的局限性，这对多摄像机广播和虚拟制作等应用至关重要。作者提出了SyncLight，这是一种利用多视图扩散变换器，通过潜在桥接匹配公式进行训练的方法，能够基于单一参考编辑实现可控且一致的重光照。关键实验结果表明，SyncLight能够零-shot推广到任意数量的视点，有效地在所有视图中传播照明变化，而无需相机位置信息，从而促进多视图捕获系统的实际重光照工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Differentially Private Computation of the Cross-Attention Mechanism</div>
<div class="meta-line">Authors: Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang</div>
<div class="meta-line">First: 2024-07-20T01:02:27+00:00 · Latest: 2026-01-23T18:38:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.14717v3">Abs</a> · <a href="https://arxiv.org/pdf/2407.14717v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-attention has emerged as a cornerstone module in modern artificial intelligence, underpinning critical applications such as retrieval-augmented generation (RAG), system prompting, and guided stable diffusion. However, this is a rising concern about securing the privacy of cross-attention, as the underlying key and value matrices frequently encode sensitive data or private user information. In this work, we introduce a novel data structure designed to enforce differential privacy (DP) for cross-attention mechanisms, accompanied by provable theoretical guarantees. Specifically, letting $n$ denote the input sequence length, $d$ the feature dimension, $R$ the maximum magnitude of query and key matrices, $R_w$ the maximum magnitude of the value matrix, and $r, s, ε_s$ the parameters for polynomial kernel methods, our proposed structure achieves $\widetilde{O}(ndr^2)$ space and initialization complexity, with a query time of $\widetilde{O}(d r^2)$ per token. Moreover, we demonstrate that our mechanism satisfies $(ε, δ)$-DP, incurring an additive error of $\widetilde{O}((1-ε_s)^{-1} n^{-1} ε^{-1} R^{2s} R_w r^2)$ and a relative error of $2ε_s/(1-ε_s)$ with respect to the ground truth. Crucially, our framework maintains robustness against adaptive queries, ensuring security even in adversarial settings. To the best of our knowledge, this constitutes the first approach providing provable differential privacy for cross-attention, establishing a foundation for future privacy-preserving algorithms in large generative models (LGMs).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明的差分隐私交叉注意力机制计算</div>
<div class="mono" style="margin-top:8px">交叉注意力已成为现代人工智能的基石模块，支撑着检索增强生成（RAG）、系统提示和引导稳定扩散等关键应用。然而，关于保护交叉注意力隐私的担忧日益增加，因为底层的键和值矩阵经常编码敏感数据或用户私人信息。在本研究中，我们引入了一种新颖的数据结构，旨在为交叉注意力机制强制执行差分隐私（DP），并附带可证明的理论保证。具体而言，设输入序列长度为$n$，特征维度为$d$，查询和键矩阵的最大幅度为$R$，值矩阵的最大幅度为$R_w$，以及多项式核方法的参数$r, s, ε_s$，我们提出的结构实现了$\widetilde{O}(ndr^2)$的空间和初始化复杂度，每个令牌的查询时间为$\widetilde{O}(d r^2)$。此外，我们证明了我们的机制满足$(ε, δ)$-DP，产生的附加误差为$\widetilde{O}((1-ε_s)^{-1} n^{-1} ε^{-1} R^{2s} R_w r^2)$，相对于真实值的相对误差为$2ε_s/(1-ε_s)$。至关重要的是，我们的框架对自适应查询保持鲁棒性，确保在对抗性环境中也能保证安全。据我们所知，这是第一个提供可证明差分隐私的交叉注意力方法，为未来在大型生成模型（LGM）中保护隐私的算法奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing concern for privacy in cross-attention mechanisms, which are essential in various AI applications but often involve sensitive data. The authors propose a novel data structure that enforces differential privacy (DP) for cross-attention, providing theoretical guarantees for its effectiveness. The experimental results indicate that the proposed structure achieves a space and initialization complexity of \widetilde{O}(ndr^2) and a query time of \widetilde{O}(d r^2) per token, while ensuring $(ε, δ)$-DP with manageable error rates, marking a significant advancement in privacy-preserving techniques for large generative models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决与交叉注意机制相关的隐私问题，该机制在各种人工智能应用中至关重要，但通常涉及敏感数据。作者提出了一种新颖的数据结构，强制执行交叉注意的差分隐私（DP），并为其有效性提供理论保证。实验结果表明，所提出的结构在空间和初始化复杂度上达到了\widetilde{O}(ndr^2)，每个令牌的查询时间为\widetilde{O}(d r^2)，同时确保了$(ε, δ)$-DP，具有可控的误差率，从而为大型生成模型中的隐私保护算法奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Multi-Contrast Super-Resolution: Self-Supervised Implicit Re-Representation for Unpaired Cross-Modal Synthesis</div>
<div class="meta-line">Authors: Yinzhe Wu, Hongyu Rui, Fanwen Wang, Jiahao Huang, Zhenxuan Zhang, Haosen Zhang, Zi Wang, Guang Yang</div>
<div class="meta-line">First: 2025-05-09T07:48:52+00:00 · Latest: 2026-01-23T18:01:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05855v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.05855v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited. They typically require large, paired low- and high-resolution (LR/HR) training datasets, which are scarce, and are trained for fixed upsampling scales. While recent self-supervised methods remove the paired data requirement, they fail to leverage valuable population-level priors. In this work, we propose a novel, decoupled MCSR framework that resolves both limitations. We reformulate MCSR into two stages: (1) an unpaired cross-modal synthesis (uCMS) module, trained once on unpaired population data to learn a robust anatomical prior; and (2) a lightweight, patient-specific implicit re-representation (IrR) module. This IrR module is optimized in a self-supervised manner to fuse the population prior with the subject&#x27;s own LR target data. This design uniquely fuses population-level knowledge with patient-specific fidelity without requiring any paired LR/HR or paired cross-modal training data. By building the IrR module on an implicit neural representation, our framework is also inherently scale-agnostic. Our method demonstrates superior quantitative performance on different datasets, with exceptional robustness at extreme scales (16x, 32x), a regime where competing methods fail. Our work presents a data-efficient, flexible, and computationally lightweight paradigm for MCSR, enabling high-fidelity, arbitrary-scale</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦多对比超分辨率：用于无配对跨模态合成的自监督隐式重表征</div>
<div class="mono" style="margin-top:8px">多对比超分辨率（MCSR）对增强MRI至关重要，但当前的深度学习方法受到限制。它们通常需要大量配对的低分辨率和高分辨率（LR/HR）训练数据集，而这些数据集稀缺，并且是针对固定的上采样比例进行训练的。尽管最近的自监督方法消除了配对数据的需求，但它们未能利用有价值的人群级先验。在本研究中，我们提出了一种新颖的解耦MCSR框架，解决了这两个限制。我们将MCSR重新构建为两个阶段：（1）一个无配对跨模态合成（uCMS）模块，基于无配对人群数据进行一次训练，以学习稳健的解剖先验；（2）一个轻量级、患者特定的隐式重表征（IrR）模块。该IrR模块以自监督的方式进行优化，将人群先验与受试者自己的LR目标数据融合。该设计独特地将人群级知识与患者特定的保真度融合，而无需任何配对的LR/HR或配对的跨模态训练数据。通过在隐式神经表征上构建IrR模块，我们的框架也本质上是尺度无关的。我们的方法在不同数据集上展示了优越的定量性能，在极端尺度（16x，32x）下表现出卓越的鲁棒性，而在这一范围内竞争方法失败。我们的工作提出了一种数据高效、灵活且计算轻量的MCSR范式，实现高保真度和任意尺度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current deep learning methods for multi-contrast super-resolution (MCSR) in MRI, which often require large paired datasets that are not readily available. The authors propose a decoupled MCSR framework that consists of two main components: an unpaired cross-modal synthesis (uCMS) module that learns a robust anatomical prior from unpaired population data, and a patient-specific implicit re-representation (IrR) module that integrates this prior with individual low-resolution data in a self-supervised manner. Experimental results show that this method achieves superior quantitative performance across various datasets, demonstrating exceptional robustness at extreme upsampling scales of 16x and 32x, where other methods typically fail.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前深度学习方法在MRI多对比度超分辨率（MCSR）中的局限性，这些方法通常需要难以获得的大型配对数据集。作者提出了一种新颖的解耦MCSR框架，主要由两个部分组成：一个从未配对的人群数据中学习稳健解剖先验的未配对跨模态合成（uCMS）模块，以及一个以自监督方式优化的患者特定隐式重表示（IrR）模块，用于将人群先验与个体低分辨率数据结合。实验结果表明，该方法在不同数据集上实现了优越的定量性能，在16倍和32倍的极端上采样尺度下表现出卓越的鲁棒性，而现有方法在这些情况下表现不佳。</div>
</details>
</div>
<div class="card">
<div class="title">T-LoRA: Single Image Diffusion Model Customization Without Overfitting</div>
<div class="meta-line">Authors: Vera Soboleva, Aibek Alanov, Andrey Kuznetsov, Konstantin Sobolev</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-07-08T13:14:10+00:00 · Latest: 2026-01-23T17:14:49+00:00</div>
<div class="meta-line">Comments: AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05964v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.05964v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://controlgenai.github.io/T-LoRA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. We show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments on SD-XL and FLUX-1.dev show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques, achieving a superior balance between concept fidelity and text alignment. Project page is available at https://controlgenai.github.io/T-LoRA/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T-LoRA：无过拟合的单图像扩散模型定制</div>
<div class="mono" style="margin-top:8px">尽管扩散模型微调为定制预训练模型以生成特定对象提供了强大的方法，但在训练样本有限时，它常常会遭遇过拟合，损害了泛化能力和输出多样性。本文解决了使用单个概念图像适应扩散模型这一具有挑战性但影响深远的任务，因为单图像定制具有最大的实际潜力。我们介绍了T-LoRA，一种专为扩散模型个性化设计的时间步依赖低秩适应框架。我们表明，较高的扩散时间步比较低的更容易过拟合，因此需要一种时间步敏感的微调策略。T-LoRA包含两个关键创新：（1）一种动态微调策略，根据扩散时间步调整秩约束更新；（2）一种权重参数化技术，通过正交初始化确保适配器组件之间的独立性。在SD-XL和FLUX-1.dev上的大量实验表明，T-LoRA及其各个组件优于标准LoRA和其他扩散模型个性化技术，在概念保真度和文本对齐之间实现了更好的平衡。项目页面可在https://controlgenai.github.io/T-LoRA/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the overfitting issues commonly encountered in fine-tuning diffusion models with limited training samples, particularly when customizing them using a single concept image. The authors propose T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework that employs a dynamic fine-tuning strategy sensitive to diffusion timesteps and a weight parametrization technique to maintain independence among adapter components. Experimental results demonstrate that T-LoRA significantly outperforms standard LoRA and other personalization methods in terms of concept fidelity and text alignment, particularly when applied to the SD-XL and FLUX-1.dev datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在使用有限训练样本微调扩散模型时常见的过拟合问题，特别是在使用单张图像进行定制时。作者提出了T-LoRA，一种针对扩散模型个性化设计的时间步依赖低秩适应框架。关键实验结果表明，T-LoRA通过其动态微调策略和权重参数化技术，在SD-XL和FLUX-1.dev的广泛测试中显著优于标准LoRA和其他个性化方法，实现了概念保真度和文本对齐之间的更好平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Large Vision-language Models for Surgical Tool Detection</div>
<div class="meta-line">Authors: Nakul Poudel, Richard Simon, Cristian A. Linte</div>
<div class="meta-line">First: 2026-01-23T17:00:46+00:00 · Latest: 2026-01-23T17:00:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型视觉语言模型在外科工具检测中的应用</div>
<div class="mono" style="margin-top:8px">外科手术是一个高度复杂的过程，人工智能已成为支持外科指导和决策的变革性力量。然而，大多数当前AI系统的单模态特性限制了它们对外科工作流程的全面理解。这突显了需要通用外科AI系统，能够全面建模外科场景中相互关联的组件。最近在整合多模态数据处理的大型视觉语言模型方面的进展，为建模外科任务和提供类人场景推理与理解提供了强大潜力。尽管前景广阔，但在外科应用中对视觉语言模型的系统研究仍然有限。在本研究中，我们评估了大型视觉语言模型在检测外科工具这一基本外科视觉任务中的有效性。具体而言，我们在零样本和参数高效的LoRA微调设置下，研究了三种最先进的视觉语言模型：Qwen2.5、LLaVA1.5和InternVL3.5，使用GraSP机器人手术数据集。我们的结果表明，Qwen2.5在评估的视觉语言模型中，在这两种配置下均持续实现了优越的检测性能。此外，与开放集检测基线Grounding DINO相比，Qwen2.5表现出更强的零样本泛化能力和可比的微调性能。值得注意的是，Qwen2.5在工具识别方面表现优越，而Grounding DINO在定位方面表现更强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the limitations of current unimodal AI systems in understanding complex surgical workflows, highlighting the need for general-purpose surgical AI that can model interrelated components of surgical scenes. The researchers evaluated three state-of-the-art vision-language models (VLMs) - Qwen2.5, LLaVA1.5, and InternVL3.5 - for the task of detecting surgical tools using the GraSP robotic surgery dataset, applying both zero-shot and parameter-efficient LoRA fine-tuning methods. The findings reveal that Qwen2.5 consistently outperforms the other models in detection performance across both configurations, demonstrating superior zero-shot generalization and instrument recognition compared to the baseline model Grounding DINO, which, while excelling in localization, does not match Qwen2.5&#x27;s overall effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前单模态人工智能系统在全面理解外科工作流程方面的局限性，强调了需要先进的外科人工智能来建模复杂的外科场景。该研究评估了三种大型视觉-语言模型（VLM），即Qwen2.5、LLaVA1.5和InternVL3.5，在GraSP机器人手术数据集上检测外科工具的有效性，采用零样本和参数高效的LoRA微调方法。研究结果表明，Qwen2.5在两种配置下的检测性能均优于其他模型，显示出比基线模型Grounding DINO更强的零样本泛化能力和工具识别能力，而Grounding DINO在定位方面表现更强。</div>
</details>
</div>
<div class="card">
<div class="title">ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models</div>
<div class="meta-line">Authors: Chenxi Ruan, Yu Xiao, Yihan Hou, Guosheng Hu, Wei Zeng</div>
<div class="meta-line">First: 2026-01-23T15:36:02+00:00 · Latest: 2026-01-23T15:36:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16836v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16836v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ColorConceptBench：文本到图像模型中概率颜色概念理解的基准</div>
<div class="mono" style="margin-top:8px">尽管文本到图像（T2I）模型已经取得了显著进展，但它们将颜色与隐含概念关联的能力仍然未得到充分探索。为了解决这一问题，我们引入了ColorConceptBench，这是一个新的人工标注基准，旨在通过概率颜色分布的视角系统评估颜色概念关联。ColorConceptBench超越了显式颜色名称或代码，通过探讨模型如何利用6,369个人工注释的基础翻译1,281个隐含颜色概念。我们对七个领先的T2I模型的评估表明，当前模型对抽象语义缺乏敏感性，且这一局限性似乎对标准干预（例如，缩放和引导）具有抵抗力。这表明，实现类人颜色语义不仅需要更大的模型，还需要在模型学习和表示隐含意义的方式上进行根本性转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the underexplored capability of text-to-image models in associating colors with implicit concepts. The authors introduce ColorConceptBench, a human-annotated benchmark designed to evaluate color-concept associations through probabilistic color distributions, utilizing 1,281 implicit color concepts and 6,369 human annotations. The evaluation of seven leading text-to-image models reveals that these models exhibit a lack of sensitivity to abstract semantics, and this limitation persists despite standard interventions, indicating that improving human-like color semantics requires a fundamental change in model learning and representation of implicit meanings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像模型在将颜色与隐含概念关联方面的不足。作者提出了ColorConceptBench，这是一个基于6,369个人工注释和1,281个隐含颜色概念的基准，评估颜色概念关联，采用概率颜色分布的方法。对七个领先的文本到图像模型的评估表明，这些模型通常缺乏对抽象语义的敏感性，并且这种局限性在标准干预下仍然存在，这表明改善颜色语义需要对模型训练和隐含意义的表示进行根本性的改变。</div>
</details>
</div>
<div class="card">
<div class="title">Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models</div>
<div class="meta-line">Authors: Zahraa Al Sahili, Ioannis Patras, Matthew Purver</div>
<div class="meta-line">First: 2025-01-22T21:08:30+00:00 · Latest: 2026-01-23T15:12:35+00:00</div>
<div class="meta-line">Comments: Published at TMLR; updated version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.13223v7">Abs</a> · <a href="https://arxiv.org/pdf/2501.13223v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors -- model size, training-data scale, and training-data source -- by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M/2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies -- Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据至关重要：对比视觉语言模型中的社会偏见审计</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在零样本识别中表现强劲，但经常从训练数据中继承社会偏见。我们通过比较CLIP和OpenCLIP这两个共享相同对比目标但在编码器宽度和预训练图像-文本语料库（400M专有对比400M/2B LAION）上存在差异的模型，系统性地解开三个设计因素——模型大小、训练数据规模和训练数据来源。在平衡的面部分析基准测试中，增大编码器在CLIP中减少了性别偏差，但在OpenCLIP中加剧了性别和种族偏差；将LAION语料库从400M增加到2B进一步增加了OpenCLIP的偏见。在匹配的模型和数据预算下，用LAION替代专有数据提高了性别公平性，同时增加了种族偏差，强调数据来源是偏见模式的主要驱动因素。我们还评估了三种后期测试去偏见策略——偏见提示、提示数组和SANER。去偏见减少了但并未消除伤害，其有效性依赖于来源和规模：在较小模型规模下，偏见提示最有效地减少了CLIP中的性别偏差，而提示数组和SANER在OpenCLIP中更可靠地减少了种族偏差；扩展LAION重新配置了最公平的方法。综合来看，这些发现挑战了更大模型或数据集自动更公平的假设，并突出了训练数据来源作为偏见和缓解有效性的关键决定因素。我们发布了代码和评估脚本，以便对未来的VLM进行透明、可重复的审计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the social biases present in vision-language models (VLMs) due to their training data, motivated by the need for fairer AI systems. The study systematically examines the effects of model size, training-data scale, and training-data source by comparing CLIP and OpenCLIP, which differ in encoder width and the datasets used for pre-training. Key findings reveal that increasing the encoder size reduces gender bias in CLIP but exacerbates both gender and racial biases in OpenCLIP, while expanding the LAION dataset from 400M to 2B increases bias further; substituting proprietary data with LAION improves gender fairness but increases racial skew, highlighting the significance of data source in bias patterns. Additionally, three debiasing strategies were evaluated, showing varying effectiveness based on model size and data source, ultimately challenging the notion that larger models or datasets inherently lead to fairer outcomes.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLM）中存在的社会偏见，这些偏见源于其训练数据，旨在推动更公平的人工智能系统。研究系统地分析了模型大小、训练数据规模和训练数据来源的影响，通过比较CLIP和OpenCLIP这两个在编码器宽度和预训练数据集上存在差异的模型。主要发现表明，尽管在CLIP中增加编码器大小可以减少性别偏见，但在OpenCLIP中却加剧了性别和种族偏见，而扩展LAION数据集则进一步增加了偏见；用LAION替代专有数据可以改善性别公平性，但增加了种族偏见，强调了数据来源在偏见模式中的重要性。此外，评估了三种去偏见策略，结果显示虽然它们可以减少偏见，但其有效性因模型和数据特征而异，这挑战了更大模型或数据集必然更公平的观念，并强调了训练数据来源在偏见缓解中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation</div>
<div class="meta-line">Authors: Carolin Holtermann, Florian Schneider, Anne Lauscher</div>
<div class="meta-line">First: 2026-01-23T14:55:11+00:00 · Latest: 2026-01-23T14:55:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16803v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16803v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt&#x27;s semantics. Yet a comprehensive analysis of this behavior, which we dub Surface-over-Semantics (SoS), is missing. We present the first analysis of T2I models&#x27; SoS tendencies. To this end, we create a set of prompts covering 171 cultural identities, translated into 14 languages, and use it to prompt seven T2I models. To quantify SoS tendencies across models, languages, and cultures, we introduce a novel measure and analyze how the tendencies we identify manifest visually. We show that all but one model exhibit strong surface-level tendency in at least two languages, with this effect intensifying across the layers of T2I text encoders. Moreover, these surface tendencies frequently correlate with stereotypical visual depictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoS：多语言文本到图像生成中的表面与语义分析</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在全球用户中越来越普遍。然而，先前的研究指出，T2I对特定输入语言的敏感性很高——当面对非英语语言（即同一提示的不同表面形式）时，T2I模型往往产生文化刻板印象的描绘，优先考虑表面而非提示的语义。然而，关于这种行为的全面分析，即我们称之为表面优于语义（SoS），仍然缺失。我们首次分析了T2I模型的SoS倾向。为此，我们创建了一组涵盖171种文化身份的提示，翻译成14种语言，并用它来提示七个T2I模型。为了量化模型、语言和文化之间的SoS倾向，我们引入了一种新颖的度量方法，并分析我们识别的倾向如何在视觉上表现。我们显示，除了一个模型外，所有模型在至少两种语言中都表现出强烈的表面倾向，这种效应在T2I文本编码器的各层中加剧。此外，这些表面倾向通常与刻板印象的视觉描绘相关。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the sensitivity of text-to-image (T2I) models to different input languages, which often leads to culturally stereotypical outputs that prioritize surface forms over semantic meaning. The authors conducted a comprehensive analysis by creating a set of prompts that represent 171 cultural identities, translated into 14 languages, and utilized these prompts to evaluate seven T2I models. The findings reveal that all but one model demonstrate a strong tendency to prioritize surface-level interpretations in at least two languages, with this tendency becoming more pronounced across the layers of T2I text encoders, and these surface tendencies frequently align with stereotypical visual representations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了文本到图像（T2I）模型中的表面优先于语义（SoS）现象，动机是观察到这些模型在非英语语言提示下常常生成文化刻板印象的图像。作者通过创建涵盖171种文化身份的提示，翻译成14种语言，并测试七个T2I模型来评估其反应，从而进行了全面分析。研究结果显示，除了一个模型外，所有模型在至少两种语言中都表现出显著的表面特征优先于语义内容的倾向，这种倾向在T2I文本编码器的更深层次中变得更加明显，并且这些表面倾向通常与刻板印象的视觉表现相一致。</div>
</details>
</div>
<div class="card">
<div class="title">CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts</div>
<div class="meta-line">Authors: Shuai Huang, Xuhan Lin, Yuwu Lu</div>
<div class="meta-line">First: 2026-01-23T14:19:04+00:00 · Latest: 2026-01-23T14:19:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16773v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16773v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model&#x27;s ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CASP：基于CLS Token注意力引导提示的少样本类增量学习</div>
<div class="mono" style="margin-top:8px">少样本类增量学习（FSCIL）在持续学习中提出了核心挑战，要求模型在样本极其有限的情况下快速适应新类别，同时减轻灾难性遗忘。最近的基于提示的方法将预训练的主干与任务特定的提示相结合，取得了显著进展。然而，在极端的少样本增量设置下，模型的迁移和泛化能力变得至关重要，因此必须利用预训练知识学习可以在基础会话中跨未来类别共享的特征表示。受到CLS token机制的启发，该机制类似于人类注意力并逐步过滤掉与任务无关的信息，我们提出了CLS Token注意力引导提示（CASP）。该方法将类共享的可训练偏置参数引入CLS token的查询、键和值投影中，以明确调节自注意力权重。为了进一步增强泛化能力，我们还设计了一种注意力扰动策略，并在浅层特征空间中执行流形Token混合，合成潜在的新类特征以提高泛化能力，并为即将到来的任务保留表示能力。在CUB200、CIFAR100和ImageNet-R数据集上的实验表明，CASP在标准和细粒度FSCIL设置中均优于最先进的方法，无需在增量阶段进行微调，同时显著减少了参数开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of few-shot class-incremental learning (FSCIL), particularly the need for models to adapt quickly to new classes with limited samples while avoiding catastrophic forgetting. The authors propose a novel method called CLS Token Attention Steering Prompts (CASP), which incorporates class-shared trainable bias parameters into the self-attention mechanism of the CLS token to enhance feature representation and generalization. Experimental results on datasets such as CUB200, CIFAR100, and ImageNet-R show that CASP significantly outperforms existing state-of-the-art methods in both standard and fine-grained FSCIL scenarios, achieving this without the need for fine-tuning during incremental learning phases and with reduced parameter overhead.</div>
<div class="mono" style="margin-top:8px">本研究解决了少样本类增量学习（FSCIL）中的挑战，该挑战要求模型在有限样本下快速适应新类别，同时最小化灾难性遗忘。作者提出了一种新方法，称为CLS Token Attention Steering Prompts（CASP），该方法将类共享的可训练偏置参数引入CLS token的自注意力机制，以增强特征表示和泛化能力。在CUB200、CIFAR100和ImageNet-R数据集上的实验结果表明，CASP在标准和细粒度FSCIL场景中显著优于现有方法，且在增量学习阶段无需微调，同时减少了参数开销。</div>
</details>
</div>
<div class="card">
<div class="title">Adoption of Generative Artificial Intelligence in the German Software Engineering Industry: An Empirical Study</div>
<div class="meta-line">Authors: Ludwig Felder, Tobias Eisenreich, Mahsa Fischer, Stefan Wagner, Chunyang Chen</div>
<div class="meta-line">First: 2026-01-23T12:42:33+00:00 · Latest: 2026-01-23T12:42:33+00:00</div>
<div class="meta-line">Comments: Submitted to FSE &#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16700v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative artificial intelligence (GenAI) tools have seen rapid adoption among software developers. While adoption rates in the industry are rising, the underlying factors influencing the effective use of these tools, including the depth of interaction, organizational constraints, and experience-related considerations, have not been thoroughly investigated. This issue is particularly relevant in environments with stringent regulatory requirements, such as Germany, where practitioners must address the GDPR and the EU AI Act while balancing productivity gains with intellectual property considerations. Despite the significant impact of GenAI on software engineering, to the best of our knowledge, no empirical study has systematically examined the adoption dynamics of GenAI tools within the German context. To address this gap, we present a comprehensive mixed-methods study on GenAI adoption among German software engineers. Specifically, we conducted 18 exploratory interviews with practitioners, followed by a developer survey with 109 participants. We analyze patterns of tool adoption, prompting strategies, and organizational factors that influence effectiveness. Our results indicate that experience level moderates the perceived benefits of GenAI tools, and productivity gains are not evenly distributed among developers. Further, organizational size affects both tool selection and the intensity of tool use. Limited awareness of the project context is identified as the most significant barrier. We summarize a set of actionable implications for developers, organizations, and tool vendors seeking to advance artificial intelligence (AI) assisted software development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>德国软件工程行业生成性人工智能的采用：一项实证研究</div>
<div class="mono" style="margin-top:8px">生成性人工智能（GenAI）工具在软件开发人员中迅速普及。尽管行业的采用率在上升，但影响这些工具有效使用的潜在因素，包括互动深度、组织限制和经验相关考虑，尚未得到彻底研究。这个问题在具有严格监管要求的环境中尤为相关，例如德国，实践者必须在遵守GDPR和欧盟人工智能法案的同时，平衡生产力提升与知识产权的考虑。尽管GenAI对软件工程的影响显著，但据我们所知，尚无实证研究系统地考察德国背景下GenAI工具的采用动态。为填补这一空白，我们呈现了一项关于德国软件工程师GenAI采用的综合混合方法研究。具体而言，我们与18位实践者进行了探索性访谈，随后进行了109名参与者的开发者调查。我们分析了工具采用模式、提示策略和影响有效性的组织因素。我们的结果表明，经验水平调节了对GenAI工具的感知收益，生产力提升在开发者之间并不均匀分布。此外，组织规模影响工具选择和使用强度。对项目背景的有限认知被确定为最显著的障碍。我们总结了一系列可行的建议，供希望推进人工智能（AI）辅助软件开发的开发者、组织和工具供应商参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid adoption of generative artificial intelligence (GenAI) tools among software developers in Germany raises questions about the factors influencing their effective use, particularly in a regulatory environment. To investigate this, the study employs a mixed-methods approach, conducting 18 exploratory interviews followed by a survey of 109 developers. The findings reveal that experience level significantly moderates the perceived benefits of GenAI tools, with productivity gains unevenly distributed among developers, while organizational size impacts tool selection and usage intensity, and limited project context awareness emerges as a major barrier to effective adoption.</div>
<div class="mono" style="margin-top:8px">在德国，生成性人工智能（GenAI）工具在软件开发人员中的快速采用引发了对影响其有效使用因素的关注，尤其是在监管环境中。为此，作者进行了混合方法研究，包括18次探索性访谈和对109名开发人员的调查。研究结果表明，经验水平显著调节了对GenAI工具的感知收益，生产力提升在开发人员中并不均匀分布，而组织规模则影响工具选择和使用强度。研究还发现，项目背景意识的不足是主要障碍，这为开发人员、组织和工具供应商在提升AI辅助软件开发方面提供了可行的建议。</div>
</details>
</div>
<div class="card">
<div class="title">EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</div>
<div class="meta-line">Authors: Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</div>
<div class="meta-line">First: 2026-01-23T12:09:59+00:00 · Latest: 2026-01-23T12:09:59+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent&#x27;s own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMemBench：针对VLM代理的情节记忆交互基准测试</div>
<div class="mono" style="margin-top:8px">我们介绍了EMemBench，这是一个通过交互游戏评估代理长期记忆的程序化基准测试。EMemBench不是使用固定的问题集，而是从每个代理的自身轨迹生成问题，涵盖文本和视觉游戏环境。每个模板从基础游戏信号计算可验证的真实答案，控制可回答性并在记忆技能上实现平衡覆盖：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗。我们使用强大的语言模型/视觉语言模型作为基础，通过上下文提示作为基线评估记忆代理。在15个文本游戏和多个视觉种子中，结果远未饱和：归纳和空间推理是持续的瓶颈，尤其是在视觉设置中。持久记忆在文本游戏中为开放基础模型带来了明显的收益，但对于视觉语言模型代理的改进则不那么一致，这表明视觉基础的情节记忆仍然是一个开放的挑战。一项人类研究进一步确认了EMemBench的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate the long-term memory capabilities of agents in interactive environments. The authors introduce EMemBench, a benchmark that generates questions based on each agent&#x27;s trajectory in text and visual games, allowing for a comprehensive assessment of various memory skills. Experimental results reveal that while persistent memory improves performance for text-based agents, challenges remain for visual language models, particularly in induction and spatial reasoning tasks, indicating that episodic memory in visual contexts is still a significant hurdle. A human study corroborates the difficulties identified in the benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过互动游戏评估虚拟语言模型（VLM）代理的长期记忆能力。作者开发了EMemBench，一个基准测试，通过生成基于每个代理轨迹的问题，涵盖文本和视觉环境，从而全面评估各种记忆技能。实验结果表明，尽管持久记忆提高了文本游戏的表现，但VLM代理在归纳和空间推理任务上仍面临挑战，突显了实现有效视觉基础的情节记忆的持续困难。</div>
</details>
</div>
<div class="card">
<div class="title">Fast, faithful and photorealistic diffusion-based image super-resolution with enhanced Flow Map models</div>
<div class="meta-line">Authors: Maxence Noble, Gonzalo Iñaki Quintana, Benjamin Aubin, Clément Chadebec</div>
<div class="meta-line">First: 2026-01-23T11:25:04+00:00 · Latest: 2026-01-23T11:25:04+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16660v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based image super-resolution (SR) has recently attracted significant attention by leveraging the expressive power of large pre-trained text-to-image diffusion models (DMs). A central practical challenge is resolving the trade-off between reconstruction faithfulness and photorealism. To address inference efficiency, many recent works have explored knowledge distillation strategies specifically tailored to SR, enabling one-step diffusion-based approaches. However, these teacher-student formulations are inherently constrained by information compression, which can degrade perceptual cues such as lifelike textures and depth of field, even with high overall perceptual quality. In parallel, self-distillation DMs, known as Flow Map models, have emerged as a promising alternative for image generation tasks, enabling fast inference while preserving the expressivity and training stability of standard DMs. Building on these developments, we propose FlowMapSR, a novel diffusion-based framework for image super-resolution explicitly designed for efficient inference. Beyond adapting Flow Map models to SR, we introduce two complementary enhancements: (i) positive-negative prompting guidance, based on a generalization of classifier free-guidance paradigm to Flow Map models, and (ii) adversarial fine-tuning using Low-Rank Adaptation (LoRA). Among the considered Flow Map formulations (Eulerian, Lagrangian, and Shortcut), we find that the Shortcut variant consistently achieves the best performance when combined with these enhancements. Extensive experiments show that FlowMapSR achieves a better balance between reconstruction faithfulness and photorealism than recent state-of-the-art methods for both x4 and x8 upscaling, while maintaining competitive inference time. Notably, a single model is used for both upscaling factors, without any scale-specific conditioning or degradation-guided mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速、真实且具照片真实感的基于扩散的图像超分辨率与增强的流图模型</div>
<div class="mono" style="margin-top:8px">基于扩散的图像超分辨率（SR）最近因利用大型预训练文本到图像扩散模型（DMs）的表现力而受到广泛关注。一个主要的实际挑战是解决重建真实性与照片真实感之间的权衡。为了解决推理效率，许多近期工作探索了专门针对SR的知识蒸馏策略，使得一步扩散方法成为可能。然而，这些教师-学生的形式在信息压缩上固有地受到限制，可能会降低感知线索，如逼真的纹理和景深，即使整体感知质量较高。与此同时，自蒸馏DMs，即流图模型，作为图像生成任务的有前景的替代方案出现，能够在保持标准DMs的表现力和训练稳定性的同时实现快速推理。在这些发展的基础上，我们提出了FlowMapSR，一个专门为高效推理设计的基于扩散的图像超分辨率框架。除了将流图模型适配到SR外，我们还引入了两个互补的增强：(i) 基于流图模型的分类器自由引导范式的推广的正负提示指导，以及 (ii) 使用低秩适应（LoRA）的对抗微调。在考虑的流图形式（欧拉、拉格朗日和快捷）中，我们发现快捷变体在结合这些增强时始终实现最佳性能。大量实验表明，FlowMapSR在x4和x8放大时在重建真实性与照片真实感之间取得了比近期最先进方法更好的平衡，同时保持了竞争力的推理时间。值得注意的是，单个模型用于两个放大因子，而无需任何特定于尺度的条件或降级引导机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve diffusion-based image super-resolution (SR) by addressing the trade-off between reconstruction faithfulness and photorealism, which has been a significant challenge in the field. The authors propose a novel framework called FlowMapSR that adapts Flow Map models for efficient inference in SR tasks, incorporating enhancements such as positive-negative prompting guidance and adversarial fine-tuning with Low-Rank Adaptation (LoRA). Experimental results demonstrate that FlowMapSR outperforms recent state-of-the-art methods in achieving a better balance between reconstruction faithfulness and photorealism for both x4 and x8 upscaling, while also maintaining competitive inference times using a single model for both scaling factors.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善扩散基础图像超分辨率(SR)方法中重建真实性与照片真实感之间的权衡。作者提出了一种名为FlowMapSR的新框架，该框架将流图模型适配于高效推理，同时引入了正负提示引导和使用低秩适应(LoRA)的对抗微调等增强措施。实验结果表明，FlowMapSR在x4和x8放大时，能够在重建真实性和照片真实感之间实现更好的平衡，且推理时间具有竞争力，并且使用单一模型处理不同的放大因子。</div>
</details>
</div>
<div class="card">
<div class="title">Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</div>
<div class="meta-line">Authors: Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-23T11:06:51+00:00 · Latest: 2026-01-23T11:06:51+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16645v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16645v1">PDF</a> · <a href="https://github.com/gongms00/SPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model&#x27;s generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过具有新颖结构保留损失的扩散模型进行边缘感知图像处理</div>
<div class="mono" style="margin-top:8px">最近的图像编辑进展利用潜在扩散模型（LDMs）进行多样化的、基于文本提示的编辑。然而，保持像素级边缘结构——对于如照片真实感风格转移或图像色调调整等任务至关重要——仍然是潜在扩散编辑的一大挑战。为克服这一限制，我们提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异。我们的无训练方法将SPL直接集成到扩散模型的生成过程中，以确保结构的保真性。该核心机制通过后处理步骤来减轻LDM解码失真、精确编辑定位的掩蔽策略以及保留未编辑区域色调的颜色保留损失进行补充。实验确认SPL增强了结构保真性，在基于潜在扩散的图像编辑中实现了最先进的性能。我们的代码将公开发布在https://github.com/gongms00/SPL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of maintaining pixel-level edge structures in image editing using latent diffusion models (LDMs), which is essential for tasks like photorealistic style transfer. The authors propose a novel Structure Preservation Loss (SPL) that quantifies structural differences between input and edited images using local linear models, integrating this loss into the generative process of the diffusion model without requiring additional training. Experimental results demonstrate that the SPL significantly enhances structural fidelity, achieving state-of-the-art performance in latent-diffusion-based image editing tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在使用潜在扩散模型（LDMs）进行图像编辑时保持像素级边缘结构的挑战，这对如逼真的风格转移等任务至关重要。作者提出了一种新颖的结构保留损失（SPL），利用局部线性模型量化输入图像与编辑图像之间的结构差异，将该损失直接整合到扩散模型的生成过程中，无需额外训练。实验结果表明，SPL显著增强了结构保真度，在基于潜在扩散的图像编辑任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">RONOM: Reduced-Order Neural Operator Modeling</div>
<div class="meta-line">Authors: Sven Dummer, Dongwei Ye, Christoph Brune</div>
<div class="meta-line">First: 2025-07-17T06:14:19+00:00 · Latest: 2026-01-23T09:47:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12814v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.12814v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-dependent partial differential equations are ubiquitous in physics-based modeling, but they remain computationally intensive in many-query scenarios, such as real-time forecasting, optimal control, and uncertainty quantification. Reduced-order modeling (ROM) addresses these challenges by constructing a low-dimensional surrogate model but relies on a fixed discretization, which limits flexibility across varying meshes during evaluation. Operator learning approaches, such as neural operators, offer an alternative by parameterizing mappings between infinite-dimensional function spaces, enabling adaptation to data across different resolutions. Whereas ROM provides rigorous numerical error estimates, neural operator learning largely focuses on discretization convergence and invariance without quantifying the error between the infinite-dimensional and the discretized operators. This work introduces the reduced-order neural operator modeling (RONOM) framework, which bridges concepts from ROM and operator learning. We establish a discretization error bound analogous to those in ROM, and get insights into RONOM&#x27;s discretization convergence and discretization robustness. Moreover, three numerical examples are presented that compare RONOM to existing neural operators for solving partial differential equations. The results demonstrate that RONOM using standard vector-to-vector neural networks can achieve comparable performance in input generalization and achieves superior performance in both spatial super-resolution and discretization robustness, while also offering novel insights into temporal super-resolution scenarios and ROM-based approaches for learning on time-dependent data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RONOM：降阶神经算子建模</div>
<div class="mono" style="margin-top:8px">时间相关的偏微分方程在基于物理的建模中无处不在，但在实时预测、最优控制和不确定性量化等多查询场景中仍然计算密集。降阶建模（ROM）通过构建低维代理模型来应对这些挑战，但依赖于固定的离散化，这限制了在评估过程中对不同网格的灵活性。算子学习方法，如神经算子，通过参数化无限维函数空间之间的映射，提供了一种替代方案，使其能够适应不同分辨率的数据。虽然ROM提供严格的数值误差估计，但神经算子学习主要关注离散化收敛性和不变性，而未量化无限维算子与离散化算子之间的误差。本研究介绍了降阶神经算子建模（RONOM）框架，桥接了ROM和算子学习的概念。我们建立了类似于ROM的离散化误差界限，并深入了解RONOM的离散化收敛性和离散化鲁棒性。此外，提供了三个数值示例，将RONOM与现有神经算子在求解偏微分方程方面进行比较。结果表明，使用标准向量到向量神经网络的RONOM在输入泛化方面可以实现可比的性能，并在空间超分辨率和离散化鲁棒性方面表现优越，同时还提供了对时间超分辨率场景和基于ROM的时间相关数据学习方法的新见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational challenges posed by time-dependent partial differential equations in various applications, such as real-time forecasting and optimal control, particularly in many-query scenarios. The authors propose the reduced-order neural operator modeling (RONOM) framework, which combines reduced-order modeling (ROM) with operator learning to create a flexible low-dimensional surrogate model that adapts to different discretizations. Experimental results indicate that RONOM, utilizing standard vector-to-vector neural networks, achieves comparable performance in input generalization and outperforms existing neural operators in spatial super-resolution and discretization robustness, while providing new insights into temporal super-resolution and ROM-based learning for time-dependent data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决时间依赖的偏微分方程在实时预测和最优控制等应用中带来的计算挑战。作者提出了减少阶神经算子建模（RONOM）框架，该框架结合了减少阶建模和算子学习，创建了一个灵活的低维替代模型，能够适应不同的离散化。实验结果表明，使用标准的向量到向量神经网络的RONOM在输入泛化方面表现出可比的性能，并在空间超分辨率和离散化鲁棒性方面优于现有的神经算子，同时还提供了关于时间超分辨率和基于ROM的时间依赖数据学习的新见解。</div>
</details>
</div>
<div class="card">
<div class="title">Text-to-Image Diffusion Models Cannot Count, and Prompt Refinement Cannot Help</div>
<div class="meta-line">Authors: Xuyang Guo, Jiayan Huo, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang, Zhen Zhuang</div>
<div class="meta-line">First: 2025-03-10T03:28:18+00:00 · Latest: 2026-01-23T07:51:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06884v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.06884v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling is widely regarded as one of the most essential problems in today&#x27;s AI community, with text-to-image generation having gained unprecedented real-world impacts. Among various approaches, diffusion models have achieved remarkable success and have become the de facto solution for text-to-image generation. However, despite their impressive performance, these models exhibit fundamental limitations in adhering to numerical constraints in user instructions, frequently generating images with an incorrect number of objects. While several prior works have mentioned this issue, a comprehensive and rigorous evaluation of this limitation remains lacking. To address this gap, we introduce T2ICountBench, a novel benchmark designed to rigorously evaluate the counting ability of state-of-the-art text-to-image diffusion models. Our benchmark encompasses a diverse set of generative models, including both open-source and private systems. It explicitly isolates counting performance from other capabilities, provides structured difficulty levels, and incorporates human evaluations to ensure high reliability.
  Extensive evaluations with T2ICountBench reveal that all state-of-the-art diffusion models fail to generate the correct number of objects, with accuracy dropping significantly as the number of objects increases. Additionally, an exploratory study on prompt refinement demonstrates that such simple interventions generally do not improve counting accuracy. Our findings highlight the inherent challenges in numerical understanding within diffusion models and point to promising directions for future improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散模型无法计数，提示优化无济于事</div>
<div class="mono" style="margin-top:8px">生成建模被广泛认为是当今人工智能社区中最重要的问题之一，文本到图像生成在现实世界中产生了前所未有的影响。在各种方法中，扩散模型取得了显著成功，成为文本到图像生成的事实标准解决方案。然而，尽管它们的表现令人印象深刻，这些模型在遵循用户指令中的数字约束方面存在根本性限制，常常生成物体数量不正确的图像。虽然一些先前的研究提到了这个问题，但对这一限制的全面和严格评估仍然缺乏。为了解决这一空白，我们引入了T2ICountBench，这是一个新颖的基准，旨在严格评估最先进的文本到图像扩散模型的计数能力。我们的基准涵盖了一组多样的生成模型，包括开源和私有系统。它明确将计数性能与其他能力隔离，提供结构化的难度级别，并纳入人类评估以确保高可靠性。使用T2ICountBench进行的广泛评估表明，所有最先进的扩散模型都未能生成正确数量的物体，随着物体数量的增加，准确性显著下降。此外，对提示优化的探索性研究表明，这种简单的干预通常不会提高计数准确性。我们的发现突显了扩散模型中数字理解的固有挑战，并指向未来改进的有希望方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the significant issue of text-to-image diffusion models&#x27; inability to accurately count objects as specified in user prompts, despite their success in generative modeling. The authors introduce T2ICountBench, a new benchmark designed to evaluate the counting capabilities of various state-of-the-art diffusion models, isolating counting performance from other generative abilities and incorporating human evaluations for reliability. The evaluation results indicate that all tested models struggle with counting accuracy, particularly as the number of objects increases, and attempts at prompt refinement do not yield improvements in counting performance, underscoring the limitations of these models in numerical understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像扩散模型的基本局限性，特别是它们无法遵循用户指令中的数字约束。作者引入了T2ICountBench，这是一个旨在严格评估各种最先进的文本到图像扩散模型计数能力的基准。实验结果表明，这些模型在生成正确数量的对象方面始终失败，随着对象数量的增加，准确性显著下降，而提示优化并未改善计数准确性，突显了这些模型在数字理解方面的挑战，并提出了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer</div>
<div class="meta-line">Authors: Tongcheng Fang, Hanling Zhang, Ruiqi Xie, Zhuo Han, Xin Tao, Tianchen Zhao, Pengfei Wan, Wenbo Ding, Wanli Ouyang, Xuefei Ning, Yu Wang</div>
<div class="meta-line">First: 2026-01-23T07:28:53+00:00 · Latest: 2026-01-23T07:28:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16515v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SALAD：通过高效线性注意力调优实现视频扩散变换器的高稀疏注意力</div>
<div class="mono" style="margin-top:8px">扩散变换器最近在视频生成中表现出色。然而，长输入序列导致由于全注意力的平方复杂度而产生高计算延迟。已经提出了各种稀疏注意力机制。无训练的稀疏注意力受限于稀疏性有限，因此提供的加速效果有限，而基于训练的方法可以达到更高的稀疏性，但需要大量数据和计算进行训练。在本研究中，我们提出了SALAD，引入了一个与稀疏注意力并行的轻量级线性注意力分支。通过结合一个依赖输入的门控机制来精细平衡两个分支，我们的方法实现了90%的稀疏性和1.72倍的推理加速，同时保持生成质量与全注意力基线相当。此外，我们的微调过程非常高效，仅需2,000个视频样本和1,600个训练步骤，批量大小为8。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational latency in video generation caused by the quadratic complexity of full attention in Diffusion Transformers. The authors propose SALAD, a method that integrates a lightweight linear attention branch alongside sparse attention, utilizing an input-dependent gating mechanism to optimize performance. Experimental results demonstrate that SALAD achieves 90% sparsity and a 1.72x speedup in inference while preserving generation quality comparable to the full attention baseline, requiring only 2,000 video samples and 1,600 training steps for fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散变换器中全注意力的二次复杂性导致的视频生成高计算延迟。作者提出了SALAD方法，引入了轻量级线性注意力分支与稀疏注意力并行，利用输入依赖的门控机制来优化性能。实验结果表明，SALAD实现了90%的稀疏性和1.72倍的推理加速，同时保持与全注意力基线相当的生成质量，仅需2000个视频样本和1600个训练步骤进行微调。</div>
</details>
</div>
<div class="card">
<div class="title">LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning</div>
<div class="meta-line">Authors: Obed Junias, Maria Leonor Pacheco</div>
<div class="meta-line">First: 2026-01-23T07:07:19+00:00 · Latest: 2026-01-23T07:07:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16504v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16504v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑常识问答：逻辑常识推理的基准测试</div>
<div class="mono" style="margin-top:8px">常识推理通常涉及评估多个合理的解释，而不是选择单一的原子答案，但大多数基准依赖于单标签评估，模糊了陈述是否共同合理、相互排斥或共同不合理。我们引入了LOGICAL-COMMONSENSEQA，一个将常识推理重新框架为对原子陈述对的逻辑组合的基准，使用合理性级别运算符（与、或、既不/也不）。在零-shot、少-shot和思维链提示下评估指令调优、推理专用和微调模型，我们发现模型在合取推理上表现合理，在析取推理上表现中等，但在基于否定的问题上性能急剧下降。LOGICAL-COMMONSENSEQA揭示了基本推理的局限性，并提供了一个受控框架，以推动组合常识推理的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing commonsense reasoning benchmarks that typically rely on single-label evaluations, which do not adequately capture the complexity of evaluating multiple plausible interpretations. The authors introduce LOGICAL-COMMONSENSEQA, a benchmark that reformulates commonsense reasoning as logical composition of atomic statements using plausibility-level operators such as AND, OR, and NEITHER/NOR. Experimental results show that while models perform reasonably well on conjunctive reasoning and moderately on disjunctive reasoning, their performance significantly declines on questions involving negation, highlighting fundamental limitations in current reasoning capabilities and providing a framework for improving compositional commonsense reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有常识推理基准的局限性，这些基准通常依赖于单标签评估，无法充分捕捉评估多个合理解释的复杂性。作者提出了LOGICAL-COMMONSENSEQA，这是一个将常识推理重新构建为对原子语句对的逻辑组合的基准，使用了诸如AND、OR和NEITHER/NOR等合理性级别运算符。实验结果表明，尽管模型在合取推理上表现合理，在析取推理上表现中等，但在基于否定的问题上性能显著下降，突显了基本推理的局限性，并提供了一个结构化框架以增强组合常识推理。</div>
</details>
</div>
<div class="card">
<div class="title">DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses</div>
<div class="meta-line">Authors: Wei Song, Zhenchang Xing, Liming Zhu, Yulei Sui, Jingling Xue</div>
<div class="meta-line">First: 2026-01-23T06:04:43+00:00 · Latest: 2026-01-23T06:04:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16473v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of realistic deepfakes has raised urgent concerns over their misuse, motivating the use of defensive watermarks in synthetic images for reliable detection and provenance tracking. However, this defense paradigm assumes such watermarks are inherently resistant to removal. We challenge this assumption with DeMark, a query-free black-box attack framework that targets defensive image watermarking schemes for deepfakes. DeMark exploits latent-space vulnerabilities in encoder-decoder watermarking models through a compressive sensing based sparsification process, suppressing watermark signals while preserving perceptual and structural realism appropriate for deepfakes. Across eight state-of-the-art watermarking schemes, DeMark reduces watermark detection accuracy from 100% to 32.9% on average while maintaining natural visual quality, outperforming existing attacks. We further evaluate three defense strategies, including image super resolution, sparse watermarking, and adversarial training, and find them largely ineffective. These results demonstrate that current encoder decoder watermarking schemes remain vulnerable to latent-space manipulations, underscoring the need for more robust watermarking methods to safeguard against deepfakes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeMark：一种无查询黑箱攻击深度伪造水印防御的框架</div>
<div class="mono" style="margin-top:8px">逼真的深度伪造技术的快速传播引发了对其滥用的紧迫担忧，促使在合成图像中使用防御性水印以实现可靠的检测和来源追踪。然而，这种防御范式假设这些水印本质上是抗去除的。我们通过DeMark挑战这一假设，DeMark是一个无查询黑箱攻击框架，针对深度伪造的防御性图像水印方案。DeMark通过基于压缩感知的稀疏化过程利用编码器-解码器水印模型中的潜在空间漏洞，抑制水印信号，同时保持适合深度伪造的感知和结构现实性。在八种最先进的水印方案中，DeMark将水印检测准确率从100%平均降低到32.9%，同时保持自然视觉质量，超越了现有攻击。我们进一步评估了三种防御策略，包括图像超分辨率、稀疏水印和对抗训练，发现它们在很大程度上无效。这些结果表明，当前的编码器-解码器水印方案仍然容易受到潜在空间操控的影响，强调了需要更强大的水印方法来保护免受深度伪造的威胁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of realistic deepfakes has prompted the implementation of watermarking techniques for detection and provenance tracking, yet these methods are assumed to be resistant to removal. This study introduces DeMark, a query-free black-box attack framework that targets watermarking defenses in deepfakes by exploiting vulnerabilities in encoder-decoder models through a compressive sensing-based sparsification process. The findings reveal that DeMark significantly decreases watermark detection accuracy from 100% to an average of 32.9% across eight advanced watermarking schemes while preserving the visual quality of the images, indicating that current watermarking strategies are susceptible to such attacks and highlighting the necessity for more robust defenses against deepfakes.</div>
<div class="mono" style="margin-top:8px">随着逼真深度伪造技术的迅速普及，采用水印技术进行检测和来源追踪的需求日益迫切，但这些方法被假定为抗去除的。该研究提出了DeMark，这是一种无查询的黑箱攻击框架，针对这些水印防御，通过基于压缩感知的稀疏化过程利用编码器-解码器模型中的脆弱性。实验结果表明，DeMark在八种先进水印方案中将水印检测准确率从100%显著降低到平均32.9%，同时保持了图像的视觉质量，并揭示现有的防御策略在抵御此类攻击方面基本无效。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology</div>
<div class="meta-line">Authors: Peixian Liang, Songhao Li, Shunsuke Koga, Yutong Li, Zahra Alipour, Yucheng Tang, Daguang Xu, Zhi Huang</div>
<div class="meta-line">First: 2026-01-23T05:06:57+00:00 · Latest: 2026-01-23T05:06:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16451v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16451v1">PDF</a> · <a href="https://github.com/zhihuanglab/VISTA-PATH">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA-PATH：一种用于计算病理学中病理图像分割和定量分析的交互式基础模型</div>
<div class="mono" style="margin-top:8px">准确的组织病理图像语义分割对定量组织分析和下游临床建模至关重要。最近的分割基础模型通过大规模预训练提高了泛化能力，但由于将分割视为静态视觉预测任务，仍与病理学不够对齐。我们提出了VISTA-PATH，这是一种交互式、类感知的病理分割基础模型，旨在解决异质结构，结合专家反馈，并生成对临床解释直接有意义的像素级分割。VISTA-PATH将分割共同条件化于视觉上下文、语义组织描述和可选的专家提供的空间提示，从而实现跨异质病理图像的精确多类分割。为了支持这一范式，我们策划了VISTA-PATH数据集，这是一个大规模的病理分割语料库，包含超过160万对图像-掩膜-文本三元组，涵盖9个器官和93个组织类别。在广泛的保留和外部基准测试中，VISTA-PATH始终优于现有的分割基础模型。重要的是，VISTA-PATH支持动态人机协作的细化，通过将稀疏的、基于补丁的边界框注释反馈传播到全幻灯片分割中。最后，我们展示了VISTA-PATH生成的高保真、类感知分割是计算病理学的首选模型。它通过提出的肿瘤相互作用评分（TIS）改善组织微环境分析，该评分与患者生存率表现出强烈和显著的关联。总之，这些结果确立了VISTA-PATH作为一种基础模型，将病理图像分割从静态预测提升为数字病理学的交互式和临床基础表示。源代码和演示可以在https://github.com/zhihuanglab/VISTA-PATH找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the accuracy of semantic segmentation in histopathology images, which is essential for quantitative tissue analysis and clinical modeling. The authors developed VISTA-PATH, an interactive and class-aware segmentation foundation model that integrates expert feedback and visual context to achieve precise multi-class segmentation of heterogeneous pathology images. Experimental results demonstrate that VISTA-PATH outperforms existing models across various benchmarks and supports dynamic human-in-the-loop refinement, ultimately improving tissue microenvironment analysis through the Tumor Interaction Score, which correlates significantly with patient survival outcomes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高组织病理图像中语义分割的准确性，这对定量组织分析和临床建模至关重要。作者开发了VISTA-PATH，这是一种互动的、类别感知的分割基础模型，能够整合专家反馈和视觉上下文，以实现对多种病理图像的精确多类分割。实验结果表明，VISTA-PATH在各种基准测试中优于现有模型，并支持通过人类反馈进行动态优化，最终通过新的肿瘤相互作用评分改善组织微环境分析，该评分与患者生存结果显著相关。</div>
</details>
</div>
<div class="card">
<div class="title">A Cosine Network for Image Super-Resolution</div>
<div class="meta-line">Authors: Chunwei Tian, Chengyuan Zhang, Bob Zhang, Zhiwu Li, C. L. Philip Chen, David Zhang</div>
<div class="meta-line">First: 2026-01-23T02:58:57+00:00 · Latest: 2026-01-23T02:58:57+00:00</div>
<div class="meta-line">Comments: in IEEE Transactions on Image Processing (2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16413v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16413v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于图像超分辨率的余弦网络</div>
<div class="mono" style="margin-top:8px">深度卷积神经网络可以利用层次信息逐步提取结构信息以恢复高质量图像。然而，在图像超分辨率中，保持所获得的结构信息的有效性至关重要。本文提出了一种用于图像超分辨率的余弦网络（CSRNet），通过改进网络架构和优化训练策略。为了提取互补的同源结构信息，设计了奇偶异构块以扩大架构差异并提高图像超分辨率的性能。结合线性和非线性结构信息可以克服同源信息的缺点，并增强所获得的结构信息在图像超分辨率中的鲁棒性。考虑到梯度下降的局部最小值，采用余弦退火机制通过执行热重启和调整学习率来优化训练过程。实验结果表明，所提出的CSRNet在图像超分辨率方面与最先进的方法具有竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of structural information preservation in image super-resolution using deep convolutional neural networks. The authors propose a cosine network for image super-resolution (CSRNet), which incorporates a novel network architecture and an optimized training strategy that includes a cosine annealing mechanism. Key experimental findings demonstrate that CSRNet effectively extracts complementary homologous structural information through the design of odd and even heterogeneous blocks, resulting in performance that is competitive with state-of-the-art methods in image super-resolution.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过深度卷积神经网络增强图像超分辨率中结构信息的有效性。作者提出了一种用于图像超分辨率的余弦网络（CSRNet），该网络结合了新颖的网络架构和优化的训练策略，包括余弦退火机制以更好地调整学习率。实验结果表明，CSRNet在图像超分辨率方面的性能与最先进的方法具有竞争力，能够通过异构块的设计有效提取和保留互补的结构信息。</div>
</details>
</div>
<div class="card">
<div class="title">On The Robustness of Foundational 3D Medical Image Segmentation Models Against Imprecise Visual Prompts</div>
<div class="meta-line">Authors: Soumitri Chattopadhyay, Basar Demir, Marc Niethammer</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-23T00:55:02+00:00 · Latest: 2026-01-23T00:55:02+00:00</div>
<div class="meta-line">Comments: Accepted at ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16383v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16383v1">PDF</a> · <a href="https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While 3D foundational models have shown promise for promptable segmentation of medical volumes, their robustness to imprecise prompts remains under-explored. In this work, we aim to address this gap by systematically studying the effect of various controlled perturbations of dense visual prompts, that closely mimic real-world imprecision. By conducting experiments with two recent foundational models on a multi-organ abdominal segmentation task, we reveal several facets of promptable medical segmentation, especially pertaining to reliance on visual shape and spatial cues, and the extent of resilience of models towards certain perturbations. Codes are available at: https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基础3D医学图像分割模型对不精确视觉提示的鲁棒性研究</div>
<div class="mono" style="margin-top:8px">尽管3D基础模型在医学体积的可提示分割中显示出潜力，但它们对不精确提示的鲁棒性仍然未得到充分探索。本研究旨在通过系统研究各种受控扰动的密集视觉提示的影响，填补这一空白，这些扰动与现实世界的不精确性密切相似。通过在多脏器腹部分割任务上对两个近期基础模型进行实验，我们揭示了可提示医学分割的多个方面，特别是与对视觉形状和空间线索的依赖以及模型对某些扰动的韧性程度相关。代码可在以下链接获取：https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the robustness of 3D foundational models for medical image segmentation against imprecise visual prompts, a topic that has not been thoroughly explored. The authors systematically examined the impact of various controlled perturbations of dense visual prompts that simulate real-world imprecision, using two recent foundational models on a multi-organ abdominal segmentation task. The experiments revealed important insights into the reliance of promptable medical segmentation on visual shape and spatial cues, as well as the models&#x27; resilience to specific perturbations.</div>
<div class="mono" style="margin-top:8px">本研究旨在填补对3D基础模型在面对不精确视觉提示时的医学图像分割鲁棒性理解的空白。作者系统地研究了对密集视觉提示的控制扰动对医学图像分割的影响，这些扰动模拟了现实世界中的不准确性，使用了两个最新的基础模型进行多脏器腹部分割任务。实验结果揭示了对视觉形状和空间线索的依赖程度，以及模型对特定扰动的鲁棒性，突显了可提示医学分割的重要方面。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：文档的基础视觉-语言前端</div>
<div class="mono" style="margin-top:8px">GutenOCR是一系列通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的基础OCR前端。生成的单检查点视觉-语言模型通过统一的基于提示的接口展示阅读、检测和定位。模型在商业文档、科学文章和合成定位数据上进行训练，支持全页和局部阅读，具有行和段落级的边界框以及条件查询“x在哪里？”。我们引入了一种基础OCR评估协议，并显示GutenOCR-7B在10.5K保留的商业和科学页面上，其复合基础OCR得分超过Qwen2.5-VL-7B主干的两倍（从0.40提高到0.82）。在Fox和OmniDocBench v1.5上，我们的方法显著提高了区域和行级OCR以及文本检测召回率，但在页面级线性化、颜色引导OCR和公式密集布局方面显示出权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GutenOCR is to enhance the capabilities of optical character recognition (OCR) for various document types by integrating vision and language processing. The method involves fine-tuning two models, Qwen2.5-VL-3B and Qwen2.5-VL-7B, to create a unified, prompt-based interface that supports reading and grounding tasks on business documents and scientific articles. Key experimental findings indicate that GutenOCR-7B significantly improves the composite grounded OCR score from 0.40 to 0.82 on a dataset of 10.5K pages, while also enhancing region- and line-level OCR performance, although it presents challenges in page-level linearization and handling complex layouts.</div>
<div class="mono" style="margin-top:8px">GutenOCR的研究动机在于通过统一的视觉语言模型增强光学字符识别（OCR）在处理各种文档类型中的能力。研究人员对Qwen2.5-VL-3B和Qwen2.5-VL-7B模型进行了微调，以创建一个支持全页和局部阅读的基础OCR前端，采用基于提示的接口。实验结果表明，GutenOCR-7B在10.5K商业和科学页面的数据集上将复合基础OCR得分从0.40显著提高到0.82，同时增强了区域和行级OCR性能，尽管在页面级线性化和处理复杂布局方面存在一些权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</div>
<div class="meta-line">Authors: Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</div>
<div class="meta-line">First: 2026-01-22T18:58:16+00:00 · Latest: 2026-01-22T18:58:16+00:00</div>
<div class="meta-line">Comments: website: https://rae-dit.github.io/scale-rae/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rae-dit.github.io/scale-rae/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用表示自编码器扩展文本到图像扩散变换器</div>
<div class="mono" style="margin-top:8px">表示自编码器（RAEs）在ImageNet上的扩散建模中显示出明显优势，通过在高维语义潜在空间中进行训练。在这项工作中，我们研究了该框架是否可以扩展到大规模、自由形式的文本到图像（T2I）生成。我们首先在冻结的表示编码器（SigLIP-2）上扩展RAE解码器，超越ImageNet，训练于网络、合成和文本渲染数据，发现尽管规模提高了整体保真度，但特定领域（如文本）的目标数据组合至关重要。然后，我们严格测试了最初为ImageNet提出的RAE设计选择。我们的分析表明，扩展简化了框架：尽管维度相关的噪声调度仍然至关重要，但诸如宽扩散头和噪声增强解码等架构复杂性在规模上提供的好处微乎其微。在这个简化的框架基础上，我们对RAE与最先进的FLUX VAE在0.5B到9.8B参数的扩散变换器规模上进行了受控比较。RAEs在所有模型规模的预训练中始终优于VAEs。此外，在高质量数据集上进行微调时，基于VAE的模型在64个周期后灾难性过拟合，而RAE模型在256个周期内保持稳定并实现持续更好的性能。在所有实验中，基于RAE的扩散模型表现出更快的收敛速度和更好的生成质量，确立了RAEs作为大规模T2I生成的比VAEs更简单、更强大的基础。此外，由于视觉理解和生成都可以在共享表示空间中操作，多模态模型可以直接对生成的潜在变量进行推理，为统一模型开辟了新的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to explore the scalability of Representation Autoencoders (RAEs) for large-scale, freeform text-to-image (T2I) generation, building on their advantages in diffusion modeling observed in ImageNet. The authors scaled RAE decoders using a frozen representation encoder (SigLIP-2) and trained on diverse datasets, revealing that while increasing scale enhances general fidelity, the composition of targeted data is crucial for specific domains like text. The study found that scaling simplifies the RAE framework, with dimension-dependent noise scheduling being critical, while complex architectural features provided minimal benefits. In a controlled comparison against the state-of-the-art FLUX VAE, RAEs consistently outperformed VAEs during pretraining across various model scales and maintained stability during finetuning, achieving better performance and faster convergence, thus establishing RAEs as a more effective foundation for large-scale T2I generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨表示自编码器（RAEs）在大规模自由形式文本到图像生成中的可扩展性，基于其在ImageNet中观察到的扩散建模优势。作者使用冻结的表示编码器（SigLIP-2）对RAE解码器进行了扩展，并在多样化的数据集上进行训练，发现尽管增加规模提高了整体保真度，但特定数据组合对文本相关任务至关重要。研究发现，扩展简化了RAE框架，关键的噪声调度仍然重要，但架构复杂性带来的好处微乎其微；RAEs在各种模型规模的预训练中优于最先进的FLUX VAEs，并在微调过程中保持稳定，取得了更好的性能和更快的收敛速度，生成高质量图像。</div>
</details>
</div>
<div class="card">
<div class="title">360Anything: Geometry-Free Lifting of Images and Videos to 360°</div>
<div class="meta-line">Authors: Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker, Saurabh Saxena</div>
<div class="meta-line">First: 2026-01-22T18:45:59+00:00 · Latest: 2026-01-22T18:45:59+00:00</div>
<div class="meta-line">Comments: Project page: https://360anything.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://360anything.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything&#x27;s deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>360Anything：无几何图像和视频提升至360°</div>
<div class="mono" style="margin-top:8px">将透视图像和视频提升为360°全景可以实现沉浸式3D世界生成。现有方法通常依赖于透视图与等矩形投影（ERP）空间之间的显式几何对齐。然而，这需要已知的相机元数据，这使得在缺乏或噪声较大的野外数据中应用变得困难。我们提出了360Anything，一个基于预训练扩散变换器的无几何框架。通过将透视输入和全景目标简单地视为令牌序列，360Anything以纯数据驱动的方式学习透视到等矩形的映射，消除了对相机信息的需求。我们的方法在图像和视频透视到360°生成上实现了最先进的性能，超越了使用真实相机信息的先前工作。我们还追踪到ERP边界处接缝伪影的根本原因是VAE编码器中的零填充，并引入了循环潜在编码以促进无缝生成。最后，我们在零-shot相机视场和方向估计基准测试中展示了竞争结果，证明了360Anything的深层几何理解和在计算机视觉任务中的更广泛应用。更多结果可在https://360anything.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enable the lifting of perspective images and videos to 360° panoramas without relying on explicit geometric alignment, which is often hindered by the absence of accurate camera metadata in real-world data. The authors propose a geometry-free framework called 360Anything, which utilizes pre-trained diffusion transformers to learn the mapping from perspective inputs to equirectangular outputs in a data-driven manner. Experimental results demonstrate that 360Anything achieves state-of-the-art performance in generating 360° panoramas from both images and videos, surpassing previous methods that depended on ground-truth camera information, while also addressing seam artifacts through Circular Latent Encoding and showing strong performance in zero-shot camera field of view and orientation estimation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于实现将透视图像和视频提升为360°全景，而不依赖于明确的几何对齐，因为在真实世界数据中，准确的相机元数据通常缺失。作者提出了一种名为360Anything的无几何框架，该框架利用预训练的扩散变换器，将透视输入与等矩形输出之间的映射作为令牌序列进行学习，从而消除了对相机信息的需求。实验结果表明，360Anything在生成360°图像和视频方面达到了最先进的性能，超越了依赖真实相机数据的先前方法，同时通过循环潜在编码解决了接缝伪影问题，并在零样本相机视场和方向估计任务中显示出竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">From Text to Image: Exploring GPT-4Vision&#x27;s Potential in Advanced Radiological Analysis across Subspecialties</div>
<div class="meta-line">Authors: Felix Busch, Tianyu Han, Marcus Makowski, Daniel Truhn, Keno Bressem, Lisa Adams</div>
<div class="meta-line">Venue: J Med Internet Res 2024;26:e54948</div>
<div class="meta-line">First: 2023-11-24T15:39:29+00:00 · Latest: 2026-01-22T18:06:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.14777v2">Abs</a> · <a href="https://arxiv.org/pdf/2311.14777v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The study evaluates and compares GPT-4 and GPT-4Vision for radiological tasks, suggesting GPT-4Vision may recognize radiological features from images, thereby enhancing its diagnostic potential over text-based descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从文本到图像：探索GPT-4Vision在各亚专业高级放射学分析中的潜力</div>
<div class="mono" style="margin-top:8px">本研究评估并比较了GPT-4和GPT-4Vision在放射学任务中的表现，建议GPT-4Vision可能从图像中识别放射学特征，从而增强其相较于基于文本描述的诊断潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the capabilities of GPT-4 and GPT-4Vision in performing advanced radiological analysis, motivated by the need for improved diagnostic tools in radiology. The researchers conducted evaluations and comparisons of both models to assess their performance on radiological tasks. The key findings indicate that GPT-4Vision demonstrates a superior ability to recognize radiological features from images, suggesting its enhanced diagnostic potential compared to traditional text-based descriptions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了GPT-4Vision在高级放射学分析中的能力，旨在满足放射学中对改进诊断工具的需求。研究人员比较了GPT-4和GPT-4Vision在识别图像中的放射学特征与基于文本的描述方面的表现。结果表明，GPT-4Vision通过有效识别图像中的特征，展现出更强的诊断潜力，暗示其优于传统的基于文本的方法。</div>
</details>
</div>
<div class="card">
<div class="title">ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</div>
<div class="meta-line">Authors: Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier</div>
<div class="meta-line">First: 2026-01-22T17:41:13+00:00 · Latest: 2026-01-22T17:41:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16148v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes &quot;in action&quot; in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed &quot;temporal 3D diffusion&quot;. Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActionMesh：基于时间3D扩散的动画3D网格生成</div>
<div class="mono" style="margin-top:8px">生成动画3D对象是许多应用的核心，但大多数先进的工作通常因其有限的设置、长时间的运行或有限的质量而难以实际应用。我们介绍了ActionMesh，这是一种生成模型，以前馈方式预测“动态”生产就绪的3D网格。我们的关键见解是修改现有的3D扩散模型以包含时间轴，从而形成我们称之为“时间3D扩散”的框架。具体而言，我们首先调整3D扩散阶段，以生成一系列同步的潜在变量，表示时间变化和独立的3D形状。其次，我们设计了一个时间3D自编码器，将一系列独立形状转换为预定义参考形状的相应变形，从而构建动画。结合这两个组件，ActionMesh能够从单目视频、文本描述或甚至带有描述其动画的文本提示的3D网格等不同输入生成动画3D网格。此外，与之前的方法相比，我们的方法快速且生成的结果无骨架且拓扑一致，从而实现快速迭代和无缝应用，如纹理处理和重定向。我们在标准视频到4D基准（Consistent4D，Objaverse）上评估了我们的模型，并报告了在几何精度和时间一致性方面的最新性能，证明我们的模型能够以空前的速度和质量交付动画3D网格。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of animated 3D objects, which is often hindered by practical limitations such as setup complexity and runtime. The authors introduce ActionMesh, a generative model that employs a novel approach called temporal 3D diffusion, which modifies existing 3D diffusion models to incorporate a temporal axis for generating synchronized sequences of 3D shapes. Experimental results show that ActionMesh achieves state-of-the-art performance in geometric accuracy and temporal consistency on standard benchmarks, enabling the rapid generation of rig-free and topology-consistent animated 3D meshes from various input sources, including monocular videos and text descriptions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善动画3D对象的生成，这一过程常常受到设置复杂性和处理时间长等实际限制的困扰。作者提出了ActionMesh，这是一种生成模型，通过修改3D扩散方法以纳入时间轴，从而实现以前馈方式生成3D网格。实验结果表明，ActionMesh在标准基准测试中在几何精度和时间一致性方面达到了最先进的性能，能够快速生成无骨架且拓扑一致的动画3D网格，输入类型包括单目视频和文本描述。</div>
</details>
</div>
<div class="card">
<div class="title">ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation</div>
<div class="meta-line">Authors: Yuan Lin, Murong Xu, Marc Hölle, Chinmay Prabhakar, Andreas Maier, Vasileios Belagiannis, Bjoern Menze, Suprosanna Shit</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2026-01-22T15:56:21+00:00 · Latest: 2026-01-22T15:56:21+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures. It has been accepted by IEEE ISBI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16060v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProGiDiff：基于提示引导的扩散医学图像分割</div>
<div class="mono" style="margin-top:8px">广泛采用的医学图像分割方法虽然高效，但主要是确定性的，且对自然语言提示的适应性较差。因此，它们缺乏估计多个提案、人机交互和跨模态适应的能力。最近，文本到图像的扩散模型显示出弥合这一差距的潜力。然而，从头开始训练它们需要大量数据集，这对医学图像分割构成了限制。此外，它们通常仅限于二元分割，无法基于自然语言提示进行条件化。为此，我们提出了一种新框架ProGiDiff，利用现有的图像生成模型进行医学图像分割。具体而言，我们提出了一种ControlNet风格的条件机制，配备定制编码器，适合图像条件化，以引导预训练的扩散模型输出分割掩膜。通过提示目标器官，它自然扩展到多类设置。我们在CT图像的器官分割实验中展示了与之前方法相比的强大性能，并且在专家参与的设置中可以大大受益于多个提案。重要的是，我们证明了学习到的条件机制可以通过低秩、少量样本适应轻松转移到MR图像分割。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve medical image segmentation methods, which are typically deterministic and not well-suited for natural language prompts, limiting their ability to handle multiple proposals and cross-modality adaptation. The authors propose a novel framework called ProGiDiff that utilizes existing image generation models and introduces a ControlNet-style conditioning mechanism with a custom encoder to guide a pre-trained diffusion model in producing segmentation masks. Experimental results on organ segmentation from CT images show that ProGiDiff outperforms previous methods and can effectively incorporate expert feedback, while also demonstrating the capability for low-rank, few-shot adaptation to segment MR images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有医学图像分割方法的局限性，这些方法主要是确定性的，且未能有效利用自然语言提示。作者提出了一种名为ProGiDiff的新框架，该框架采用ControlNet风格的条件机制和自定义编码器，引导预训练的扩散模型生成分割掩膜。在CT图像的器官分割实验中，ProGiDiff的表现优于以往的方法，并且能够通过低秩、少量适应来适应MR图像的分割，突显了其在利用多个提案的专家参与应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Embracing Ambiguity: Bayesian Nonparametrics and Stakeholder Participation for Ambiguity-Aware Safety Evaluation</div>
<div class="meta-line">Authors: Yanan Long</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-04-21T16:31:15+00:00 · Latest: 2026-01-22T15:49:05+00:00</div>
<div class="meta-line">Comments: AAAI 2026 workshop MURE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15211v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.15211v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluations of generative AI models often collapse nuanced behaviour into a single number computed for a single decoding configuration. Such point estimates obscure tail risks, demographic disparities, and the existence of multiple near-optimal operating points. We propose a unified framework that embraces multiplicity by modelling the distribution of harmful behaviour across the entire space of decoding knobs and prompts, quantifying risk through tail-focused metrics, and integrating stakeholder preferences. Our technical contributions are threefold: (i) we formalise decoding Rashomon sets, regions of knob space whose risk is near-optimal under given criteria and measure their size and disagreement; (ii) we develop a dependent Dirichlet process (DDP) mixture with stakeholder-conditioned stick-breaking weights to learn multi-modal harm surfaces; and (iii) we introduce an active sampling pipeline that uses Bayesian deep learning surrogates to explore knob space efficiently. Our approach bridges multiplicity theory, Bayesian nonparametrics, and stakeholder-aligned sensitivity analysis, paving the way for trustworthy deployment of generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拥抱模糊性：贝叶斯非参数方法与利益相关者参与的模糊安全评估</div>
<div class="mono" style="margin-top:8px">生成性人工智能模型的评估通常将细微的行为简化为单一数字，该数字是针对单一解码配置计算的。这种点估计掩盖了尾部风险、人口差异以及多个近似最优操作点的存在。我们提出了一个统一框架，通过对整个解码旋钮和提示空间中有害行为的分布建模，量化风险并整合利益相关者偏好，从而拥抱多样性。我们的技术贡献有三方面：（i）我们形式化了解码Rashomon集，即在给定标准下风险接近最优的旋钮空间区域，并测量其大小和分歧；（ii）我们开发了一种依赖于Dirichlet过程（DDP）混合模型，使用利益相关者条件的粘性权重来学习多模态危害表面；（iii）我们引入了一种主动采样管道，利用贝叶斯深度学习代理高效探索旋钮空间。我们的方法桥接了多样性理论、贝叶斯非参数方法和利益相关者对齐的敏感性分析，为生成模型的可信部署铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional evaluations of generative AI models, which often reduce complex behaviors to single point estimates, thereby obscuring important risks and disparities. The authors propose a unified framework that models the distribution of harmful behavior across various decoding configurations, utilizing tail-focused metrics and incorporating stakeholder preferences. Key findings include the formalization of decoding Rashomon sets to measure risk and disagreement, the development of a dependent Dirichlet process mixture for learning multi-modal harm surfaces, and the introduction of an active sampling pipeline that leverages Bayesian deep learning to efficiently explore the decoding space.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前生成AI模型评估的局限性，这些评估通常将复杂行为简化为单一的点估计，无法捕捉尾部风险和人口差异。作者提出了一个统一框架，通过建模不同解码配置下有害行为的分布，利用关注尾部的指标并结合利益相关者的偏好。主要实验结果包括对解码Rashomon集合的形式化，以测量风险和分歧，开发了依赖于Dirichlet过程的混合模型以学习多模态伤害表面，以及引入了一个主动采样管道，使用贝叶斯深度学习代理高效探索解码空间。</div>
</details>
</div>
<div class="card">
<div class="title">HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models</div>
<div class="meta-line">Authors: Xin Xie, Jiaxian Guo, Dong Gong</div>
<div class="meta-line">First: 2026-01-22T13:49:47+00:00 · Latest: 2026-01-22T13:49:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15968v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15968v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model&#x27;s generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperAlign：高效测试时对齐扩散模型的超网络</div>
<div class="mono" style="margin-top:8px">扩散模型实现了最先进的性能，但往往无法生成与人类偏好和意图一致的输出，导致图像的美学质量差和语义不一致。现有的对齐方法存在困难的权衡：微调方法在奖励过度优化时会失去多样性，而测试时缩放方法则引入了显著的计算开销，并倾向于欠优化。为了解决这些局限性，我们提出了HyperAlign，一个训练超网络以实现高效和有效测试时对齐的新框架。HyperAlign动态生成低秩适应权重来调节扩散模型的生成算子，而不是修改潜在状态。这使得去噪轨迹能够根据输入潜在、时间步和奖励条件对齐的提示进行自适应调整。我们引入了多个HyperAlign的变体，它们在超网络应用的频率上有所不同，以平衡性能和效率。此外，我们使用带有偏好数据的奖励分数目标来优化超网络，以减少奖励黑客行为。我们在多个扩展生成范式上评估HyperAlign，包括Stable Diffusion和FLUX。它在增强语义一致性和视觉吸引力方面显著优于现有的微调和测试时缩放基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of diffusion model outputs with human preferences, addressing issues of poor aesthetic quality and semantic inconsistencies. The authors propose HyperAlign, a framework that utilizes a hypernetwork to generate low-rank adaptation weights for modulating the diffusion model&#x27;s generation operators without altering latent states. Experimental results demonstrate that HyperAlign significantly enhances semantic consistency and visual appeal compared to existing fine-tuning and test-time scaling methods across various generative paradigms, including Stable Diffusion and FLUX.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善扩散模型与人类偏好和意图的对齐，解决生成图像中美学质量差和语义不一致的问题。作者提出了一种名为HyperAlign的新框架，该框架利用超网络动态生成低秩适应权重，以调节扩散模型的生成操作，而不是修改潜在状态。实验结果表明，HyperAlign在多个生成范式（包括Stable Diffusion和FLUX）中显著优于现有的微调和测试时缩放方法，在增强语义一致性和视觉吸引力方面表现出色。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
