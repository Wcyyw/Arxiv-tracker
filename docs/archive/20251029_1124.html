<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-29 11:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251029_1124</div>
    <div class="row"><div class="card">
<div class="title">Frequency-Aware Vision Transformers for High-Fidelity Super-Resolution   of Earth System Models</div>
<div class="meta-line">Authors: Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić</div>
<div class="meta-line">First: 2025-02-18T01:52:41+00:00 · Latest: 2025-10-28T16:06:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.12427v4">Abs</a> · <a href="http://arxiv.org/pdf/2502.12427v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth
System Model (ESM) outputs, allowing fine-scale structures vital to climate
science to be recovered from coarse simulations. However, traditional deep
super-resolution methods, including convolutional and transformer-based models,
tend to exhibit spectral bias, reconstructing low-frequency content more
readily than valuable high-frequency details. In this work, we introduce two
frequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit
Representation (ViSIR), combining Vision Transformers and sinusoidal
activations to mitigate spectral bias, and the Vision Transformer Fourier
Representation Network (ViFOR), which integrates explicit Fourier-based
filtering for independent low- and high-frequency learning. Evaluated on the
E3SM-HR Earth system dataset across surface temperature, shortwave, and
longwave fluxes, these models outperform leading CNN, GAN, and vanilla
transformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in
PSNR and significantly higher SSIM. Detailed ablation and scaling studies
highlight the benefit of full-field training, the impact of frequency
hyperparameters, and the potential for generalization. The results establish
ViFOR as a state-of-the-art, scalable solution for climate data downscaling.
Future extensions will address temporal super-resolution, multimodal climate
variables, automated parameter selection, and integration of physical
conservation constraints to broaden scientific applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>频率感知视觉变换器用于地球系统模型的高保真超分辨率</div>
<div class="mono" style="margin-top:8px">超分辨率（SR）对于提高地球系统模型（ESM）输出的空间保真度至关重要，使得气候科学所需的细尺度结构能够从粗糙的模拟中恢复。然而，传统的深度超分辨率方法，包括卷积和基于变换器的模型，往往表现出光谱偏差，更容易重建低频内容而非有价值的高频细节。在本研究中，我们引入了两个频率感知框架：视觉变换器调谐的正弦隐式表示（ViSIR），结合视觉变换器和正弦激活以减轻光谱偏差，以及视觉变换器傅里叶表示网络（ViFOR），它集成了显式傅里叶滤波以实现独立的低频和高频学习。在E3SM-HR地球系统数据集上评估，涵盖表面温度、短波和长波通量，这些模型的表现超越了领先的CNN、GAN和普通变换器基线，ViFOR在PSNR上显示出高达2.6~dB的改善，并且SSIM显著更高。详细的消融和缩放研究突出了全场训练的好处、频率超参数的影响以及泛化的潜力。结果确立了ViFOR作为气候数据下采样的最先进、可扩展的解决方案。未来的扩展将解决时间超分辨率、多模态气候变量、自动参数选择以及物理守恒约束的整合，以扩大科学适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the spatial fidelity of Earth System Model outputs through super-resolution techniques, which are essential for recovering fine-scale structures important for climate science. The authors propose two frequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit Representation (ViSIR) and the Vision Transformer Fourier Representation Network (ViFOR), which address the spectral bias commonly found in traditional deep super-resolution methods. Experimental results on the E3SM-HR Earth system dataset show that these models outperform existing CNN, GAN, and vanilla transformer baselines, with ViFOR achieving up to 2.6 dB improvements in PSNR and significantly higher SSIM, indicating their effectiveness in enhancing climate data downscaling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过超分辨率技术提高地球系统模型输出的空间保真度，这对于恢复气候科学中重要的细尺度结构至关重要。作者提出了两种频率感知框架：视觉变换器调谐的正弦隐式表示（ViSIR）和视觉变换器傅里叶表示网络（ViFOR），旨在解决传统深度超分辨率方法中常见的光谱偏差。对E3SM-HR地球系统数据集的实验结果表明，这些模型在性能上优于现有的CNN、GAN和普通变换器基线，其中ViFOR在PSNR上提高了多达2.6 dB，并显著提高了SSIM，表明其在气候数据下采样中的有效性及未来在时间超分辨率和多模态气候变量中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Delving into Cascaded Instability: A Lipschitz Continuity View on Image   Restoration and Object Detection Synergy</div>
<div class="meta-line">Authors: Qing Zhao, Weijian Deng, Pengxu Wei, ZiYi Dong, Hannan Lu, Xiangyang Ji, Liang Lin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-28T09:41:42+00:00 · Latest: 2025-10-28T09:41:42+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24232v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector&#x27;s feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入探讨级联不稳定性：基于Lipschitz连续性的图像恢复与目标检测协同视角</div>
<div class="mono" style="margin-top:8px">为了提高在恶劣条件下（例如雾霾和低光照）的检测鲁棒性，图像恢复通常作为预处理步骤应用，以增强检测器的图像质量。然而，恢复和检测网络之间的功能不匹配可能会引入不稳定性，并阻碍有效整合——这是一个尚未充分探讨的问题。我们通过Lipschitz连续性的视角重新审视这一局限，分析恢复和检测网络在输入空间和参数空间中的功能差异。我们的分析表明，恢复网络执行平滑、连续的变换，而目标检测器则在不连续的决策边界上操作，使其对微小扰动高度敏感。这种不匹配在传统级联框架中引入不稳定性，甚至恢复中的微小噪声在检测过程中被放大，干扰梯度流并阻碍优化。为了解决这个问题，我们提出了Lipschitz正则化目标检测（LROD），这是一个简单而有效的框架，将图像恢复直接整合到检测器的特征学习中，在训练过程中协调两个任务的Lipschitz连续性。我们将该框架实现为Lipschitz正则化YOLO（LR-YOLO），无缝扩展到现有的YOLO检测器。在雾霾和低光照基准上的大量实验表明，LR-YOLO始终提高了检测稳定性、优化平滑性和整体准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance detection robustness in challenging conditions such as haze and low light, where traditional image restoration methods can introduce instability due to functional mismatches with detection networks. The authors analyze this issue using Lipschitz continuity to highlight the differences in behavior between restoration and detection networks, revealing that restoration networks provide smooth transformations while detection networks have discontinuous decision boundaries. They propose a new framework called Lipschitz-regularized object detection (LROD), implemented as Lipschitz-regularized YOLO (LR-YOLO), which integrates image restoration into the detector&#x27;s feature learning process, resulting in improved detection stability, smoother optimization, and higher accuracy in extensive experiments on haze and low-light benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高在雾霾和低光等恶劣条件下的检测鲁棒性，传统的图像恢复方法在与检测网络的功能不匹配时可能引入不稳定性。作者通过利普希茨连续性的概念分析了这一问题，揭示了恢复网络提供平滑变换，而检测网络则具有不连续的决策边界，从而对扰动敏感。为了减轻这种不稳定性，他们提出了一种利普希茨正则化的目标检测框架（LROD），并实现为利普希茨正则化的YOLO（LR-YOLO），将图像恢复直接集成到检测器的特征学习过程中。针对雾霾和低光基准的实验结果表明，LR-YOLO显著提高了检测的稳定性、优化的平滑性和整体准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution   with Fourier Constraints</div>
<div class="meta-line">Authors: Kazutoshi Akita, Norimichi Ukita</div>
<div class="meta-line">First: 2025-10-28T01:19:54+00:00 · Latest: 2025-10-28T01:19:54+00:00</div>
<div class="meta-line">Comments: 9 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23978v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有傅里叶约束的高效成本与质量可控任意尺度超分辨率</div>
<div class="mono" style="margin-top:8px">在任意尺度超分辨率中，成本与质量（CQ）可控性至关重要。现有方法使用递归神经网络逐个预测傅里叶分量。然而，这种方法由于独立预测导致性能下降和效率低下。本文提出联合预测多个分量，以提高质量和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution, addressing the inefficiencies and performance degradation of existing methods that predict Fourier components independently using recurrent neural networks. The authors propose a novel method that predicts multiple Fourier components jointly, which aims to improve both the quality and efficiency of the super-resolution process. Experimental results demonstrate that this joint prediction approach significantly outperforms traditional methods in terms of both quality and computational efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高任意尺度超分辨率中的成本和质量（CQ）可控性，因为现有方法由于对傅里叶分量的独立预测而导致效率低下和性能下降。作者提出了一种新方法，通过联合预测多个傅里叶分量，而不是使用递归神经网络逐个预测。实验结果表明，与传统方法相比，这种方法显著提高了超分辨率结果的质量和效率。</div>
</details>
</div>
<div class="card">
<div class="title">RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution   of Rare-Earth Features</div>
<div class="meta-line">Authors: Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</div>
<div class="meta-line">First: 2025-10-27T19:56:43+00:00 · Latest: 2025-10-27T19:56:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23816v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow&#x27;s core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model&#x27;s
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RareFlow：面向物理的跨传感器稀土特征超分辨率流匹配</div>
<div class="mono" style="margin-top:8px">遥感图像的超分辨率（SR）在分布外（OOD）条件下常常失败，例如由不同传感器捕获的稀有地貌特征，产生视觉上合理但物理上不准确的结果。我们提出了RareFlow，一个旨在提高OOD鲁棒性的面向物理的SR框架。RareFlow的核心是一个双条件架构。门控控制网络从低分辨率输入中保留细粒度的几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上合理的输出，我们引入了一个多方面的损失函数，强制执行与传感器特性的一致性，包括光谱和辐射一致性。此外，该框架通过采用随机前向传播方法量化自身的预测不确定性；结果输出方差直接识别不熟悉的输入，减轻特征幻觉。我们在一个新的、精心策划的多传感器卫星图像基准上验证了RareFlow。在盲评中，地球物理专家将我们模型的输出评为接近真实图像的保真度，显著优于最先进的基线。这种定性优越性通过感知指标的定量提升得到了证实，包括FID减少近40%。RareFlow为数据稀缺的科学领域提供了一个高保真合成的强大框架，并为在严重领域转移下的受控生成提供了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of super-resolution (SR) in remote sensing imagery, particularly under out-of-distribution conditions where rare geomorphic features are captured by various sensors. The authors propose RareFlow, a physics-aware SR framework that utilizes a dual-conditioning architecture, combining a Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Experimental results demonstrate that RareFlow significantly outperforms existing methods, with geophysical experts rating its outputs as nearly equivalent to ground truth imagery, and achieving a nearly 40% reduction in Fréchet Inception Distance (FID), indicating its effectiveness in generating high-fidelity images in data-scarce environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决遥感图像超分辨率（SR）在分布外条件下的挑战，特别是当各种传感器捕捉到稀有地貌特征时，导致物理不准确的结果。作者开发了RareFlow，这是一种物理感知的SR框架，采用双重条件架构，利用门控控制网络保持几何保真度，并通过文本提示提供语义指导。实验结果表明，RareFlow显著优于最先进的方法，地球物理专家将其输出评为几乎等同于真实图像，并且FID减少近40%，表明其在数据稀缺的科学应用中实现高保真合成的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">An Efficient Remote Sensing Super Resolution Method Exploring Diffusion   Priors and Multi-Modal Constraints for Crop Type Mapping</div>
<div class="meta-line">Authors: Songxi Yang, Tang Sui, Qunying Huang</div>
<div class="meta-line">First: 2025-10-27T14:34:52+00:00 · Latest: 2025-10-27T14:34:52+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23382v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super resolution offers a way to harness medium even lowresolution but
historically valuable remote sensing image archives. Generative models,
especially diffusion models, have recently been applied to remote sensing super
resolution (RSSR), yet several challenges exist. First, diffusion models are
effective but require expensive training from scratch resources and have slow
inference speeds. Second, current methods have limited utilization of auxiliary
information as real-world constraints to reconstruct scientifically realistic
images. Finally, most current methods lack evaluation on downstream tasks. In
this study, we present a efficient LSSR framework for RSSR, supported by a new
multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built
on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention
with auxiliary knowledge (Digital Elevation Model, land cover, month) and
Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier
NDVI loss to balance spatial details and spectral fidelity. Extensive
experiments demonstrate that LSSR significantly improves crop boundary
delineation and recovery, achieving state-of-the-art performance with Peak
Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)
and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while
maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers
effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,
yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:
0.85). These results highlight the potential of RSSR to advance precision
agriculture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的遥感超分辨率方法：探索扩散先验和多模态约束用于作物类型映射</div>
<div class="mono" style="margin-top:8px">超分辨率提供了一种利用中等甚至低分辨率但历史上有价值的遥感图像档案的方法。生成模型，特别是扩散模型，最近被应用于遥感超分辨率（RSSR），但仍存在几个挑战。首先，扩散模型有效但需要昂贵的从零开始的训练资源，并且推理速度较慢。其次，当前方法对辅助信息的利用有限，无法作为现实世界约束来重建科学上真实的图像。最后，大多数当前方法缺乏对下游任务的评估。在本研究中，我们提出了一种高效的LSSR框架用于RSSR，支持一个新的多模态数据集，包含配对的30米Landsat 8和10米Sentinel 2影像。基于冻结的预训练稳定扩散，LSSR将跨模态注意力与辅助知识（数字高程模型、土地覆盖、月份）和合成孔径雷达指导相结合，通过适配器和定制的傅里叶NDVI损失来平衡空间细节和光谱保真度。大量实验表明，LSSR显著改善了作物边界的描绘和恢复，达到最先进的性能，峰值信噪比/结构相似性指数分别为32.63/0.84（RGB）和23.99/0.78（IR），并且NDVI均方误差最低（0.042），同时保持高效推理（0.39秒/图像）。此外，LSSR有效转移到NASA协调的Landsat和Sentinel（HLS）超分辨率，产生比Sentinel-2更可靠的作物分类（F1: 0.86，Sentinel-2: F1: 0.85）。这些结果突显了RSSR在推进精准农业方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the utilization of low-resolution remote sensing images for crop type mapping, addressing challenges such as expensive training and slow inference speeds associated with diffusion models. The study introduces an efficient LSSR framework that leverages a new multimodal dataset of paired Landsat 8 and Sentinel 2 imagery, integrating cross-modal attention with auxiliary knowledge and Synthetic Aperture Radar guidance. Experimental results show that LSSR enhances crop boundary delineation and recovery, achieving state-of-the-art performance metrics and efficient inference times, while also demonstrating effective transferability to NASA&#x27;s HLS super resolution for improved crop classification accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高低分辨率遥感图像在作物类型映射中的利用，解决与扩散模型相关的高训练成本和慢推理速度等挑战。作者提出了一种高效的LSSR框架，利用一组新的多模态数据集，该数据集由配对的Landsat 8和Sentinel 2影像组成，结合了跨模态注意力、辅助知识和定制的傅里叶NDVI损失。实验结果表明，LSSR显著提高了作物边界划分和恢复，达到了最先进的性能指标，包括峰值信噪比32.63和均方误差0.042，同时在NASA的HLS超分辨率中有效转移，提升了作物分类的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Residual Diffusion Bridge Model for Image Restoration</div>
<div class="meta-line">Authors: Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</div>
<div class="meta-line">First: 2025-10-27T08:35:49+00:00 · Latest: 2025-10-27T08:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23116v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23116v1">PDF</a> · <a href="https://github.com/MiliLab/RDBM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion bridge models establish probabilistic paths between arbitrary
paired distributions and exhibit great potential for universal image
restoration. Most existing methods merely treat them as simple variants of
stochastic interpolants, lacking a unified analytical perspective. Besides,
they indiscriminately reconstruct images through global noise injection and
removal, inevitably distorting undegraded regions due to imperfect
reconstruction. To address these challenges, we propose the Residual Diffusion
Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic
differential equations of generalized diffusion bridge and derive the
analytical formulas of its forward and reverse processes. Crucially, we
leverage the residuals from given distributions to modulate the noise injection
and removal, enabling adaptive restoration of degraded regions while preserving
intact others. Moreover, we unravel the fundamental mathematical essence of
existing bridge models, all of which are special cases of RDBM and empirically
demonstrate the optimality of our proposed models. Extensive experiments are
conducted to demonstrate the state-of-the-art performance of our method both
qualitatively and quantitatively across diverse image restoration tasks. Code
is publicly available at https://github.com/MiliLab/RDBM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像恢复的残差扩散桥模型</div>
<div class="mono" style="margin-top:8px">扩散桥模型在任意配对分布之间建立概率路径，展现出在通用图像恢复中的巨大潜力。现有大多数方法仅将其视为随机插值的简单变体，缺乏统一的分析视角。此外，它们通过全局噪声注入和去除不加区分地重建图像，因不完美的重建不可避免地扭曲未退化区域。为了解决这些挑战，我们提出了残差扩散桥模型（RDBM）。具体而言，我们从理论上重新表述了广义扩散桥的随机微分方程，并推导出其正向和反向过程的解析公式。关键是，我们利用给定分布的残差来调节噪声的注入和去除，实现对退化区域的自适应恢复，同时保留完整的其他区域。此外，我们揭示了现有桥模型的基本数学本质，所有这些模型都是RDBM的特例，并实证证明了我们提出模型的最优性。我们进行了广泛的实验，以定性和定量的方式展示我们方法在各种图像恢复任务中的最先进性能。代码可在 https://github.com/MiliLab/RDBM 上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image restoration techniques that often distort undegraded regions due to global noise injection and removal. The authors propose the Residual Diffusion Bridge Model (RDBM), which reformulates the stochastic differential equations of generalized diffusion bridges and derives analytical formulas for its processes. Experimental results show that RDBM enables adaptive restoration of degraded areas while preserving intact regions, demonstrating superior performance compared to existing methods across various image restoration tasks both qualitatively and quantitatively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有扩散桥模型的局限性来改进图像恢复技术，这些模型在重建过程中往往会扭曲未降解区域。作者提出了残差扩散桥模型（RDBM），重新公式化了广义扩散桥的随机微分方程，并推导出其过程的解析公式。实验结果表明，RDBM有效地利用给定分布的残差来调节噪声注入和去除，实现了对降解区域的自适应恢复，同时保留了完整区域，在各种图像恢复任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</div>
<div class="meta-line">Authors: Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-23T17:59:54+00:00 · Latest: 2025-10-26T09:13:56+00:00</div>
<div class="meta-line">Comments: Accepted as a poster at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20819v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.20819v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/lddbm/home">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对比和预测的潜在扩散桥的通用模态翻译</div>
<div class="mono" style="margin-top:8px">最近的生成建模进展使扩散模型成为从复杂数据分布中采样的最先进工具。尽管这些模型在图像和音频等单一模态领域取得了显著成功，但将其能力扩展到模态翻译（MT），即在不同感官模态之间翻译信息，仍然是一个开放的挑战。现有方法通常依赖于限制性假设，包括共享维度、高斯源先验和特定模态架构，这限制了它们的通用性和理论基础。在本研究中，我们提出了潜在去噪扩散桥模型（LDDBM），这是一个基于去噪扩散桥模型的潜变量扩展的通用模态翻译框架。通过在共享潜在空间中操作，我们的方法学习了任意模态之间的桥梁，而无需对齐维度。我们引入了一种对比对齐损失，以强制配对样本之间的语义一致性，并设计了一种针对潜在空间噪声预测的领域无关编码器-解码器架构。此外，我们提出了一种预测损失，以指导训练朝着准确的跨域翻译，并探索几种训练策略以提高稳定性。我们的方法支持任意模态对，并在多视图到3D形状生成、图像超分辨率和多视图场景合成等多种MT任务中表现出色。全面的实验和消融验证了我们框架的有效性，确立了通用模态翻译的新强基线。有关更多信息，请参见我们的项目页面：https://sites.google.com/view/lddbm/home。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing approaches in Modality Translation (MT), which often rely on restrictive assumptions that hinder their generality. The authors propose the Latent Denoising Diffusion Bridge Model (LDDBM), a framework that operates in a shared latent space to facilitate translation between arbitrary modalities without the need for aligned dimensions. Experimental results demonstrate that LDDBM achieves strong performance across various MT tasks, including multi-view to 3D shape generation and image super-resolution, validating its effectiveness and establishing a new baseline in the field.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有模态翻译方法的局限性，这些方法通常依赖于限制性假设，妨碍了其广泛适用性。作者提出了潜在去噪扩散桥模型（LDDBM），该框架在共享潜在空间中操作，以便在不需要对齐维度的情况下促进任意模态之间的翻译。实验结果表明，LDDBM在多种模态翻译任务中表现出色，包括多视图到3D形状生成和图像超分辨率，确立了该领域的新基准。</div>
</details>
</div>
<div class="card">
<div class="title">SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution   with Spatially Re-Focused Text-Conditioning</div>
<div class="meta-line">Authors: Chen Chen, Majid Abdolshah, Violetta Shevchenko, Hongdong Li, Chang Xu, Pulak Purkait</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-26T05:03:55+00:00 · Latest: 2025-10-26T05:03:55+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22534v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22534v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing diffusion-based super-resolution approaches often exhibit semantic
ambiguities due to inaccuracies and incompleteness in their text conditioning,
coupled with the inherent tendency for cross-attention to divert towards
irrelevant pixels. These limitations can lead to semantic misalignment and
hallucinated details in the generated high-resolution outputs. To address
these, we propose a novel, plug-and-play spatially re-focused super-resolution
(SRSR) framework that consists of two core components: first, we introduce
Spatially Re-focused Cross-Attention (SRCA), which refines text conditioning at
inference time by applying visually-grounded segmentation masks to guide
cross-attention. Second, we introduce a Spatially Targeted Classifier-Free
Guidance (STCFG) mechanism that selectively bypasses text influences on
ungrounded pixels to prevent hallucinations. Extensive experiments on both
synthetic and real-world datasets demonstrate that SRSR consistently
outperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR
and SSIM) across all datasets, and in perceptual quality measures (LPIPS and
DISTS) on two real-world benchmarks, underscoring its effectiveness in
achieving both high semantic fidelity and perceptual quality in
super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRSR：通过空间重新聚焦的文本条件增强现实世界图像超分辨率的语义准确性</div>
<div class="mono" style="margin-top:8px">现有的基于扩散的超分辨率方法由于文本条件的不准确和不完整，常常表现出语义模糊，加上交叉注意力固有的倾向于偏向无关像素。这些限制可能导致生成的高分辨率输出中的语义不对齐和幻觉细节。为了解决这些问题，我们提出了一种新颖的即插即用的空间重新聚焦超分辨率（SRSR）框架，包含两个核心组件：首先，我们引入了空间重新聚焦交叉注意力（SRCA），通过应用视觉基础的分割掩码在推理时精炼文本条件，以引导交叉注意力。其次，我们引入了一种空间定向无分类器引导（STCFG）机制，选择性地绕过对无基础像素的文本影响，以防止幻觉。在合成和真实世界数据集上的广泛实验表明，SRSR在所有数据集的标准保真度指标（PSNR和SSIM）上始终优于七个最先进的基线，并在两个真实世界基准的感知质量测量（LPIPS和DISTS）上表现出色，强调了其在超分辨率中实现高语义保真度和感知质量的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the semantic ambiguities and inaccuracies in existing diffusion-based super-resolution methods, which often lead to misaligned semantics and hallucinated details in high-resolution images. The authors propose a novel framework called Spatially Re-focused Super-Resolution (SRSR), which includes two main components: Spatially Re-focused Cross-Attention (SRCA) for refining text conditioning using segmentation masks, and Spatially Targeted Classifier-Free Guidance (STCFG) to minimize the influence of text on irrelevant pixels. Experimental results show that SRSR outperforms seven state-of-the-art methods in standard fidelity metrics and perceptual quality measures across various datasets, demonstrating its effectiveness in enhancing both semantic fidelity and perceptual quality in super-resolution tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有基于扩散的超分辨率方法中的语义模糊和不准确问题，这些问题常常导致高分辨率图像中的不一致和幻觉细节。作者提出了一种名为空间重聚焦超分辨率（SRSR）的新框架，其中包括两个主要组件：空间重聚焦交叉注意力（SRCA），通过使用分割掩码来精炼文本条件，以及空间定向无分类引导（STCFG），以减轻对无关像素的文本影响。实验结果表明，SRSR在各种数据集上显著优于七种最先进的方法，在保真度指标和感知质量方面均表现出色，证明了其在超分辨率任务中提高语义准确性的能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
