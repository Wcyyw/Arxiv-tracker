<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-21 04:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260121_0414</div>
    <div class="row"><div class="card">
<div class="title">Building Production-Ready Probes For Gemini</div>
<div class="meta-line">Authors: János Kramár, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy</div>
<div class="meta-line">First: 2026-01-16T18:54:29+00:00 · Latest: 2026-01-16T18:54:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11516v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google&#x27;s frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为Gemini构建生产就绪探针</div>
<div class="mono" style="margin-top:8px">前沿语言模型的能力正在迅速提升。因此，我们需要更强的措施来防止不法分子滥用日益强大的系统。先前的研究表明，激活探针可能是一种有前景的滥用缓解技术，但我们识别出一个关键的挑战：探针在重要的生产分布变化下无法泛化。特别是，我们发现从短上下文到长上下文输入的转变对现有探针架构来说是困难的。我们提出几种新的探针架构，以处理这种长上下文分布变化。
我们在网络攻击领域评估这些探针，测试它们对各种与生产相关的变化的鲁棒性，包括多轮对话、静态越狱和自适应红队。我们的结果表明，尽管多重最大值解决了上下文长度问题，但需要架构选择和在多样化分布上训练的结合，以实现广泛的泛化。此外，我们还表明，将探针与提示分类器配对可以以低成本实现最佳准确性，因为探针的计算效率。
这些发现为在用户面向的Gemini实例中成功部署滥用缓解探针提供了依据，Gemini是谷歌的前沿语言模型。最后，我们发现使用AlphaEvolve自动化改进探针架构搜索和自适应红队的早期积极结果，表明自动化某些AI安全研究已经成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the rapid advancements in frontier language model capabilities, which necessitate stronger safeguards against potential misuse. The authors propose new probe architectures designed to address the challenge of generalizing under significant production distribution shifts, particularly from short-context to long-context inputs. Their evaluation in the cyber-offensive domain reveals that while the multimax approach effectively manages context length, achieving broad generalization requires a combination of architectural choices and training on diverse distributions. Furthermore, they demonstrate that integrating probes with prompted classifiers enhances accuracy while maintaining computational efficiency, leading to successful deployment in Google&#x27;s Gemini language model and indicating the potential for automating improvements in AI safety research using AlphaEvolve.</div>
<div class="mono" style="margin-top:8px">前沿语言模型的快速发展需要有效的策略来防止恶意行为者的滥用。本研究解决了激活探针的挑战，尽管其在减轻滥用方面显示出潜力，但在重要的生产分布变化下难以泛化，尤其是在短上下文到长上下文输入的转变中。作者提出了新的探针架构，旨在更好地处理这些长上下文的变化，并在网络攻击领域评估其在各种相关变化下的表现。研究结果表明，尽管multimax架构改善了上下文长度的处理，但最佳的泛化需要架构选择和多样化训练分布的结合，探针与提示分类器的配对在低计算成本下实现了高准确率。这些见解促成了滥用减轻探针在谷歌的先进语言模型Gemini中的成功实施，并展示了使用AlphaEvolve自动改进探针架构和自适应红队的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model</div>
<div class="meta-line">Authors: Shuai Yuan, Tianwu Lin, Shuang Chen, Yu Xia, Peng Qin, Xiangyu Liu, Xiaoqing Xu, Nan Xu, Hongsheng Zhang, Jie Wang, Peng Gong</div>
<div class="meta-line">First: 2026-01-16T16:10:32+00:00 · Latest: 2026-01-16T16:10:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11400v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11400v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于稀疏标注的湿地制图：卫星图像时间序列与时序感知的任意分割模型</div>
<div class="mono" style="margin-top:8px">准确的湿地制图对生态系统监测至关重要，但密集的像素级标注成本过高，实际应用通常依赖稀疏的点标签，在这种情况下，现有的深度学习模型表现不佳，而强烈的季节性和年际湿地动态进一步使单日期图像不足，导致显著的制图误差；尽管像SAM这样的基础模型在点提示下显示出良好的泛化能力，但它们本质上是为静态图像设计的，无法建模时间信息，导致在异质湿地中产生碎片化的掩膜。为克服这些限制，我们提出了WetSAM，一个基于SAM的框架，通过双分支设计集成卫星图像时间序列，从稀疏点监督中进行湿地制图，其中一个时间提示分支通过分层适配器和动态时间聚合扩展SAM，以解开湿地特征与物候变异的关系，另一个空间分支采用时间约束的区域生长策略生成可靠的密集伪标签，同时双向一致性正则化共同优化两个分支。在八个约5000平方公里的全球区域进行的广泛实验表明，WetSAM显著优于最先进的方法，平均F1-score达到85.58%，并以最小的标注努力提供准确且结构一致的湿地分割，突显其强大的泛化能力和可扩展、低成本、高分辨率湿地制图的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for accurate wetland mapping for ecosystem monitoring, which is challenged by the high cost of dense pixel-level annotations and the inadequacy of single-date imagery due to wetland dynamics. The authors propose WetSAM, a framework that utilizes satellite image time series and a dual-branch design to enhance the Segment Anything Model (SAM) for wetland mapping from sparse point supervision. Key experimental results show that WetSAM significantly outperforms existing methods, achieving an average F1-score of 85.58% across eight global regions, demonstrating its effectiveness in providing accurate and consistent wetland segmentation with reduced labeling effort.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于准确的湿地映射对于生态系统监测的重要性，而密集的像素级注释成本高昂且单日期影像无法满足湿地动态的需求。作者提出了WetSAM框架，利用卫星影像时间序列和双分支设计来增强Segment Anything Model（SAM），以从稀疏点监督中映射湿地。实验结果显示，WetSAM在八个全球区域的测试中显著优于现有方法，平均F1分数达到85.58%，并以较少的标注工作提供准确且结构一致的湿地分割。</div>
</details>
</div>
<div class="card">
<div class="title">A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery</div>
<div class="meta-line">Authors: Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Oktay Karakus</div>
<div class="meta-line">First: 2025-08-08T15:50:40+00:00 · Latest: 2026-01-16T15:13:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06407v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06407v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种针对SAR图像中船舶目标的分类感知超分辨率框架</div>
<div class="mono" style="margin-top:8px">高分辨率图像在提高分类、检测和分割等视觉识别任务的性能中发挥着关键作用。在许多领域，包括遥感和监视，低分辨率图像可能限制自动分析的准确性。为了解决这个问题，超分辨率（SR）技术被广泛采用，试图从低分辨率输入重建高分辨率图像。相关的传统方法仅专注于基于像素级指标增强图像质量，导致超分辨率图像的保真度与下游分类性能之间的关系在很大程度上未被探索。这引发了一个关键问题：将分类目标直接整合到超分辨率过程中是否能进一步提高分类准确性？在本文中，我们尝试通过部署专门的算法策略来调查超分辨率与分类之间的关系，以回应这一问题。我们提出了一种新方法，通过优化考虑图像质量和分类性能的损失函数来提高合成孔径雷达图像的分辨率。我们的方法提高了图像质量，依据科学确定的图像质量指标进行测量，同时也增强了分类准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of visual recognition tasks in low-resolution synthetic aperture radar (SAR) imagery, which can hinder automated analysis in fields like remote sensing and surveillance. The authors propose a novel super-resolution framework that integrates classification objectives into the super-resolution process, optimizing loss functions to enhance both image quality and classification performance. Experimental results demonstrate that this approach not only improves image quality based on established metrics but also significantly enhances classification accuracy compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高低分辨率合成孔径雷达（SAR）图像在视觉识别任务中的表现，这在遥感和监视等领域可能会阻碍自动分析。作者提出了一种新颖的超分辨率框架，通过优化考虑图像质量和分类性能的损失函数，将分类目标整合到超分辨率过程中。实验结果表明，该方法不仅提高了图像质量（根据已建立的图像质量指标），而且与传统方法相比，显著改善了分类准确性。</div>
</details>
</div>
<div class="card">
<div class="title">SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</div>
<div class="meta-line">Authors: Gergely Dinya, András Gelencsér, Krisztina Kupán, Clemens Küpper, Kristóf Karacs, Anna Gelencsér-Horváth</div>
<div class="meta-line">First: 2026-01-16T13:55:10+00:00 · Latest: 2026-01-16T13:55:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11301v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine&#x27;&#x27; workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAMannot：基于SAM2的内存高效、本地开源交互视频实例分割框架</div>
<div class="mono" style="margin-top:8px">当前精确视频分割的研究工作流程常常在劳动密集型手动整理、昂贵的商业平台和/或侵犯隐私的云服务之间妥协。对高保真视频实例分割的需求常常受到手动标注瓶颈和云工具隐私问题的阻碍。我们提出了SAMannot，一个开源的本地框架，将Segment Anything Model 2（SAM2）集成到人机协作的工作流程中。为了解决基础模型的高资源需求，我们修改了SAM2的依赖关系，并实现了一个处理层，以最小化计算开销并最大化吞吐量，确保高度响应的用户界面。主要功能包括持久的实例身份管理、带障碍帧的自动“锁定和细化”工作流程，以及基于掩码骨架化的自动提示机制。SAMannot促进了YOLO和PNG格式的研究准备数据集的生成，并附带结构化交互日志。通过动物行为追踪用例和LVOS及DAVIS基准数据集的子集进行验证，该工具为复杂视频标注任务提供了可扩展、私密且具有成本效益的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of manual curation, high costs of commercial platforms, and privacy issues associated with cloud-based services in video instance segmentation. The authors developed SAMannot, an open-source, local framework that incorporates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow, while optimizing for resource efficiency. Experimental results demonstrate that SAMannot effectively manages persistent instance identities and automates workflows, proving to be a scalable and cost-effective solution for generating research-ready datasets, as validated through animal behavior tracking and benchmark datasets like LVOS and DAVIS.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频实例分割中手动标注、商业平台高成本和云服务隐私问题的挑战。作者开发了SAMannot，这是一个开源的本地框架，将Segment Anything Model 2（SAM2）集成到人机协作工作流程中，同时优化资源效率。实验结果表明，SAMannot有效地促进了YOLO和PNG格式研究准备数据集的创建，具有持久实例身份管理和自动化工作流程等功能，并通过动物行为追踪和LVOS、DAVIS基准数据集的用例进行了验证。</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-16T13:04:59+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：一个统一的视觉生成器，具有交错的全模态上下文</div>
<div class="mono" style="margin-top:8px">我们提出了VINO，一个统一的视觉生成器，在一个框架内执行图像和视频的生成与编辑。VINO不依赖于特定任务的模型或每种模态的独立模块，而是使用一个共享的扩散骨干网络，基于文本、图像和视频进行条件处理，从而在一个模型下实现广泛的视觉创作和编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）结合，其中多模态输入被编码为交错的条件标记，然后用于指导扩散过程。该设计支持多参考基础、长格式指令跟随以及在静态和动态内容中保持一致的身份，同时避免特定模态的架构组件。为了训练这样一个统一的系统，我们引入了一个多阶段训练管道，逐步将视频生成基础模型扩展为一个统一的多任务生成器，能够处理图像和视频的输入和输出。在多样的生成和编辑基准测试中，VINO展示了强大的视觉质量、忠实的指令跟随、改进的参考和属性保留，以及更可控的多身份编辑。我们的结果突显了可扩展统一视觉生成的实际路径，以及交错的上下文计算作为通用视觉创作基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the development of VINO is to create a unified visual generator that can handle both image and video generation and editing tasks within a single framework, eliminating the need for task-specific models. The main method involves coupling a vision-language model with a Multimodal Diffusion Transformer, which encodes multimodal inputs as interleaved conditioning tokens to guide the diffusion process. Key experimental findings indicate that VINO achieves strong visual quality, effectively follows instructions, preserves references and attributes, and allows for more controllable multi-identity edits across various generation and editing benchmarks.</div>
<div class="mono" style="margin-top:8px">VINO的开发动机是创建一个统一的视觉生成器，能够在单一框架内处理图像和视频的生成与编辑任务，从而消除对特定任务模型的需求。该方法结合了视觉语言模型和多模态扩散变换器，将多模态输入编码为交错的条件标记，以引导扩散过程。主要实验结果表明，VINO在各种生成和编辑基准测试中实现了强大的视觉质量，有效地遵循指令，保持参考和属性，并允许更可控的多身份编辑。</div>
</details>
</div>
<div class="card">
<div class="title">Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification</div>
<div class="meta-line">Authors: Zhiqi Pang, Lingling Zhao, Yang Liu, Chunyu Wang, Gaurav Sharma</div>
<div class="meta-line">First: 2026-01-16T12:45:01+00:00 · Latest: 2026-01-16T12:45:01+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无监督多场景人物重识别的图像-文本知识建模</div>
<div class="mono" style="margin-top:8px">我们提出无监督多场景（UMS）人物重识别（ReID）作为一种新任务，在单一一致框架内扩展ReID到多样化场景（跨分辨率、换装等）。为了解决UMS-ReID，我们引入图像-文本知识建模（ITKM）——一个三阶段框架，有效利用视觉-语言模型的表征能力。我们从一个预训练的CLIP模型开始，包含图像编码器和文本编码器。在第一阶段，我们在图像编码器中引入场景嵌入，并微调编码器以自适应地利用来自多个场景的知识。在第二阶段，我们优化一组学习到的文本嵌入，以与第一阶段的伪标签关联，并引入多场景分离损失，以增加场景间文本表征的差异性。在第三阶段，我们首先引入集群级和实例级异构匹配模块，以在每个场景内获得可靠的异构正样本对（例如，同一人的可见图像和红外图像）。接下来，我们提出动态文本表征更新策略，以保持文本和图像监督信号之间的一致性。多个场景的实验结果证明了ITKM的优越性和泛化能力；它不仅优于现有的场景特定方法，还通过整合来自多个场景的知识提升了整体性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of unsupervised multi-scenario person re-identification (ReID), which involves recognizing individuals across varying conditions such as resolution and clothing changes. The authors propose a novel image-text knowledge modeling (ITKM) framework that consists of three stages, starting with a pre-trained CLIP model to adaptively incorporate scenario knowledge. Key experimental findings indicate that ITKM significantly outperforms existing methods tailored to specific scenarios and improves overall performance by effectively integrating knowledge from diverse scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决无监督多场景人物重识别（ReID）中的挑战，该任务涉及在不同条件下（如分辨率和服装变化）识别个体。作者提出了一种名为图像-文本知识建模（ITKM）的三阶段框架，该框架利用视觉-语言模型，从预训练的CLIP模型开始。该方法包括使用场景嵌入微调图像编码器、优化与伪标签关联的文本嵌入，并采用异构匹配模块创建可靠的正样本对。实验结果表明，ITKM优于现有的特定场景方法，并通过有效整合多场景知识提高了整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">Bio-inspired fine-tuning for selective transfer learning in image classification</div>
<div class="meta-line">Authors: Ana Davila, Jacinto Colan, Yasuhisa Hasegawa</div>
<div class="meta-line">Venue: Published in IEEE Access, vol. 13, pp. 129234-129249, 2025</div>
<div class="meta-line">First: 2026-01-16T12:28:49+00:00 · Latest: 2026-01-16T12:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11235v1">PDF</a> · <a href="https://github.com/davilac/BioTune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune&#x27;s key components on overall performance. The source code is available at https://github.com/davilac/BioTune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生物启发的图像分类选择性迁移学习微调</div>
<div class="mono" style="margin-top:8px">深度学习在各个领域的图像分析中取得了显著进展，但通常依赖于大量标注数据集。迁移学习通过利用预训练模型来应对这一挑战，以处理有限标注数据的新任务。然而，源领域和目标领域之间的差异可能会阻碍有效的迁移学习。我们提出了BioTune，一种利用进化优化的创新自适应微调技术。BioTune通过最佳选择冻结层和调整未冻结层的学习率来增强迁移学习。通过对九个图像分类数据集的广泛评估，涵盖自然和专业领域（如医学成像），BioTune在准确性和效率上优于最先进的微调方法，包括AutoRGN和LoRA，突显了其对各种数据特征和分布变化的适应性。此外，BioTune在四种不同的CNN架构中始终实现最佳性能，强调了其灵活性。消融研究提供了关于BioTune关键组件对整体性能影响的宝贵见解。源代码可在https://github.com/davilac/BioTune获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve transfer learning in image classification, particularly when faced with limited labeled data and discrepancies between source and target domains. The authors propose BioTune, an adaptive fine-tuning technique that employs evolutionary optimization to determine which layers to freeze and how to adjust learning rates for the remaining layers. Experimental results across nine diverse image classification datasets indicate that BioTune outperforms existing fine-tuning methods, achieving higher accuracy and efficiency while demonstrating adaptability across various data characteristics and CNN architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善图像分类中的迁移学习，特别是在面对有限标注数据和源域与目标域之间差异时。作者提出了BioTune，这是一种自适应微调技术，采用进化优化方法选择性地冻结层并调整未冻结层的学习率。通过在九个图像分类数据集上的实验结果表明，BioTune在准确性和效率上优于现有的微调方法，并展示了其在不同数据特征和卷积神经网络架构中的适应性，消融研究突显了其关键组件的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">LoRA as Oracle</div>
<div class="meta-line">Authors: Marco Arazzi, Antonino Nocera</div>
<div class="meta-line">First: 2026-01-16T11:32:32+00:00 · Latest: 2026-01-16T11:32:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Backdoored and privacy-leaking deep neural networks pose a serious threat to the deployment of machine learning systems in security-critical settings. Existing defenses for backdoor detection and membership inference typically require access to clean reference models, extensive retraining, or strong assumptions about the attack mechanism. In this work, we introduce a novel LoRA-based oracle framework that leverages low-rank adaptation modules as a lightweight, model-agnostic probe for both backdoor detection and membership inference.
  Our approach attaches task-specific LoRA adapters to a frozen backbone and analyzes their optimization dynamics and representation shifts when exposed to suspicious samples. We show that poisoned and member samples induce distinctive low-rank updates that differ significantly from those generated by clean or non-member data. These signals can be measured using simple ranking and energy-based statistics, enabling reliable inference without access to the original training data or modification of the deployed model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoRA作为Oracle</div>
<div class="mono" style="margin-top:8px">后门和隐私泄露的深度神经网络对安全关键环境中机器学习系统的部署构成严重威胁。现有的后门检测和成员推断防御通常需要访问干净的参考模型、广泛的重新训练或对攻击机制的强假设。在本研究中，我们引入了一种新颖的基于LoRA的Oracle框架，利用低秩适应模块作为轻量级、模型无关的探测器，用于后门检测和成员推断。我们的方案将任务特定的LoRA适配器附加到冻结的主干网络上，并分析其在暴露于可疑样本时的优化动态和表示变化。我们表明，污染样本和成员样本会引发显著不同于干净或非成员数据生成的低秩更新。这些信号可以通过简单的排名和基于能量的统计进行测量，从而在不访问原始训练数据或修改部署模型的情况下实现可靠推断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by backdoored and privacy-leaking deep neural networks in security-critical applications, where existing defenses often rely on clean reference models or extensive retraining. The authors propose a novel LoRA-based oracle framework that utilizes low-rank adaptation modules as a lightweight and model-agnostic method for backdoor detection and membership inference. Their experimental results demonstrate that poisoned and member samples produce unique low-rank updates that can be effectively distinguished from clean or non-member data, allowing for reliable inference without needing access to the original training data or altering the deployed model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在安全关键应用中，后门和隐私泄露的深度神经网络所带来的重大威胁，而现有的防御措施通常需要干净的参考模型或大量的再训练。作者提出了一种新颖的基于LoRA的oracle框架，利用低秩适配模块作为轻量级、模型无关的方法来进行后门检测和成员推断。关键实验结果表明，污染样本和成员样本在任务特定的LoRA适配器的优化动态中产生了独特的低秩更新，这些更新可以通过简单的排名和基于能量的统计方法有效测量，从而在不需要访问原始训练数据或修改已部署模型的情况下实现可靠推断。</div>
</details>
</div>
<div class="card">
<div class="title">Shape-morphing programming of soft materials on complex geometries via neural operator</div>
<div class="meta-line">Authors: Lu Chen, Gengxiang Chen, Xu Liu, Jingyan Su, Xuhao Lyu, Lihui Wang, Yingguang Li</div>
<div class="meta-line">First: 2026-01-16T09:36:58+00:00 · Latest: 2026-01-16T09:36:58+00:00</div>
<div class="meta-line">Comments: 20 pages,5 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11126v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator&#x27;s discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过神经算子在复杂几何体上进行软材料的形状变换编程</div>
<div class="mono" style="margin-top:8px">形状变换的软材料可以通过体素级材料分布设计实现多样的目标形态，为各种应用提供了显著潜力。尽管在简单几何体的基本形状变换设计方面取得了进展，但实现诸如符合性植入部署或气动变形等高级应用需要在复杂几何体上进行准确和多样的变换设计，这仍然具有挑战性。在此，我们提出了一种谱空间神经算子（S2NO），它能够在复杂几何体上实现高保真度的变换预测。S2NO通过整合拉普拉斯特征函数编码和空间卷积，有效捕捉不规则计算域上的全局和局部变换行为。将S2NO与进化算法相结合，可以实现对各种复杂几何体（包括不规则边界形状、多孔结构和薄壁结构）进行体素级材料分布的优化。此外，神经算子的离散不变性特性使得超分辨率材料分布设计成为可能，进一步扩展了变换设计的多样性和复杂性。这些进展显著提高了编程复杂形状变换的效率和能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for advanced shape-morphing designs in soft materials to enable applications like conformal implants and aerodynamic morphing on complex geometries. The authors introduce a Spectral and Spatial Neural Operator (S2NO) that predicts morphing behaviors with high fidelity by integrating Laplacian eigenfunction encoding and spatial convolutions, allowing for voxel-level optimization of material distributions. Key experimental results demonstrate that combining S2NO with evolutionary algorithms significantly enhances the efficiency and capability of programming complex shape morphing across various geometries, including irregular shapes and porous structures.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高形状变形软材料在复杂几何体上的设计能力，以满足先进应用的需求，尽管在简单形状上已有进展。作者提出了一种谱空间神经算子（S2NO），通过结合拉普拉斯特征函数编码和空间卷积，以高保真度预测变形行为，从而有效实现材料分布的体素级优化。主要实验结果表明，将S2NO与进化算法相结合显著提高了复杂形状变形编程的效率和能力，使得在不规则边界形状、多孔结构和薄壁结构上实现多样化设计成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation</div>
<div class="meta-line">Authors: Shuai Tan, Biao Gong, Ke Ma, Yutong Feng, Qiyuan Zhang, Yan Wang, Yujun Shen, Hengshuang Zhao</div>
<div class="meta-line">First: 2026-01-16T08:53:09+00:00 · Latest: 2026-01-16T08:53:09+00:00</div>
<div class="meta-line">Comments: https://lucaria-academy.github.io/CoDance/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lucaria-academy.github.io/CoDance/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoDance：一种用于鲁棒多主体动画的解绑定-重绑定范式</div>
<div class="mono" style="margin-top:8px">角色图像动画在各个领域变得越来越重要，受到对鲁棒和灵活的多主体渲染需求的驱动。虽然现有方法在单人动画方面表现出色，但在处理任意主体数量、多样化角色类型以及参考图像与驱动姿势之间的空间错位时却显得力不从心。我们将这些局限归因于过于严格的空间绑定，强迫姿势与参考之间进行严格的像素级对齐，以及无法始终如一地将运动重新绑定到预期主体。为了解决这些挑战，我们提出了CoDance，一种新颖的解绑定-重绑定框架，能够在单个可能错位的姿势序列的条件下，实现任意主体数量、类型和空间配置的动画。具体而言，解绑定模块采用一种新颖的姿势偏移编码器，通过对姿势及其潜在特征引入随机扰动，打破姿势与参考之间的严格空间绑定，从而迫使模型学习位置无关的运动表示。为了确保精确控制和主体关联，我们设计了重绑定模块，利用文本提示的语义指导和主体掩码的空间指导，将学习到的运动引导到预期角色。此外，为了便于全面评估，我们引入了新的多主体CoDanceBench。在CoDanceBench和现有数据集上的大量实验表明，CoDance达到了SOTA性能，在多样化主体和空间布局中展现出显著的泛化能力。代码和权重将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve character image animation for multiple subjects, addressing the limitations of existing methods that struggle with varying subject counts and spatial misalignment. The authors propose CoDance, an Unbind-Rebind framework that utilizes a pose shift encoder to decouple the rigid spatial binding between poses and references, allowing for the animation of diverse subjects based on a single pose sequence. Experimental results demonstrate that CoDance achieves state-of-the-art performance on the newly introduced CoDanceBench and existing datasets, showcasing its ability to generalize effectively across different subjects and spatial configurations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善多主体的角色图像动画，解决现有方法在处理不同主体数量和空间不对齐时的局限性。作者提出了CoDance，一个解绑-重绑定框架，利用姿势偏移编码器解耦姿势与参考图像之间的严格空间绑定，从而实现更灵活的运动表示。实验结果表明，CoDance在新引入的CoDanceBench和现有数据集上达到了最先进的性能，展示了其在不同主体和配置下的有效泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SAM-pose2seg: Pose-Guided Human Instance Segmentation in Crowds</div>
<div class="meta-line">Authors: Constantin Kolomiiets, Miroslav Purkrabek, Jiri Matas</div>
<div class="meta-line">First: 2026-01-13T21:12:03+00:00 · Latest: 2026-01-16T08:36:57+00:00</div>
<div class="meta-line">Comments: GitHub: https://github.com/MiraPurkrabek/BBoxMaskPose/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08982v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08982v2">PDF</a> · <a href="https://github.com/MiraPurkrabek/BBoxMaskPose/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://mirapurkrabek.github.io/BBox-Mask-Pose/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segment Anything (SAM) provides an unprecedented foundation for human segmentation, but may struggle under occlusion, where keypoints may be partially or fully invisible. We adapt SAM 2.1 for pose-guided segmentation with minimal encoder modifications, retaining its strong generalization. Using a fine-tuning strategy called PoseMaskRefine, we incorporate pose keypoints with high visibility into the iterative correction process originally employed by SAM, yielding improved robustness and accuracy across multiple datasets. During inference, we simplify prompting by selecting only the three keypoints with the highest visibility. This strategy reduces sensitivity to common errors, such as missing body parts or misclassified clothing, and allows accurate mask prediction from as few as a single keypoint. Our results demonstrate that pose-guided fine-tuning of SAM enables effective, occlusion-aware human segmentation while preserving the generalization capabilities of the original model. The code and pretrained models will be available at https://mirapurkrabek.github.io/BBox-Mask-Pose/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM-pose2seg：人群中的姿态引导人类实例分割</div>
<div class="mono" style="margin-top:8px">Segment Anything (SAM) 为人类分割提供了前所未有的基础，但在遮挡情况下可能会遇到困难，此时关键点可能部分或完全不可见。我们对 SAM 2.1 进行了适应性修改，以实现姿态引导的分割，保持其强大的泛化能力。通过一种称为 PoseMaskRefine 的微调策略，我们将高可见性的姿态关键点纳入 SAM 原有的迭代修正过程中，从而在多个数据集上提高了鲁棒性和准确性。在推理过程中，我们通过仅选择三个可见性最高的关键点来简化提示。这一策略减少了对常见错误的敏感性，例如缺失身体部位或错误分类的衣物，并允许从少至一个关键点进行准确的掩膜预测。我们的结果表明，SAM 的姿态引导微调能够有效地进行考虑遮挡的人类分割，同时保留原始模型的泛化能力。代码和预训练模型将可在 https://mirapurkrabek.github.io/BBox-Mask-Pose/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of the Segment Anything Model (SAM) in human segmentation under occlusion, where keypoints may not be fully visible. The authors adapt SAM 2.1 for pose-guided segmentation by implementing a fine-tuning strategy called PoseMaskRefine, which integrates visible pose keypoints into the iterative correction process of SAM. The experimental results show that this approach enhances robustness and accuracy in human instance segmentation across various datasets, allowing for effective mask prediction even with minimal keypoint input, thus maintaining the model&#x27;s generalization capabilities while addressing occlusion challenges.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高拥挤环境中人类实例分割的效果，特别是在关键点可能不完全可见的遮挡情况下。作者通过实施一种称为PoseMaskRefine的微调策略，将可见的姿态关键点整合到SAM的迭代修正过程中，从而将Segment Anything Model (SAM) 2.1适应于姿态引导的分割。实验结果表明，这种方法显著提高了在多个数据集上的分割任务的鲁棒性和准确性，即使在关键点输入最少的情况下也能有效预测掩膜，从而展示了模型在遮挡感知能力的同时保持其泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Generation of Chest CT pulmonary Nodule Images by Latent Diffusion Models using the LIDC-IDRI Dataset</div>
<div class="meta-line">Authors: Kaito Urata, Maiko Nagao, Atsushi Teramoto, Kazuyoshi Imaizumi, Masashi Kondo, Hiroshi Fujita</div>
<div class="meta-line">First: 2026-01-16T08:36:12+00:00 · Latest: 2026-01-16T08:36:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11085v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, computer-aided diagnosis systems have been developed to support diagnosis, but their performance depends heavily on the quality and quantity of training data. However, in clinical practice, it is difficult to collect the large amount of CT images for specific cases, such as small cell carcinoma with low epidemiological incidence or benign tumors that are difficult to distinguish from malignant ones. This leads to the challenge of data imbalance. In this study, to address this issue, we proposed a method to automatically generate chest CT nodule images that capture target features using latent diffusion models (LDM) and verified its effectiveness. Using the LIDC-IDRI dataset, we created pairs of nodule images and finding-based text prompts based on physician evaluations. For the image generation models, we used Stable Diffusion version 1.5 (SDv1) and 2.0 (SDv2), which are types of LDM. Each model was fine-tuned using the created dataset. During the generation process, we adjusted the guidance scale (GS), which indicates the fidelity to the input text. Both quantitative and subjective evaluations showed that SDv2 (GS = 5) achieved the best performance in terms of image quality, diversity, and text consistency. In the subjective evaluation, no statistically significant differences were observed between the generated images and real images, confirming that the quality was equivalent to real clinical images. We proposed a method for generating chest CT nodule images based on input text using LDM. Evaluation results demonstrated that the proposed method could generate high-quality images that successfully capture specific medical features.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LIDC-IDRI数据集的潜在扩散模型生成胸部CT肺结节图像</div>
<div class="mono" style="margin-top:8px">近年来，计算机辅助诊断系统已被开发以支持诊断，但其性能在很大程度上依赖于训练数据的质量和数量。然而，在临床实践中，收集特定病例的大量CT图像是困难的，例如流行病学发生率低的小细胞癌或难以与恶性肿瘤区分的良性肿瘤。这导致了数据不平衡的问题。在本研究中，为了解决这一问题，我们提出了一种方法，利用潜在扩散模型（LDM）自动生成捕捉目标特征的胸部CT结节图像，并验证了其有效性。使用LIDC-IDRI数据集，我们根据医生评估创建了结节图像和基于发现的文本提示的配对。对于图像生成模型，我们使用了稳定扩散版本1.5（SDv1）和2.0（SDv2），这两种都是LDM类型。每个模型都使用创建的数据集进行了微调。在生成过程中，我们调整了指导比例（GS），该比例表示对输入文本的忠实度。定量和主观评估均显示，SDv2（GS = 5）在图像质量、多样性和文本一致性方面表现最佳。在主观评估中，生成图像与真实图像之间未观察到统计学显著差异，确认其质量等同于真实临床图像。我们提出了一种基于输入文本使用LDM生成胸部CT结节图像的方法。评估结果表明，该方法能够生成成功捕捉特定医学特征的高质量图像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of data imbalance in computer-aided diagnosis systems, particularly due to the difficulty in collecting sufficient CT images for rare cases. The authors proposed a method using latent diffusion models (LDM) to automatically generate chest CT nodule images that reflect target features, utilizing the LIDC-IDRI dataset for training. The experimental results indicated that the Stable Diffusion version 2.0 model, with a guidance scale of 5, produced the highest quality images, showing no significant differences in quality compared to real clinical images, thus validating the effectiveness of the proposed image generation method.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决计算机辅助诊断系统中数据不平衡的问题，特别是由于难以收集稀有病例的足够CT图像。作者提出了一种使用潜在扩散模型（LDM）自动生成反映目标特征的胸部CT结节图像的方法，利用LIDC-IDRI数据集进行训练。结果表明，使用指导尺度为5的稳定扩散版本2.0模型生成的图像质量最高，生成图像与真实临床图像之间没有显著差异，从而确认了所提方法在生成高质量医学图像方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration</div>
<div class="meta-line">Authors: Subhajit Sanyal, Srinivas Soumitri Miriyala, Akshay Janardan Bankar, Manjunath Arveti, Sowmya Vajrala, Shreyas Pandith, Sravanth Kodavanti, Abhishek Ameta, Harshit, Amit Satish Unde</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-14T19:30:53+00:00 · Latest: 2026-01-16T07:49:33+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09823v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09823v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NanoSD：用于实时图像恢复的边缘高效基础模型</div>
<div class="mono" style="margin-top:8px">潜在扩散模型如Stable Diffusion 1.5提供了强大的生成先验，对于图像恢复非常有价值，但其完整的管道在边缘设备上的部署仍然过于计算密集。现有的轻量级变体主要压缩去噪U-Net或减少扩散轨迹，这破坏了潜在流形并限制了超出单一任务的泛化。我们引入NanoSD，这是一系列通过网络手术、特征生成蒸馏和结构化架构缩放共同应用于U-Net和VAE编码器-解码器的Pareto最优扩散基础模型。这种全管道共同设计保留了生成先验，同时生成在准确性-延迟-大小边界上占据不同操作点的模型（例如，130M-315M参数，在移动级NPU上实现高达20ms的实时推理）。我们表明，仅仅减少参数与硬件效率并不相关，并提供分析揭示了架构平衡、特征路由和潜在空间保留如何共同影响真实的设备延迟。当作为插入骨干网使用时，NanoSD在图像超分辨率、图像去模糊、面部恢复和单目深度估计等任务中实现了最先进的性能，在感知质量和实际可部署性上超越了先前的轻量级扩散模型。NanoSD建立了一个适用于边缘设备实时视觉生成和恢复的通用扩散基础模型系列。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational challenges of deploying latent diffusion models like Stable Diffusion 1.5 for image restoration on edge devices. The authors introduce NanoSD, a family of Pareto-optimal diffusion foundation models achieved through techniques such as network surgery, feature-wise generative distillation, and structured architectural scaling applied to both the U-Net and VAE encoder-decoder. Experimental results demonstrate that NanoSD maintains generative priors while achieving real-time inference times as low as 20ms on mobile-class NPUs, outperforming existing lightweight diffusion models in various tasks including image super-resolution and face restoration, thus establishing a versatile model suitable for real-time visual generation on edge devices.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在边缘设备上部署潜在扩散模型（如Stable Diffusion 1.5）进行实时图像恢复的计算挑战。作者提出了NanoSD，一系列通过网络手术和特征生成蒸馏等技术实现的帕累托最优扩散基础模型，这些模型在优化准确性、延迟和模型大小的同时保持了生成先验。实验结果表明，NanoSD模型的参数范围从1.3亿到3.15亿，能够在移动级NPU上实现低至20毫秒的实时推理，在图像超分辨率、去模糊、面部恢复和单目深度估计方面的感知质量和可部署性均优于现有的轻量级模型。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Known Fakes: Generalized Detection of AI-Generated Images via Post-hoc Distribution Alignment</div>
<div class="meta-line">Authors: Li Wang, Wenyu Chen, Xiangtao Meng, Zheng Li, Shanqing Guo</div>
<div class="meta-line">First: 2025-02-15T13:55:34+00:00 · Latest: 2026-01-16T06:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.10803v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.10803v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of highly realistic AI-generated images poses serious security threats such as misinformation and identity fraud. Detecting generated images in open-world settings is particularly challenging when they originate from unknown generators, as existing methods typically rely on model-specific artifacts and require retraining on new fake data, limiting their generalization and scalability. In this work, we propose Post-hoc Distribution Alignment (PDA), a generalized and model-agnostic framework for detecting AI-generated images under unknown generative threats. Specifically, PDA reformulates detection as a distribution alignment task by regenerating test images through a known generative model. When real images are regenerated, they inherit model-specific artifacts and align with the known fake distribution. In contrast, regenerated unknown fakes contain incompatible or mixed artifacts and remain misaligned. This difference allows an existing detector, trained on the known generative model, to accurately distinguish real images from unknown fakes without requiring access to unseen data or retraining. Extensive experiments across 16 state-of-the-art generative models, including GANs, diffusion models, and commercial text-to-image APIs (e.g., Midjourney), demonstrate that PDA achieves average detection accuracy of 96.69%, outperforming the best baseline by 10.71%. Comprehensive ablation studies and robustness analyses further confirm PDA&#x27;s generalizability and resilience to distribution shifts and image transformations. Overall, our work provides a practical and scalable solution for real-world AI-generated image detection where new generative models emerge continuously.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越已知伪造品：通过后验分布对齐实现AI生成图像的通用检测</div>
<div class="mono" style="margin-top:8px">高度逼真的AI生成图像的快速传播带来了严重的安全威胁，如虚假信息和身份欺诈。在开放世界环境中检测生成的图像尤其具有挑战性，特别是当它们来自未知生成器时，因为现有方法通常依赖于特定模型的伪影，并需要在新的伪造数据上进行再训练，这限制了它们的泛化能力和可扩展性。在本研究中，我们提出了后验分布对齐（PDA），这是一个通用且与模型无关的框架，用于在未知生成威胁下检测AI生成的图像。具体而言，PDA将检测重新表述为分布对齐任务，通过已知生成模型重新生成测试图像。当真实图像被重新生成时，它们继承了特定模型的伪影，并与已知的伪造分布对齐。相比之下，重新生成的未知伪造品包含不兼容或混合的伪影，并保持不对齐。这一差异使得在已知生成模型上训练的现有检测器能够准确区分真实图像和未知伪造品，而无需访问未见数据或再训练。对16种最先进的生成模型（包括GAN、扩散模型和商业文本到图像API（如Midjourney））的广泛实验表明，PDA实现了96.69%的平均检测准确率，超越了最佳基线10.71%。全面的消融研究和鲁棒性分析进一步确认了PDA的泛化能力和对分布变化及图像变换的韧性。总体而言，我们的工作为在新生成模型不断出现的现实世界中提供了一个实用且可扩展的AI生成图像检测解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of realistic AI-generated images raises significant concerns regarding misinformation and identity fraud, motivating the need for effective detection methods. This study introduces Post-hoc Distribution Alignment (PDA), a model-agnostic framework that reformulates the detection of AI-generated images as a distribution alignment task, allowing for the identification of unknown generative threats without retraining on new data. Experimental results demonstrate that PDA achieves an average detection accuracy of 96.69% across 16 advanced generative models, significantly surpassing existing methods by 10.71%, while also showing robustness to distribution shifts and image transformations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是现实中高度逼真的AI生成图像带来的安全威胁，这使得在开放世界场景中检测这些图像变得复杂，尤其是当这些图像来自未知生成器时。作者提出了后验分布对齐（PDA），这是一种模型无关的框架，将AI生成图像的检测重新表述为分布对齐任务，通过使用已知生成模型重新生成测试图像。实验结果表明，PDA在16种最先进的生成模型中实现了平均96.69%的检测准确率，显著超过最佳基线10.71%，并展示了对各种分布变化和图像变换的强泛化能力和韧性。</div>
</details>
</div>
<div class="card">
<div class="title">MERGETUNE: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-16T04:31:59+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v2">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalisation without adding parameters. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at https://github.com/Surrey-UP-Lab/MERGETUNE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MERGETUNE：视觉-语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉-语言模型（VLMs），如CLIP，往往会导致预训练知识的灾难性遗忘。之前的工作主要旨在减轻适应过程中的遗忘；然而，在此过程中，遗忘往往是不可避免的。我们引入了一种新范式，持续微调（CFT），旨在在零-shot模型已经适应后恢复预训练知识。我们提出了一种简单的、与模型无关的CFT策略（称为MERGETUNE），该策略由线性模式连接（LMC）指导，可以在现有微调模型上事后应用，而无需架构更改。给定一个微调模型，我们继续微调其可训练参数（例如，软提示或线性头），以寻找一个具有两个低损失路径的持续模型，分别通向零-shot（例如，CLIP）和微调（例如，CoOp）解决方案。通过利用损失景观的几何特性，持续模型隐式地合并了这两种解决方案，恢复了在微调对应模型中丢失的预训练知识。一个挑战是，普通的LMC约束需要来自预训练任务的数据重放。我们通过二阶代理来近似这一约束，消除了对大规模数据重放的需求。实验表明，MERGETUNE在不增加参数的情况下，将CoOp的调和平均提高了5.6%。在稳健微调评估中，来自MERGETUNE的LMC合并模型超越了具有较低推理成本的集成基线，与零-shot模型集成时实现了进一步的增益和最先进的结果。我们的代码可在https://github.com/Surrey-UP-Lab/MERGETUNE获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of catastrophic forgetting in vision-language models (VLMs) like CLIP during fine-tuning, which often results in the loss of pretrained knowledge. The authors introduce a method called MERGETUNE, a continued fine-tuning (CFT) strategy that operates post hoc on existing fine-tuned models without requiring changes to their architecture. Experimental results demonstrate that MERGETUNE enhances the harmonic mean of CoOp by 5.6% on base-novel generalization and outperforms ensemble baselines in robust fine-tuning evaluations, achieving state-of-the-art results while maintaining lower inference costs and without adding parameters.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在对视觉语言模型（VLMs）如CLIP进行微调时出现的灾难性遗忘问题，这通常导致预训练知识的丧失。作者提出了一种名为MERGETUNE的持续微调（CFT）新方法，利用线性模式连通性（LMC）来恢复这些丢失的知识，而无需更改模型架构。实验结果表明，MERGETUNE在基础-新颖泛化上提高了CoOp的调和平均值5.6%，并在稳健微调评估中超越了集成基线，取得了最新的成果，同时保持了较低的推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">Controllable Video Generation: A Survey</div>
<div class="meta-line">Authors: Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen</div>
<div class="meta-line">First: 2025-07-22T06:05:34+00:00 · Latest: 2026-01-16T03:30:27+00:00</div>
<div class="meta-line">Comments: project page: https://github.com/mayuelala/Awesome-Controllable-Video-Generation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.16869v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.16869v2">PDF</a> · <a href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of AI-generated content (AIGC), video generation has emerged as one of its most dynamic and impactful subfields. In particular, the advancement of video generation foundation models has led to growing demand for controllable video generation methods that can more accurately reflect user intent. Most existing foundation models are designed for text-to-video generation, where text prompts alone are often insufficient to express complex, multi-modal, and fine-grained user requirements. This limitation makes it challenging for users to generate videos with precise control using current models. To address this issue, recent research has explored the integration of additional non-textual conditions, such as camera motion, depth maps, and human pose, to extend pretrained video generation models and enable more controllable video synthesis. These approaches aim to enhance the flexibility and practical applicability of AIGC-driven video generation systems. In this survey, we provide a systematic review of controllable video generation, covering both theoretical foundations and recent advances in the field. We begin by introducing the key concepts and commonly used open-source video generation models. We then focus on control mechanisms in video diffusion models, analyzing how different types of conditions can be incorporated into the denoising process to guide generation. Finally, we categorize existing methods based on the types of control signals they leverage, including single-condition generation, multi-condition generation, and universal controllable generation. For a complete list of the literature on controllable video generation reviewed, please visit our curated repository at https://github.com/mayuelala/Awesome-Controllable-Video-Generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可控视频生成：综述</div>
<div class="mono" style="margin-top:8px">随着人工智能生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。特别是视频生成基础模型的进步，导致对可控视频生成方法的需求日益增长，这些方法能够更准确地反映用户意图。现有的大多数基础模型是为文本到视频生成设计的，仅依靠文本提示往往不足以表达复杂的多模态和细粒度的用户需求。这一限制使得用户在使用当前模型时难以精确控制视频生成。为了解决这个问题，最近的研究探索了集成额外的非文本条件，如相机运动、深度图和人体姿态，以扩展预训练的视频生成模型，并实现更可控的视频合成。这些方法旨在增强AIGC驱动的视频生成系统的灵活性和实用性。在本综述中，我们对可控视频生成进行了系统的回顾，涵盖了该领域的理论基础和最新进展。我们首先介绍关键概念和常用的开源视频生成模型。然后，我们重点关注视频扩散模型中的控制机制，分析如何将不同类型的条件纳入去噪过程以指导生成。最后，我们根据所利用的控制信号类型对现有方法进行分类，包括单条件生成、多条件生成和通用可控生成。有关可控视频生成文献的完整列表，请访问我们整理的库：https://github.com/mayuelala/Awesome-Controllable-Video-Generation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this survey is the increasing demand for controllable video generation methods that accurately reflect user intent, particularly as AI-generated content evolves. The authors systematically review the theoretical foundations and recent advances in controllable video generation, focusing on the integration of non-textual conditions such as camera motion and human pose into pretrained video generation models. Key findings highlight the categorization of existing methods based on control signals, including single-condition, multi-condition, and universal controllable generation, which enhance the flexibility and applicability of video synthesis systems.</div>
<div class="mono" style="margin-top:8px">本综述的动机是随着人工智能生成内容的发展，对能够准确反映用户意图的可控视频生成方法的需求日益增加。作者系统地回顾了现有文献和方法，重点研究将相机运动、深度图和人体姿态等非文本条件整合到视频生成模型中，以增强控制和灵活性。主要发现表明，将各种控制信号纳入视频扩散模型的去噪过程显著提高了视频合成的精确性，从而根据单一、多重和通用控制条件对方法进行了分类。</div>
</details>
</div>
<div class="card">
<div class="title">MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement</div>
<div class="meta-line">Authors: Meidan Ding, Jipeng Zhang, Wenxuan Wang, Haiqin Zhong, Xiaoling Luo, Wenting Chen, Linlin Shen</div>
<div class="meta-line">First: 2026-01-16T02:32:07+00:00 · Latest: 2026-01-16T02:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10949v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedExpert-R1：通过领域特定适应和临床指南强化加强多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型（MedVLMs）在感知任务中表现出色，但在现实场景中所需的复杂临床推理方面存在困难。尽管强化学习（RL）已被探索以增强推理能力，但现有方法面临关键不匹配：深度推理数据稀缺、冷启动限制多专业对齐，标准RL算法未能建模临床推理的多样性。我们提出MMedExpert-R1，这是一种新颖的推理MedVLM，通过领域特定适应和临床指南强化来解决这些挑战。我们构建了MMedExpert，这是一个包含四个专业的1万样本高质量数据集，具有逐步推理痕迹。我们的领域特定适应（DSA）创建了专业特定的LoRA模块，以提供多样化的初始化，而基于指南的优势（GBA）明确建模不同的临床推理视角，以与现实世界的诊断策略对齐。冲突感知能力集成随后将这些专业专家合并为一个统一的代理，确保稳健的多专业对齐。全面的实验表明，性能达到最先进水平，我们的7B模型在MedXpert-MM上获得27.50，在OmniMedVQA上获得83.03，为可靠的多模态医学推理系统奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reasoning capabilities of Medical Vision-Language Models (MedVLMs), which currently excel at perception tasks but struggle with complex clinical reasoning in real-world scenarios. The authors propose MMedExpert-R1, a novel reasoning MedVLM that employs domain-specific adaptation and clinical guideline reinforcement to address challenges such as the scarcity of deep reasoning data and the limitations of standard reinforcement learning algorithms. Experimental results show that MMedExpert-R1 achieves state-of-the-art performance, with its 7B model scoring 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, indicating a significant advancement in multimodal medical reasoning systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高医学视觉语言模型（MedVLM）在复杂临床推理方面的能力，这些模型目前在感知任务上表现出色，但在现实场景中面临挑战。作者提出了MMedExpert-R1，这是一种新型推理MedVLM，利用特定领域的适应和临床指南强化来解决深度推理数据不足和标准强化学习算法的局限性等问题。实验结果表明，MMedExpert-R1达到了最先进的性能，其7B模型在MedXpert-MM上得分27.50，在OmniMedVQA上得分83.03，从而为有效的多模态医学推理系统奠定了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions</div>
<div class="meta-line">Authors: Tasneem Shaffee, Sherief Reda</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2026-01-16T00:41:42+00:00 · Latest: 2026-01-16T00:41:42+00:00</div>
<div class="meta-line">Comments: Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10921v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10921v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust Multi-Task Learning (MTL) is crucial for autonomous systems operating in real-world environments, where adverse weather conditions can severely degrade model performance and reliability. In this paper, we introduce RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. Our framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. To validate our approach, we evaluated it on the PASCAL and NYUD-v2 datasets and compared it against single-task models, standard MTL baselines, and state-of-the-art methods. On the PASCAL benchmark, RobuMTL delivers a +2.8% average relative improvement under single perturbations and up to +44.4% under mixed weather conditions compared to the MTL baseline. On NYUD-v2, RobuMTL achieves a +9.7% average relative improvement across tasks. The code is available at GitHub.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RobuMTL：增强多任务学习在天气条件下的鲁棒性</div>
<div class="mono" style="margin-top:8px">鲁棒的多任务学习（MTL）对于在现实环境中运行的自主系统至关重要，因为恶劣的天气条件会严重降低模型的性能和可靠性。本文介绍了RobuMTL，这是一种新颖的架构，旨在通过动态选择特定任务的分层低秩适应（LoRA）模块和基于输入扰动的LoRA专家小组，以混合专家的方式自适应地解决视觉退化。我们的框架能够基于输入特征实现自适应专业化，提高在多样化现实条件下的鲁棒性。为了验证我们的方法，我们在PASCAL和NYUD-v2数据集上进行了评估，并与单任务模型、标准MTL基线和最先进的方法进行了比较。在PASCAL基准上，RobuMTL在单一扰动下提供了+2.8%的平均相对提升，在混合天气条件下则提高了+44.4%，相比于MTL基线。在NYUD-v2上，RobuMTL在各任务间实现了+9.7%的平均相对提升。代码可在GitHub上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the robustness of multi-task learning (MTL) models for autonomous systems in adverse weather conditions, which can significantly impair performance. The authors propose RobuMTL, a novel architecture that utilizes task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a mixture-of-experts approach to dynamically select adaptations based on input perturbations. Experimental results demonstrate that RobuMTL achieves a +2.8% average relative improvement on the PASCAL dataset under single perturbations and up to +44.4% under mixed weather conditions, as well as a +9.7% average relative improvement across tasks on the NYUD-v2 dataset compared to standard MTL baselines and other state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强多任务学习（MTL）模型在面对不利天气条件时的鲁棒性，这些条件可能会影响性能。作者提出了RobuMTL，这是一种新颖的架构，利用任务特定的分层低秩适应（LoRA）模块和混合专家方法，根据输入扰动动态选择适应。实验结果表明，RobuMTL在PASCAL数据集上在单一扰动下实现了+2.8%的平均相对提升，在混合天气条件下提升高达+44.4%，并且在NYUD-v2数据集上实现了跨任务的+9.7%的平均相对提升，相较于标准MTL基线和最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation</div>
<div class="meta-line">Authors: Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian</div>
<div class="meta-line">First: 2026-01-15T22:18:14+00:00 · Latest: 2026-01-15T22:18:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10880v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10880v1">PDF</a> · <a href="https://github.com/AIM-Research-Lab/Medical-SAM3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3&#x27;s model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Medical SAM3：通用提示驱动医学图像分割的基础模型</div>
<div class="mono" style="margin-top:8px">可提示分割基础模型如SAM3通过交互式和基于概念的提示展示了强大的泛化能力。然而，由于严重的领域转移、缺乏特权空间提示以及需要对复杂解剖和体积结构进行推理，其在医学图像分割中的直接应用仍然有限。在此，我们提出Medical SAM3，这是一个通用提示驱动医学图像分割的基础模型，通过在大规模异构的2D和3D医学成像数据集上对SAM3进行全面微调而获得，数据集包含配对的分割掩码和文本提示。通过对原始SAM3的系统分析，我们观察到其在医学数据上的性能显著下降，其明显的竞争力在很大程度上依赖于强几何先验，如基于真实值的边界框。这些发现促使我们在提示工程之外进行全面模型适应。通过在涵盖10种医学成像模式的33个数据集上微调SAM3的模型参数，Medical SAM3获得了稳健的领域特定表示，同时保持了提示驱动的灵活性。针对器官、成像模式和维度的广泛实验表明，在语义模糊、复杂形态和长距离3D上下文等具有挑战性的场景中，性能持续显著提升。我们的结果确立了Medical SAM3作为医学成像的通用文本引导分割基础模型，并强调了在严重领域转移下实现稳健提示驱动分割的整体模型适应的重要性。代码和模型将发布在https://github.com/AIM-Research-Lab/Medical-SAM3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing promptable segmentation models like SAM3 in the context of medical image segmentation, which faces challenges such as domain shifts and complex anatomical structures. The authors developed Medical SAM3 by fully fine-tuning the original SAM3 model on a diverse set of 2D and 3D medical imaging datasets, incorporating paired segmentation masks and text prompts. The experimental results show that Medical SAM3 significantly improves segmentation performance across various organs and imaging modalities, particularly in scenarios with semantic ambiguity and complex morphology, demonstrating its effectiveness as a universal, text-guided segmentation model for medical imaging.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高像SAM3这样的可提示分割模型在医学图像分割中的适用性，因为它面临着领域转移和复杂解剖结构的挑战。作者通过在包含配对分割掩码和文本提示的多样化2D和3D医学影像数据集上对原始SAM3模型进行全面微调，开发了Medical SAM3。实验结果表明，Medical SAM3在各种器官和成像模式下显著提高了分割性能，尤其是在复杂场景中，从而确立了它作为医学成像的强大文本引导基础模型的地位，解决了先前模型的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">ViSTA: Visual Storytelling using Multi-modal Adapters for Text-to-Image Diffusion Models</div>
<div class="meta-line">Authors: Sibo Dong, Ismail Shaheen, Maggie Shen, Rupayan Mallick, Sarah Adel Bargal</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-13T19:57:40+00:00 · Latest: 2026-01-15T21:21:42+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12198v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.12198v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models have achieved remarkable success, yet generating coherent image sequences for visual storytelling remains challenging. A key challenge is effectively leveraging all previous text-image pairs, referred to as history text-image pairs, which provide contextual information for maintaining consistency across frames. Existing auto-regressive methods condition on all past image-text pairs but require extensive training, while training-free subject-specific approaches ensure consistency but lack adaptability to narrative prompts. To address these limitations, we propose a multi-modal history adapter for text-to-image diffusion models, \textbf{ViSTA}. It consists of (1) a multi-modal history fusion module to extract relevant history features and (2) a history adapter to condition the generation on the extracted relevant features. We also introduce a salient history selection strategy during inference, where the most salient history text-image pair is selected, improving the quality of the conditioning. Furthermore, we propose to employ a Visual Question Answering-based metric TIFA to assess text-image alignment in visual storytelling, providing a more targeted and interpretable assessment of generated images. Evaluated on the StorySalon and FlintStonesSV dataset, our proposed ViSTA model is not only consistent across different frames, but also well-aligned with the narrative text descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSTA：使用多模态适配器进行文本到图像扩散模型的视觉讲故事</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型取得了显著成功，但生成连贯的图像序列以进行视觉讲故事仍然具有挑战性。一个关键挑战是有效利用所有先前的文本-图像对，称为历史文本-图像对，这些对提供了保持帧间一致性的上下文信息。现有的自回归方法对所有过去的图像-文本对进行条件化，但需要大量训练，而无训练的特定主题方法确保一致性但缺乏对叙事提示的适应性。为了解决这些限制，我们提出了一种用于文本到图像扩散模型的多模态历史适配器，\textbf{ViSTA}。它由（1）一个多模态历史融合模块来提取相关的历史特征和（2）一个历史适配器来基于提取的相关特征进行生成。我们还在推理过程中引入了一种显著历史选择策略，选择最显著的历史文本-图像对，从而提高条件化的质量。此外，我们建议采用基于视觉问答的度量TIFA来评估视觉讲故事中的文本-图像对齐，提供对生成图像更有针对性和可解释的评估。在StorySalon和FlintStonesSV数据集上的评估表明，我们提出的ViSTA模型不仅在不同帧之间保持一致，而且与叙事文本描述良好对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of coherent image sequences for visual storytelling using text-to-image diffusion models, which face challenges in maintaining consistency across frames. The authors propose a multi-modal history adapter, ViSTA, which includes a multi-modal history fusion module to extract relevant features from previous text-image pairs and a history adapter to condition the image generation on these features. Experimental results on the StorySalon and FlintStonesSV datasets demonstrate that ViSTA achieves consistency across frames and aligns well with narrative text descriptions, outperforming existing methods in terms of quality and coherence.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善使用文本到图像扩散模型生成连贯图像序列的能力，以应对视觉叙事中的一致性挑战。作者提出了一种多模态历史适配器ViSTA，其中包括一个多模态历史融合模块，用于从先前的文本-图像对中提取相关特征，以及一个历史适配器，用于基于这些特征进行生成。对StorySalon和FlintStonesSV数据集的实验结果表明，ViSTA在不同帧之间实现了一致性，并与叙事文本描述良好对齐，超越了现有方法在质量和连贯性方面的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks</div>
<div class="meta-line">Authors: Julia Nakhleh, Robert D. Nowak</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-27T21:46:27+00:00 · Latest: 2026-01-15T21:01:49+00:00</div>
<div class="meta-line">Comments: Update to final published version (NeurIPS 2025). Removed $\ell^\infty$ boundedness constraint in multivariate problem and updated Theorem 4.1 proof accordingly to show that this is possible. Also updated Proposition 4.1 to show existence of solutions and that solutions for any $0 \leq p &lt; 1$ have no more than N neurons</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.21791v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.21791v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Overparameterized neural networks can interpolate a given dataset in many different ways, prompting the fundamental question: which among these solutions should we prefer, and what explicit regularization strategies will provably yield these solutions? This paper addresses the challenge of finding the sparsest interpolating ReLU network--i.e., the network with the fewest nonzero parameters or neurons--a goal with wide-ranging implications for efficiency, generalization, interpretability, theory, and model compression. Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere differentiable training objective whose global minima are guaranteed to correspond to the sparsest single-hidden-layer ReLU networks that fit the data. This result marks a conceptual advance: it recasts the combinatorial problem of sparse interpolation as a smooth optimization task, potentially enabling the use of gradient-based training methods. Our objective is based on minimizing $\ell^p$ quasinorms of the weights for $0 &lt; p &lt; 1$, a classical sparsity-promoting strategy in finite-dimensional settings. However, applying these ideas to neural networks presents new challenges: the function class is infinite-dimensional, and the weights are learned using a highly nonconvex objective. We prove that, under our formulation, global minimizers correspond exactly to sparsest solutions. Our work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be leveraged to recover sparse networks through training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全局最小化的 $\ell^p$-正则化目标产生最稀疏的 ReLU 神经网络</div>
<div class="mono" style="margin-top:8px">过参数化的神经网络可以以多种方式插值给定的数据集，这引发了一个基本问题：我们应该偏好这些解决方案中的哪一个，以及什么明确的正则化策略可以证明地产生这些解决方案？本文解决了寻找最稀疏插值 ReLU 网络的挑战，即具有最少非零参数或神经元的网络，这一目标对效率、泛化、可解释性、理论和模型压缩具有广泛的影响。与事后剪枝方法不同，我们提出了一种连续的、几乎处处可微的训练目标，其全局最小值保证对应于适合数据的最稀疏单隐层 ReLU 网络。这个结果标志着一个概念上的进步：它将稀疏插值的组合问题重新表述为平滑优化任务，可能使基于梯度的训练方法的使用成为可能。我们的目标是基于最小化 $\ell^p$ 准范数的权重，适用于 $0 &lt; p &lt; 1$，这是有限维设置中经典的稀疏促进策略。然而，将这些思想应用于神经网络带来了新的挑战：函数类是无限维的，权重是通过高度非凸的目标学习的。我们证明，在我们的表述下，全局最小化者恰好对应于最稀疏的解决方案。我们的工作为理解何时以及如何利用连续稀疏诱导目标通过训练恢复稀疏网络奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of identifying the sparsest ReLU neural networks that can interpolate a given dataset, which is crucial for improving efficiency, generalization, interpretability, and model compression. The authors propose a continuous, differentiable training objective based on minimizing $\ell^p$ quasinorms of the weights, which guarantees that the global minima correspond to the sparsest single-hidden-layer ReLU networks fitting the data. The findings demonstrate that this approach successfully recasts the combinatorial problem of sparse interpolation into a smooth optimization task, allowing for the recovery of sparse networks through gradient-based training methods.</div>
<div class="mono" style="margin-top:8px">本文研究了识别最稀疏插值ReLU神经网络的挑战，这对提高效率、泛化能力、可解释性和模型压缩至关重要。作者提出了一种连续的、几乎处处可微的训练目标，保证全局最小值对应于拟合数据的最稀疏单隐层ReLU网络，从而将稀疏插值的组合问题转化为平滑优化任务。关键发现是，通过最小化0 &lt; p &lt; 1的\ell^p准范数，全局最小值产生最稀疏的解，为在神经网络训练中利用连续稀疏诱导目标奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">TetriServe: Efficient DiT Serving for Heterogeneous Image Generation</div>
<div class="meta-line">Authors: Runyu Lu, Shiqi He, Wenxuan Tan, Shenggui Li, Ruofan Wu, Jeff J. Ma, Ang Chen, Mosharaf Chowdhury</div>
<div class="meta-line">First: 2025-10-02T01:23:32+00:00 · Latest: 2026-01-15T20:28:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01565v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01565v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformer (DiT) models excel at generating high-quality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.
  In this paper, we propose step-level sequence parallelism to dynamically adjust the degree of parallelism of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TetriServe：异构图像生成的高效DiT服务</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiT）模型通过迭代去噪步骤在生成高质量图像方面表现出色，但在严格的服务水平目标（SLO）下提供服务具有挑战性，因为其计算成本高，尤其是在大分辨率下。现有的服务系统使用固定的并行度序列，这对于具有混合分辨率和截止日期的异构工作负载效率低下，导致GPU利用率低和SLO达成率低。
  在本文中，我们提出了步骤级序列并行性，根据请求的截止日期动态调整个别请求的并行度。我们提出了TetriServe，一个实现该策略的DiT服务系统，以实现高效的图像生成。具体而言，TetriServe引入了一种新颖的基于轮次的调度机制，提高了SLO达成率：（1）将时间离散化为固定轮次，使得截止日期感知调度可行，（2）在步骤级别适应并行性并最小化GPU小时消耗，以及（3）联合打包请求以最小化延迟完成。对最先进的DiT模型进行的广泛评估表明，TetriServe的SLO达成率比现有解决方案高出多达32%，且不降低图像质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of serving Diffusion Transformer (DiT) models for high-quality image generation under strict Service Level Objectives (SLOs), particularly due to their high computational costs at large resolutions. The authors propose a novel approach called step-level sequence parallelism, implemented in a system named TetriServe, which dynamically adjusts the degree of parallelism for individual requests based on their deadlines. Experimental results demonstrate that TetriServe achieves up to 32% higher SLO attainment compared to existing serving systems while maintaining image quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在严格的服务水平目标（SLO）下为图像生成服务扩散变换器（DiT）模型所面临的挑战，特别是在大分辨率下的高计算需求。作者提出了一种名为TetriServe的新方法，该方法采用逐步序列并行性，根据单个请求的截止日期动态调整并行度。实验结果表明，TetriServe在保持高图像质量的同时，通过创新的基于轮次的调度机制显著提高了SLO达成率，提升幅度高达32%，并优化了GPU利用率，减少了延迟完成的情况。</div>
</details>
</div>
<div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：编辑图像中物体的内在属性</div>
<div class="mono" style="margin-top:8px">我们介绍了Alterbute，一种基于扩散的方法，用于编辑图像中物体的内在属性。我们允许改变物体的颜色、纹理、材料，甚至形状，同时保持其感知身份和场景上下文。现有方法要么依赖于常常无法保持身份的无监督先验，要么使用过于严格的监督，阻碍有意义的内在变化。我们的方法依赖于：（i）一个放宽的训练目标，允许模型在身份参考图像、描述目标内在属性的文本提示以及定义外部上下文的背景图像和物体掩码的条件下，改变内在和外在属性。在推理时，我们通过重用原始背景和物体掩码来限制外在变化，从而确保仅改变所需的内在属性；（ii）视觉命名实体（VNEs）- 细粒度视觉身份类别（例如，“保时捷911 Carrera”），将共享身份定义特征的物体分组，同时允许内在属性的变化。我们使用视觉-语言模型从大型公共图像数据集中自动提取VNE标签和内在属性描述，实现可扩展的、保持身份的监督。Alterbute在保持身份的物体内在属性编辑方面优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the editing of intrinsic attributes of objects in images while maintaining their identity and context, addressing limitations in existing methods that either fail to preserve identity or impose restrictive supervision. The authors propose Alterbute, a diffusion-based method that employs a relaxed training objective to allow changes in intrinsic and extrinsic attributes based on an identity reference image, a textual prompt, and a background image with an object mask. Key experimental results demonstrate that Alterbute significantly outperforms existing methods in preserving the identity of objects while enabling meaningful variations in intrinsic attributes such as color, texture, and shape.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善图像中物体的内在属性编辑，同时保持其感知身份和场景上下文，解决了现有方法在身份保持方面的局限性。作者提出了Alterbute，这是一种基于扩散的方法，采用放宽的训练目标，允许根据身份参考图像、文本提示和带有物体掩膜的背景图像改变内在和外在属性。主要实验结果表明，Alterbute在保持身份的同时，在内在属性（如颜色、纹理、材料和形状）的编辑上显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</div>
<div class="meta-line">Authors: Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu</div>
<div class="meta-line">First: 2026-01-15T18:52:59+00:00 · Latest: 2026-01-15T18:52:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&#x27; prior expertise and interaction strategies through prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成性人工智能对建筑概念设计的影响：绩效、创造性自我效能和认知负荷</div>
<div class="mono" style="margin-top:8px">我们的研究考察了生成性人工智能（GenAI）如何影响建筑概念设计任务中的绩效、创造性自我效能和认知负荷。来自建筑工程及其他学科的三十六名学生参与者完成了一个两阶段的建筑设计任务，首先独立完成，然后使用外部工具（GenAI辅助条件和使用现有建筑项目在线库的对照条件）。设计结果由专家评审，创造性自我效能和认知负荷在每个阶段后由参与者自我报告。差异中的差异分析显示，参与者之间GenAI没有整体绩效优势；然而，子组分析表明，GenAI显著提高了新手设计师的设计绩效。相反，使用GenAI的学生的创造性自我效能普遍下降。不同条件下的认知负荷没有显著差异，尽管提示使用模式显示，迭代的创意生成和视觉反馈提示与认知负荷的更大减少相关。这些发现表明，GenAI的有效性取决于用户的先前专业知识和通过提示的互动策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the influence of generative AI (GenAI) on performance, creative self-efficacy, and cognitive load in architectural conceptual design. Thirty-six student participants engaged in a two-phase design task, first independently and then with GenAI assistance or a control condition using an online repository. The results indicated no overall performance advantage for GenAI, but novice designers showed significant improvement in design performance. Additionally, while general creative self-efficacy decreased for students using GenAI, cognitive load remained similar across conditions, with specific prompting strategies linked to reduced cognitive load.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成性人工智能（GenAI）对建筑概念设计中表现、创造性自我效能和认知负荷的影响。来自不同学科的36名学生参与了一个两阶段的设计任务，首先独立工作，然后使用GenAI或控制条件下的在线建筑项目库。分析显示，GenAI在整体表现上没有优势，但显著提高了新手设计师的设计表现。然而，使用GenAI的学生报告了创造性自我效能的下降，尽管不同条件下的认知负荷没有显著差异，但某些提示策略与认知负荷的减少相关联。</div>
</details>
</div>
<div class="card">
<div class="title">Moonworks Lunara Aesthetic Dataset</div>
<div class="meta-line">Authors: Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan</div>
<div class="meta-line">First: 2026-01-12T19:11:41+00:00 · Latest: 2026-01-15T18:27:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07941v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07941v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Moonworks Lunara 美学数据集</div>
<div class="mono" style="margin-top:8px">该数据集涵盖多种艺术风格，包括来自中东、北欧、东亚和南亚的区域性美学，以及素描和油画等一般类别。所有图像均使用 Moonworks Lunara 模型生成，旨在体现独特的高质量美学风格，形成首个具有显著更高美学评分的数据集，超越了以美学为重点的数据集和通用数据集。每幅图像都附有经过人工精炼的提示和结构化注释，联合描述显著对象、属性、关系和风格线索。与强调广度而非精确度的大规模网络衍生数据集不同，Lunara 美学数据集优先考虑美学质量、风格多样性和许可透明度，并在 Apache 2.0 许可下发布，以支持研究和无限制的学术及商业使用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the creation of the Moonworks Lunara Aesthetic Dataset is to provide a high-quality resource that emphasizes aesthetic diversity and precision, contrasting with existing datasets that prioritize breadth. The dataset is generated using the Moonworks Lunara model, which produces images across various artistic styles, including regionally grounded aesthetics and general categories like sketch and oil painting. Key experimental findings indicate that this dataset achieves significantly higher aesthetic scores than both aesthetics-focused and general-purpose datasets, while also offering structured annotations that enhance its usability for research and applications in art and design.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要一个高质量的数据集，以捕捉多样的艺术风格，同时保持美学完整性。作者开发了Moonworks Lunara美学数据集，其中包括由Lunara模型生成的图像，涵盖各种区域美学和艺术类别。主要发现表明，该数据集的美学评分显著高于现有的美学专注和通用数据集，并且包含详细的注释，增强了其在研究中的可用性。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</div>
<div class="meta-line">Authors: Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong</div>
<div class="meta-line">First: 2026-01-15T16:16:34+00:00 · Latest: 2026-01-15T16:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10551v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放大型视觉语言模型在道路基础设施智能感知中的能力</div>
<div class="mono" style="margin-top:8px">城市道路基础设施的自动感知对智慧城市管理至关重要，但通用模型往往难以捕捉必要的细粒度属性和领域规则。虽然大型视觉语言模型（VLMs）在开放世界识别方面表现出色，但它们在准确解释符合工程标准的复杂设施状态时常常遇到困难，导致在实际应用中的性能不可靠。为了解决这个问题，我们提出了一种领域适应框架，将VLMs转变为智能基础设施分析的专业代理。我们的方法结合了数据高效的微调策略和基于知识的推理机制。具体而言，我们利用开放词汇微调Grounding DINO，以最小的监督强健地定位多样资产，随后在Qwen-VL上进行基于LoRA的适应，以进行深层语义属性推理。为了减轻幻觉并强制执行专业合规性，我们引入了一个双模态检索增强生成（RAG）模块，在推理过程中动态检索权威行业标准和视觉示例。在一个全面的新城市道路场景数据集上评估，我们的框架实现了58.9 mAP的检测性能和95.5%的属性识别准确率，展示了智能基础设施监测的强大解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the automated perception of urban roadside infrastructure, which is essential for effective smart city management, as existing general-purpose models often fail to capture the necessary details and domain-specific rules. The authors propose a domain-adapted framework that enhances Large Vision Language Models (VLMs) for specialized infrastructure analysis by employing a data-efficient fine-tuning strategy combined with a knowledge-grounded reasoning mechanism. Their experimental results, evaluated on a new dataset of urban roadside scenes, show that the framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, indicating a significant advancement in intelligent infrastructure monitoring capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高城市路边基础设施的自动感知能力，这对有效的智慧城市管理至关重要，因为现有的通用模型无法准确解释复杂的设施状态。作者提出了一种领域适应框架，将大型视觉语言模型（VLMs）转变为专门的基础设施分析代理，利用数据高效的微调策略和知识驱动的推理机制。实验结果表明，该框架在新的城市路边场景数据集上实现了58.9 mAP的检测性能和95.5%的属性识别准确率，表明其在智能基础设施监测中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CoGen: Creation of Reusable UI Components in Figma via Textual Commands</div>
<div class="meta-line">Authors: Ishani Kanapathipillai, Obhasha Priyankara</div>
<div class="meta-line">First: 2026-01-15T15:57:59+00:00 · Latest: 2026-01-15T15:57:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10536v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10536v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.
  The project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoGen：通过文本命令在Figma中创建可重用的UI组件</div>
<div class="mono" style="margin-top:8px">用户界面设计的发展强调了高效、可重用和可编辑组件的需求，以确保高效的设计过程。本研究介绍了CoGen，一个使用机器学习技术直接在Figma中生成可重用UI组件的系统，Figma是最流行的UI设计工具之一。CoGen解决了当前系统中的空白，专注于使用结构化JSON和自然语言提示创建原子组件，如按钮、标签和输入字段。
该项目集成了Figma API数据提取、Seq2Seq模型和微调的T5变换器用于组件生成。关键结果表明，T5模型在提示生成中的效率，准确率为98%，BLEU分数为0.2668，确保了JSON与描述性提示的映射。对于JSON创建，CoGen在生成指定组件类型的简单JSON输出方面的成功率高达100%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for efficient and reusable UI components in user interface design by introducing CoGen, a system that generates these components in Figma using machine learning techniques. The method involves the integration of Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers to create atomic components like buttons and labels from structured JSON and natural language prompts. The experimental results indicate that the T5 model achieves a prompt generation accuracy of 98% and a BLEU score of 0.2668, while CoGen successfully generates simple JSON outputs for specified component types with a 100% success rate.</div>
<div class="mono" style="margin-top:8px">本研究通过引入CoGen系统，解决了设计过程中对高效和可重用UI组件的需求，该系统利用机器学习技术在Figma中生成这些组件。该方法涉及从Figma API提取数据，并使用Seq2Seq模型和微调的T5变换器，从结构化JSON和自然语言提示中创建原子组件，如按钮和标签。实验结果表明，T5模型在提示生成方面的准确率达到98%，BLEU分数为0.2668，而CoGen在为指定组件类型生成简单JSON输出时的成功率达到100%。</div>
</details>
</div>
<div class="card">
<div class="title">SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery</div>
<div class="meta-line">Authors: Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang</div>
<div class="meta-line">First: 2026-01-15T15:57:18+00:00 · Latest: 2026-01-15T15:57:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVII-3D：利用分米级3D定位和稀疏街景图像推进路边基础设施清单</div>
<div class="mono" style="margin-top:8px">数字双胞胎和精确资产清单的自动创建是智慧城市建设和设施生命周期管理中的关键任务。然而，由于鲁棒性有限、定位不准确以及缺乏细粒度状态理解，利用成本效益高的稀疏图像仍然具有挑战性。为了解决这些局限性，提出了SVII-3D，一个用于整体资产数字化的统一框架。首先，将LoRA微调的开放集检测与空间注意力匹配网络融合，以稳健地关联稀疏视图中的观察。其次，引入几何引导的精细化机制以解决结构错误，实现精确的分米级3D定位。第三，超越静态几何映射，结合多模态提示的视觉-语言模型代理被纳入，以自动诊断细粒度操作状态。实验表明，SVII-3D显著提高了识别准确性并最小化了定位误差。因此，该框架提供了一种可扩展、成本效益高的高保真基础设施数字化解决方案，有效弥合了稀疏感知与自动智能维护之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the automated creation of digital twins and asset inventories for smart city development, addressing challenges related to sparse imagery such as localization inaccuracies and limited understanding of asset states. The authors propose SVII-3D, a unified framework that integrates LoRA fine-tuned open-set detection with a spatial-attention matching network for robust observation association, employs a geometry-guided refinement mechanism for achieving decimeter-level 3D localization, and incorporates a Vision-Language Model agent for diagnosing operational states. Experimental results indicate that SVII-3D significantly improves identification accuracy and reduces localization errors, providing a scalable and cost-effective solution for high-fidelity infrastructure digitization and maintenance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强智能城市发展中数字双胞胎和资产清单的自动创建，解决与稀疏图像、定位精度和资产状态理解相关的挑战。作者提出了SVII-3D，这是一个统一框架，集成了经过LoRA微调的开放集检测与空间注意力匹配网络，以实现稳健的观察关联，采用几何引导的精炼机制以实现分米级3D定位，并结合视觉-语言模型代理以诊断操作状态。实验结果表明，SVII-3D显著提高了识别准确性并减少了定位误差，为高保真基础设施数字化和维护提供了可扩展且具有成本效益的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Curvature Tuning: Provable Training-free Model Steering From a Single Parameter</div>
<div class="meta-line">Authors: Leyang Hu, Matteo Gamba, Randall Balestriero</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-11T18:59:57+00:00 · Latest: 2026-01-15T15:36:28+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.07783v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.07783v5">PDF</a> · <a href="https://github.com/Leon-Leyang/curvature-tuning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model&#x27;s decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 8.59%/8.34% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>曲率调节：从单一参数可证明的无训练模型引导</div>
<div class="mono" style="margin-top:8px">模型和数据规模的扩展重塑了人工智能的格局，使得微调预训练模型成为解决下游任务的标准范式。然而，主流的微调方法通常依赖于权重适应，往往缺乏可解释性，并依赖于启发式选择的超参数。本文从不同的角度出发，将重点从权重转向激活函数，通过样条算子的视角来看待它们。我们提出了曲率调节（CT），这是一种可解释且有原则的引导方法，通过将单一超参数注入激活函数来调节模型的决策边界。我们证明了CT可以调整模型决策边界的曲率，更根本地将模型投影到光滑函数的空间，从而补充当前主要在特征适应方面发挥作用的微调方法。使该超参数可训练产生了一种新颖且高效的微调方法。实证结果表明，CT提高了泛化能力和鲁棒性。例如，它在12个数据集上分别提高了ResNet-50/152的下游准确率8.59%/8.34%（相较于线性探测）和4.64%/1.70%（相较于LoRA），并在RobustBench的$\ell_\infty$基准上提高了鲁棒准确率1032.64%/1494.46%。我们的代码可在https://github.com/Leon-Leyang/curvature-tuning获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional finetuning methods that rely on weight adaptation, which often lack interpretability and depend on heuristically chosen hyperparameters. The authors propose a novel approach called Curvature Tuning (CT), which focuses on modulating a model&#x27;s decision boundary through a single hyperparameter injected into its activation functions, viewed as spline operators. Experimental results demonstrate that CT significantly enhances both generalization and robustness, achieving improvements in downstream accuracy of ResNet-50/152 by 8.59%/8.34% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, while also increasing robust accuracy on the $\ell_\infty$ benchmark from RobustBench by 1032.64%/1494.46%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决传统微调方法的局限性，这些方法依赖于权重调整，往往缺乏可解释性，并依赖于启发式选择的超参数。作者提出了一种新方法，称为曲率调节（CT），该方法通过单个超参数修改激活函数，以调整模型的决策边界。实验结果表明，CT显著提高了模型的泛化能力和鲁棒性，在12个数据集上，ResNet-50/152的下游准确率分别比线性探测提高了8.59%/8.34%，比LoRA提高了4.64%/1.70%，并且在RobustBench的$\ell_\infty$基准测试中，鲁棒准确率分别提高了1032.64%/1494.46%。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous Diffusion for Mixed-Type Tabular Data</div>
<div class="meta-line">Authors: Markus Mueller, Kathrin Gruber, Dennis Fok</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2023-12-16T12:21:03+00:00 · Latest: 2026-01-15T15:08:41+00:00</div>
<div class="meta-line">Comments: published at ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.10431v6">Abs</a> · <a href="https://arxiv.org/pdf/2312.10431v6">PDF</a> · <a href="https://github.com/muellermarkus/cdtd">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes. To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at https://github.com/muellermarkus/cdtd.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合类型表格数据的连续扩散</div>
<div class="mono" style="margin-top:8px">基于评分的生成模型，通常称为扩散模型，已被证明在生成文本和图像数据方面成功。然而，它们在混合类型表格数据上的适应性仍然未被充分探索。在本研究中，我们提出了CDTD，一种用于混合类型表格数据的连续扩散模型。CDTD基于评分匹配和评分插值的新颖组合，以强制对连续和分类特征施加统一的连续噪声分布。我们明确承认通过依赖于特定模型的损失校准和初始化方案来同质化不同数据类型的必要性。为了进一步解决混合类型表格数据中的高异质性，我们引入了自适应特征或类型特定的噪声调度。这些确保了特征之间的生成性能平衡，并优化了模型容量在特征和扩散时间之间的分配。我们的实验结果表明，CDTD始终优于最先进的基准模型，异常良好地捕捉特征相关性，并且噪声调度设计中的异质性提高了样本质量。复现代码可在https://github.com/muellermarkus/cdtd获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the generation of mixed-type tabular data using diffusion models, which have been successful in text and image generation but are underutilized in this area. The authors propose a Continuous Diffusion model for mixed-type Tabular Data (CDTD), employing a combination of score matching and score interpolation to create a unified noise distribution for both continuous and categorical features. Experimental results demonstrate that CDTD outperforms existing benchmark models, effectively captures feature correlations, and improves sample quality through a tailored noise schedule that addresses the heterogeneity of mixed-type data.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索基于分数的生成模型，特别是扩散模型在混合类型表格数据中的适应性，这一领域尚未得到充分研究。作者提出了一种混合类型表格数据的连续扩散模型（CDTD），该模型采用分数匹配和分数插值的创新组合，为连续和分类特征创建统一的连续噪声分布。实验结果表明，CDTD在性能上优于现有基准模型，能够有效捕捉特征相关性，并通过针对混合类型数据异质性的定制噪声调度提高样本质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
