<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 03:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0327</div>
    <div class="row"><div class="card">
<div class="title">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</div>
<div class="meta-line">Authors: Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Ziwei Liu, Chenyang Si</div>
<div class="meta-line">First: 2026-01-21T18:59:02+00:00 · Latest: 2026-01-21T18:59:02+00:00</div>
<div class="meta-line">Comments: 17 pages, 21 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StableWorld：朝着稳定和一致的长交互视频生成</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中被忽视的稳定性和时间一致性挑战，该生成通过相机移动和文本提示等交互行为合成动态和可控的视频世界。尽管在世界建模方面取得了显著进展，但当前方法仍然面临严重的不稳定性和时间退化，常常导致长时间交互中的空间漂移和场景崩溃。为了更好地理解这个问题，我们最初调查了不稳定性的根本原因，并确定主要的误差积累来源于同一场景，其中生成的帧逐渐偏离初始干净状态，并将误差传播到后续帧。基于这一观察，我们提出了一种简单而有效的方法，\textbf{StableWorld}，动态帧驱逐机制。通过持续过滤掉退化帧，同时保留几何一致的帧，StableWorld有效地防止了在源头的累积漂移，从而提高了交互生成的稳定性和时间一致性。在多个交互视频模型上（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）取得的良好结果表明，StableWorld是模型无关的，可以应用于不同的交互视频生成框架，以显著提高稳定性、时间一致性和在多样化交互场景中的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of stability and temporal consistency in interactive video generation, which is crucial for synthesizing dynamic video worlds through user interactions. The authors investigate the causes of instability, identifying that error accumulation primarily occurs within the same scene, leading to spatial drift and scene collapse. They propose a method called StableWorld, which employs a Dynamic Frame Eviction Mechanism to filter out degraded frames while preserving geometrically consistent ones, resulting in improved stability and temporal consistency. Experimental results across various interactive video models, including Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld enhances stability and generalization in interactive video generation frameworks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决交互视频生成中的稳定性和时间一致性挑战，这通常导致在长时间交互中出现空间漂移和场景崩溃等问题。作者调查了不稳定性的原因，发现错误积累主要发生在同一场景中，生成的帧逐渐偏离初始状态。为此，他们提出了StableWorld，一种动态帧驱逐机制，通过过滤掉退化帧并保留几何一致的帧来减轻这一问题。对多个交互视频模型（如Matrix-Game、Open-Oasis和Hunyuan-GameCraft）的实验结果表明，StableWorld显著提高了稳定性和时间一致性，展示了其在不同框架中的模型无关适用性。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion</div>
<div class="meta-line">Authors: Zichen Xi, Hao-Xiang Chen, Nan Xue, Hongyu Yan, Qi-Yuan Feng, Levent Burak Kara, Joaquim Jorge, Qun-Ce Xu</div>
<div class="meta-line">First: 2026-01-21T18:32:27+00:00 · Latest: 2026-01-21T18:32:27+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSSC：通过一步潜在扩散实现通用生成单目语义场景补全</div>
<div class="mono" style="margin-top:8px">从单目RGB图像进行语义场景补全（SSC）是一项基本但具有挑战性的任务，因为从单一视角推断被遮挡的3D几何形状固有地存在模糊性。尽管前馈方法取得了一定进展，但它们在生成被遮挡区域的可信细节和保持物体的基本空间关系方面常常面临困难。对于整个3D空间的准确生成推理能力在实际应用中至关重要。本文提出了FlowSSC，这是第一个直接应用于单目语义场景补全的生成框架。FlowSSC将SSC任务视为条件生成问题，并可以与现有的前馈SSC方法无缝集成，以显著提升其性能。为了实现实时推理而不影响质量，我们引入了在紧凑三平面潜在空间中操作的Shortcut Flow-matching。与需要数百步的标准扩散模型不同，我们的方法利用快捷机制在一步内实现高保真生成，使其能够在自主系统中实际部署。在SemanticKITTI上的大量实验表明，FlowSSC实现了最先进的性能，显著超越了现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of Semantic Scene Completion (SSC) from monocular RGB images, particularly the difficulty in accurately inferring occluded 3D geometry. The authors propose FlowSSC, a generative framework that treats SSC as a conditional generation problem and integrates with existing feed-forward methods to enhance their performance. Experimental results on the SemanticKITTI dataset show that FlowSSC achieves state-of-the-art performance, significantly surpassing existing baselines while enabling real-time inference through its innovative Shortcut Flow-matching mechanism.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从单目RGB图像进行语义场景补全（SSC）时面临的挑战，特别是在推断被遮挡的3D几何形状方面的困难。作者提出了FlowSSC，这是一种新颖的生成框架，将SSC视为条件生成问题，并与现有的前馈方法集成，以提高其性能。在SemanticKITTI数据集上的实验结果表明，FlowSSC实现了最先进的性能，显著超越了现有基准，同时通过其创新的Shortcut Flow-matching机制实现了实时推理，允许在单步中生成高保真结果。</div>
</details>
</div>
<div class="card">
<div class="title">PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu</div>
<div class="meta-line">First: 2026-01-21T17:56:59+00:00 · Latest: 2026-01-21T17:56:59+00:00</div>
<div class="meta-line">Comments: Website: https://progresslm.github.io/ProgressLM/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://progresslm.github.io/ProgressLM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PROGRESSLM：面向视觉语言模型的进展推理</div>
<div class="mono" style="margin-top:8px">估计任务进展需要对长期动态进行推理，而不是仅仅识别静态视觉内容。虽然现代视觉语言模型（VLMs）在描述可见内容方面表现出色，但尚不清楚它们是否能够从部分观察中推断任务的进展程度。为此，我们引入了Progress-Bench，这是一个系统评估VLMs中进展推理的基准。除了基准测试，我们还通过无训练提示和基于训练的方法探索了一种人类启发的两阶段进展推理范式，基于精心策划的数据集ProgressLM-45K。对14个VLM的实验表明，大多数模型尚未准备好进行任务进展估计，对演示方式和视角变化敏感，并且在处理无法回答的案例时表现不佳。虽然强制结构化进展推理的无训练提示带来了有限且依赖于模型的收益，但基于训练的ProgressLM-3B即使在小模型规模下也实现了一致的改进，尽管其训练任务集与评估任务完全不重叠。进一步分析揭示了特征错误模式，并阐明了进展推理何时以及为何成功或失败。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the capability of Vision-Language Models (VLMs) in estimating task progress, which involves reasoning over dynamic changes rather than merely recognizing static images. The authors introduce Progress-Bench, a benchmark designed for assessing progress reasoning in VLMs, and investigate a two-stage progress reasoning paradigm using both training-free prompting and a training-based approach with the ProgressLM-45K dataset. Experimental results indicate that most VLMs struggle with task progress estimation, showing sensitivity to changes in demonstration modality and viewpoint, while the training-based ProgressLM-3B model demonstrates consistent improvements, even when trained on unrelated tasks, highlighting specific error patterns in progress reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估视觉语言模型（VLMs）是否能够有效地从部分观察中估计任务进展，因为当前模型在识别静态内容方面表现出色，但在动态推理方面存在困难。作者引入了Progress-Bench，这是一个旨在评估VLMs进展推理能力的基准，并使用无训练提示和基于ProgressLM-45K数据集的训练方法研究了两阶段进展推理范式。实验结果表明，大多数VLM在任务进展估计方面尚不具备能力，对演示方式和视角变化敏感，而基于训练的ProgressLM-3B模型即使在与评估任务无关的情况下训练，仍表现出一致的改进，突显了进展推理中的特定错误模式。</div>
</details>
</div>
<div class="card">
<div class="title">ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation</div>
<div class="meta-line">Authors: Hanlei Guo, Jiahao Shao, Xinya Chen, Xiyang Tan, Sheng Miao, Yujun Shen, Yiyi Liao</div>
<div class="meta-line">First: 2026-01-21T17:53:21+00:00 · Latest: 2026-01-21T17:53:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15221v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15221v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScenDi：用于城市生成的3D到2D场景扩散级联</div>
<div class="mono" style="margin-top:8px">最近，使用扩散模型进行3D物体生成的进展取得了显著成功，但生成逼真的3D城市场景仍然具有挑战性。仅依赖3D扩散模型的现有方法往往在外观细节上出现退化，而仅使用2D扩散模型的方法通常妥协了相机可控性。为克服这一限制，我们提出了ScenDi，一种结合3D和2D扩散模型的城市场景生成方法。我们首先训练一个3D潜在扩散模型以生成3D高斯分布，从而以相对较低的分辨率渲染图像。为了实现可控合成，这一3DGS生成过程可以通过指定输入（如3D边界框、道路地图或文本提示）进行条件化。然后，我们训练一个2D视频扩散模型，以增强基于3D高斯渲染图像的外观细节。通过利用粗糙的3D场景作为2D视频扩散的指导，ScenDi根据输入条件生成所需场景，并成功遵循准确的相机轨迹。在两个具有挑战性的真实世界数据集Waymo和KITTI-360上的实验表明了我们方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in generating realistic 3D urban scenes using diffusion models, which often suffer from either a loss of detail or lack of camera controllability. The authors propose ScenDi, a novel method that combines 3D and 2D diffusion models for urban scene generation. They first train a 3D latent diffusion model to produce 3D Gaussians for low-resolution image rendering, which can be conditioned on various inputs for controllability. Subsequently, a 2D video diffusion model is trained to enhance the appearance details based on the rendered images from the 3D model. Experimental results on the Waymo and KITTI-360 datasets show that ScenDi effectively generates urban scenes while maintaining accurate camera trajectories and improved visual fidelity.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用现有扩散模型生成逼真的3D城市场景所面临的挑战，这些模型要么妥协了外观细节，要么影响了相机可控性。作者提出了ScenDi，这是一种将3D和2D扩散模型结合用于城市场景生成的新方法。他们首先训练了一个3D潜在扩散模型，以创建3D高斯分布，用于低分辨率图像渲染，并可以根据各种输入进行条件控制。随后，训练了一个2D视频扩散模型，以基于3D高斯渲染图像增强外观细节。在Waymo和KITTI-360数据集上的实验结果表明，ScenDi能够有效生成所需场景，同时保持准确的相机轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">BBoxMaskPose v2: Expanding Mutual Conditioning to 3D</div>
<div class="meta-line">Authors: Miroslav Purkrabek, Constantin Kolomiiets, Jiri Matas</div>
<div class="meta-line">First: 2026-01-21T17:18:04+00:00 · Latest: 2026-01-21T17:18:04+00:00</div>
<div class="meta-line">Comments: GitHub repository: https://github.com/MiraPurkrabek/BBoxMaskPose/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15200v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15200v1">PDF</a> · <a href="https://github.com/MiraPurkrabek/BBoxMaskPose/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://MiraPurkrabek.github.io/BBox-Mask-Pose/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP&#x27;s 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BBoxMaskPose v2：扩展互条件到3D</div>
<div class="mono" style="margin-top:8px">大多数2D人体姿态估计基准几乎已饱和，拥挤场景除外。我们介绍PMPose，一种自上而下的2D姿态估计器，结合了概率公式和掩码条件。PMPose在不牺牲标准场景性能的情况下改善了拥挤姿态估计。在此基础上，我们提出了整合PMPose和增强的基于SAM的掩码细化模块的BBoxMaskPose v2（BMPv2）。BMPv2在COCO上超越了最先进的技术1.5平均精度（AP）点，在OCHuman上超越6 AP点，成为第一个在OCHuman上超过50 AP的方法。我们证明了BMP对3D模型的2D提示改善了拥挤场景中的3D姿态估计，并且2D姿态质量的进步直接有利于3D估计。新OCHuman-Pose数据集的结果表明，多人表现更受姿态预测准确性的影响，而非检测。代码、模型和数据可在https://MiraPurkrabek.github.io/BBox-Mask-Pose/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the saturation of most 2D human pose estimation benchmarks, particularly in crowded scenes. The authors introduce PMPose, a top-down 2D pose estimator that utilizes a probabilistic formulation and mask-conditioning to enhance pose estimation in crowded environments without compromising performance in standard scenes. Building on PMPose, BBoxMaskPose v2 (BMPv2) integrates an improved SAM-based mask refinement module, achieving a 1.5 average precision point increase on COCO and a 6 point increase on OCHuman, marking it as the first method to surpass 50 average precision on OCHuman. The findings indicate that the 2D prompting of a 3D model significantly enhances 3D pose estimation in crowded scenarios, with results showing that multi-person performance is more influenced by pose prediction accuracy than by detection accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是2D人体姿态估计基准几乎饱和，尤其是在拥挤场景中。作者介绍了PMPose，这是一种自上而下的2D姿态估计器，利用概率公式和掩码条件来增强拥挤环境中的姿态估计，而不影响标准场景的性能。在此基础上，BBoxMaskPose v2 (BMPv2)将PMPose与改进的基于SAM的掩码细化模块结合，在COCO上提高了1.5个平均精度点，在OCHuman上提高了6个点，成为第一个在OCHuman上超过50个平均精度的算法。研究表明，2D对3D模型的提示提高了拥挤场景中的3D姿态估计，结果显示多人物表现更受姿态预测准确性的影响，而非检测准确性。</div>
</details>
</div>
<div class="card">
<div class="title">BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation</div>
<div class="meta-line">Authors: Andrey Moskalenko, Danil Kuznetsov, Irina Dudko, Anastasiia Iasakova, Nikita Boldyrev, Denis Shepelev, Andrei Spiridonov, Andrey Kuznetsov, Vlad Shakhuro</div>
<div class="meta-line">First: 2026-01-21T16:02:21+00:00 · Latest: 2026-01-21T16:02:21+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15123v1">PDF</a> · <a href="https://github.com/emb-ai/BREPS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BREPS：可提示分割的边界框鲁棒性评估</div>
<div class="mono" style="margin-top:8px">可提示分割模型如SAM建立了一种强大的范式，使其能够在最小用户输入（包括点、边界框和文本提示）的情况下，对未见对象和领域进行强泛化。其中，边界框特别有效，通常优于点，同时显著降低注释成本。然而，目前的训练和评估协议通常依赖于通过简单启发式生成的合成提示，提供的对真实世界鲁棒性的洞察有限。本文研究了可提示分割模型对边界框提示自然变化的鲁棒性。首先，我们进行了一项受控用户研究，收集了数千个真实的边界框注释。我们的分析显示，对于相同模型和实例，不同用户的分割质量存在显著变异，表明类似SAM的模型对自然提示噪声高度敏感。然后，由于对所有可能用户输入进行全面测试在计算上是不可行的，我们将鲁棒性评估重新表述为边界框提示空间上的白盒优化问题。我们引入了BREPS，一种生成对抗性边界框的方法，旨在最小化或最大化分割错误，同时遵循自然性约束。最后，我们在10个数据集上基准测试了最先进的模型，涵盖日常场景到医学成像。代码 - https://github.com/emb-ai/BREPS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to evaluate the robustness of promptable segmentation models, particularly in the context of bounding box prompts, which are known to be effective yet underexplored in terms of real-world variability. The authors conducted a controlled user study to collect thousands of real bounding box annotations, revealing significant variability in segmentation quality across different users. To address the challenge of exhaustive testing, they reformulated robustness evaluation as a white-box optimization problem and introduced BREPS, a method for generating adversarial bounding boxes that either minimize or maximize segmentation error while maintaining naturalness constraints. The benchmarking of state-of-the-art models across 10 diverse datasets demonstrated the sensitivity of these models to natural prompt noise and highlighted the effectiveness of the proposed method.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估可提示分割模型的鲁棒性，特别是与边界框提示相关的鲁棒性，这些提示有效但通常使用合成启发式方法进行评估，无法反映真实世界的情况。作者进行了受控用户研究，收集了数千个真实的边界框注释，揭示了用户之间在相同模型和实例上的分割质量存在显著差异，表明像SAM这样的模型对自然提示噪声非常敏感。为了解决全面测试的挑战，他们将鲁棒性评估重新构建为一个白盒优化问题，并引入了BREPS，这是一种生成对抗性边界框的方法，能够在保持自然性的同时最小化或最大化分割错误，并在十个不同的数据集上对最先进的模型进行了基准测试，从日常场景到医学成像。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning</div>
<div class="meta-line">Authors: Shuonan Yang, Yuchen Zhang, Zeyu Fu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:52:26+00:00 · Latest: 2026-01-21T15:52:26+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026. \c{opyright} 2026 IEEE. This is the author accepted manuscript. The final published version will be available via IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15115v1">PDF</a> · <a href="https://github.com/Multimodal-Intelligence-Lab-MIL/MARS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无训练且可解释的仇恨视频检测通过多阶段对抗推理</div>
<div class="mono" style="margin-top:8px">仇恨视频通过加剧歧视、煽动暴力和破坏在线安全，带来了严重风险。现有的基于训练的仇恨视频检测方法受到有限训练数据的限制，且缺乏可解释性，而直接提示大型视觉-语言模型往往难以提供可靠的仇恨检测。为了解决这些挑战，本文提出了MARS，一个无训练的多阶段对抗推理框架，能够实现可靠且可解释的仇恨内容检测。MARS首先对视频内容进行客观描述，为后续分析建立中立基础。在此基础上，开发基于证据的推理，支持潜在的仇恨解释，同时并行纳入反证据推理，以捕捉合理的非仇恨视角。最后，这些视角被综合成一个结论性且可解释的决策。在两个真实世界数据集上的广泛评估表明，MARS在某些骨干网络和设置下相比其他无训练方法提高了最多10%的性能，并在一个数据集上超越了最先进的基于训练的方法。此外，MARS生成可被人理解的理由，从而支持合规监督并增强内容审核工作流程的透明度。代码可在https://github.com/Multimodal-Intelligence-Lab-MIL/MARS获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for effective detection of hateful videos, which pose risks to online safety and can incite violence, while existing methods face limitations in training data and interpretability. The authors propose MARS, a training-free Multi-stage Adversarial Reasoning framework that begins with a neutral description of video content and employs evidence-based reasoning to identify potential hateful interpretations alongside counter-evidence reasoning for non-hateful perspectives. Experimental results demonstrate that MARS achieves up to a 10% improvement over other training-free methods and outperforms state-of-the-art training-based approaches on one dataset, while also providing human-understandable justifications to enhance transparency in content moderation workflows.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决仇恨视频带来的风险，这些视频可能加剧歧视并煽动暴力，同时克服现有基于训练的检测方法的局限性。作者提出了MARS，一个无训练的多阶段对抗推理框架，该框架首先对视频内容进行客观描述，并采用基于证据的推理来识别潜在的仇恨解释，同时结合反证据推理来捕捉非仇恨的观点。实验结果表明，MARS在某些基础模型和设置下比其他无训练方法提高了多达10%的性能，并在一个数据集上超越了最先进的基于训练的方法，同时还提供了可理解的决策理由，增强了内容审核的透明度。</div>
</details>
</div>
<div class="card">
<div class="title">Field-Space Autoencoder for Scalable Climate Emulators</div>
<div class="meta-line">Authors: Johannes Meuer, Maximilian Witte, Étiénne Plésiat, Thomas Ludwig, Christopher Kadow</div>
<div class="meta-line">First: 2026-01-21T15:43:53+00:00 · Latest: 2026-01-21T15:43:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15102v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15102v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展气候模拟器的场空间自编码器</div>
<div class="mono" style="margin-top:8px">公里尺度的地球系统模型对于捕捉局部气候变化至关重要。然而，这些模型计算成本高，产生的输出达到PB级别，这限制了它们在概率风险评估等应用中的实用性。在此，我们提出了场空间自编码器，这是一种基于球形压缩模型的可扩展气候模拟框架，克服了这些挑战。通过利用场空间注意力，该模型有效地处理原生气候模型输出，从而避免了将球形数据强制映射到欧几里得网格所造成的几何失真。这种方法在保留物理结构方面显著优于卷积基线。通过生成结构化的压缩场，它为下游生成模拟提供了良好的基线。此外，该模型可以执行零-shot超分辨率，将低分辨率的大型集合和稀缺的高分辨率数据映射到共享表示中。我们在这些压缩场上训练了生成扩散模型。该模型可以同时从丰富的低分辨率数据中学习内部变异性，并从稀疏的高分辨率数据中学习细尺度物理。我们的工作弥合了低分辨率集合统计的高容量与高分辨率物理细节的稀缺之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of kilometer-scale Earth system models, which are computationally intensive and generate massive outputs, hindering their application in areas like probabilistic risk assessment. The authors propose the Field-Space Autoencoder, a scalable climate emulation framework that employs a spherical compression model and Field-Space Attention to efficiently process native climate model outputs without geometric distortions. Experimental results demonstrate that this method preserves physical structures better than traditional convolutional approaches and enables zero-shot super-resolution, allowing the model to integrate low-resolution ensemble data with sparse high-resolution information effectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决千米尺度地球系统模型的局限性，这些模型计算密集且生成大量数据，妨碍了它们在概率风险评估等领域的应用。作者提出了场空间自编码器，这是一种可扩展的气候仿真框架，采用球面压缩模型和场空间注意力，能够高效处理本地气候模型输出，避免几何失真。实验结果表明，该方法比传统卷积方法更好地保留物理结构，并且能够实现零样本超分辨率，使模型能够同时从低分辨率和稀疏高分辨率数据中学习，有效弥合低分辨率集合统计与高分辨率物理细节之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Autiverse: Eliciting Autistic Adolescents&#x27; Daily Narratives through AI-guided Multimodal Journaling</div>
<div class="meta-line">Authors: Migyeong Yang, Kyungah Lee, Jinyoung Han, SoHyun Park, Young-Ho Kim</div>
<div class="meta-line">First: 2025-09-22T08:02:09+00:00 · Latest: 2026-01-21T14:47:03+00:00</div>
<div class="meta-line">Comments: 19 pages excluding reference. Conditionally accepted to ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17466v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17466v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Journaling can potentially serve as an effective method for autistic adolescents to improve narrative skills. However, its text-centric nature and high executive functioning demands present barriers to practice. We present Autiverse, an AI-guided multimodal journaling app for tablets that scaffolds storytelling through conversational prompts and visual supports. Autiverse elicits key details through a stepwise dialogue with peer-like, customizable AI and composes them into an editable four-panel comic strip. Through a two-week deployment study with 10 autistic adolescent-parent dyads, we examine how Autiverse supports autistic adolescents to organize their daily experience and emotion. Autiverse scaffolded adolescents&#x27; coherent narratives, while enabling parents to learn additional details of their child&#x27;s events and emotions. The customized AI peer created a comfortable space for sharing, fostering enjoyment and a strong sense of agency. We discuss implications for adaptive scaffolding across autism profiles, socio-emotionally appropriate AI peer design, and balancing autonomy with parental involvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Autiverse：通过AI引导的多模态日记引发自闭症青少年的日常叙事</div>
<div class="mono" style="margin-top:8px">日记写作可能成为自闭症青少年提高叙事技能的有效方法。然而，其以文本为中心的特性和高执行功能需求给实践带来了障碍。我们提出了Autiverse，一款为平板电脑设计的AI引导多模态日记应用，通过对话提示和视觉支持来辅助讲故事。Autiverse通过与类似同龄人的可定制AI进行逐步对话，提取关键细节，并将其编排成可编辑的四格漫画。通过与10对自闭症青少年-家长的为期两周的部署研究，我们考察了Autiverse如何支持自闭症青少年组织他们的日常经历和情感。Autiverse帮助青少年构建连贯的叙事，同时使家长能够了解孩子事件和情感的更多细节。定制的AI同伴创造了一个舒适的分享空间，促进了乐趣和强烈的自主感。我们讨论了在自闭症谱系中适应性支架的影响、社会情感适宜的AI同伴设计，以及在自主性与家长参与之间的平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance narrative skills in autistic adolescents, who face challenges with traditional text-based journaling due to its demands on executive functioning. The study introduces Autiverse, an AI-guided multimodal journaling app that facilitates storytelling through conversational prompts and visual aids, allowing users to create editable comic strips. In a two-week deployment study involving 10 autistic adolescent-parent pairs, findings indicate that Autiverse effectively helped adolescents organize their daily experiences and emotions, while also providing parents with deeper insights into their child&#x27;s life, fostering enjoyment and a sense of agency through the customizable AI peer interface.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高自闭症青少年的叙事能力，因为他们在传统的日记方法中面临由于文本中心特性和高执行功能要求而带来的挑战。该研究介绍了Autiverse，这是一款利用对话提示和视觉支持来促进讲故事的AI引导多模态日记应用程序。通过对10对自闭症青少年与父母的为期两周的部署研究发现，Autiverse有效地帮助青少年组织他们的日常经历和情感，促进连贯的叙事，并使父母能够深入了解孩子的感受和事件，同时可定制的AI同伴营造了一个舒适的分享环境，增强了青少年的自主感。</div>
</details>
</div>
<div class="card">
<div class="title">RadixMLP -- Intra-batch Deduplication for Causal Transformers</div>
<div class="meta-line">Authors: Michael Feil, Julius Lipp</div>
<div class="meta-line">First: 2026-01-21T14:11:46+00:00 · Latest: 2026-01-21T14:11:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15013v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15013v1">PDF</a> · <a href="https://github.com/michaelfeil/radix-mlp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\times$ speedups in realistic reranking workloads, with up to $5\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RadixMLP -- 因果变换器的批内去重</div>
<div class="mono" style="margin-top:8px">因果变换器模型的批推理工作负载经常处理共享公共前缀的序列，如系统提示、少量示例或共享查询。标准推理引擎独立处理每个序列，为每个共享前缀的副本冗余地重新计算相同的MLP激活。我们引入了RadixMLP，这是一种利用MLP、LayerNorm、线性投影和嵌入的逐位置特性来消除这种冗余的技术。RadixMLP动态将批次映射到前缀字典树，将共享段聚集到压缩表示中以进行逐位置计算，并仅在注意力边界处散布结果。RadixMLP是无状态的，并在单次前向传递中操作。在使用Qwen3模型（参数从0.6B到8B）的MS~MARCO v1.1的端到端服务基准测试中，RadixMLP在现实的重排序工作负载中实现了1.44-1.59$\times$的加速，在具有更长共享前缀的合成基准测试中实现了高达$5\times$的加速。我们的代码可在https://github.com/michaelfeil/radix-mlp获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of batch inference workloads for causal transformer models, which often process sequences with common prefixes, leading to redundant computations. The authors introduce RadixMLP, a method that leverages the position-wise characteristics of MLPs and related components to eliminate this redundancy by dynamically mapping batches to a prefix trie. Experimental results demonstrate that RadixMLP achieves speedups of 1.44-1.59 times in end-to-end serving benchmarks on MS MARCO v1.1 with Qwen3 models, and up to 5 times on synthetic benchmarks with longer shared prefixes.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高因果变换器模型的批量推理工作负载的效率，这些模型通常处理具有共同前缀的序列，导致冗余计算。作者提出了RadixMLP，这是一种利用MLP及其他组件的逐位置特性来消除冗余的方法，通过动态将批次映射到前缀字典树，并压缩共享段以实现高效计算。实验结果表明，RadixMLP在MS MARCO v1.1的Qwen3模型的端到端服务基准测试中实现了1.44-1.59倍的加速，并在具有较长共享前缀的合成基准测试中实现了高达5倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers</div>
<div class="meta-line">Authors: Xinyu Peng, Han Li, Yuyang Huang, Ziyang Zheng, Yaoming Wang, Xin Chen, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</div>
<div class="meta-line">First: 2026-01-21T12:58:52+00:00 · Latest: 2026-01-21T12:58:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14959v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14959v1">PDF</a> · <a href="https://github.com/xypeng9903/LDF-VFI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \textbf{L}ocal \textbf{D}iffusion \textbf{F}orcing for \textbf{V}ideo \textbf{F}rame \textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自回归扩散变换器的全局视频帧插值建模</div>
<div class="mono" style="margin-top:8px">现有的视频帧插值（VFI）方法通常采用以帧为中心的方法，将视频处理为独立的短片段（例如，三元组），这导致时间不一致和运动伪影。为了解决这个问题，我们提出了一种名为\textbf{L}ocal \textbf{D}iffusion \textbf{F}orcing for \textbf{V}ideo \textbf{F}rame \textbf{I}nterpolation (LDF-VFI)的全局视频中心范式。我们的框架基于自回归扩散变换器，建模整个视频序列以确保长时间范围内的时间一致性。为了减轻自回归生成中固有的误差积累，我们引入了一种新颖的跳跃连接采样策略，有效保持时间稳定性。此外，LDF-VFI结合了稀疏的局部注意力和瓦片VAE编码，这种组合不仅能够高效处理长序列，还允许在推理时对任意空间分辨率（例如，4K）进行泛化，而无需重新训练。增强的条件VAE解码器利用输入视频的多尺度特征，进一步提高了重建保真度。从经验上看，LDF-VFI在具有挑战性的长序列基准测试中实现了最先进的性能，展示了优越的每帧质量和时间一致性，特别是在大运动场景中。源代码可在https://github.com/xypeng9903/LDF-VFI获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing video frame interpolation methods that often lead to temporal inconsistencies and motion artifacts due to their frame-centric approach. The authors propose a holistic video-centric paradigm called Local Diffusion Forcing for Video Frame Interpolation (LDF-VFI), which utilizes an auto-regressive diffusion transformer to model entire video sequences for improved long-range temporal coherence. Key experimental results show that LDF-VFI achieves state-of-the-art performance on long-sequence benchmarks, demonstrating enhanced per-frame quality and temporal consistency, particularly in scenes with significant motion.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视频帧插值方法的局限性，这些方法由于采用帧中心的方法，常常导致时间不一致和运动伪影。作者提出了一种称为视频帧插值的局部扩散强制（LDF-VFI）的整体视频中心框架，该框架利用自回归扩散变换器对整个视频序列进行建模，以提高时间一致性。关键实验结果表明，LDF-VFI在长序列基准测试中实现了最先进的性能，尤其是在大运动场景中，显示出增强的每帧质量和时间一致性。</div>
</details>
</div>
<div class="card">
<div class="title">TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models</div>
<div class="meta-line">Authors: Carolin Holtermann, Nina Krebs, Anne Lauscher</div>
<div class="meta-line">First: 2026-01-21T12:52:23+00:00 · Latest: 2026-01-21T12:52:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14951v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TempViz：文本到图像模型中时间知识的评估</div>
<div class="mono" style="margin-top:8px">时间改变了我们世界中实体的视觉外观，如物体、地点和动物。因此，为了准确生成与上下文相关的图像，关于时间的知识和推理可能至关重要（例如，生成春季与冬季的风景）。然而，尽管在自然语言处理领域对理解和改善时间知识的研究已有相当多的工作，但关于时间现象在文本到图像（T2I）模型中如何出现和处理的研究仍然稀缺。我们通过TempViz解决了这一空白，这是第一个全面评估图像生成中时间知识的数据集，包含7.9k个提示和600多个参考图像。使用TempViz，我们研究了五个T2I模型在五个时间知识类别中的能力。人类评估显示，时间能力普遍较弱，没有模型在各类别中超过75%的准确率。为了进行更大规模的研究，我们还考察了自动评估方法，将几种已建立的方法与人类判断进行比较。然而，这些方法都未能提供对时间线索的可靠评估，进一步表明未来在T2I中对时间知识进行研究的迫切需要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of understanding regarding how temporal knowledge is represented and utilized in text-to-image (T2I) models, which is crucial for generating contextually relevant images. The authors introduce TempViz, a dataset comprising 7.9k prompts and over 600 reference images, to evaluate the temporal knowledge capabilities of five T2I models across five categories. The findings reveal that the temporal competence of these models is generally weak, with none achieving over 75% accuracy in human evaluations, and automated evaluation methods fail to provide reliable assessments of temporal cues, highlighting the need for further research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像（T2I）模型中时间知识的表示和利用缺乏理解的问题，这对于生成上下文相关的图像至关重要。作者引入了TempViz数据集，该数据集包含7.9k个提示和超过600个参考图像，用于评估五个T2I模型在五个类别中的时间知识能力。研究结果表明，这些模型的时间能力普遍较弱，任何类别的准确率均未超过75%，而自动评估方法也未能可靠地评估时间线索，这凸显了该领域进一步研究的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：用于遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有的视觉语言模型（VLMs）主要在网络抓取的嘈杂图像-文本数据上训练，缺乏对遥感（RS）专业领域的充分接触。这一缺陷导致在RS特定任务上的表现不佳，因为常用数据集往往缺乏详细、科学准确的文本描述，而仅强调日期和位置等属性。为弥补这一关键缺口，我们推出了GAIA，一个为多尺度、多传感器和多模态RS图像分析设计的新数据集。GAIA包含201,005对精心策划的RS图像-文本对，代表了与不同空间分辨率相关的多样化RS模态。与现有的RS视觉语言数据集不同，GAIA特别关注捕捉多样化的RS应用，提供有关环境变化、自然灾害和其他各种动态现象的独特信息。该数据集提供了空间和时间上平衡的分布，覆盖全球，涵盖过去25年，观察的时间分布也很平衡。GAIA的构建涉及两个阶段的过程：（1）从可靠的RS相关来源有针对性地抓取图像及其伴随文本，和（2）为每张图像生成五个高质量、科学基础的合成标题，使用精心设计的提示，利用GPT-4o的先进视觉语言能力。我们的广泛实验，包括对CLIP和BLIP2模型的微调，表明GAIA显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。我们在项目的GitHub仓库上公开了我们的数据集、自动处理框架和微调模型权重：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing Vision-Language Models (VLMs) that are primarily trained on noisy web-scraped data, which inadequately supports remote sensing (RS) tasks due to a lack of scientifically accurate textual descriptions. The authors introduce GAIA, a new dataset consisting of 201,005 carefully curated RS image-text pairs, designed for multi-scale, multi-sensor, and multi-modal analysis, created through targeted web-scraping and the generation of high-quality synthetic captions using GPT-4o. Experimental results show that fine-tuning models like CLIP and BLIP2 on GAIA leads to significant improvements in RS image classification, cross-modal retrieval, and image captioning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视觉语言模型（VLM）在噪声较大的网络抓取数据上训练的局限性，这导致其在遥感（RS）任务上的表现不佳，因为缺乏科学准确的文本描述。作者提出了GAIA，一个包含201,005对精心策划的RS图像-文本对的数据集，该数据集通过两阶段过程创建，包括针对性地从可靠的RS相关来源抓取图像和使用GPT-4o生成高质量的合成标题。实验结果表明，对CLIP和BLIP2等模型进行GAIA微调显著提高了RS图像分类、跨模态检索和图像标题生成任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DroneVLA: VLA based Aerial Manipulation</div>
<div class="meta-line">Authors: Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:08:00+00:00 · Latest: 2026-01-21T10:32:20+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13809v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system&#x27;s efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DroneVLA：基于VLA的空中操作</div>
<div class="mono" style="margin-top:8px">随着空中平台从被动观察者演变为主动操作者，挑战转向设计直观的界面，使非专业用户能够自然地指挥这些系统。本研究介绍了一种新颖的自主空中操作系统，能够解释高级自然语言命令以检索物体并将其交付给人类用户。该系统旨在集成基于Grounding DINO的MediaPipe和视觉-语言-动作（VLA）模型与一架配备1-DOF抓手和Intel RealSense RGB-D相机的定制无人机。VLA执行语义推理以解释用户提示的意图，并生成优先任务队列以抓取场景中的相关物体。使用Grounding DINO和动态A*规划算法进行导航和安全重新定位物体。为了确保在交接阶段的安全和自然交互，系统采用了由MediaPipe驱动的人本控制器。该模块提供实时的人体姿态估计，使无人机能够利用视觉伺服保持在用户正前方的稳定、清晰位置，促进舒适的交接。我们通过实际实验展示了系统在定位和导航方面的有效性，结果显示最大、平均欧几里得误差和均方根误差分别为0.164m、0.070m和0.084m，突显了VLA在空中操作中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for intuitive interfaces that enable non-expert users to control aerial manipulation systems as drones evolve from passive observers to active manipulators. The authors developed an autonomous aerial manipulation system that interprets high-level natural language commands using a combination of a MediaPipe-based Grounding DINO and a Vision-Language-Action (VLA) model, integrated with a custom drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. Experimental results demonstrated the system&#x27;s effectiveness in localization and navigation, achieving maximum, mean, and root-mean squared errors of 0.164m, 0.070m, and 0.084m respectively, confirming the practicality of VLA for aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要直观的界面，使非专业用户能够指挥空中操作系统。作者开发了一种自主空中操作系统，该系统使用基于MediaPipe的Grounding DINO和视觉-语言-动作（VLA）模型，结合配备夹持器和RGB-D相机的定制无人机，来解释高级自然语言命令。实验结果表明，该系统在定位和导航方面的有效性，最大、平均和均方根误差分别为0.164米、0.070米和0.084米，确认了VLA在空中操作任务中的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking</div>
<div class="meta-line">Authors: Qihua Liang, Liang Chen, Yaozong Zheng, Jian Nong, Zhiyi Mo, Bineng Zhong</div>
<div class="meta-line">First: 2026-01-21T09:24:19+00:00 · Latest: 2026-01-21T09:24:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14799v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba&#x27;s long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UBATrack：通用多模态跟踪的时空状态空间模型</div>
<div class="mono" style="margin-top:8px">多模态物体跟踪通过整合多种互补输入（例如，热成像、深度和事件数据）来实现卓越性能，受到了广泛关注。尽管当前的通用多模态跟踪器主要通过提示学习统一各种模态跟踪任务（即RGB-热成像、RGB-深度或RGB-事件跟踪），但它们仍然忽视了有效捕捉时空线索。在本研究中，我们引入了一种基于曼巴风格状态空间模型的新型多模态跟踪框架，称为UBATrack。我们的UBATrack包含两个简单而有效的模块：时空曼巴适配器（STMA）和动态多模态特征混合器。前者利用曼巴的长序列建模能力，以适配器调优的方式共同建模跨模态依赖关系和时空视觉线索。后者进一步增强了跨多个特征维度的多模态表示能力，以提高跟踪的鲁棒性。通过这种方式，UBATrack消除了对昂贵的全参数微调的需求，从而提高了多模态跟踪算法的训练效率。实验表明，UBATrack在RGB-T、RGB-D和RGB-E跟踪基准上超越了最先进的方法，在LasHeR、RGBT234、RGBT210、DepthTrack、VOT-RGBD22和VisEvent数据集上取得了卓越的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multi-modal object tracking by effectively capturing spatio-temporal cues, which current trackers fail to do despite their integration of various inputs. The authors propose a novel framework called UBATrack, which utilizes a mamba-style state space model featuring two main components: a Spatio-temporal Mamba Adapter for modeling cross-modal dependencies and spatio-temporal cues, and a Dynamic Multi-modal Feature Mixer to improve representation capacity. Experimental results demonstrate that UBATrack surpasses existing state-of-the-art methods across multiple benchmarks, including RGB-T, RGB-D, and RGB-E tracking, achieving significant performance improvements on several datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效捕捉时空线索来增强多模态目标跟踪，而当前的跟踪器在整合各种输入模态时未能做到这一点。作者提出了一种名为UBATrack的新框架，该框架利用一种蛇形状态空间模型，包含两个模块：时空蛇形适配器用于建模跨模态依赖关系和时空线索，以及动态多模态特征混合器以提高表示能力。实验结果表明，UBATrack在多个基准测试中超越了现有的最先进方法，在LasHeR和VOT-RGBD22等多个数据集上表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach</div>
<div class="meta-line">Authors: Ziyao Ling, Silvia Mirri, Paola Salomoni, Giovanni Delnevo</div>
<div class="meta-line">First: 2026-01-21T09:14:22+00:00 · Latest: 2026-01-21T09:14:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scarcity of training data presents a fundamental challenge in applying deep learning to archaeological artifact classification, particularly for the rare types of Chinese porcelain. This study investigates whether synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) can effectively augment limited real datasets for multi-task CNN-based porcelain classification. Using MobileNetV3 with transfer learning, we conducted controlled experiments comparing models trained on pure real data against those trained on mixed real-synthetic datasets (95:5 and 90:10 ratios) across four classification tasks: dynasty, glaze, kiln and type identification. Results demonstrate task-specific benefits: type classification showed the most substantial improvement (5.5\% F1-macro increase with 90:10 ratio), while dynasty and kiln tasks exhibited modest gains (3-4\%), suggesting that synthetic augmentation effectiveness depends on the alignment between generated features and task-relevant visual signatures. Our work contributes practical guidelines for deploying generative AI in archaeological research, demonstrating both the potential and limitations of synthetic data when archaeological authenticity must be balanced with data diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多任务中国瓷器分类的合成数据增强：一种稳定扩散方法</div>
<div class="mono" style="margin-top:8px">训练数据的稀缺在将深度学习应用于考古文物分类时，尤其是对于稀有类型的中国瓷器，构成了根本挑战。本研究探讨通过低秩适应（LoRA）的稳定扩散生成的合成图像是否能有效增强有限的真实数据集，以进行基于CNN的多任务瓷器分类。我们使用MobileNetV3进行迁移学习，进行了对照实验，比较了在纯真实数据上训练的模型与在混合真实-合成数据集（95:5和90:10比例）上训练的模型在四个分类任务（朝代、釉料、窑口和类型识别）上的表现。结果显示任务特定的益处：类型分类显示出最显著的改善（90:10比例下F1-macro提高5.5%），而朝代和窑口任务则表现出适度的增益（3-4%），这表明合成增强的有效性依赖于生成特征与任务相关视觉特征之间的对齐。我们的工作为在考古研究中部署生成性人工智能提供了实用指南，展示了在考古真实性与数据多样性之间必须平衡时合成数据的潜力和局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of limited training data in deep learning for classifying rare Chinese porcelain artifacts. The researchers employed synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) to augment real datasets for multi-task convolutional neural network (CNN) classification. Experimental results indicated that while type classification benefited significantly from synthetic data (5.5% F1-macro increase with a 90:10 ratio), dynasty and kiln classifications showed modest improvements of 3-4%, highlighting that the effectiveness of synthetic augmentation is influenced by the alignment of generated features with task-specific visual signatures.</div>
<div class="mono" style="margin-top:8px">本研究解决了深度学习在分类稀有中国瓷器文物时面临的训练数据不足问题。研究采用通过稳定扩散与低秩适应（LoRA）生成的合成图像来增强多任务卷积神经网络（CNN）分类的真实数据集。实验结果表明，使用混合真实-合成数据集训练的模型在分类任务中表现出显著改善，特别是在类型分类任务中，90:10比例下F1-macro得分提高了5.5%，而朝代和窑口任务则获得了3-4%的适度提升，这表明合成增强的有效性受到生成特征与任务特定视觉特征对齐程度的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph</div>
<div class="meta-line">Authors: Ziyu Zheng, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyan Huang, Weigang Lu</div>
<div class="meta-line">First: 2025-10-10T13:48:34+00:00 · Latest: 2026-01-21T08:54:49+00:00</div>
<div class="meta-line">Comments: Accepted by WWW2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09394v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.09394v3">PDF</a> · <a href="https://github.com/zhengziyu77/MSGCOT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ``pre-train, prompt&quot; paradigm, designed to bridge the gap between pre-training tasks and downstream objectives, has been extended from the NLP domain to the graph domain and has achieved remarkable progress. Current mainstream graph prompt-tuning methods modify input or output features using learnable prompt vectors. However, existing approaches are confined to single-granularity (e.g., node-level or subgraph-level) during prompt generation, overlooking the inherently multi-scale structural information in graph data, which limits the diversity of prompt semantics. To address this issue, we pioneer the integration of multi-scale information into graph prompt and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework. Specifically, we design a lightweight, low-rank coarsening network to efficiently capture multi-scale structural features as hierarchical basis vectors for prompt generation. Subsequently, mimicking human cognition from coarse-to-fine granularity, we dynamically integrate multi-scale information at each reasoning step, forming a progressive coarse-to-fine prompt chain. Extensive experiments on eight benchmark datasets demonstrate that MSGCOT outperforms the state-of-the-art single-granularity graph prompt-tuning method, particularly in few-shot scenarios, showcasing superior performance. The code is available at: https://github.com/zhengziyu77/MSGCOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单粒度提示：一种用于图的多尺度思维链提示学习</div>
<div class="mono" style="margin-top:8px">“预训练，提示”范式旨在弥合预训练任务与下游目标之间的差距，已从NLP领域扩展到图领域，并取得了显著进展。目前主流的图提示调优方法通过可学习的提示向量修改输入或输出特征。然而，现有方法在提示生成过程中局限于单粒度（例如，节点级或子图级），忽视了图数据中固有的多尺度结构信息，限制了提示语义的多样性。为了解决这个问题，我们首创将多尺度信息整合到图提示中，并提出了一种多尺度图思维链（MSGCOT）提示框架。具体而言，我们设计了一种轻量级、低秩的粗化网络，以高效捕捉多尺度结构特征作为提示生成的层次基础向量。随后，模仿人类从粗到细的认知过程，我们在每个推理步骤动态整合多尺度信息，形成一个渐进的粗到细提示链。在八个基准数据集上的大量实验表明，MSGCOT在少样本场景中优于最先进的单粒度图提示调优方法，展示了卓越的性能。代码可在：https://github.com/zhengziyu77/MSGCOT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing graph prompt-tuning methods that rely on single-granularity approaches, which fail to utilize the multi-scale structural information present in graph data. The authors propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework that incorporates multi-scale information through a lightweight, low-rank coarsening network to generate hierarchical basis vectors for prompts. Experimental results across eight benchmark datasets indicate that MSGCOT significantly outperforms traditional single-granularity methods, especially in few-shot learning scenarios, demonstrating enhanced performance and versatility in graph-based tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有的图提示调优方法仅利用单一粒度的提示，这限制了图数据中提示语义的多样性。为了解决这个问题，作者提出了一种多尺度图思维链（MSGCOT）提示框架，将多尺度结构信息整合到提示生成中。该方法采用轻量级低秩粗化网络捕捉层次特征，并在每个推理步骤动态结合这些特征，从而形成渐进的提示链。在八个基准数据集上的实验结果表明，MSGCOT显著优于传统的单粒度方法，特别是在少样本学习场景中，证明了其在提升图提示调优性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution</div>
<div class="meta-line">Authors: Chongbin Yi, Yuxin Liang, Ziqi Zhou, Peng Yang</div>
<div class="meta-line">First: 2026-01-21T07:55:37+00:00 · Latest: 2026-01-21T07:55:37+00:00</div>
<div class="meta-line">Comments: Accpeted by ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14741v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14741v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users&#x27; Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过端边协作混合超分辨率增强文本到图像生成</div>
<div class="mono" style="margin-top:8px">人工智能生成内容（AIGC）取得了显著进展，高分辨率文本到图像（T2I）生成对于提升用户体验质量（QoE）变得越来越重要。尽管资源受限的边缘计算能够支持快速的低分辨率T2I生成，但实现高分辨率输出仍面临在延迟成本下确保图像保真度的挑战。为了解决这个问题，我们首先研究了超分辨率（SR）方法在图像增强中的性能，确认了一个基本的权衡：轻量级学习型SR难以恢复细节，而基于扩散的SR在较高计算成本下实现了更高的保真度。基于这些观察，我们提出了一种端边协作生成增强框架。在接收到T2I生成任务后，系统首先根据自适应选择的去噪步骤和超分辨率尺度在边缘侧生成低分辨率图像，然后将其划分为补丁，并由区域感知混合SR策略处理。该策略对前景补丁应用基于扩散的SR模型以恢复细节，对背景补丁应用轻量级学习型SR模型以高效放大，最终将增强后的图像拼接成高分辨率图像。实验表明，我们的系统在保持竞争性图像质量的同时，将服务延迟减少了33%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the quality of experience in high-resolution text-to-image generation, which is crucial for artificial intelligence-generated content. The authors propose an end-edge collaborative generation-enhancement framework that first generates a low-resolution image at the edge, which is then processed using a hybrid super-resolution policy that combines diffusion-based and lightweight learning-based methods. Experimental results demonstrate that this approach reduces service latency by 33% compared to baseline methods while preserving competitive image quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高高分辨率文本到图像生成的用户体验质量，这对人工智能生成内容至关重要。作者提出了一种端边协作生成增强框架，首先使用自适应去噪步骤和超分辨率尺度生成低分辨率图像，然后通过区域感知混合超分辨率策略进行处理。实验结果表明，该方法在保持竞争性图像质量的同时，将服务延迟减少了33%。</div>
</details>
</div>
<div class="card">
<div class="title">Human detectors are surprisingly powerful reward models</div>
<div class="meta-line">Authors: Kumar Ashutosh, XuDong Wang, Xi Yin, Kristen Grauman, Adam Polyak, Ishan Misra, Rohit Girdhar</div>
<div class="meta-line">First: 2026-01-15T18:48:17+00:00 · Latest: 2026-01-21T07:24:31+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14037v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类检测器是令人惊讶的强大奖励模型</div>
<div class="mono" style="margin-top:8px">视频生成模型最近在视觉保真度和时间一致性方面取得了令人印象深刻的成果。然而，它们在合成执行动态动作（如体育、舞蹈等）的人类时，仍然面临复杂的非刚性运动的挑战。生成的视频常常出现缺失或多余的肢体、扭曲的姿势或物理上不合理的动作。在这项工作中，我们提出了一种非常简单的奖励模型HuDA，以量化和改善生成视频中的人类运动。HuDA结合了人类检测置信度以评估外观质量，以及时间提示对齐分数以捕捉运动真实感。我们展示了这个简单的奖励函数利用现成模型而无需额外训练，超越了使用手动标注数据微调的专门模型。通过在视频模型的后训练中使用HuDA进行群体奖励策略优化（GRPO），我们显著提升了视频生成，尤其是在生成复杂人类运动时，超越了如Wan 2.1等最先进模型，胜率达到73%。最后，我们证明HuDA不仅改善了人类的生成质量，还显著提高了动物视频和人类与物体交互的生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by video generation models in synthesizing complex human motions, which often result in unrealistic outputs. The authors propose a simple reward model called HuDA, which quantifies human motion quality by integrating human detection confidence and a temporal prompt alignment score. Experimental results show that HuDA significantly enhances video generation, particularly for dynamic human actions, achieving a win-rate of 73% against state-of-the-art models like Wan 2.1, and also improves the quality of videos involving animals and human-object interactions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频生成模型在准确合成复杂人类动作时面临的挑战，这些挑战常常导致不现实的表现。作者提出了一种名为HuDA的简单奖励模型，通过整合人类检测置信度和时间提示对齐分数来量化人类动作质量。实验结果表明，HuDA显著提升了视频生成的效果，特别是在动态人类动作方面，其胜率达到73%，超越了如Wan 2.1等最先进模型，并且还改善了涉及动物和人类与物体交互的视频生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">When Text-as-Vision Meets Semantic IDs in Generative Recommendation: An Empirical Study</div>
<div class="meta-line">Authors: Shutong Qiao, Wei Yuan, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin</div>
<div class="meta-line">First: 2026-01-21T06:18:57+00:00 · Latest: 2026-01-21T06:18:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14697v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.
  In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当文本作为视觉与生成推荐中的语义ID相遇：一项实证研究</div>
<div class="mono" style="margin-top:8px">语义ID学习是生成推荐（GR）模型中的一个关键接口，将项目映射到基于侧面信息的离散标识符，通常通过预训练的文本编码器实现。然而，这些文本编码器主要针对结构良好的自然语言进行优化。在现实世界的推荐数据中，项目描述往往是符号化和属性中心的，包含数字、单位和缩写。这些文本编码器可能会将这些信号分解为碎片化的标记，削弱语义连贯性并扭曲属性之间的关系。更糟糕的是，当转向多模态GR时，依赖标准文本编码器会引入额外障碍：文本和图像嵌入通常表现出不匹配的几何结构，使得跨模态融合效果较差且不稳定。本文通过将文本视为视觉信号，重新审视语义ID学习的表示设计。我们对基于OCR的文本表示进行了系统的实证研究，通过将项目描述渲染为图像并使用基于视觉的OCR模型进行编码。四个数据集和两个生成骨干网络的实验表明，OCR文本在单模态和多模态设置中始终与标准文本嵌入相匹配或超越，适用于语义ID学习。此外，我们发现基于OCR的语义ID在极端空间分辨率压缩下仍然保持稳健，表明在实际部署中具有强大的鲁棒性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to improve Semantic ID learning in Generative Recommendation models, which often struggle with item descriptions that are not well-formed natural language. The authors propose a method that utilizes OCR-based text representations by converting item descriptions into images and encoding them with vision-based OCR models. The experimental results demonstrate that OCR-text representations consistently match or exceed the performance of standard text embeddings in Semantic ID learning across various datasets and generative backbones, while also showing robustness under extreme spatial-resolution compression.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善生成推荐模型中的语义ID学习，这些模型通常在处理符号和属性中心的项目描述时面临困难。作者提出了一种新方法，通过将项目描述转换为图像并使用基于视觉的OCR模型进行编码，利用基于OCR的文本表示。实验结果表明，OCR文本表示在多个数据集和生成基础上，语义ID学习的表现始终与标准文本嵌入相匹配或超越，同时在极端空间分辨率压缩下也表现出强大的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Text-Image Generation with Weakness-Targeted Post-Training</div>
<div class="meta-line">Authors: Jiahui Chen, Philippe Hansen-Estruch, Xiaochuang Han, Yushi Hu, Emily Dinan, Amita Kamath, Michal Drozdzal, Reyhane Askari-Hemmat, Luke Zettlemoyer, Marjan Ghazvininejad</div>
<div class="meta-line">First: 2026-01-07T19:19:44+00:00 · Latest: 2026-01-21T05:54:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04339v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04339v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对弱点的后训练统一文本-图像生成</div>
<div class="mono" style="margin-top:8px">统一的多模态生成架构最近作为文本到图像（T2I）合成的有前景方向而出现，这些架构共同生成文本和图像。然而，许多现有系统依赖于显式的模态切换，在手动切换到图像生成之前生成推理文本。这种分离的顺序推理过程限制了跨模态耦合，并禁止自动多模态生成。本研究探索后训练以实现完全统一的文本-图像生成，其中模型在单一推理过程中自主地从文本推理过渡到视觉合成。我们考察了联合文本-图像生成对T2I性能的影响以及后训练期间每种模态的相对重要性。我们还探索了不同的后训练数据策略，显示出针对特定限制的目标数据集相比于广泛的图像-标题语料库或基准对齐数据取得了更优的结果。通过离线、奖励加权的后训练，使用完全自生成的合成数据，我们的方法在四个不同的T2I基准上实现了多模态图像生成的改进，证明了奖励加权两种模态和战略性设计的后训练数据的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance text-to-image synthesis by overcoming the limitations of existing systems that rely on explicit modality switching. The authors propose a method of post-training that allows for fully unified text-image generation, enabling models to transition autonomously from textual reasoning to visual synthesis in a single inference process. The key experimental findings indicate that using a targeted dataset that addresses specific limitations leads to superior performance in multimodal image generation across four diverse benchmarks, highlighting the effectiveness of reward-weighted post-training with self-generated synthetic data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过消除显式模态切换的限制，增强文本到图像合成的统一多模态生成架构。作者提出了一种后训练方法，使模型能够在单次推理过程中自主地从文本推理过渡到图像生成。关键发现表明，联合文本-图像生成显著提高了文本到图像的性能，使用针对特定限制的目标数据集比更广泛的数据集产生更好的结果，该方法通过离线、奖励加权的后训练与自生成的合成数据，在四个不同的文本到图像基准上实现了显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models</div>
<div class="meta-line">Authors: Wenda Zhang, Hongyu Jin, Siyi Wang, Zhiqiang Wei, Ting Dang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T03:32:24+00:00 · Latest: 2026-01-21T03:32:24+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14620v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩放模糊性：利用音频语言模型增强语音情感识别中的人类标注</div>
<div class="mono" style="margin-top:8px">语音情感识别模型通常使用单一类别标签，忽视了人类情感的固有模糊性。模糊情感识别通过将情感表示为概率分布来解决这一问题，但由于从稀疏的人类标注中推断出的不可靠真实分布，进展有限。本文探讨大型音频语言模型（ALMs）是否可以通过生成高质量的合成标注来缓解标注瓶颈。我们引入一个框架，利用ALMs创建合成感知代理，增强人类标注以提高真实分布的可靠性。我们通过统计分析验证这些代理与人类分布的一致性，并通过用增强的情感分布微调ALMs来评估其影响。此外，为了解决类别不平衡并实现无偏评估，我们提出了DiME-Aug，一种分布感知的多模态情感增强策略。在IEMOCAP和MSP-Podcast上的实验表明，合成标注增强了情感分布，特别是在标注一致性高的低模糊区域。然而，对于高度模糊且人类意见分歧较大的情感，收益减少。这项工作提供了ALMs可以解决模糊情感识别中标注稀缺的首个证据，但强调了需要更先进的提示或生成策略来处理高度模糊的案例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Speech Emotion Recognition (SER) by addressing the limitations of single categorical labels and the unreliable ground-truth distributions derived from sparse human annotations. The authors propose a framework that utilizes Large Audio-Language Models (ALMs) to generate high-quality synthetic annotations, referred to as Synthetic Perceptual Proxies, which augment human annotations to enhance the reliability of ground-truth distributions. Experimental results on IEMOCAP and MSP-Podcast demonstrate that these synthetic annotations significantly improve emotion distribution, particularly in low-ambiguity regions with high annotation agreement, although the benefits are less pronounced for highly ambiguous emotions characterized by greater human disagreement.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决单一类别标签的局限性和稀疏人类注释所推导的可靠性不足的真实分布，来改善语音情感识别（SER）。作者提出了一种利用大型音频语言模型（ALMs）生成高质量合成注释的框架，称为合成感知代理，这些代理增强了人类注释并提高了情感分布的可靠性。在IEMOCAP和MSP-Podcast上的实验结果表明，这些合成注释显著改善了情感分布，特别是在注释一致性高的低模糊区域，尽管在高度模糊且人类意见分歧较大的情感中，效果不明显。</div>
</details>
</div>
<div class="card">
<div class="title">E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models</div>
<div class="meta-line">Authors: Jiaheng Dong, Hong Jia, Soumyajit Chatterjee, Abhirup Ghosh, James Bailey, Ting Dang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-08T10:33:37+00:00 · Latest: 2026-01-21T01:27:07+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07078v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07078v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech Foundation Models encounter significant performance degradation when deployed in real-world scenarios involving acoustic domain shifts, such as background noise and speaker accents. Test-time adaptation (TTA) has recently emerged as a viable strategy to address such domain shifts at inference time without requiring access to source data or labels. However, existing TTA approaches, particularly those relying on backpropagation, are memory-intensive, limiting their applicability in speech tasks and resource-constrained settings. Although backpropagation-free methods offer improved efficiency, existing ones exhibit poor accuracy. This is because they are predominantly developed for vision tasks, which fundamentally differ from speech task formulations, noise characteristics, and model architecture, posing unique transferability challenges. In this paper, we introduce E-BATS, the first Efficient BAckpropagation-free TTA framework designed explicitly for speech foundation models. E-BATS achieves a balance between adaptation effectiveness and memory efficiency through three key components: (i) lightweight prompt adaptation for a forward-pass-based feature alignment, (ii) a multi-scale loss to capture both global (utterance-level) and local distribution shifts (token-level) and (iii) a test-time exponential moving average mechanism for stable adaptation across utterances. Experiments conducted on four noisy speech datasets spanning sixteen acoustic conditions demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods. By enabling scalable and robust adaptation under acoustic variability, this work paves the way for developing more efficient adaptation approaches for practical speech processing systems in real-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>E-BATS：针对语音基础模型的高效无反向传播测试时适应</div>
<div class="mono" style="margin-top:8px">语音基础模型在实际场景中遇到显著的性能下降，尤其是在涉及声学领域转变的情况下，如背景噪声和说话者口音。测试时适应（TTA）最近成为解决此类领域转变的可行策略，无需访问源数据或标签。然而，现有的TTA方法，特别是依赖反向传播的方法，内存消耗大，限制了它们在语音任务和资源受限环境中的适用性。尽管无反向传播的方法提供了更高的效率，但现有方法的准确性较差。这是因为它们主要是为视觉任务开发的，而视觉任务与语音任务的表述、噪声特征和模型架构根本不同，带来了独特的可转移性挑战。本文介绍了E-BATS，这是第一个专门为语音基础模型设计的高效无反向传播TTA框架。E-BATS通过三个关键组件实现了适应效果与内存效率之间的平衡：（i）轻量级提示适应用于基于前向传播的特征对齐，（ii）多尺度损失以捕捉全局（话语级）和局部分布转变（标记级），以及（iii）用于跨话语稳定适应的测试时指数移动平均机制。在四个噪声语音数据集上进行的实验显示出一致的改进，相较于无反向传播基线，准确性提高了4.1%-13.5%，与基于反向传播的方法相比，GPU内存节省了2.0-6.4倍。通过在声学变异性下实现可扩展和稳健的适应，这项工作为在实际语音处理系统中开发更高效的适应方法铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation of Speech Foundation Models in real-world scenarios characterized by acoustic domain shifts, such as background noise and speaker accents. The authors propose E-BATS, a backpropagation-free test-time adaptation (TTA) framework specifically designed for speech tasks, which enhances efficiency while maintaining accuracy. Key experimental findings indicate that E-BATS achieves accuracy improvements of 4.1%-13.5% over existing backpropagation-free methods and offers 2.0-6.4 times GPU memory savings compared to backpropagation-based approaches, demonstrating its effectiveness in adapting to acoustic variability in practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决语音基础模型在背景噪声和说话者口音等声学领域转变的真实场景中性能下降的问题。作者提出了E-BATS，这是一种专门为语音任务设计的无反向传播测试时间适应（TTA）框架，旨在提高效率的同时保持准确性。实验结果表明，E-BATS在现有无反向传播方法的基础上实现了4.1%-13.5%的准确性提升，并且与基于反向传播的方法相比，提供了2.0-6.4倍的GPU内存节省，证明了其在实际应用中适应声学变化的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-20T21:26:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：文档的基础视觉语言前端</div>
<div class="mono" style="margin-top:8px">GutenOCR是一系列通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的基础OCR前端。生成的单检查点视觉语言模型通过统一的基于提示的接口展示阅读、检测和定位。模型在商业文档、科学文章和合成定位数据上进行训练，支持全页和局部阅读，具有行和段落级的边界框以及条件“x在哪里？”查询。我们引入了一种基础OCR评估协议，并显示GutenOCR-7B在10.5K保留的商业和科学页面上将其Qwen2.5-VL-7B主干的复合基础OCR得分提高了两倍以上（从0.40提高到0.82）。在Fox和OmniDocBench v1.5上，我们的方法显著改善了区域和行级OCR以及文本检测召回，但在页面级线性化、颜色引导OCR和公式密集布局方面显示出权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GutenOCR is to enhance the capabilities of optical character recognition (OCR) for various document types by integrating vision and language processing. The method involves fine-tuning two versions of the Qwen2.5-VL model to create a unified, prompt-based interface that supports reading and detection at both full-page and localized levels. Experimental results demonstrate that GutenOCR-7B significantly improves the grounded OCR score from 0.40 to 0.82 on a dataset of 10.5K business and scientific pages, while also enhancing region- and line-level OCR performance, although some trade-offs were noted in specific areas like page-level linearization and handling complex layouts.</div>
<div class="mono" style="margin-top:8px">GutenOCR的研究动机在于通过整合视觉和语言处理来增强光学字符识别（OCR）的能力，以改善文档的阅读和理解。该方法涉及对两种版本的Qwen2.5-VL模型进行微调，以创建一个统一的基于提示的接口，支持对商业文档和科学文章的各种阅读任务。主要实验结果表明，GutenOCR-7B显著提高了基础OCR得分，在10.5K页面的数据集上达到了0.82的复合得分，同时也增强了区域和行级OCR性能，尽管在页面级线性化和处理复杂布局方面存在挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing autoregressive forecasts in chaotic systems via multi-rate latent recurrence</div>
<div class="meta-line">Authors: Mrigank Dhingra, Omer San</div>
<div class="meta-line">First: 2026-01-20T21:16:37+00:00 · Latest: 2026-01-20T21:16:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14487v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14487v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon autoregressive forecasting of chaotic dynamical systems remains challenging due to rapid error amplification and distribution shift: small one-step inaccuracies compound into physically inconsistent rollouts and collapse of large-scale statistics. We introduce MSR-HINE, a hierarchical implicit forecaster that augments multiscale latent priors with multi-rate recurrent modules operating at distinct temporal scales. At each step, coarse-to-fine recurrent states generate latent priors, an implicit one-step predictor refines the state with multiscale latent injections, and a gated fusion with posterior latents enforces scale-consistent updates; a lightweight hidden-state correction further aligns recurrent memories with fused latents. The resulting architecture maintains long-term context on slow manifolds while preserving fast-scale variability, mitigating error accumulation in chaotic rollouts. Across two canonical benchmarks, MSR-HINE yields substantial gains over a U-Net autoregressive baseline: on Kuramoto-Sivashinsky it reduces end-horizon RMSE by 62.8% at H=400 and improves end-horizon ACC by +0.983 (from -0.155 to 0.828), extending the ACC &gt;= 0.5 predictability horizon from 241 to 400 steps; on Lorenz-96 it reduces RMSE by 27.0% at H=100 and improves end horizon ACC by +0.402 (from 0.144 to 0.545), extending the ACC &gt;= 0.5 horizon from 58 to 100 steps.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多速率潜在递归稳定混沌系统的自回归预测</div>
<div class="mono" style="margin-top:8px">由于快速的误差放大和分布转移，混沌动力系统的长期自回归预测仍然具有挑战性：小的单步不准确性会累积成物理不一致的结果和大规模统计的崩溃。我们引入了MSR-HINE，一种层次隐式预测器，通过在不同时间尺度上操作的多速率递归模块增强多尺度潜在先验。在每一步中，粗到细的递归状态生成潜在先验，一个隐式单步预测器通过多尺度潜在注入来细化状态，带有后验潜在的门控融合强制执行尺度一致的更新；轻量级的隐藏状态校正进一步将递归记忆与融合潜在对齐。最终的架构在慢流形上保持长期上下文，同时保留快速尺度的变异性，减轻混沌结果中的误差积累。在两个经典基准上，MSR-HINE相较于U-Net自回归基线取得了显著的提升：在Kuramoto-Sivashinsky上，它在H=400时将终端RMSE降低了62.8%，并将终端ACC提高了+0.983（从-0.155到0.828），将ACC &gt;= 0.5的可预测性范围从241步扩展到400步；在Lorenz-96上，它在H=100时将RMSE降低了27.0%，并将终端ACC提高了+0.402（从0.144到0.545），将ACC &gt;= 0.5的范围从58步扩展到100步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of long-horizon autoregressive forecasting in chaotic dynamical systems, where small inaccuracies can lead to significant error amplification and inconsistent predictions. The authors propose a method called MSR-HINE, which is a hierarchical implicit forecaster that utilizes multiscale latent priors and multi-rate recurrent modules to enhance forecasting accuracy. Experimental results demonstrate that MSR-HINE significantly outperforms a U-Net autoregressive baseline, achieving a 62.8% reduction in end-horizon RMSE on the Kuramoto-Sivashinsky benchmark and a 27.0% reduction on the Lorenz-96 benchmark, while also extending the predictability horizon for accurate forecasts.</div>
<div class="mono" style="margin-top:8px">本研究解决了混沌动力系统中长期自回归预测的挑战，其中小误差可能导致时间上的显著不准确。作者提出了MSR-HINE，这是一种层次隐式预测器，结合了多尺度潜在先验和多速率递归模块，以提高预测准确性。实验结果表明，MSR-HINE显著优于U-Net自回归基线，在Kuramoto-Sivashinsky基准上实现了62.8%的终端RMSE减少，并延长了可预测性范围，同时在Lorenz-96基准上减少了27.0%的RMSE，并提高了预测准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text</div>
<div class="meta-line">Authors: Piyush Singh Pasi</div>
<div class="meta-line">First: 2026-01-15T05:56:37+00:00 · Latest: 2026-01-20T20:40:18+00:00</div>
<div class="meta-line">Comments: EACL 2026 Findings accepted. Camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10096v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10096v2">PDF</a> · <a href="https://github.com/piyushsinghpasi/M2M">Code1</a> · <a href="https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k">Code2</a> · <a href="https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely on machine translation, while advances in multilingual text modeling remain underutilized. We introduce M2M, a lightweight alignment method that learns only a few linear layers--using English text alone--to map multilingual text embeddings into multimodal space. Despite its simplicity, M2M matches baseline performance in English (94.9% Recall@10) and achieves strong zero-shot transfer (89.5% Recall@10 averaged across 11 languages, 10 unseen) on XTD Text-to-Image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, M2M demonstrates robustness across datasets and tasks, extending to Audio-Text retrieval and Text-to-Image generation. We release code and checkpoints (https://github.com/piyushsinghpasi/M2M) along with multilingual evaluation datasets: MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多语言到多模态 (M2M)：通过单语文本解锁新语言</div>
<div class="mono" style="margin-top:8px">多模态模型在英语中表现出色，得益于丰富的图像-文本和音频-文本数据，但由于多语言多模态资源有限，其他语言的性能急剧下降。现有解决方案依赖于机器翻译，而多语言文本建模的进展仍未得到充分利用。我们介绍了M2M，这是一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间。尽管其简单性，M2M在英语中匹配基线性能（94.9% Recall@10），并在XTD文本到图像检索中实现强大的零样本迁移（11种语言中平均89.5% Recall@10，10种未见语言）。定性t-SNE可视化显示多语言嵌入与多模态表示紧密对齐，而权重分析揭示变换重塑了嵌入几何，而不是进行微不足道的旋转。除了图像-文本检索，M2M在数据集和任务中表现出鲁棒性，扩展到音频-文本检索和文本到图像生成。我们发布了代码和检查点（https://github.com/piyushsinghpasi/M2M），以及多语言评估数据集：MSCOCO多语言30K（https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k），AudioCaps多语言（https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual），和Clotho多语言（https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of poor performance of multimodal models in languages other than English due to a lack of multilingual multimodal resources. The authors propose M2M, a lightweight alignment method that utilizes English text to create a mapping of multilingual text embeddings into multimodal space using only a few linear layers. Experimental results show that M2M achieves baseline performance in English with a Recall@10 of 94.9% and demonstrates strong zero-shot transfer capabilities with an average Recall@10 of 89.5% across 11 languages, including 10 unseen languages, in XTD Text-to-Image retrieval, while also proving effective in Audio-Text retrieval and Text-to-Image generation tasks.</div>
<div class="mono" style="margin-top:8px">该研究解决了多模态模型在英语以外语言中的性能挑战，这一问题由于缺乏多语言多模态资源而加剧。作者提出了M2M，这是一种轻量级对齐方法，仅利用英语文本通过少量线性层将多语言文本嵌入映射到多模态空间。实验结果表明，M2M在英语中的Recall@10达到了94.9%的基准性能，并在XTD文本到图像检索中展示了强大的零样本迁移能力，在11种语言中平均Recall@10为89.5%，其中包括10种未见语言，同时在音频-文本检索和文本到图像生成等多种数据集和任务中也表现出稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Super-resolution by Adaptive Non-local Observables</div>
<div class="meta-line">Authors: Hsin-Yi Lin, Huan-Hsin Tseng, Samuel Yen-Chi Chen, Shinjae Yoo</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-20T19:40:59+00:00 · Latest: 2026-01-20T19:40:59+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14433v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14433v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) seeks to reconstruct high-resolution (HR) data from low-resolution (LR) observations. Classical deep learning methods have advanced SR substantially, but require increasingly deeper networks, large datasets, and heavy computation to capture fine-grained correlations. In this work, we present the \emph{first study} to investigate quantum circuits for SR. We propose a framework based on Variational Quantum Circuits (VQCs) with \emph{Adaptive Non-Local Observable} (ANO) measurements. Unlike conventional VQCs with fixed Pauli readouts, ANO introduces trainable multi-qubit Hermitian observables, allowing the measurement process to adapt during training. This design leverages the high-dimensional Hilbert space of quantum systems and the representational structure provided by entanglement and superposition. Experiments demonstrate that ANO-VQCs achieve up to five-fold higher resolution with a relatively small model size, suggesting a promising new direction at the intersection of quantum machine learning and super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应非局部可观测量实现量子超分辨率</div>
<div class="mono" style="margin-top:8px">超分辨率（SR）旨在从低分辨率（LR）观测中重建高分辨率（HR）数据。经典深度学习方法在SR方面取得了显著进展，但需要越来越深的网络、大型数据集和大量计算来捕捉细粒度的相关性。在本研究中，我们提出了第一个研究量子电路用于SR的工作。我们提出了一种基于变分量子电路（VQCs）与自适应非局部可观测量（ANO）测量的框架。与具有固定保利读数的传统VQCs不同，ANO引入了可训练的多量子比特厄米可观测量，使测量过程在训练期间能够自适应。这一设计利用了量子系统的高维希尔伯特空间以及纠缠和叠加所提供的表征结构。实验表明，ANO-VQCs在相对较小的模型规模下实现了高达五倍的分辨率，暗示了量子机器学习与超分辨率交叉领域的一个有前景的新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance super-resolution (SR) techniques, which aim to reconstruct high-resolution data from low-resolution observations, while addressing the limitations of classical deep learning methods that require deep networks and extensive datasets. The authors propose a novel framework utilizing Variational Quantum Circuits (VQCs) with Adaptive Non-Local Observable (ANO) measurements, which allows for trainable multi-qubit Hermitian observables that adapt during training. Experimental results indicate that ANO-VQCs can achieve up to five-fold higher resolution compared to traditional methods, highlighting the potential of integrating quantum machine learning with super-resolution techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强超分辨率(SR)技术，该技术旨在从低分辨率观察中重建高分辨率数据，同时克服传统深度学习方法的局限性，这些方法需要深层网络和大量数据集。作者提出了一种新颖的框架，利用变分量子电路(VQCs)和自适应非局部可观测量(ANO)测量，允许在训练过程中适应的可训练多量子比特厄米可观测量。实验结果表明，ANO-VQCs可以实现比传统方法高出五倍的分辨率，展示了将量子电路与超分辨率任务结合的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Meta Flow Maps enable scalable reward alignment</div>
<div class="meta-line">Authors: Peter Potaptchik, Adhi Saravanan, Abbas Mammadov, Alvaro Prat, Michael S. Albergo, Yee Whye Teh</div>
<div class="meta-line">First: 2026-01-20T19:39:56+00:00 · Latest: 2026-01-20T19:39:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>元流图实现可扩展的奖励对齐</div>
<div class="mono" style="margin-top:8px">控制生成模型的计算成本高昂。这是因为与奖励函数的最佳对齐——无论是通过推理时的引导还是微调——都需要估计价值函数。这个任务要求访问条件后验 $p_{1|t}(x_1|x_t)$，即与中间状态 $x_t$ 一致的干净数据 $x_1$ 的分布，这通常迫使方法诉诸于昂贵的轨迹模拟。为了解决这个瓶颈，我们引入了元流图（MFMs），一个将一致性模型和流图扩展到随机领域的框架。MFMs 被训练用于执行随机的一步后验采样，从任何中间状态生成任意数量的独立同分布的干净数据 $x_1$。关键是，这些样本提供了一个可微的重参数化，解锁了高效的价值函数估计。我们利用这一能力解决了两种范式中的瓶颈：在没有内部回合的情况下实现推理时的引导，并促进对一般奖励的无偏离线微调。从经验上看，我们的单粒子引导-MFM 采样器在多个奖励下以较少的计算量超越了 ImageNet 上的最佳1000基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of computational expense in controlling generative models, particularly in aligning them with reward functions through value function estimation. The authors propose a novel framework called Meta Flow Maps (MFMs), which extends consistency models and flow maps to enable stochastic one-step posterior sampling, allowing for the generation of numerous independent samples of clean data from intermediate states without costly trajectory simulations. Experimental results demonstrate that the single-particle steered-MFM sampler significantly outperforms a Best-of-1000 baseline on ImageNet across various rewards while using considerably less computational resources.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决与奖励函数对齐生成模型相关的计算挑战，这通常需要耗费大量计算资源的轨迹模拟来进行最优价值函数估计。作者提出了一种名为Meta Flow Maps (MFMs)的新框架，该框架将一致性模型和流映射扩展到随机领域，允许高效的随机一步后验采样。实验结果表明，单粒子引导的MFM采样器在多个奖励下显著优于ImageNet上的Best-of-1000基线，同时使用的计算资源大幅减少。</div>
</details>
</div>
<div class="card">
<div class="title">APEX-Agents</div>
<div class="meta-line">Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</div>
<div class="meta-line">First: 2026-01-20T18:53:44+00:00 · Latest: 2026-01-20T18:53:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-Agents</div>
<div class="mono" style="margin-top:8px">我们介绍了代理的人工智能生产力指数（APEX-Agents），这是一个评估人工智能代理是否能够执行由投资银行分析师、管理顾问和企业律师创建的长期跨应用任务的基准。APEX-Agents要求代理在具有文件和工具的现实工作环境中导航。我们使用Pass@1测试了八个代理以进行排行榜。Gemini 3 Flash（思维=高）获得最高分24.0%，其次是GPT-5.2（思维=高）、Claude Opus 4.5（思维=高）和Gemini 3 Pro（思维=高）。我们开源了APEX-Agents基准（n=480），包括所有提示、评分标准、金标准输出、文件和元数据。我们还开源了Archipelago，这是我们用于代理执行和评估的基础设施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the APEX-Agents study is to evaluate the capability of AI agents in performing complex, long-term tasks across various professional domains such as investment banking, management consulting, and corporate law. The researchers developed the AI Productivity Index for Agents (APEX-Agents) as a benchmark that requires agents to operate within realistic work environments using files and tools. The experimental results showed that among the eight tested agents, Gemini 3 Flash (Thinking=High) achieved the highest score of 24.0%, followed by GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro, all also rated as Thinking=High, and the benchmark, along with its infrastructure for agent execution and evaluation, has been made open source for further research.</div>
<div class="mono" style="margin-top:8px">APEX-Agents研究的动机是创建一个基准，以评估AI代理在投资银行、管理咨询和企业法律等领域执行复杂的长期任务的能力。研究人员开发了AI生产力指数（APEX-Agents），并在涉及文件和工具的真实工作环境中使用Pass@1指标测试了八种不同的AI代理。结果显示，Gemini 3 Flash（Thinking=High）获得了最高分24.0%，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro，并且该基准及其相关资源已向公众开放。</div>
</details>
</div>
<div class="card">
<div class="title">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</div>
<div class="meta-line">Authors: Rotem Gatenyo, Ohad Fried</div>
<div class="meta-line">First: 2026-01-20T18:12:55+00:00 · Latest: 2026-01-20T18:12:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复制-变换-粘贴：由视觉-语言和几何约束引导的零-shot物体-物体对齐</div>
<div class="mono" style="margin-top:8px">我们研究了给定两个网格的零-shot 3D 对齐，使用描述其空间关系的文本提示——这是内容创作和场景组装的基本能力。早期的方法主要依赖几何对齐程序，而最近的工作利用预训练的2D扩散模型来建模语言条件的物体-物体空间关系。相比之下，我们在测试时直接优化相对姿态，通过可微渲染器使用CLIP驱动的梯度更新平移、旋转和各向同性缩放，而无需训练新模型。我们的框架通过几何感知目标增强语言监督：一种软迭代最近点（ICP）项的变体以鼓励表面附着，以及一个穿透损失以防止相互穿透。分阶段的调度随着时间的推移加强接触约束，摄像机控制将优化集中在交互区域。为了实现评估，我们策划了一个包含多样类别和关系的基准，并与基线进行比较。我们的方法优于所有替代方案，产生语义上真实且物理上合理的对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of zero-shot 3D alignment of two meshes using a text prompt to describe their spatial relationship, which is crucial for content creation and scene assembly. The authors propose a method that optimizes the relative pose at test time by updating translation, rotation, and isotropic scale using CLIP-driven gradients through a differentiable renderer, without the need for training a new model. Experimental results demonstrate that their approach, which incorporates language supervision with geometry-aware objectives, significantly outperforms existing methods, achieving semantically accurate and physically plausible alignments.</div>
<div class="mono" style="margin-top:8px">本研究解决了使用文本提示描述空间关系的网格零-shot 3D 对齐的挑战，这对内容创作和场景组装至关重要。作者提出了一种方法，通过使用 CLIP 驱动的梯度在可微渲染器中更新平移、旋转和缩放，在测试时优化网格的相对姿态，而无需重新训练模型。实验结果表明，他们的方法结合了语言监督和几何感知目标，显著优于现有方法，实现了语义准确和物理合理的对齐。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
