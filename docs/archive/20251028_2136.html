<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-28 21:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251028_2136</div>
    <div class="row"><div class="card">
<div class="title">An Efficient Remote Sensing Super Resolution Method Exploring Diffusion   Priors and Multi-Modal Constraints for Crop Type Mapping</div>
<div class="meta-line">Authors: Songxi Yang, Tang Sui, Qunying Huang</div>
<div class="meta-line">First: 2025-10-27T14:34:52+00:00 · Latest: 2025-10-27T14:34:52+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23382v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super resolution offers a way to harness medium even lowresolution but
historically valuable remote sensing image archives. Generative models,
especially diffusion models, have recently been applied to remote sensing super
resolution (RSSR), yet several challenges exist. First, diffusion models are
effective but require expensive training from scratch resources and have slow
inference speeds. Second, current methods have limited utilization of auxiliary
information as real-world constraints to reconstruct scientifically realistic
images. Finally, most current methods lack evaluation on downstream tasks. In
this study, we present a efficient LSSR framework for RSSR, supported by a new
multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built
on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention
with auxiliary knowledge (Digital Elevation Model, land cover, month) and
Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier
NDVI loss to balance spatial details and spectral fidelity. Extensive
experiments demonstrate that LSSR significantly improves crop boundary
delineation and recovery, achieving state-of-the-art performance with Peak
Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)
and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while
maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers
effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,
yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:
0.85). These results highlight the potential of RSSR to advance precision
agriculture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的遥感超分辨率方法：探索扩散先验和多模态约束用于作物类型映射</div>
<div class="mono" style="margin-top:8px">超分辨率提供了一种利用中等甚至低分辨率但历史上有价值的遥感图像档案的方法。生成模型，特别是扩散模型，最近被应用于遥感超分辨率（RSSR），但仍存在几个挑战。首先，扩散模型有效但需要昂贵的从零开始的训练资源，并且推理速度较慢。其次，当前方法对辅助信息的利用有限，无法作为现实世界约束来重建科学上真实的图像。最后，大多数当前方法缺乏对下游任务的评估。在本研究中，我们提出了一种高效的LSSR框架用于RSSR，支持一个新的多模态数据集，包含配对的30米Landsat 8和10米Sentinel 2影像。基于冻结的预训练稳定扩散，LSSR将跨模态注意力与辅助知识（数字高程模型、土地覆盖、月份）和合成孔径雷达指导相结合，通过适配器和定制的傅里叶NDVI损失来平衡空间细节和光谱保真度。大量实验表明，LSSR显著改善了作物边界的描绘和恢复，达到最先进的性能，峰值信噪比/结构相似性指数分别为32.63/0.84（RGB）和23.99/0.78（IR），NDVI均方误差最低（0.042），同时保持高效推理（0.39秒/图像）。此外，LSSR有效转移到NASA协调的Landsat和Sentinel（HLS）超分辨率，产生比Sentinel-2更可靠的作物分类（F1: 0.86，Sentinel-2: F1: 0.85）。这些结果突显了RSSR在推进精准农业方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the utilization of low-resolution remote sensing images for crop type mapping, addressing challenges associated with existing diffusion models in terms of training costs and inference speed. The authors propose an efficient LSSR framework that leverages a multimodal dataset of paired Landsat 8 and Sentinel 2 imagery, integrating crossmodal attention with auxiliary information such as Digital Elevation Model and land cover. Experimental results indicate that LSSR significantly enhances crop boundary delineation and recovery, achieving state-of-the-art performance metrics, including a Peak Signal-to-Noise Ratio of 32.63 and a Mean Squared Error of 0.042, while ensuring efficient inference times, and demonstrating effective transferability to NASA&#x27;s HLS super resolution for improved crop classification.</div>
<div class="mono" style="margin-top:8px">本研究通过利用生成扩散模型解决遥感超分辨率（RSSR）中的挑战，这些模型通常需要大量训练并且推理速度较慢。作者提出了一种高效的LSSR框架，利用新的配对Landsat 8和Sentinel 2影像的多模态数据集，结合跨模态注意力和辅助知识，如数字高程模型和合成孔径雷达指导。实验结果表明，LSSR显著增强了作物边界的描绘和恢复，达到了最先进的性能指标，包括峰值信噪比32.63和均方误差0.042，同时在NASA的HLS超分辨率中有效转移，改善了作物分类。</div>
</details>
</div>
<div class="card">
<div class="title">Residual Diffusion Bridge Model for Image Restoration</div>
<div class="meta-line">Authors: Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</div>
<div class="meta-line">First: 2025-10-27T08:35:49+00:00 · Latest: 2025-10-27T08:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23116v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23116v1">PDF</a> · <a href="https://github.com/MiliLab/RDBM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion bridge models establish probabilistic paths between arbitrary
paired distributions and exhibit great potential for universal image
restoration. Most existing methods merely treat them as simple variants of
stochastic interpolants, lacking a unified analytical perspective. Besides,
they indiscriminately reconstruct images through global noise injection and
removal, inevitably distorting undegraded regions due to imperfect
reconstruction. To address these challenges, we propose the Residual Diffusion
Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic
differential equations of generalized diffusion bridge and derive the
analytical formulas of its forward and reverse processes. Crucially, we
leverage the residuals from given distributions to modulate the noise injection
and removal, enabling adaptive restoration of degraded regions while preserving
intact others. Moreover, we unravel the fundamental mathematical essence of
existing bridge models, all of which are special cases of RDBM and empirically
demonstrate the optimality of our proposed models. Extensive experiments are
conducted to demonstrate the state-of-the-art performance of our method both
qualitatively and quantitatively across diverse image restoration tasks. Code
is publicly available at https://github.com/MiliLab/RDBM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像恢复的残差扩散桥模型</div>
<div class="mono" style="margin-top:8px">扩散桥模型在任意配对分布之间建立概率路径，展现出在通用图像恢复中的巨大潜力。现有大多数方法仅将其视为随机插值的简单变体，缺乏统一的分析视角。此外，它们通过全局噪声注入和去除不加区分地重建图像，因不完美的重建不可避免地扭曲未退化区域。为了解决这些挑战，我们提出了残差扩散桥模型（RDBM）。具体而言，我们从理论上重新表述了广义扩散桥的随机微分方程，并推导出其正向和反向过程的解析公式。关键是，我们利用给定分布的残差来调节噪声的注入和去除，实现对退化区域的自适应恢复，同时保留完整的其他区域。此外，我们揭示了现有桥模型的基本数学本质，所有这些模型都是RDBM的特例，并实证证明了我们提出模型的最优性。进行了大量实验，以定性和定量的方式展示我们方法在各种图像恢复任务中的最先进性能。代码可在https://github.com/MiliLab/RDBM公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image restoration techniques that often distort undegraded regions due to inadequate noise handling. The authors introduce the Residual Diffusion Bridge Model (RDBM), which reformulates the stochastic differential equations of generalized diffusion bridges and derives analytical formulas for its processes. Key experimental findings indicate that RDBM effectively modulates noise injection and removal using residuals from given distributions, leading to adaptive restoration that preserves intact areas while achieving state-of-the-art performance across various image restoration tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善图像恢复技术，这些技术往往由于全局噪声注入和去除而扭曲未退化区域。作者提出了残差扩散桥模型（RDBM），该模型重新公式化了广义扩散桥的随机微分方程，并推导出其过程的解析公式。实验结果表明，RDBM能够自适应地恢复退化区域，同时保留完整区域，广泛测试证实其在各种图像恢复任务中的优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</div>
<div class="meta-line">Authors: Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-23T17:59:54+00:00 · Latest: 2025-10-26T09:13:56+00:00</div>
<div class="meta-line">Comments: Accepted as a poster at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20819v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.20819v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/lddbm/home">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对比和预测的潜在扩散桥的通用模态翻译</div>
<div class="mono" style="margin-top:8px">最近的生成建模进展使扩散模型成为从复杂数据分布中采样的最先进工具。尽管这些模型在图像和音频等单一模态领域取得了显著成功，但将其能力扩展到模态翻译（MT），即在不同感官模态之间翻译信息，仍然是一个未解决的挑战。现有方法通常依赖于限制性假设，包括共享维度、高斯源先验和特定模态的架构，这限制了它们的通用性和理论基础。在本研究中，我们提出了潜在去噪扩散桥模型（LDDBM），这是一个基于去噪扩散桥模型的潜变量扩展的通用模态翻译框架。通过在共享潜在空间中操作，我们的方法学习了任意模态之间的桥梁，而无需对齐维度。我们引入了一种对比对齐损失，以强制配对样本之间的语义一致性，并设计了一种针对潜在空间噪声预测的领域无关编码器-解码器架构。此外，我们提出了一种预测损失，以指导训练朝着准确的跨域翻译，并探索了几种训练策略以提高稳定性。我们的方法支持任意模态对，并在多视图到3D形状生成、图像超分辨率和多视图场景合成等多种MT任务中表现出色。全面的实验和消融验证了我们框架的有效性，确立了通用模态翻译的新强基线。有关更多信息，请访问我们的项目页面： https://sites.google.com/view/lddbm/home。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing approaches in Modality Translation (MT), which often rely on restrictive assumptions that hinder their general applicability. The authors propose the Latent Denoising Diffusion Bridge Model (LDDBM), a framework that operates in a shared latent space to facilitate translation between arbitrary modalities without requiring aligned dimensions. Key experimental findings demonstrate that LDDBM effectively supports various MT tasks, such as multi-view to 3D shape generation and image super-resolution, and establishes a new strong baseline for general modality translation through comprehensive experiments and ablation studies.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有模态翻译（MT）方法的局限性，这些方法通常依赖于限制性假设，阻碍了其广泛适用性。作者提出了潜在去噪扩散桥模型（LDDBM），这是一种新颖的框架，通过在共享潜在空间中操作，促进任意模态之间的翻译，而无需对齐维度。实验结果表明，LDDBM在多视图到3D形状生成和图像超分辨率等多种MT任务中表现出色，验证了其有效性，并在该领域建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution   with Spatially Re-Focused Text-Conditioning</div>
<div class="meta-line">Authors: Chen Chen, Majid Abdolshah, Violetta Shevchenko, Hongdong Li, Chang Xu, Pulak Purkait</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-26T05:03:55+00:00 · Latest: 2025-10-26T05:03:55+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22534v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22534v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing diffusion-based super-resolution approaches often exhibit semantic
ambiguities due to inaccuracies and incompleteness in their text conditioning,
coupled with the inherent tendency for cross-attention to divert towards
irrelevant pixels. These limitations can lead to semantic misalignment and
hallucinated details in the generated high-resolution outputs. To address
these, we propose a novel, plug-and-play spatially re-focused super-resolution
(SRSR) framework that consists of two core components: first, we introduce
Spatially Re-focused Cross-Attention (SRCA), which refines text conditioning at
inference time by applying visually-grounded segmentation masks to guide
cross-attention. Second, we introduce a Spatially Targeted Classifier-Free
Guidance (STCFG) mechanism that selectively bypasses text influences on
ungrounded pixels to prevent hallucinations. Extensive experiments on both
synthetic and real-world datasets demonstrate that SRSR consistently
outperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR
and SSIM) across all datasets, and in perceptual quality measures (LPIPS and
DISTS) on two real-world benchmarks, underscoring its effectiveness in
achieving both high semantic fidelity and perceptual quality in
super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRSR：通过空间重新聚焦的文本条件增强真实世界图像超分辨率的语义准确性</div>
<div class="mono" style="margin-top:8px">现有的基于扩散的超分辨率方法由于文本条件的不准确和不完整，常常表现出语义模糊，加上交叉注意力固有的倾向于偏向无关像素。这些限制可能导致生成的高分辨率输出中的语义不对齐和幻觉细节。为了解决这些问题，我们提出了一种新颖的即插即用的空间重新聚焦超分辨率（SRSR）框架，包含两个核心组件：首先，我们引入了空间重新聚焦交叉注意力（SRCA），通过应用视觉基础的分割掩码在推理时精炼文本条件，以引导交叉注意力。其次，我们引入了一种空间定向无分类器引导（STCFG）机制，选择性地绕过对无基础像素的文本影响，以防止幻觉。在合成和真实世界数据集上的广泛实验表明，SRSR在所有数据集的标准保真度指标（PSNR和SSIM）上始终优于七个最先进的基线，并在两个真实世界基准上的感知质量指标（LPIPS和DISTS）中表现出色，强调了其在超分辨率中实现高语义保真度和感知质量的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the semantic ambiguities and inaccuracies in existing diffusion-based super-resolution methods, which often lead to misaligned semantics and hallucinated details in high-resolution images. The authors propose a novel framework called Spatially Re-focused Super-Resolution (SRSR), which includes two main components: Spatially Re-focused Cross-Attention (SRCA) that refines text conditioning using segmentation masks, and Spatially Targeted Classifier-Free Guidance (STCFG) that mitigates text influence on ungrounded pixels. Experimental results show that SRSR outperforms seven state-of-the-art methods in standard fidelity metrics and perceptual quality measures across various datasets, demonstrating its effectiveness in enhancing both semantic fidelity and perceptual quality in super-resolution tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服现有扩散基础超分辨率方法中的语义模糊和不准确性，这些问题常常导致高分辨率图像中的细节不对齐。作者提出了一种名为空间重聚焦超分辨率（SRSR）的新框架，该框架包含两个主要组件：空间重聚焦交叉注意力（SRCA），通过使用分割掩码来精炼文本条件，以及空间定向无分类器引导（STCFG），以最小化文本对无关像素的影响。实验结果表明，SRSR在各种数据集上显著优于七种最先进的方法，无论是在保真度指标还是感知质量方面，展示了其在超分辨率任务中提高语义准确性的能力。</div>
</details>
</div>
<div class="card">
<div class="title">RestoreVAR: Visual Autoregressive Generation for All-in-One Image   Restoration</div>
<div class="meta-line">Authors: Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel</div>
<div class="meta-line">First: 2025-05-23T15:52:26+00:00 · Latest: 2025-10-25T22:06:16+00:00</div>
<div class="meta-line">Comments: Project page: https://sudraj2002.github.io/restorevarpage/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18047v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18047v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sudraj2002.github.io/restorevarpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The use of latent diffusion models (LDMs) such as Stable Diffusion has
significantly improved the perceptual quality of All-in-One image Restoration
(AiOR) methods, while also enhancing their generalization capabilities.
However, these LDM-based frameworks suffer from slow inference due to their
iterative denoising process, rendering them impractical for time-sensitive
applications. Visual autoregressive modeling (VAR), a recently introduced
approach for image generation, performs scale-space autoregression and achieves
comparable performance to that of state-of-the-art diffusion transformers with
drastically reduced computational costs. Moreover, our analysis reveals that
coarse scales in VAR primarily capture degradations while finer scales encode
scene detail, simplifying the restoration process. Motivated by this, we
propose RestoreVAR, a novel VAR-based generative approach for AiOR that
significantly outperforms LDM-based models in restoration performance while
achieving over $10\times$ faster inference. To optimally exploit the advantages
of VAR for AiOR, we propose architectural modifications and improvements,
including intricately designed cross-attention mechanisms and a latent-space
refinement module, tailored for the AiOR task. Extensive experiments show that
RestoreVAR achieves state-of-the-art performance among generative AiOR methods,
while also exhibiting strong generalization capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RestoreVAR：全能图像修复的视觉自回归生成</div>
<div class="mono" style="margin-top:8px">使用潜在扩散模型（LDM）如稳定扩散显著提高了全能图像修复（AiOR）方法的感知质量，同时增强了其泛化能力。然而，这些基于LDM的框架由于其迭代去噪过程而导致推理速度缓慢，使其在时间敏感的应用中不切实际。视觉自回归建模（VAR）是一种最近提出的图像生成方法，执行尺度空间自回归，并在计算成本大幅降低的情况下实现了与最先进的扩散变换器相当的性能。此外，我们的分析表明，VAR中的粗尺度主要捕捉退化，而细尺度则编码场景细节，从而简化了修复过程。基于此，我们提出了RestoreVAR，一种新颖的基于VAR的生成方法，显著优于基于LDM的模型在修复性能上，同时实现了超过$10\times$的推理加速。为了最佳利用VAR在AiOR中的优势，我们提出了架构修改和改进，包括精心设计的交叉注意机制和针对AiOR任务的潜在空间精炼模块。大量实验表明，RestoreVAR在生成AiOR方法中实现了最先进的性能，同时展现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the slow inference times of latent diffusion models (LDMs) used in All-in-One image Restoration (AiOR), which limits their practicality for time-sensitive applications. The authors introduce RestoreVAR, a novel visual autoregressive modeling (VAR) approach that performs scale-space autoregression, achieving comparable restoration performance to state-of-the-art diffusion transformers but with over 10 times faster inference. Experimental results demonstrate that RestoreVAR not only outperforms LDM-based models in restoration quality but also shows strong generalization capabilities, aided by architectural enhancements such as cross-attention mechanisms and a latent-space refinement module tailored for AiOR tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决潜在扩散模型（LDM）在一体化图像修复（AiOR）方法中存在的慢推理时间问题，这限制了其在时间敏感应用中的实用性。作者提出了RestoreVAR，这是一种新颖的视觉自回归建模（VAR）方法，利用尺度空间自回归来提高修复性能，同时显著降低计算成本。实验结果表明，RestoreVAR在修复质量上优于基于LDM的模型，并且推理速度提高了超过10倍，展示了其在生成AiOR任务中的有效性和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Kernel Density Steering: Inference-Time Scaling via Mode Seeking for   Image Restoration</div>
<div class="meta-line">Authors: Yuyang Hu, Kangfu Mei, Mojtaba Sahraee-Ardakan, Ulugbek S. Kamilov, Peyman Milanfar, Mauricio Delbracio</div>
<div class="meta-line">First: 2025-07-08T02:33:44+00:00 · Latest: 2025-10-25T16:42:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.05604v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.05604v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as &quot;collective wisdom&quot;, steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>核密度引导：通过模式寻求进行图像恢复的推理时间缩放</div>
<div class="mono" style="margin-top:8px">扩散模型在图像恢复中显示出潜力，但现有方法往往在一致性保真度和不良伪影方面存在困难。为了解决这个问题，我们引入了核密度引导（KDS），这是一种新颖的推理时间框架，通过显式的局部模式寻求促进稳健的高保真输出。KDS采用$N$粒子扩散样本集，从它们的集体输出中计算补丁级核密度估计梯度。这些梯度引导每个粒子中的补丁朝向集体中识别的共享高密度区域。这个集体局部模式寻求机制，作为“集体智慧”，将样本引导远离易受伪影影响的虚假模式，这些模式源于独立采样或模型缺陷，并朝向更稳健的高保真结构。这使我们能够在同时采样多个粒子的情况下，以更高的计算成本获得更高质量的样本。作为一个即插即用的框架，KDS无需重新训练或外部验证器，能够与各种扩散采样器无缝集成。大量数值验证表明，KDS在具有挑战性的真实世界超分辨率和图像修复任务中显著提高了定量和定性性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the fidelity and reduce artifacts in image restoration using diffusion models, which often face challenges in these areas. The authors propose a novel framework called Kernel Density Steering (KDS), which utilizes an ensemble of diffusion samples to compute patch-wise kernel density estimation gradients, guiding the samples towards higher-density regions. Experimental results indicate that KDS significantly improves both the quantitative and qualitative performance in complex tasks such as super-resolution and image inpainting, demonstrating its effectiveness as a plug-and-play solution without the need for retraining or external verification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高图像恢复中扩散模型的保真度并减少伪影，尽管扩散模型显示出潜力，但通常会产生不一致的结果。作者提出了一种名为核密度引导（KDS）的新框架，该框架利用扩散样本的集成计算补丁级核密度估计梯度，指导样本朝向更高密度区域。实验结果表明，KDS在超分辨率和图像修复等具有挑战性的任务中显著提高了定量和定性性能，证明了其作为即插即用解决方案的有效性，无需重新训练或外部验证。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
