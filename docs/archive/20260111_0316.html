<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-11 03:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260111_0316</div>
    <div class="row"><div class="card">
<div class="title">Pixel-Perfect Visual Geometry Estimation</div>
<div class="meta-line">Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</div>
<div class="meta-line">First: 2026-01-08T18:59:49+00:00 · Latest: 2026-01-08T18:59:49+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/gangweix/pixel-perfect-depth</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05246v1">PDF</a> · <a href="https://github.com/gangweix/pixel-perfect-depth">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素完美视觉几何估计</div>
<div class="mono" style="margin-top:8px">从图像中恢复干净且准确的几何形状对机器人技术和增强现实至关重要。然而，现有的几何基础模型仍然严重受到飞行像素和细节丢失的影响。本文提出了像素完美视觉几何模型，利用像素空间中的生成建模预测高质量、无飞行像素的点云。我们首先介绍了像素完美深度（PPD），这是一个基于像素空间扩散变换器（DiT）的单目深度基础模型。为了解决与像素空间扩散相关的高计算复杂性，我们提出了两个关键设计：1）语义提示的DiT，它结合了来自视觉基础模型的语义表示以提示扩散过程，保留全局语义的同时增强细粒度视觉细节；2）级联DiT架构，逐步增加图像标记的数量，提高效率和准确性。为了进一步将PPD扩展到视频（PPVD），我们引入了一种新的语义一致DiT，它从多视角几何基础模型中提取时间一致的语义。然后，我们在DiT中执行参考引导的标记传播，以在最小的计算和内存开销下保持时间一致性。我们的模型在所有生成的单目和视频深度估计模型中表现最佳，并产生比其他所有模型显著更干净的点云。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy and cleanliness of geometry recovery from images, which is crucial for applications in robotics and augmented reality. The authors propose pixel-perfect visual geometry models, specifically the Pixel-Perfect Depth (PPD) model, which utilizes pixel-space diffusion transformers to generate high-quality point clouds free from flying pixels. Key innovations include the Semantics-Prompted DiT for enhancing fine details while maintaining global semantics, and a Cascade DiT architecture that boosts efficiency and accuracy by progressively increasing image tokens. Additionally, the extension to video through the Semantics-Consistent DiT allows for temporally consistent semantics extraction, resulting in superior performance in depth estimation and cleaner point clouds compared to existing models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像几何恢复的准确性和清晰度，这对机器人技术和增强现实应用至关重要。作者提出了一种新方法，称为像素完美深度（PPD），利用像素空间扩散变换器生成高质量、无飞行像素的点云。关键创新包括语义提示的DiT，用于增强视觉细节同时保留全局语义，以及级联DiT架构以优化计算效率。此外，通过语义一致的DiT扩展到视频，实现了时间一致的深度估计，结果显示其性能优于现有模型，并生成显著更清晰的点云。</div>
</details>
</div>
<div class="card">
<div class="title">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</div>
<div class="meta-line">Authors: Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-01-08T18:59:22+00:00 · Latest: 2026-01-08T18:59:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboVIP：通过视觉身份提示增强机器人操作的多视角视频生成</div>
<div class="mono" style="margin-top:8px">操作数据的多样性、数量和质量对训练有效的机器人策略至关重要。然而，由于硬件和物理设置的限制，收集大规模的真实世界操作数据在不同环境中仍然难以扩展。最近的研究使用文本提示条件的图像扩散模型，通过改变视觉观察中的背景和桌面物体来增强操作数据。然而，这些方法往往忽视了最先进策略模型所需的多视角和时间一致观察的实际需求。此外，仅靠文本提示无法可靠地指定场景设置。为了为扩散模型提供明确的视觉指导，我们引入了视觉身份提示，提供示例图像作为条件输入，以指导所需场景设置的生成。为此，我们还建立了一个可扩展的管道，从大型机器人数据集中策划视觉身份池。使用我们增强的操作数据训练下游视觉-语言-动作和视觉运动策略模型，在模拟和真实机器人环境中均获得了一致的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of collecting large-scale manipulation data for training effective robot policies, which is hindered by hardware and environmental constraints. The authors propose a method called visual identity prompting that enhances existing text-prompt conditioned image diffusion models by providing exemplar images to guide scene generation, ensuring multi-view and temporally coherent observations. Experimental results demonstrate that using the augmented manipulation data to train vision-language-action and visuomotor policy models leads to consistent performance improvements in both simulated and real robotic environments.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于硬件和环境限制而导致的收集大规模操作数据的挑战，这对训练有效的机器人策略至关重要。作者提出了一种名为视觉身份提示的方法，通过结合示例图像为生成多视角和时间一致的观察提供明确的视觉指导，从而增强现有的文本提示条件图像扩散模型。实验结果表明，使用增强的操作数据显著提高了视觉语言-动作和视觉运动策略模型在模拟和真实机器人环境中的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</div>
<div class="meta-line">Authors: William Rudman, Michal Golovanevsky, Dana Arad, Yonatan Belinkov, Ritambhara Singh, Carsten Eickhoff, Kyle Mahowald</div>
<div class="meta-line">First: 2026-01-08T18:23:03+00:00 · Latest: 2026-01-08T18:23:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中提示引发幻觉的机制</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）能力强大，但常常偏向文本提示而忽视视觉证据，从而产生幻觉。我们在一个受控的物体计数环境中研究这种失效模式，其中提示夸大了图像中的物体数量（例如，要求模型描述四朵睡莲，而实际上只有三朵）。在物体数量较少时，模型通常会纠正这种过高估计，但随着物体数量的增加，它们越来越倾向于遵循提示，而不考虑差异。通过对三种VLM的机制分析，我们识别出一小组注意力头，其消融显著减少了提示引发的幻觉（PIH），减少幅度至少为40%，且无需额外训练。在不同模型中，PIH头以特定于模型的方式介导提示复制。我们描述了这些差异，并表明PIH消融增加了对视觉证据的纠正。我们的发现为提示引发幻觉的内部机制提供了见解，揭示了这些行为实现中的模型特异性差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand the phenomenon of prompt-induced hallucination in large vision-language models (VLMs), where models tend to favor textual prompts over visual evidence. The study employs a controlled object-counting setting to analyze how VLMs respond to prompts that exaggerate the number of objects in images. The key experimental findings reveal that while models can correct overestimations at low object counts, they increasingly conform to prompts as object counts rise, with specific attention heads identified that, when ablated, reduce prompt-induced hallucinations by at least 40% without additional training, thus enhancing alignment with visual evidence.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLMs）中提示引发的幻觉现象，即模型倾向于优先考虑文本提示而非视觉证据，特别是在物体计数任务中。研究采用受控实验设置分析VLMs如何响应夸大图像中物体数量的提示。关键发现表明，尽管模型在低物体计数时能够纠正过高估计，但在物体计数增加时，它们越来越倾向于遵循提示，特定的注意力头被确定为调节这种行为的关键；去除这些头部可以在不进一步训练的情况下将幻觉减少至少40%，并增强与视觉证据的一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving and Evaluating Open Deep Research Agents</div>
<div class="meta-line">Authors: Doaa Allabadi, Kyle Bradbury, Jordan M. Malof</div>
<div class="meta-line">First: 2025-08-13T19:32:01+00:00 · Latest: 2026-01-08T17:54:58+00:00</div>
<div class="meta-line">Comments: 8 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10152v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10152v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We focus here on Deep Research Agents (DRAs), which are systems that can take a natural language prompt from a user, and then autonomously search for, and utilize, internet-based content to address the prompt. Recent DRAs have demonstrated impressive capabilities on public benchmarks however, recent research largely involves proprietary closed-source systems. At the time of this work, we only found one open-source DRA, termed Open Deep Research (ODR). In this work we adapt the challenging recent BrowseComp benchmark to compare ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small), comprising a subset of BrowseComp, as a more computationally-tractable DRA benchmark for academic labs. We benchmark ODR and two other proprietary systems on BC-Small: one system from Anthropic and one system from Google. We find that all three systems achieve 0% accuracy on the test set of 60 questions. We introduce three strategic improvements to ODR, resulting in the ODR+ model, which achieves a state-of-the-art 10% success rate on BC-Small among both closed-source and open-source systems. We report ablation studies indicating that all three of our improvements contributed to the success of ODR+.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进与评估开放深度研究代理</div>
<div class="mono" style="margin-top:8px">我们在这里关注深度研究代理（DRA），这是一种能够接受用户自然语言提示并自主搜索和利用基于互联网的内容来解决提示的系统。最近的DRA在公共基准测试中展示了令人印象深刻的能力，但最近的研究主要涉及专有的闭源系统。在本研究时，我们只找到一个开源DRA，称为开放深度研究（ODR）。在这项工作中，我们调整了具有挑战性的最近BrowseComp基准，以比较ODR与现有的专有系统。我们提出了BrowseComp-Small（BC-Small），它是BrowseComp的一个子集，作为一个更具计算可行性的DRA基准，供学术实验室使用。我们在BC-Small上对ODR和另外两个专有系统进行了基准测试：一个来自Anthropic，另一个来自Google。我们发现这三个系统在60个问题的测试集上均达到0%的准确率。我们为ODR引入了三项战略改进，形成了ODR+模型，该模型在BC-Small上在闭源和开源系统中实现了10%的最新成功率。我们报告了消融研究，表明我们的三项改进均对ODR+的成功做出了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of open-source Deep Research Agents (DRAs), which autonomously search for and utilize internet content based on user prompts, as existing benchmarks primarily focus on proprietary systems. The authors adapted the BrowseComp benchmark to create a more manageable version called BrowseComp-Small (BC-Small) for academic evaluation, and they compared the performance of the Open Deep Research (ODR) system with two proprietary systems from Anthropic and Google. The results showed that all three systems initially achieved 0% accuracy on a test set of 60 questions, but after implementing three strategic improvements to ODR, the enhanced model ODR+ achieved a state-of-the-art success rate of 10% on BC-Small, with ablation studies confirming the contributions of each improvement to this performance increase.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升开源深度研究代理（DRA）的能力，这些代理能够根据用户提示自主搜索和利用互联网内容，而与占主导地位的专有系统形成对比。作者调整了BrowseComp基准，以评估Open Deep Research（ODR）系统与Anthropic和Google的专有系统的性能，并提出了一个更易于管理的子集BrowseComp-Small（BC-Small）供学术使用。实验结果显示，所有三个系统，包括ODR，在60个问题的测试集上最初均达到了0%的准确率；然而，在实施三项战略改进后，ODR+模型达到了10%的成功率，标志着在开源和闭源系统中性能的显著提升，消融研究确认了每项改进对这一成功的贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</div>
<div class="meta-line">Authors: Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai</div>
<div class="meta-line">First: 2026-01-08T17:13:00+00:00 · Latest: 2026-01-08T17:13:00+00:00</div>
<div class="meta-line">Comments: 13 pages, 9 figures, project page: https://github.com/hrz2000/realign</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05124v1">PDF</a> · <a href="https://github.com/hrz2000/realign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model&#x27;s overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Re-Align：基于结构化推理的上下文图像生成与编辑对齐</div>
<div class="mono" style="margin-top:8px">上下文图像生成与编辑（ICGE）使用户能够通过交错的图像-文本提示指定视觉概念，要求对用户意图进行精确理解和忠实执行。尽管最近的统一多模态模型展现出良好的理解能力，但这些优势往往无法有效转移到图像生成上。我们提出了Re-Align，一个通过结构化推理引导的对齐统一框架，弥合理解与生成之间的差距。其核心是上下文思维链（IC-CoT），一种结构化推理范式，解耦语义指导和参考关联，提供清晰的文本目标并减轻参考图像之间的混淆。此外，Re-Align引入了一种有效的强化学习训练方案，利用替代奖励来衡量结构化推理文本与生成图像之间的对齐，从而提高模型在ICGE任务上的整体表现。大量实验验证了Re-Align在上下文图像生成和编辑任务上超越了可比模型规模和资源的竞争方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance in-context image generation and editing (ICGE) by improving the understanding and execution of user intent through visual concepts specified in image-text prompts. The authors propose Re-Align, a unified framework that employs a structured reasoning-guided alignment method, specifically utilizing the In-Context Chain-of-Thought (IC-CoT) to separate semantic guidance from reference association, which clarifies textual targets and reduces confusion with reference images. Experimental results demonstrate that Re-Align significantly outperforms existing methods of similar scale and resources in both in-context image generation and editing tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改善用户意图的理解和执行，增强上下文图像生成和编辑（ICGE），用户通过图像-文本提示指定视觉概念。作者提出了Re-Align，这是一个统一框架，利用结构化推理引导的对齐，特别是通过上下文思维链（IC-CoT）方法，将语义指导与参考关联分离，以减少图像生成中的混淆。实验结果表明，Re-Align在图像生成和编辑任务中显著优于现有的相似规模和资源的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models</div>
<div class="meta-line">Authors: Arghyadeep Das, Sai Sreenivas Chintha, Rishiraj Girmal, Kinjal Pandey, Sharvi Endait</div>
<div class="meta-line">First: 2026-01-08T16:19:43+00:00 · Latest: 2026-01-08T16:19:43+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05076v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清洗思维链：堵塞大型推理模型中的个人身份信息泄露</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过生成明确的思维链（CoT）推理来提高性能、可靠性和可解释性，但这种透明性带来了严重的隐私风险：即使最终答案经过清洗，中间推理也常常泄露个人身份信息（PII）。我们研究如何引导隐私优先的推理，使模型在不暴露敏感信息的情况下进行推理，采用可部署的干预措施而非事后修订。我们引入了PII-CoT-Bench，这是一个带有隐私意识的CoT注释的监督数据集，以及一个涵盖现实和对抗性泄露场景的类别平衡评估基准。我们的结果揭示了一种能力依赖的趋势：最先进的模型最能从基于提示的控制中受益，而较弱的模型则需要微调以实现有意义的泄露减少。在模型和类别之间，这两种方法都显著减少了PII暴露，且对效用的影响最小，证明了可以在不牺牲性能的情况下实现私密推理。总体而言，我们表明可以在最小效用损失的情况下实现私密CoT推理，为构建隐私保护的推理系统提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the privacy risks associated with Large Reasoning Models (LRMs), which can inadvertently leak personally identifiable information (PII) during their chain-of-thought (CoT) reasoning processes. The authors propose a method for inducing privacy-first reasoning through deployable interventions, rather than relying on post-hoc redaction, and introduce the PII-CoT-Bench dataset with privacy-aware CoT annotations. Experimental results indicate that state-of-the-art models benefit significantly from prompt-based controls, while weaker models require fine-tuning for effective leakage reduction; both approaches effectively minimize PII exposure with little impact on overall utility, demonstrating the feasibility of private reasoning without compromising performance.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型推理模型（LRMs）在生成思维链（CoT）推理时可能泄露个人可识别信息（PII）的隐私风险。作者提出了一种通过可部署干预措施诱导隐私优先推理的方法，而不是依赖事后修正。他们的实验结果使用新引入的PII-CoT-Bench数据集，表明尽管最先进的模型受益于基于提示的控制，但较弱的模型需要微调才能有效减少PII泄露。这两种方法显著降低了PII暴露，且对模型性能的影响最小，表明在不牺牲效用的情况下实现隐私保护推理是可行的。</div>
</details>
</div>
<div class="card">
<div class="title">From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</div>
<div class="meta-line">Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar</div>
<div class="meta-line">First: 2026-01-08T16:02:56+00:00 · Latest: 2026-01-08T16:02:56+00:00</div>
<div class="meta-line">Comments: Contributed original research to top tier conference in VLM; currently undergoing peer review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05059v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05059v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut &amp; Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从理解到参与：通过视觉语言模型（VLMs）个性化药学视频剪辑</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）有望通过实现智能、可扩展和自动化的多模态内容处理，彻底改变制药行业的数字化转型。传统的异构数据模态（文本、图像、视频、音频和网页链接）的手动标注容易出现不一致、质量下降和内容利用效率低下的问题。长视频和音频数据的庞大体量进一步加剧了这些挑战（例如，长时间的临床试验访谈和教育研讨会）。在此，我们介绍了一种领域适应的视频到视频剪辑生成框架，该框架集成了音频语言模型（ALMs）和视觉语言模型（VLMs）以生成精彩剪辑。我们的贡献有三方面：（i）可重复的剪切与合并算法，具有淡入/淡出和时间戳归一化，确保平滑过渡和音视频对齐；（ii）基于角色定义和提示注入的个性化机制，以实现定制输出（市场营销、培训、合规）；（iii）成本高效的端到端管道策略，平衡ALM/VLM增强处理。在视频MME基准（900）和我们在14个疾病领域的16,159个药学视频的专有数据集上的评估表明，速度提升3到4倍，成本降低4倍，剪辑质量具有竞争力。除了效率提升，我们还报告了我们的方法在剪辑连贯性评分（0.348）和信息量评分（0.721）上优于最先进的VLM基线（例如，Gemini 2.5 Pro），突显了透明、定制提取和支持合规的视频摘要在生命科学中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies and inconsistencies in traditional manual annotation of diverse data modalities in the pharmaceutical industry, particularly in processing long video and audio content. The authors propose a Video to Video Clip Generation framework that combines Audio Language Models and Vision Language Models to automate the creation of highlight clips. Experimental results on a benchmark of 900 videos and a proprietary dataset of 16,159 pharmacy videos show a 3 to 4 times increase in processing speed, a 4 times reduction in costs, and improved clip coherence and informativeness scores compared to existing state-of-the-art models, indicating significant advancements in video summarization for life sciences.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决制药行业中传统手动注释多种数据模态的低效和不一致性，特别是在处理长视频和音频内容时。作者提出了一种视频到视频剪辑生成框架，结合了音频语言模型和视觉语言模型来创建高亮剪辑，具有可重复的剪切与合并算法以实现平滑过渡、个性化机制以提供定制输出，以及成本高效的端到端管道。对900个视频的基准测试和一个包含16,159个制药视频的专有数据集的实验结果显示，处理速度提高了3到4倍，成本降低了4倍，并且剪辑的连贯性和信息量评分相比现有的视觉语言模型基线有显著改善，表明在生命科学领域的视频摘要方面取得了重要进展。</div>
</details>
</div>
<div class="card">
<div class="title">Guiding diffusion models to reconstruct flow fields from sparse data</div>
<div class="meta-line">Authors: Marc Amorós-Trepat, Luis Medrano-Navarro, Qiang Liu, Luca Guastoni, Nils Thuerey</div>
<div class="meta-line">Venue: Physics of Fluids 1 January 2026; 38 (1): 015112</div>
<div class="meta-line">First: 2025-10-22T19:01:50+00:00 · Latest: 2026-01-08T15:36:06+00:00</div>
<div class="meta-line">Comments: Published on Physics of Fluids, code and data can be found at https://github.com/tum-pbs/sparse-reconstruction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19971v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19971v2">PDF</a> · <a href="https://github.com/tum-pbs/sparse-reconstruction">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reconstruction of unsteady flow fields from limited measurements is a challenging and crucial task for many engineering applications. Machine learning models are gaining popularity for solving this problem due to their ability to learn complex patterns from data and to generalize across diverse conditions. Among these, diffusion models have emerged as being particularly powerful for generative tasks, producing high-quality samples by iteratively refining noisy inputs. In contrast to other methods, these generative models are capable of reconstructing the smallest scales of the fluid spectrum. In this work, we introduce a novel sampling method for diffusion models that enables the reconstruction of high-fidelity samples by guiding the reverse process using the available sparse data. Moreover, we enhance the reconstructions with available physics knowledge using a conflict-free update method during training. To evaluate the effectiveness of our method, we conduct experiments on 2 and 3-dimensional turbulent flow data. Our method consistently outperforms other diffusion-based methods in predicting the fluid&#x27;s structure and in pixel-wise accuracy. This study underscores the remarkable potential of diffusion models in reconstructing flow field data, paving the way for leveraging them in fluid dynamics research and applications ranging from super-resolution to reconstructions of experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>引导扩散模型从稀疏数据重建流场</div>
<div class="mono" style="margin-top:8px">从有限测量中重建非稳态流场是许多工程应用中的一项具有挑战性和关键性的任务。由于机器学习模型能够从数据中学习复杂模式并在多种条件下进行泛化，因此它们在解决此问题上越来越受欢迎。在这些模型中，扩散模型在生成任务中表现出特别强大的能力，通过迭代精炼噪声输入生成高质量样本。与其他方法相比，这些生成模型能够重建流体光谱的最小尺度。在本研究中，我们介绍了一种新颖的扩散模型采样方法，通过使用可用的稀疏数据引导反向过程，从而实现高保真样本的重建。此外，我们在训练过程中使用无冲突更新方法，结合可用的物理知识来增强重建效果。为了评估我们方法的有效性，我们在二维和三维湍流数据上进行了实验。我们的算法在预测流体结构和像素级准确性方面始终优于其他基于扩散的方法。本研究强调了扩散模型在重建流场数据方面的显著潜力，为在流体动力学研究和从超分辨率到实验重建等应用中利用它们铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The reconstruction of unsteady flow fields from limited measurements is essential for various engineering applications, motivating the exploration of machine learning models for this task. This study introduces a novel sampling method for diffusion models that enhances the reconstruction of high-fidelity samples by guiding the reverse process with sparse data and incorporating physics knowledge through a conflict-free update method during training. Experimental results on 2 and 3-dimensional turbulent flow data demonstrate that the proposed method consistently outperforms other diffusion-based approaches in predicting fluid structure and achieving higher pixel-wise accuracy.</div>
<div class="mono" style="margin-top:8px">从有限测量中重建非稳态流场对各种工程应用至关重要，这激励了对机器学习模型的探索。本研究提出了一种新颖的扩散模型采样方法，通过使用稀疏数据引导反向过程并在训练过程中通过无冲突更新方法结合物理知识，增强高保真样本的重建。对二维和三维湍流数据的实验结果表明，所提出的方法在预测流体结构和实现更高的逐像素准确性方面，始终优于其他基于扩散的方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling</div>
<div class="meta-line">Authors: Jin Gao, Saichandu Juluri</div>
<div class="meta-line">First: 2026-01-08T15:18:12+00:00 · Latest: 2026-01-08T15:18:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05016v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05016v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从创意到共创：一种用于增强代理3D建模的规划者-演员-评论家框架</div>
<div class="mono" style="margin-top:8px">我们提出了一个框架，将演员-评论家架构扩展到通过多代理自我反思和人类参与监督的创意3D建模。现有方法依赖于单提示代理，直接通过Blender MCP等工具执行建模命令，而我们的方法引入了规划者-演员-评论家架构。在该设计中，规划者协调建模步骤，演员执行这些步骤，评论家提供迭代反馈，而人类用户在整个过程中充当监督者和顾问。通过对单提示建模和我们的反思多代理方法进行系统比较，我们展示了在多种3D建模场景中几何准确性、美学质量和任务完成率的改善。我们的评估表明，与直接单提示执行相比，评论家引导的反思结合人类监督输入减少了建模错误，并提高了结果的复杂性和质量。这项工作确立了结构化代理自我反思在增强人类监督和顾问指导下，能够在保持高效工作流程集成的同时，产生更高质量的3D模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the quality of 3D modeling by integrating human supervision with multi-agent self-reflection. The authors propose a Planner-Actor-Critic framework that coordinates modeling tasks, executes them, and provides feedback, contrasting it with traditional single-prompt agent methods. Experimental results indicate that this reflective multi-agent approach significantly improves geometric accuracy, aesthetic quality, and task completion rates, demonstrating that human oversight and structured agent self-reflection lead to higher-quality 3D models while maintaining efficient workflows in real-time environments like Blender.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改进代理协作和人类监督来提高3D建模的质量和效率。作者提出了一种规划者-演员-评论家框架，该框架结合了多代理自我反思和人类参与的监督，与传统的单提示代理方法形成对比。实验结果表明，这种新方法在各种3D建模场景中显著提高了几何准确性、美学质量和任务完成率，证明了评论者引导的反思与人类输入的结合可以减少建模错误，并提高最终输出的复杂性和质量。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification</div>
<div class="meta-line">Authors: Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq</div>
<div class="meta-line">First: 2026-01-08T15:11:04+00:00 · Latest: 2026-01-08T15:11:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05011v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05011v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用预测熵进行零-shot音频语言分类中的自动提示加权</div>
<div class="mono" style="margin-top:8px">音频语言模型最近通过利用自然语言监督展示了强大的零-shot能力，可以在没有标记训练数据的情况下对音频事件进行分类。然而，它们的性能对文本提示的措辞非常敏感，细微的变化会导致准确率的大幅波动。之前的工作通过提示学习或提示集成来缓解这个问题。然而，这些策略要么需要注释数据，要么未能考虑到某些提示可能对性能产生负面影响。在本研究中，我们提出了一种基于熵的提示加权方法，旨在找到提示贡献的稳健组合，以最大化预测置信度。为此，我们制定了一个量身定制的目标函数，最小化预测熵以产生新的提示权重，利用低熵作为高置信度的代理。我们的方法可以应用于单个样本或一批音频样本，无需额外标签，并且计算开销微乎其微。在涵盖环境、城市和人声的五个音频分类数据集上的实验表明，与经典的提示集成方法相比，在零-shot设置中取得了一致的增益，准确率提升在整个基准上大约提高了5倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of audio-language models in zero-shot audio classification, which is often hindered by the sensitivity of these models to the wording of text prompts. The authors propose an entropy-guided prompt weighting method that optimizes the combination of prompt contributions to improve prediction confidence without requiring additional labeled data. Experimental results across five audio classification datasets show that this approach yields significant accuracy improvements, achieving gains up to five times larger than traditional prompt ensembling methods in a zero-shot context.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高音频语言模型在零样本音频分类中的性能，而这些模型的性能通常受到文本提示措辞敏感性的影响。作者提出了一种基于熵的提示加权方法，优化提示贡献以提高预测置信度，而无需额外的标注数据。跨五个音频分类数据集的实验结果表明，该方法在零样本环境中实现了显著的准确性提升，获得的增益是传统提示集成方法的五倍。</div>
</details>
</div>
<div class="card">
<div class="title">Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype</div>
<div class="meta-line">Authors: Haihua Luo, Xuming Ran, Zhengji Li, Huiyan Xue, Tingting Jiang, Jiangrong Shen, Tommi Kärkkäinen, Qi Xu, Fengyu Cong</div>
<div class="meta-line">First: 2026-01-08T11:59:35+00:00 · Latest: 2026-01-08T11:59:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无键值对的持续学习者通过任务特定的提示原型</div>
<div class="mono" style="margin-top:8px">持续学习旨在使模型在保留先前学习的信息的同时获取新知识。基于提示的方法在这一领域表现出色；然而，它们通常依赖于键值对，这可能引入任务间干扰并阻碍可扩展性。为克服这些限制，我们提出了一种新方法，采用任务特定的提示原型（ProP），从而消除了对键值对的需求。在我们的方法中，任务特定的提示促进了当前任务的更有效特征学习，而相应的原型捕捉输入的代表性特征。在推理过程中，通过将每个任务特定的提示与其相关的原型绑定来生成预测。此外，我们在提示初始化期间引入正则化约束，以惩罚过大的值，从而增强稳定性。在多个广泛使用的数据集上的实验表明了所提方法的有效性。与主流的基于提示的方法相比，我们的框架消除了对键值对的依赖，为未来的持续学习研究提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve continual learning by enabling models to acquire new knowledge while retaining previously learned information without the drawbacks of key-value pairing, which can cause inter-task interference. The authors propose a novel method called task-specific Prompt-Prototype (ProP) that eliminates the need for key-value pairs by using task-specific prompts for effective feature learning and corresponding prototypes to capture representative input features. Experimental results on various widely used datasets show that this approach enhances stability and performance compared to traditional prompt-based methods, providing a new direction for continual learning research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过消除键值对的缺陷，增强持续学习，使模型能够在保留先前学习信息的同时获取新知识。作者提出了一种新方法，称为任务特定的提示原型（ProP），通过使用任务特定的提示进行有效特征学习，并通过相应的原型捕捉代表性特征，从而消除了对键值对的需求。对多个广泛使用的数据集的实验结果表明，该方法在稳定性和性能上优于传统的基于提示的方法，为持续学习研究提供了新的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Single Image Reflection Separation via Dual Prior Interaction Transformer</div>
<div class="meta-line">Authors: Yue Huang, Zi&#x27;ang Li, Tianle Hu, Jie Wen, Guanbin Li, Jinglin Zhang, Guoxu Zhou, Xiaozhao Fang</div>
<div class="meta-line">First: 2025-05-19T02:50:15+00:00 · Latest: 2026-01-08T11:53:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12641v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12641v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single image reflection separation aims to separate the transmission and reflection layers from a mixed image. Existing methods typically combine general priors from pre-trained models with task-specific priors such as text prompts and reflection detection. However, the transmission prior, as the most direct task-specific prior for the target transmission layer, has not been effectively modeled or fully utilized, limiting performance in complex scenarios. To address this issue, we propose a dual-prior interaction framework based on lightweight transmission prior generation and effective prior fusion. First, we design a Local Linear Correction Network (LLCN) that finetunes pre-trained models based on the physical constraint T=SI+B, where S and B represent pixel-wise and channel-wise scaling and bias transformations. LLCN efficiently generates high-quality transmission priors with minimal parameters. Second, we construct a Dual-Prior Interaction Transformer (DPIT) that employs a dual-stream channel reorganization attention mechanism. By reorganizing features from general and transmission priors for attention computation, DPIT achieves deep fusion of both priors, fully exploiting their complementary information. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双先验交互变换器进行单幅图像反射分离</div>
<div class="mono" style="margin-top:8px">单幅图像反射分离旨在从混合图像中分离传输层和反射层。现有方法通常将预训练模型的通用先验与任务特定的先验（如文本提示和反射检测）结合。然而，作为目标传输层最直接的任务特定先验，传输先验尚未得到有效建模或充分利用，限制了在复杂场景中的性能。为了解决这个问题，我们提出了一种基于轻量级传输先验生成和有效先验融合的双先验交互框架。首先，我们设计了一个局部线性校正网络（LLCN），该网络基于物理约束T=SI+B微调预训练模型，其中S和B分别表示逐像素和逐通道的缩放和偏置变换。LLCN以最小的参数高效生成高质量的传输先验。其次，我们构建了一个双先验交互变换器（DPIT），该变换器采用双流通道重组注意力机制。通过重组来自通用和传输先验的特征进行注意力计算，DPIT实现了两种先验的深度融合，充分利用了它们的互补信息。在多个基准数据集上的实验结果表明，所提出的方法达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve single image reflection separation by effectively modeling the transmission prior, which has been underutilized in existing methods. The authors propose a dual-prior interaction framework that includes a Local Linear Correction Network (LLCN) for generating high-quality transmission priors and a Dual-Prior Interaction Transformer (DPIT) for fusing general and transmission priors through a dual-stream attention mechanism. Experimental results on various benchmark datasets indicate that this approach achieves state-of-the-art performance in separating transmission and reflection layers from mixed images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效利用传输先验来改善单幅图像反射分离，而这一先验在现有方法中未得到充分利用。作者提出了一种双先验交互框架，包括一个局部线性校正网络（LLCN）用于生成高质量的传输先验，以及一个双先验交互变换器（DPIT）通过双流注意机制融合一般先验和传输先验。多个基准数据集上的实验结果表明，该方法在从混合图像中分离传输层和反射层方面达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation</div>
<div class="meta-line">Authors: Ayush Pande</div>
<div class="meta-line">First: 2026-01-08T11:53:04+00:00 · Latest: 2026-01-08T11:53:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04860v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04860v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DivAS：通过深度加权体素聚合进行NeRF的交互式3D分割</div>
<div class="mono" style="margin-top:8px">现有的神经辐射场（NeRF）分割方法通常基于优化，需要缓慢的逐场景训练，牺牲了2D基础模型的零-shot能力。我们提出了DivAS（深度交互体素聚合分割），这是一种无优化、完全交互的框架，解决了这些局限性。我们的方法通过快速的基于GUI的工作流程运行，用户点提示生成的2D SAM掩码通过NeRF衍生的深度先验进行精炼，以提高几何精度和前景-背景分离。我们贡献的核心是一个自定义的CUDA内核，它在200毫秒内将这些精炼的多视图掩码聚合成一个统一的3D体素网格，实现实时视觉反馈。这种无优化设计消除了逐场景训练的需求。在Mip-NeRF 360°和LLFF上的实验表明，DivAS实现的分割质量可与基于优化的方法相媲美，同时端到端速度快2-2.5倍，排除用户提示时间时速度快达一个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the segmentation of Neural Radiance Fields (NeRFs) without the slow per-scene training typical of existing optimization-based methods. The authors introduce DivAS, an interactive framework that utilizes a fast GUI-based workflow to refine 2D SAM masks with NeRF-derived depth priors, enhancing geometric accuracy and separation of foreground and background. Experimental results demonstrate that DivAS achieves segmentation quality comparable to traditional methods while being 2-2.5 times faster overall, and significantly faster when excluding user prompting time.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善神经辐射场（NeRFs）的分割，避免现有优化方法中典型的慢场景训练。作者提出了DivAS，一个交互式框架，利用快速的图形用户界面工作流程，通过NeRF衍生的深度先验来细化2D SAM掩码，从而提高几何精度和前景-背景分离。实验结果表明，DivAS在分割质量上与传统方法相当，同时整体速度提高了2-2.5倍，排除用户提示时间后速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</div>
<div class="meta-line">Authors: Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</div>
<div class="meta-line">First: 2026-01-06T17:15:50+00:00 · Latest: 2026-01-08T11:08:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03193v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03193v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniCorn：通过自生成监督实现自我改进的统一多模态模型</div>
<div class="mono" style="margin-top:8px">尽管统一多模态模型（UMMs）在跨模态理解方面取得了显著成功，但它们在利用内部知识进行高质量生成方面仍存在显著差距。我们将这种差距形式化为传导性失语症，这是一种现象，模型能够准确解释多模态输入，但难以将这种理解转化为真实且可控的合成。为了解决这个问题，我们提出了UniCorn，一个简单而优雅的自我改进框架，消除了对外部数据或教师监督的需求。通过将单个UMM划分为三个协作角色：提议者、解决者和评判者，UniCorn通过自我对弈生成高质量的交互，并采用认知模式重构将潜在理解提炼为明确的生成信号。为了验证多模态一致性的恢复，我们引入了UniCycle，这是一个基于文本到图像再到文本重构循环的循环一致性基准。大量实验表明，UniCorn在六个通用图像生成基准上相较于基础模型实现了全面且显著的改进。值得注意的是，它在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle上达到了SOTA性能，同时在WISE上进一步实现了+5.0的显著提升，在OneIG上实现了+6.5的提升。这些结果突显了我们的方法显著增强了T2I生成，同时保持了强大的理解能力，展示了完全自我监督精炼在统一多模态智能中的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve Unified Multimodal Models (UMMs) in generating high-quality outputs from multimodal inputs, addressing a gap known as Conduction Aphasia. The authors propose a self-improvement framework called UniCorn, which organizes a UMM into three roles: Proposer, Solver, and Judge, allowing the model to generate interactions through self-play and refine its understanding into generative signals without external supervision. Experimental results show that UniCorn significantly enhances performance across six image generation benchmarks, achieving state-of-the-art results on several tasks and demonstrating the effectiveness of self-supervised refinement in multimodal generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型（UMMs）在从多模态输入生成高质量输出方面的能力，解决被称为传导失语症的问题。作者提出了一种名为UniCorn的自我改进框架，该框架将UMM组织为三个角色——提议者、解决者和评判者，以促进自我对弈和认知模式重构，而无需依赖外部数据。实验结果表明，UniCorn在六个图像生成基准测试中显著提高了性能，在多个任务上达到了最先进的结果，并证明了其在提高文本到图像生成能力的同时保持理解能力的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Retoucher for Text-To-Image Generation</div>
<div class="meta-line">Authors: Shaocheng Shen, Jianfeng Liang, Chunlei Cai, Cong Geng, Huiyu Duan, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</div>
<div class="meta-line">First: 2026-01-05T12:06:43+00:00 · Latest: 2026-01-08T10:57:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02046v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像生成的自主修饰器</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）扩散模型如SDXL和FLUX已实现令人印象深刻的照片真实感，但在四肢、面部、文本等方面仍存在小规模失真。现有的精细化方法要么进行昂贵的迭代再生成，要么依赖于空间基础较弱的视觉语言模型（VLM），导致语义漂移和不可靠的局部编辑。为了解决这一问题，我们提出了自主修饰器，这是一个分层决策驱动框架，将生成后的修正重新表述为类似人类的感知-推理-行动循环。具体而言，我们设计了（1）一个感知代理，学习在文本-图像一致性线索下进行细粒度失真定位的上下文显著性，（2）一个推理代理，通过渐进的偏好对齐进行人类对齐的推理诊断，以及（3）一个行动代理，根据用户偏好自适应规划局部修补。该设计将感知证据、语言推理和可控修正整合为一个统一的自我修正决策过程。为了实现细粒度监督和定量评估，我们进一步构建了GenBlemish-27K，这是一个包含6K T2I图像和12个类别中27K注释伪影区域的数据集。大量实验表明，自主修饰器在感知质量、失真定位和人类偏好对齐方面始终优于最先进的方法，为自我修正和感知可靠的T2I生成建立了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the persistent small-scale distortions in text-to-image (T2I) diffusion models, which affect the quality of generated images. The authors propose a novel framework called Agentic Retoucher, which employs a hierarchical decision-driven approach that mimics human perception and reasoning to correct these distortions. Experimental results show that Agentic Retoucher significantly outperforms existing methods in terms of perceptual quality, distortion localization, and alignment with human preferences, supported by the creation of a new dataset, GenBlemish-27K, containing 6K T2I images with 27K annotated artifact regions for evaluation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像扩散模型中持续存在的小规模失真问题，这影响了生成图像的质量。作者提出了Agentic Retoucher，一个将后生成修正重新构建为类人感知-推理-行动循环的分层框架，包含一个用于失真定位的感知代理、一个用于推理诊断的推理代理和一个用于局部修补的行动代理。实验结果表明，Agentic Retoucher在感知质量、失真定位和与人类偏好的对齐方面显著优于现有方法，并通过引入GenBlemish-27K数据集支持了细粒度评估。</div>
</details>
</div>
<div class="card">
<div class="title">CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</div>
<div class="meta-line">Authors: Dahyeon Kye, Jeahun Sung, Minkyu Jeon, Jihyong Oh</div>
<div class="meta-line">First: 2025-12-08T04:39:12+00:00 · Latest: 2026-01-08T10:29:58+00:00</div>
<div class="meta-line">Comments: Please visit our project page at https://cmlab-korea.github.io/CHIMERA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07155v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.07155v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cmlab-korea.github.io/CHIMERA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHIMERA：用于零-shot图像变形的自适应缓存注入和语义锚点提示，结合变形导向指标</div>
<div class="mono" style="margin-top:8px">扩散模型展现出卓越的生成能力，但实现平滑且语义一致的图像变形仍然是一个挑战。现有方法常常由于缺乏自适应的结构和语义对齐而导致突兀的过渡或过饱和的外观。我们提出CHIMERA，一个基于零-shot扩散的框架，将变形公式化为缓存反演引导的去噪过程。为了处理大的语义和外观差异，我们提出了自适应缓存注入和语义锚点提示。自适应缓存注入（ACI）在DDIM反演过程中缓存来自两个输入的下、中、上块特征，并在去噪过程中自适应地重新注入，从而以深度和时间自适应的方式实现空间和语义对齐，并实现自然特征融合和平滑过渡。语义锚点提示（SAP）利用视觉-语言模型生成一个共享的锚点提示，作为语义锚点，连接不同的输入并引导去噪过程朝向一致的结果。最后，我们引入了全局-局部一致性评分（GLCS），这是一种变形导向的指标，同时评估两个输入的全局协调性和局部变形过渡的平滑性。大量实验和用户研究表明，CHIMERA实现了比现有方法更平滑和更具语义对齐的过渡，确立了图像变形的新技术前沿。代码和项目页面将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of achieving smooth and semantically consistent image morphing using diffusion models, which often result in abrupt transitions or over-saturated appearances. The authors propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process, utilizing Adaptive Cache Injection (ACI) and Semantic Anchor Prompting (SAP) to manage large semantic and appearance disparities. Experimental results demonstrate that CHIMERA produces smoother and more semantically aligned transitions compared to existing methods, setting a new state of the art in image morphing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用扩散模型实现平滑且语义一致的图像变形所面临的挑战，这些模型通常导致突兀的过渡或过饱和的外观。作者提出了CHIMERA，这是一种零-shot扩散基础框架，将变形过程表述为缓存反演引导的去噪方法，结合自适应缓存注入（ACI）和语义锚定提示（SAP）来增强结构和语义对齐。实验结果表明，CHIMERA优于现有方法，实现了更平滑的过渡和更好的语义对齐，从而在图像变形领域树立了新的标准。</div>
</details>
</div>
<div class="card">
<div class="title">Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers</div>
<div class="meta-line">Authors: Lee Hyoseok, Sohwi Lim, Eunju Cha, Tae-Hyun Oh</div>
<div class="meta-line">First: 2026-01-08T10:15:35+00:00 · Latest: 2026-01-08T10:15:35+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver&#x27;s and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>测量一致的朗之万修正器：潜在扩散逆解算器的补救措施</div>
<div class="mono" style="margin-top:8px">随着生成模型的最新进展，扩散模型已成为解决各领域逆问题的强大先验。由于潜在扩散模型（LDM）提供通用先验，几项研究探讨了它们作为领域无关的零-shot 逆解算器的潜力。尽管有这些努力，现有的潜在扩散逆解算器仍然存在不稳定性，表现出不良伪影和质量下降。在这项工作中，我们首先将不稳定性识别为解算器与真实反向扩散动态之间的差异，并表明减少这一差距可以稳定解算器。在此基础上，我们引入了测量一致的朗之万修正器（MCLC），这是一个理论基础的即插即用修正模块，通过测量一致的朗之万更新来修正基于LDM的逆解算器。与依赖于线性流形假设的先前方法相比，这些假设在潜在空间中往往不成立，MCLC在没有此假设的情况下运行，从而导致更稳定和可靠的行为。我们通过实验展示了MCLC的有效性及其与现有解算器在多样图像恢复任务中的兼容性。此外，我们分析了斑点伪影并提供了其潜在原因的见解。我们强调MCLC是朝着更强大的零-shot 逆问题解算器迈出的关键一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the instability and quality degradation observed in existing latent diffusion inverse solvers, which are crucial for solving inverse problems using generative models. The authors propose a Measurement-Consistent Langevin Corrector (MCLC), a correction module designed to align the solver&#x27;s dynamics with true reverse diffusion dynamics, thereby enhancing stability. Experimental results demonstrate that MCLC significantly improves the performance of LDM-based inverse solvers across various image restoration tasks, while also providing insights into the causes of blob artifacts that were previously problematic.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有潜在扩散逆解算器中观察到的不稳定性和质量下降，这些解算器在各个领域中越来越多地用于解决逆问题。作者提出了一种测量一致的朗之万修正器（MCLC），该修正模块利用测量一致的朗之万更新来使解算器的动态与真实的反向扩散动态对齐。实验结果表明，MCLC显著提高了基于LDM的逆解算器在多个图像恢复任务中的稳定性和可靠性，同时还提供了关于在过程中可能出现的斑点伪影成因的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models</div>
<div class="meta-line">Authors: Shuyang Jiang, Yuhao Wang, Ya Zhang, Yanfeng Wang, Yu Wang</div>
<div class="meta-line">First: 2026-01-08T08:52:37+00:00 · Latest: 2026-01-08T08:52:37+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04731v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04731v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy&#x27;s intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>矿工：在大型推理模型中进行数据高效强化学习的内在掌握</div>
<div class="mono" style="margin-top:8px">当前针对大型推理模型的无评论强化学习方法在训练正向同质提示（所有回合均正确）时效率极低，导致由于零优势估计而浪费回合。我们提出了一种极其简单但强大的解决方案——内在掌握矿工（Miner），它将策略的内在不确定性重新用作自监督奖励信号，无需外部监督、辅助模型或额外推理成本。我们的方法开创了两个关键创新：（1）一种基于令牌级别的聚焦信用分配机制，动态放大关键不确定令牌的梯度，同时抑制过于自信的令牌；（2）自适应优势校准，顺畅整合内在和可验证奖励。在Qwen3-4B和Qwen3-8B基础模型的六个推理基准上评估，Miner在其他四种算法中实现了最先进的性能，与GRPO相比，在Pass@1中获得高达4.58的绝对增益，在Pass@K中获得6.66的增益。与其他针对探索增强的方法的比较进一步揭示了这两项新提出的创新的优越性。这表明，潜在不确定性的利用对于推理模型的高效和可扩展的强化学习训练既是必要的也是充分的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies of current critic-free reinforcement learning (RL) methods for large reasoning models, particularly when training on positive homogeneous prompts that lead to wasted rollouts. The authors propose a method called Miner, which utilizes the policy&#x27;s intrinsic uncertainty as a self-supervised reward signal without requiring external supervision or additional inference costs. Experimental results show that Miner outperforms four other algorithms across six reasoning benchmarks on Qwen3-4B and Qwen3-8B models, achieving up to 4.58 absolute gains in Pass@1 and 6.66 gains in Pass@K compared to the GRPO method, highlighting the effectiveness of the proposed innovations in gradient amplification and advantage calibration.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前大型推理模型中无评论的强化学习（RL）方法的低效性，特别是在训练正向同质提示时，由于零优势估计导致的回合浪费。作者提出了一种名为Miner的方法，该方法利用策略的内在不确定性作为自我监督的奖励信号，无需外部监督或额外的推理成本。实验结果表明，Miner在Qwen3-4B和Qwen3-8B模型的六个推理基准上实现了最先进的性能，与GRPO算法相比，在Pass@1上提高了4.58的绝对增益，在Pass@K上提高了6.66，突显了所提出的令牌级焦点信用分配和自适应优势校准创新的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling</div>
<div class="meta-line">Authors: Anqi Li, Wenwei Jin, Jintao Tong, Pengda Qin, Weijia Li, Guo Lu</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-08-05T10:16:04+00:00 · Latest: 2026-01-08T08:23:50+00:00</div>
<div class="meta-line">Comments: Accepted by KDD 2026. Code is available at https://github.com/lianqi1008/Hi-Guard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03296v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.03296v2">PDF</a> · <a href="https://github.com/lianqi1008/Hi-Guard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term &quot;Hierarchical&quot; reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过政策对齐推理和分层标记实现可信的多模态内容审核</div>
<div class="mono" style="margin-top:8px">社交平台革新了信息共享，但也加速了有害和违反政策内容的传播。为了在规模上确保安全和合规，审核系统必须超越效率，提供准确性和可解释性。然而，当前的方法在很大程度上依赖于嘈杂的标签驱动学习，缺乏与审核规则的对齐，产生不透明的决策，阻碍人工审核。因此，我们提出了分层守护（Hi-Guard），一个多模态审核框架，引入了一种新的政策对齐决策范式。术语“分层”反映了我们系统设计的两个关键方面：（1）分层审核管道，其中轻量级二元模型首先过滤安全内容，强模型处理细粒度风险分类；（2）第二阶段的分层分类法，模型在从粗到细的分层分类法上执行基于路径的分类。为了确保与不断发展的审核政策对齐，Hi-Guard直接将规则定义纳入模型提示。为了进一步增强结构化预测和推理，我们引入了多级软边际奖励，并使用群体相对政策优化（GRPO）进行优化，惩罚语义相邻的错误分类，提高解释质量。大量实验和实际部署表明，Hi-Guard在分类准确性、泛化能力和可解释性方面表现优越，为可扩展、透明和可信的内容安全系统铺平了道路。代码可在：https://github.com/lianqi1008/Hi-Guard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy and interpretability of moderation systems on social platforms, which currently struggle with harmful content due to reliance on noisy, label-driven learning. The authors propose a multimodal moderation framework called Hierarchical Guard (Hi-Guard), which features a hierarchical moderation pipeline that first filters safe content with a lightweight model and then classifies risks with a more robust model, alongside a hierarchical taxonomy for classification. Experimental results show that Hi-Guard significantly enhances classification accuracy, generalization, and interpretability, contributing to the development of scalable and trustworthy content moderation systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高社交平台上内容审核系统的准确性和可解释性，因为目前的系统因依赖噪声标签驱动学习而难以处理有害内容。作者提出了一种名为层次守卫（Hi-Guard）的多模态审核框架，该框架具有层次化审核流程，并通过将规则定义直接嵌入模型提示中来实现与政策对齐的决策。实验结果表明，Hi-Guard显著提高了分类准确性、泛化能力和可解释性，从而有助于开发可扩展和可信赖的内容安全系统。</div>
</details>
</div>
<div class="card">
<div class="title">HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution</div>
<div class="meta-line">Authors: Yang Zou, Xingyue Zhu, Kaiqi Han, Jun Ma, Xingyuan Li, Zhiying Jiang, Jinyuan Liu</div>
<div class="meta-line">First: 2026-01-08T07:49:02+00:00 · Latest: 2026-01-08T07:49:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04682v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04682v1">PDF</a> · <a href="https://github.com/JZ0606/HATIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HATIR：热感知扩散用于湍流红外视频超分辨率</div>
<div class="mono" style="margin-top:8px">红外视频在具有挑战性的环境下的视觉任务中备受关注，但常常受到严重的气象湍流和压缩降级的影响。现有的视频超分辨率（VSR）方法要么忽视红外和可见图像之间固有的模态差距，要么无法恢复由湍流引起的失真。将湍流缓解（TM）算法与VSR方法直接级联会导致错误传播和积累，因为湍流和分辨率之间的降级建模是解耦的。我们提出了HATIR，一种热感知扩散用于湍流红外视频超分辨率，它将热感知变形先验注入扩散采样路径，以共同建模湍流降级和结构细节损失的逆过程。具体而言，HATIR构建了一个相量引导流估计器，基于热活跃区域在时间上表现出一致的相量响应的物理原理，使得可靠的湍流感知流能够引导逆扩散过程。为了确保在非均匀失真下结构恢复的保真度，提出了一种湍流感知解码器，选择性地抑制不稳定的时间线索，并通过湍流门控和结构感知注意力增强边缘感知特征聚合。我们构建了FLIR-IVSR，这是第一个用于湍流红外VSR的数据集，包含来自FLIR T1050sc相机（1024 X 768）的配对LR-HR序列，涵盖640个具有不同相机和物体运动条件的多样场景。这鼓励未来在红外VSR领域的研究。项目页面：https://github.com/JZ0606/HATIR</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of infrared video super-resolution (VSR) under atmospheric turbulence and compression degradation, which existing methods inadequately handle due to the modality gap between infrared and visible images. The proposed method, HATIR, employs a Heat-Aware Diffusion approach that integrates heat-aware deformation priors into the diffusion sampling path, effectively modeling both turbulent degradation and structural detail loss. Experimental results demonstrate that HATIR significantly improves the fidelity of structural recovery in turbulent infrared videos, supported by the introduction of the FLIR-IVSR dataset, which consists of paired low-resolution and high-resolution sequences from diverse scenes to facilitate further research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决红外视频超分辨率（VSR）面临的大气湍流和压缩降解带来的挑战，而现有方法对此处理不足。作者提出了HATIR，这是一种新颖的方法，将热感知变形先验整合到扩散采样过程中，以同时建模湍流降解和结构细节损失。关键实验结果表明，HATIR有效利用了相位引导流估计器和湍流感知解码器，从而提高了结构恢复的保真度，并在新创建的FLIR-IVSR数据集上表现出色，该数据集包含在各种运动条件下的低分辨率和高分辨率序列配对。</div>
</details>
</div>
<div class="card">
<div class="title">HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment</div>
<div class="meta-line">Authors: Wenzhi Chen, Bo Hu, Leida Li, Lihuo He, Wen Lu, Xinbo Gao</div>
<div class="meta-line">First: 2026-01-08T05:41:06+00:00 · Latest: 2026-01-08T05:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04614v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperAlign：用于自适应文本到图像对齐评估的双曲推理锥</div>
<div class="mono" style="margin-top:8px">随着文本到图像生成技术的快速发展，准确评估生成图像与文本提示之间的对齐已成为一个关键挑战。现有方法依赖于欧几里得空间度量，忽视了语义对齐的结构特性，同时缺乏对不同样本的自适应能力。为了解决这些局限性，我们提出了HyperAlign，一个基于双曲推理几何的自适应文本到图像对齐评估框架。首先，我们使用CLIP提取欧几里得特征并将其映射到双曲空间。其次，我们设计了一种动态监督推理建模机制，将离散推理逻辑转化为连续几何结构监督。最后，我们提出了一种自适应调制回归器，利用双曲几何特征生成样本级调制参数，自适应校准欧几里得余弦相似度以预测最终得分。HyperAlign在单一数据库评估和跨数据库泛化任务中均表现出高度竞争力，充分验证了双曲几何建模在图像-文本对齐评估中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for improved assessment of alignment between generated images and text prompts in text-to-image generation technology, as existing methods fail to account for the structured nature of semantic alignment. The authors propose HyperAlign, an adaptive framework that utilizes hyperbolic entailment geometry, first extracting Euclidean features via CLIP and mapping them to hyperbolic space, followed by a dynamic-supervision entailment modeling mechanism that converts discrete entailment logic into continuous geometric supervision. The results demonstrate that HyperAlign achieves competitive performance in both single database evaluations and cross-database generalization tasks, confirming the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善文本到图像生成中生成图像与文本提示之间的对齐评估，因为现有方法未能考虑语义对齐的结构特性且缺乏适应性。作者提出了HyperAlign框架，该框架利用双曲推理几何，首先使用CLIP提取欧几里得特征并将其映射到双曲空间，然后设计了一种动态监督推理建模机制，将离散推理逻辑转化为连续几何结构监督。结果表明，HyperAlign在单数据库评估和跨数据库泛化任务中均表现出竞争力，验证了双曲几何建模在图像-文本对齐评估中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing</div>
<div class="meta-line">Authors: Zihao Lin, Wanrong Zhu, Jiuxiang Gu, Jihyung Kil, Christopher Tensmeyer, Lin Zhang, Shilong Liu, Ruiyi Zhang, Lifu Huang, Vlad I. Morariu, Tong Sun</div>
<div class="meta-line">First: 2026-01-08T04:38:07+00:00 · Latest: 2026-01-08T04:38:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04589v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04589v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MiLDEdit：基于推理的多层设计文档编辑</div>
<div class="mono" style="margin-top:8px">现实世界的设计文档（例如海报）本质上是多层的，结合了装饰、文本和图像。从自然语言指令编辑它们需要细粒度的、层意识的推理，以识别相关层并协调修改。之前的工作在很大程度上忽视了多层设计文档编辑，主要集中在单层图像编辑或多层生成上，这些方法假设一个平坦的画布，缺乏确定修改内容和位置所需的推理。为了解决这一问题，我们引入了多层文档编辑代理（MiLDEAgent），这是一个基于推理的框架，结合了经过强化学习训练的多模态推理器以实现层级理解和用于目标修改的图像编辑器。为了系统地基准测试这一设置，我们引入了MiLDEBench，这是一个包含超过2万份设计文档和多样化编辑指令的人机交互语料库。该基准测试还配备了一个特定任务的评估协议MiLDEEval，涵盖了包括指令遵循、布局一致性、美学和文本渲染在内的四个维度。在14个开源模型和2个闭源模型上的广泛实验表明，现有方法无法推广：开源模型通常无法完成多层文档编辑任务，而闭源模型则存在格式违规。相比之下，MiLDEAgent实现了强大的层意识推理和精确编辑，显著超越所有开源基线，并达到与闭源模型相当的性能，从而为多层文档编辑建立了第一个强基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the editing of multi-layer design documents, which combine various elements like text and images, as existing methods primarily focus on single-layer editing or generation without the necessary reasoning for layer-specific modifications. The authors propose the Multi-Layer Document Editing Agent (MiLDEAgent), a framework that integrates a reinforcement learning-trained multimodal reasoner for understanding layers with an image editor for precise modifications. Experimental results demonstrate that MiLDEAgent significantly outperforms existing open-source models in multi-layer document editing tasks and achieves performance levels comparable to closed-source models, thus establishing a new benchmark in this area with the introduction of MiLDEBench and MiLDEEval for systematic evaluation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多层设计文档编辑中的挑战，这些文档需要细致的推理来根据自然语言指令识别和修改特定层。作者提出了多层文档编辑代理（MiLDEAgent），这是一个将经过强化学习训练的多模态推理器与图像编辑器相结合的框架，以实现精确的修改。实验结果表明，MiLDEAgent在多层文档编辑任务中显著优于现有的开源模型，并且其性能水平与闭源模型相当，从而在该领域建立了新的基准，并引入了MiLDEBench和MiLDEEval进行系统评估。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</div>
<div class="meta-line">Authors: Delong Zeng, Yuexiang Xie, Yaliang Li, Ying Shen</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-01-08T04:02:49+00:00 · Latest: 2026-01-08T04:02:49+00:00</div>
<div class="meta-line">Comments: Accepted by ACL&#x27;2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04571v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04571v1">PDF</a> · <a href="https://github.com/zengdlong/CIEA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过互补信息提取和对齐增强多模态检索</div>
<div class="mono" style="margin-top:8px">多模态检索近年来已成为一个有前景但具有挑战性的研究方向。现有的大多数多模态检索研究集中于捕捉与其配对文本相似的多模态数据中的信息，但往往忽视了多模态数据中包含的互补信息。在本研究中，我们提出了CIEA，一种新颖的多模态检索方法，采用互补信息提取和对齐，将文档中的文本和图像转化为统一的潜在空间，并具有旨在识别和保留图像表示差异的互补信息提取器。我们使用两种互补对比损失优化CIEA，以确保语义完整性并有效捕捉图像中包含的互补信息。大量实验表明CIEA的有效性，相较于分而治之模型和通用稠密检索模型取得了显著改进。我们提供了消融研究、进一步讨论和案例研究，以突出CIEA所取得的进展。为了促进社区的进一步研究，我们已在https://github.com/zengdlong/CIEA发布了源代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing multimodal retrieval methods that primarily focus on similarities between multimodal data and their paired texts, neglecting the complementary information present in the data. The authors propose a novel approach called Complementary Information Extraction and Alignment (CIEA), which integrates text and images into a unified latent space and employs a complementary information extractor to highlight differences in image representations. Experimental results show that CIEA significantly outperforms traditional divide-and-conquer models and universal dense retrieval models, demonstrating its effectiveness in capturing complementary information and maintaining semantic integrity through the use of complementary contrastive losses.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态检索方法的局限性，这些方法主要关注多模态数据与其配对文本之间的相似性，而忽视了数据中存在的互补信息。作者提出了一种名为CIEA的新方法，该方法利用互补信息提取和对齐，将文本和图像转化为统一的潜在空间，同时采用互补信息提取器来突出图像表示中的差异。实验结果表明，CIEA在捕捉互补信息方面显著优于分治模型和通用稠密检索模型，证明了其有效性，得到了广泛的实验、消融研究和案例研究的支持。</div>
</details>
</div>
<div class="card">
<div class="title">Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models</div>
<div class="meta-line">Authors: Zitong Huang, Kaidong Zhang, Yukang Ding, Chao Gao, Rui Ding, Ying Chen, Wangmeng Zuo</div>
<div class="meta-line">First: 2026-01-07T16:32:17+00:00 · Latest: 2026-01-08T02:51:26+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04068v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04068v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关注生成细节：视频扩散模型的直接局部细节偏好优化</div>
<div class="mono" style="margin-top:8px">将文本到视频的扩散模型与人类偏好对齐对于生成高质量视频至关重要。现有的直接偏好优化（DPO）方法依赖于多样本排名和特定任务的评判模型，这效率低下且常常产生模糊的全局监督。为了解决这些局限性，我们提出了LocalDPO，这是一种新颖的后训练框架，从真实视频中构建局部偏好对，并在时空区域级别上优化对齐。我们设计了一个自动化管道，以高效收集偏好对数据，通过每个提示进行单次推理生成偏好对，消除了对外部评判模型或手动标注的需求。具体而言，我们将高质量的真实视频视为正样本，并通过随机时空掩码局部破坏它们生成相应的负样本，仅使用冻结的基础模型恢复被掩盖的区域。在训练过程中，我们引入了一种区域感知的DPO损失，将偏好学习限制在破坏区域，以实现快速收敛。在Wan2.1和CogVideoX上的实验表明，LocalDPO在视频保真度、时间一致性和人类偏好评分方面始终优于其他后训练方法，建立了一个更高效和细致的视频生成器对齐范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the alignment of text-to-video diffusion models with human preferences to produce high-quality videos. The authors introduce LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal level, addressing the inefficiencies of existing Direct Preference Optimization methods. Experimental results on Wan2.1 and CogVideoX show that LocalDPO significantly improves video fidelity, temporal coherence, and human preference scores compared to other post-training methods, establishing a more efficient approach for video generator alignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强文本到视频扩散模型与人类偏好的对齐，以生成高质量的视频。作者提出了LocalDPO，这是一种后训练框架，通过真实视频创建局部偏好对，并在时空区域级别上优化对齐，从而解决了现有直接偏好优化方法中的低效问题。在Wan2.1和CogVideoX上的实验结果表明，LocalDPO显著提高了视频的保真度、时间一致性和人类偏好评分，相较于其他后训练技术，表明了视频生成器对齐的更高效方法。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Language Models for Code-related Tasks</div>
<div class="meta-line">Authors: Zhao Tian</div>
<div class="meta-line">First: 2026-01-08T02:48:01+00:00 · Latest: 2026-01-08T02:48:01+00:00</div>
<div class="meta-line">Comments: Accepted by ICSE 2026 (DS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推进与代码相关任务的语言模型</div>
<div class="mono" style="margin-top:8px">最近语言模型（LMs）的进展推动了各种软件工程任务的显著进展。然而，现有的LMs在复杂编程场景中仍然面临挑战，原因在于数据质量、模型架构和推理能力的限制。本研究通过三个互补方向系统性地解决这些挑战：（1）通过代码差异引导的对抗增强技术（CODA）和代码去噪技术（CodeDenoise）提高代码数据质量；（2）通过语法引导的代码LMs（LEAM和LEAM++）增强模型架构；（3）通过提示技术（muFiX）和基于代理的技术（Specine）推进模型推理。这些技术旨在促进LMs在软件开发中的实际应用，并进一步推动智能软件工程的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of language models (LMs) in complex programming tasks, as existing models face challenges due to data quality, architecture, and reasoning limitations. The study employs three main methods: a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise) to improve data quality, syntax-guided code LMs (LEAM and LEAM++) to enhance model architecture, and a prompting technique (muFiX) along with an agent-based technique (Specine) to advance model reasoning. The experimental results indicate that these combined techniques significantly improve the practical applicability of LMs in software development, thereby advancing intelligent software engineering.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高语言模型（LMs）在复杂编程任务中的表现，解决其在数据质量、模型架构和推理能力方面的局限性。研究采用了三种主要方法：使用代码差异引导的对抗增强技术（CODA）和代码去噪技术（CodeDenoise）来改善数据质量，采用语法引导的代码语言模型（LEAM和LEAM++）来增强模型架构，以及使用提示技术（muFiX）和基于代理的技术（Specine）来提升推理能力。实验结果表明，这些综合技术显著提高了LMs在软件开发中的实际应用性，从而推动了智能软件工程的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data</div>
<div class="meta-line">Authors: Shogo Nakayama, Masahiro Okuda</div>
<div class="meta-line">Venue: 2025 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC), Seoul, Korea, Republic of, 2025, pp. 1-5,</div>
<div class="meta-line">First: 2026-01-08T02:32:12+00:00 · Latest: 2026-01-08T02:32:12+00:00</div>
<div class="meta-line">Comments: ITC-CSCC accepted</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04518v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将分布匹配整合到半监督对比学习中以处理标记和未标记数据</div>
<div class="mono" style="margin-top:8px">深度学习的进步极大地提高了监督图像分类的效果。然而，标记数据的成本很高，这促使研究无监督学习方法，如对比学习。在现实场景中，完全未标记的数据集是稀缺的，这使得半监督学习（SSL）在少量标记数据与大量未标记数据共存的情况下变得非常相关。一种著名的半监督对比学习方法涉及为未标记数据分配伪标签。本研究旨在通过在标记和未标记特征嵌入之间引入分布匹配来增强基于伪标签的SSL，以提高多个数据集的图像分类准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image classification accuracy in semi-supervised learning (SSL) scenarios where labeled data is scarce and unlabeled data is abundant. The authors propose a method that integrates distribution matching into a semi-supervised contrastive learning framework, enhancing the assignment of pseudo-labels to unlabeled data. Experimental results demonstrate that this approach significantly improves classification performance across multiple datasets compared to traditional SSL methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高在标记数据稀缺且未标记数据丰富的半监督学习（SSL）场景中的图像分类准确性。作者提出了一种将分布匹配整合到基于伪标签的SSL框架中的方法，重点在于对齐标记和未标记数据的特征嵌入。实验结果表明，与传统方法相比，该方法在多个数据集上显著提高了分类性能。</div>
</details>
</div>
<div class="card">
<div class="title">FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing</div>
<div class="meta-line">Authors: Mingshu Cai, Yixuan Li, Osamu Yoshie, Yuya Ieiri</div>
<div class="meta-line">First: 2025-12-24T07:21:59+00:00 · Latest: 2026-01-08T01:57:34+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Multimedia (TMM)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21015v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21015v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FluencyVE：将时间感知的Mamba与旁路注意力结合用于视频编辑</div>
<div class="mono" style="margin-top:8px">大规模文本到图像扩散模型在图像生成和编辑方面取得了前所未有的成功。然而，将这一成功扩展到视频编辑仍然具有挑战性。最近的视频编辑工作通过添加时间注意力机制来适应预训练的文本到图像模型，以处理视频任务。不幸的是，这些方法仍然存在时间不一致性问题和高计算开销。在本研究中，我们提出了FluencyVE，这是一种简单而有效的一次性视频编辑方法。FluencyVE将线性时间序列模块Mamba集成到基于预训练的稳定扩散模型的视频编辑模型中，替代了时间注意力层。这使得全局帧级注意力成为可能，同时降低了计算成本。此外，我们采用低秩近似矩阵替代因果注意力中的查询和键权重矩阵，并在训练过程中使用加权平均技术来更新注意力分数。这种方法显著保留了文本到图像模型的生成能力，同时有效降低了计算负担。实验和分析表明，在编辑现实世界视频中的各种属性、主题和位置方面取得了良好的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of temporal inconsistency and high computational overhead in video editing using pretrained text-to-image models. The authors propose FluencyVE, a one-shot video editing approach that integrates the linear time-series module Mamba into a model based on Stable Diffusion, replacing the temporal attention layer to enable global frame-level attention while reducing computational costs. Experimental results indicate that FluencyVE effectively preserves the generative capabilities of the text-to-image model and shows promising performance in editing various attributes, subjects, and locations in real-world videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用预训练文本到图像模型进行视频编辑时面临的时间不一致性和高计算开销的问题。作者提出了FluencyVE，这是一种新颖的一次性视频编辑方法，将线性时间序列模块Mamba集成到基于Stable Diffusion的视频编辑模型中，替代了传统的时间注意力层。实验结果表明，FluencyVE有效保持了文本到图像模型的生成能力，同时显著降低了计算成本，在编辑现实视频中的各种属性、主题和位置方面展示了良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation</div>
<div class="meta-line">Authors: Wei-Rui Chen, Vignesh Kothapalli, Ata Fatahibaarzi, Hejian Sang, Shao Tang, Qingquan Song, Zhipeng Wang, Muhammad Abdul-Mageed</div>
<div class="meta-line">First: 2025-12-24T06:57:35+00:00 · Latest: 2026-01-08T01:09:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21002v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21002v2">PDF</a> · <a href="https://github.com/weiruichen01/distilling-the-essence">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distilling the capabilities from a large reasoning model (LRM) to a smaller student model often involves training on substantial amounts of reasoning data. However, knowledge distillation (KD) over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) sections makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different sections (P, CoT, A) affects student performance. Our analysis shows that selective KD over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that beyond a specific length, longer training sequences provide marginal returns for downstream performance but require substantially higher memory and FLOPs. To this end, training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx91\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. Codes are available at https://github.com/weiruichen01/distilling-the-essence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提炼本质：通过序列截断实现高效推理蒸馏</div>
<div class="mono" style="margin-top:8px">将大型推理模型（LRM）的能力蒸馏到较小的学生模型通常需要在大量推理数据上进行训练。然而，针对包含提示（P）、思维链（CoT）和答案（A）部分的长序列进行知识蒸馏（KD）使得这一过程计算成本高昂。在本研究中，我们探讨了在不同部分（P、CoT、A）之间分配监督如何影响学生表现。我们的分析表明，当提示和答案信息被思维链包含时，仅对思维链标记进行选择性KD是有效的。基于这一见解，我们建立了一种截断协议，以量化计算与质量之间的权衡，作为序列长度的函数。我们观察到，超过特定长度后，较长的训练序列对下游性能的边际收益有限，但需要显著更高的内存和FLOPs。因此，仅在每个训练序列的前50%标记上进行训练，可以在数学基准测试中平均保留约91%的全序列性能，同时将训练时间、内存使用和FLOPs各减少约50%。代码可在https://github.com/weiruichen01/distilling-the-essence获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of knowledge distillation from large reasoning models to smaller student models, which is typically hindered by the computational demands of lengthy reasoning sequences. The authors investigate the impact of selectively applying knowledge distillation to the chain-of-thought (CoT) tokens, while considering the prompt and answer sections. Their findings reveal that focusing on the CoT tokens can maintain approximately 91% of the performance of full sequences on math benchmarks when training on only the first 50% of tokens, resulting in a significant reduction in training time, memory usage, and FLOPs by about 50%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高从大型推理模型到小型学生模型的知识蒸馏效率，而这一过程常常受到冗长推理序列计算需求的制约。作者研究了仅对链式思维（CoT）标记进行选择性知识蒸馏的影响，提出了一种截断协议，以优化基于序列长度的计算与质量之间的平衡。研究结果表明，仅对每个序列的前50%标记进行训练，能够在数学基准测试中实现约91%的完整序列性能，同时将训练时间、内存使用和FLOPs各减少约50%。</div>
</details>
</div>
<div class="card">
<div class="title">QUIET-SR: Quantum Image Enhancement Transformer for Single Image Super-Resolution</div>
<div class="meta-line">Authors: Siddhant Dutta, Nouhaila Innan, Khadijeh Najafi, Sadok Ben Yahia, Muhammad Shafique</div>
<div class="meta-line">First: 2025-03-11T16:06:16+00:00 · Latest: 2026-01-08T00:18:07+00:00</div>
<div class="meta-line">Comments: 13 Pages, 7 Figures (5 Main figures, 2 Sub-figures), 2 Tables, Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.08759v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.08759v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Single-Image Super-Resolution (SISR) using deep learning have significantly improved image restoration quality. However, the high computational cost of processing high-resolution images due to the large number of parameters in classical models, along with the scalability challenges of quantum algorithms for image processing, remains a major obstacle. In this paper, we propose the Quantum Image Enhancement Transformer for Super-Resolution (QUIET-SR), a hybrid framework that extends the Swin transformer architecture with a novel shifted quantum window attention mechanism, built upon variational quantum neural networks. QUIET-SR effectively captures complex residual mappings between low-resolution and high-resolution images, leveraging quantum attention mechanisms to enhance feature extraction and image restoration while requiring a minimal number of qubits, making it suitable for the Noisy Intermediate-Scale Quantum (NISQ) era. We evaluate our framework in MNIST (30.24 PSNR, 0.989 SSIM), FashionMNIST (29.76 PSNR, 0.976 SSIM) and the MedMNIST dataset collection, demonstrating that QUIET-SR achieves PSNR and SSIM scores comparable to state-of-the-art methods while using fewer parameters. Our efficient batching strategy directly enables massive parallelization on multiple QPU&#x27;s paving the way for practical quantum-enhanced image super-resolution through coordinated QPU-GPU quantum supercomputing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QUIET-SR：用于单幅图像超分辨率的量子图像增强变换器</div>
<div class="mono" style="margin-top:8px">最近，使用深度学习的单幅图像超分辨率（SISR）取得了显著的图像恢复质量提升。然而，由于经典模型中参数数量庞大，处理高分辨率图像的高计算成本以及量子算法在图像处理中的可扩展性挑战，仍然是一个主要障碍。本文提出了用于超分辨率的量子图像增强变换器（QUIET-SR），这是一个混合框架，基于变分量子神经网络，扩展了Swin变换器架构，并引入了一种新颖的偏移量子窗口注意机制。QUIET-SR有效捕捉低分辨率和高分辨率图像之间复杂的残差映射，利用量子注意机制增强特征提取和图像恢复，同时只需最少数量的量子比特，适合噪声中间规模量子（NISQ）时代。我们在MNIST（30.24 PSNR，0.989 SSIM）、FashionMNIST（29.76 PSNR，0.976 SSIM）和MedMNIST数据集上评估了我们的框架，证明QUIET-SR在使用更少参数的情况下，达到了与最先进方法相当的PSNR和SSIM分数。我们高效的批处理策略直接实现了在多个量子处理单元（QPU）上的大规模并行化，为通过协调的QPU-GPU量子超级计算实现实用的量子增强图像超分辨率铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs and scalability challenges associated with classical models in Single-Image Super-Resolution (SISR) using deep learning. The authors propose a hybrid framework called the Quantum Image Enhancement Transformer for Super-Resolution (QUIET-SR), which integrates a shifted quantum window attention mechanism into the Swin transformer architecture, leveraging variational quantum neural networks. Experimental results on the MNIST, FashionMNIST, and MedMNIST datasets show that QUIET-SR achieves competitive PSNR and SSIM scores, such as 30.24 PSNR and 0.989 SSIM for MNIST, while utilizing fewer parameters and enabling efficient parallelization on quantum processing units.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统模型在单图像超分辨率（SISR）中处理高分辨率图像时的高计算成本和可扩展性挑战。作者提出了一种名为量子图像增强变换器（QUIET-SR）的混合框架，该框架将移位量子窗口注意机制集成到Swin变换器架构中，利用变分量子神经网络。对MNIST、FashionMNIST和MedMNIST数据集的实验结果表明，QUIET-SR在使用更少参数的情况下，达到了竞争力的PSNR和SSIM分数，具体为MNIST的30.24 PSNR和0.989 SSIM，同时实现了在量子处理单元上的高效并行化。</div>
</details>
</div>
<div class="card">
<div class="title">CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction</div>
<div class="meta-line">Authors: Donghang Lyu, Marius Staring, Hildo Lamb, Mariya Doneva</div>
<div class="meta-line">First: 2026-01-07T22:23:56+00:00 · Latest: 2026-01-07T22:23:56+00:00</div>
<div class="meta-line">Comments: STACOM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04428v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04428v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRUNet-MR-Univ：多样化心脏MRI重建的基础模型</div>
<div class="mono" style="margin-top:8px">近年来，深度学习因其在心脏MRI（CMR）重建领域的优越性能而受到越来越多的关注，尤其是在处理更高加速因子方面，突显了其在实际临床应用中的潜力。然而，当前的深度学习方法在泛化能力上仍然有限。CMR扫描在图像对比度、采样模式、扫描仪供应商、解剖结构和疾病类型上表现出广泛的变异性。大多数现有模型仅设计用于处理这些变异中的单一或狭窄子集，因此在面对分布变化时性能下降。因此，开发一个能够在多样化CMR场景中泛化的统一模型是有益的。为此，我们提出了CRUNet-MR-Univ，一个利用时空相关性和基于提示的先验知识来有效处理CMR扫描全方位多样性的基础模型。我们的方法在广泛的设置中始终优于基线方法，突显了其有效性和前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current deep learning methods in cardiac MRI reconstruction, which struggle with generalizability across diverse imaging scenarios. The authors propose CRUNet-MR-Univ, a foundation model that utilizes spatio-temporal correlations and prompt-based priors to enhance performance across varying CMR conditions. Experimental results demonstrate that this model consistently outperforms baseline methods, indicating its effectiveness in handling the variability present in cardiac MRI scans.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高深度学习方法在心脏MRI（CMR）重建中的泛化能力，因为现有模型在图像对比度、采样模式等因素的变异性面前表现不佳。作者提出了CRUNet-MR-Univ，这是一种基础模型，利用时空相关性和基于提示的先验知识来应对CMR扫描中遇到的多样化场景。实验结果表明，CRUNet-MR-Univ在各种设置下始终优于基线方法，显示出其在提升CMR重建性能方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
