<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 03:32</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0332</div>
    <div class="row"><div class="card">
<div class="title">From Specialist to Generalist: Unlocking SAM&#x27;s Learning Potential on Unlabeled Medical Images</div>
<div class="meta-line">Authors: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-25T18:13:48+00:00 · Latest: 2026-01-28T18:55:46+00:00</div>
<div class="meta-line">Comments: Accepted to ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17934v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17934v2">PDF</a> · <a href="https://github.com/vnlvi2k3/SC-SAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM&#x27;s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从专家到通才：释放SAM在无标签医学图像上的学习潜力</div>
<div class="mono" style="margin-top:8px">基础模型如Segment Anything Model (SAM)展现出强大的泛化能力，但由于领域转移、标签稀缺以及参数高效微调（PEFT）无法利用无标签数据，适应医学图像仍然困难。虽然传统模型如U-Net在半监督医学学习中表现出色，但它们在辅助PEFT SAM方面的潜力被大大忽视。我们提出了SC-SAM，一个专家-通才框架，其中U-Net提供基于点的提示和伪标签来指导SAM的适应，而SAM则作为强大的通才监督者来规范U-Net。这种相互指导形成了一个双向共同训练循环，使两个模型能够有效利用无标签数据。在前列腺MRI和息肉分割基准测试中，我们的方法取得了最先进的结果，超越了其他现有的半监督SAM变体，甚至医学基础模型如MedSAM，突显了专家-通才合作在标签高效医学图像分割中的价值。我们的代码可在https://github.com/vnlvi2k3/SC-SAM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenges of adapting foundation models like the Segment Anything Model (SAM) to medical images due to domain shifts and limited labeled data. The authors propose a specialist-generalist framework called SC-SAM, where a U-Net model provides point-based prompts and pseudo-labels to assist SAM&#x27;s adaptation, while SAM acts as a generalist supervisor to regularize U-Net. Experimental results demonstrate that SC-SAM achieves state-of-the-art performance on prostate MRI and polyp segmentation tasks, surpassing existing semi-supervised SAM variants and medical foundation models, thus emphasizing the effectiveness of specialist-generalist collaboration in medical image segmentation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，由于领域转移和标注数据稀缺，将基础模型如Segment Anything Model（SAM）适应于医学图像面临挑战。作者提出了一种名为SC-SAM的专家-通用框架，其中U-Net模型提供基于点的提示和伪标签以辅助SAM的适应，而SAM则作为监督者来规范U-Net，从而形成一个双向共同训练循环，利用未标记数据。实验结果表明，SC-SAM在前列腺MRI和息肉分割基准测试中实现了最先进的性能，超越了其他半监督SAM变体和医学基础模型MedSAM，强调了专家-通用合作在医学图像分割中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BlindSight: Harnessing Sparsity for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Tharun Adithya Srikrishnan, Deval Shah, Timothy Hein, Ahmed Hasssan, Stephen Youn, Steven K. Reinhardt</div>
<div class="meta-line">First: 2025-07-11T23:15:30+00:00 · Latest: 2026-01-28T18:45:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09071v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.09071v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BlindSight：利用稀疏性提高视觉-语言模型的效率</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）能够实现文本和图像的联合处理。然而，纳入视觉数据显著增加了提示长度，导致首次令牌时间（TTFT）延长。通过利用注意力计算中的固有稀疏性，可以缓解这一瓶颈。在处理一系列图像时，我们分析了VLM中的注意力模式，观察到在相当一部分层中缺乏图像间的注意力。基于此，我们提出了BlindSight：一种优化多图像VLM推理的方法，使用输入模板感知的注意力稀疏掩码且没有运行时开销。我们利用一个数据集推导出对注意力头的提示无关分类：稠密、汇聚、图像内和图像内+汇聚。我们开发了一个基于Triton的GPU内核来利用这种稀疏性。BlindSight在注意力计算中实现了1.8-3.2倍的加速（提示长度36K-300K）。BlindSight在VLM中具有广泛的适用性（Qwen2-VL、Qwen2.5-VL、Gemma 3），在多图像理解基准上平均仅有0.78%的绝对准确度下降。最后，我们倡导设计结合BlindSight启发的稀疏和稠密层的高效VLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of increased prompt length in large vision-language models (VLMs), which leads to longer time to first token (TTFT) during inference. The authors propose BlindSight, an approach that optimizes multi-image VLM inference by utilizing an input-template-aware attention sparsity mask without adding runtime overhead. Experimental results demonstrate that BlindSight achieves a speedup of 1.8-3.2 times in attention computation across various VLMs, with only a minimal average accuracy degradation of 0.78% on multi-image comprehension benchmarks, suggesting its effectiveness in enhancing VLM efficiency while maintaining performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型视觉语言模型（VLMs）的效率，这些模型由于视觉数据的引入而面临更长的提示长度，导致首次令牌的时间（TTFT）延长。作者提出了BlindSight，这是一种通过利用输入模板感知的注意稀疏掩码来优化多图像VLM推理的方法，该方法不会引入运行时开销。实验结果表明，BlindSight在各种VLM中实现了1.8-3.2倍的注意计算加速，同时在多图像理解基准测试中仅有0.78%的平均准确度下降，表明其在提高VLM性能的同时保持准确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</div>
<div class="meta-line">Authors: Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang</div>
<div class="meta-line">First: 2026-01-28T18:34:25+00:00 · Latest: 2026-01-28T18:34:25+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &quot;sitting on a sofa&#x27;&#x27;, while supporting fine-grained functional human-scene interactions, e.g., &quot;increasing the room temperature&#x27;&#x27;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇功能性3D人类-场景交互生成</div>
<div class="mono" style="margin-top:8px">生成与3D场景功能性交互的3D人类仍然是一个未解决的问题，应用于具身AI、机器人技术和互动内容创作。关键挑战在于推理3D场景中功能元素的语义以及实现功能感知交互所需的3D人类姿态。不幸的是，现有方法通常缺乏对物体功能性及相应人类-场景接触的明确推理，导致不可信或功能性错误的交互。在本研究中，我们提出了FunHSI，一个无训练、以功能为驱动的框架，能够从开放词汇任务提示中实现功能正确的人类-场景交互。给定任务提示，FunHSI执行功能感知接触推理，以识别功能场景元素，重建其3D几何形状，并通过接触图建模高级交互。然后，我们利用视觉-语言模型合成在图像中执行任务的人类，并估计提出的3D身体和手部姿态。最后，通过阶段性优化对提出的3D身体配置进行精细化，以确保物理可信性和功能正确性。与现有方法相比，FunHSI不仅合成更可信的一般3D交互，如“坐在沙发上”，同时支持细粒度的功能性人类-场景交互，例如“提高房间温度”。大量实验表明，FunHSI在各种室内和室外场景中始终生成功能正确且物理可信的人类-场景交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of generating 3D humans that can functionally interact with 3D scenes, which is crucial for applications in embodied AI, robotics, and interactive content creation. The authors propose a training-free framework called FunHSI that utilizes functionality-aware contact reasoning to identify functional elements in scenes and model interactions through a contact graph, leveraging vision-language models to synthesize human poses. Experimental results show that FunHSI generates more plausible and functionally correct human-scene interactions, outperforming existing methods in both general interactions, such as sitting on a sofa, and specific tasks, like adjusting room temperature, across various indoor and outdoor environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成能够与3D场景功能性互动的3D人类的挑战，这对于具身人工智能、机器人技术和互动内容创作至关重要。作者提出了一种名为FunHSI的无训练框架，该框架利用功能意识接触推理来识别场景中的功能元素，重建其3D几何形状，并通过基于开放词汇任务提示的接触图建模互动。实验结果表明，FunHSI成功合成了可信且功能正确的人类-场景互动，超越了现有方法，在各种室内和室外环境中生成了现实场景，如“坐在沙发上”和“提高室温”等细致互动。</div>
</details>
</div>
<div class="card">
<div class="title">DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision</div>
<div class="meta-line">Authors: Wenlin Chen, Mingtian Zhang, Jiajun He, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber</div>
<div class="meta-line">First: 2025-02-11T23:02:14+00:00 · Latest: 2026-01-28T17:35:55+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 5 tables, 2 algorithms</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.08005v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.08005v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based distillation methods (e.g., variational score distillation) train one-step diffusion models by first pre-training a teacher score model and then distilling it into a one-step student model. However, the gradient estimator in the distillation stage usually suffers from two sources of bias: (1) biased teacher supervision due to score estimation error incurred during pre-training, and (2) the student model&#x27;s score estimation error during distillation. These biases can degrade the quality of the resulting one-step diffusion model. To address this, we propose DiffRatio, a new framework for training one-step diffusion models: instead of estimating the teacher and student scores independently and then taking their difference, we directly estimate the score difference as the gradient of a learned log density ratio between the student and data distributions across diffusion time steps. This approach greatly simplifies the training pipeline, significantly reduces gradient estimation bias, and improves one-step generation quality. Additionally, it also reduces auxiliary network size by using a lightweight density-ratio network instead of two full score networks, which improves computational and memory efficiency. DiffRatio achieves competitive one-step generation results on CIFAR-10 and ImageNet (64x64 and 512x512), outperforming most teacher-supervised distillation methods. Moreover, the learned density ratio naturally serves as a verifier, enabling a principled inference-time parallel scaling scheme that further improves the generation quality without external rewards or additional sequential computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffRatio：无教师监督训练一步扩散模型</div>
<div class="mono" style="margin-top:8px">基于评分的蒸馏方法（例如，变分评分蒸馏）通过首先预训练教师评分模型，然后将其蒸馏为一步学生模型来训练一步扩散模型。然而，蒸馏阶段的梯度估计器通常受到两种偏差的影响：（1）由于预训练期间产生的评分估计误差导致的偏倚教师监督，以及（2）蒸馏过程中学生模型的评分估计误差。这些偏差会降低所得到的一步扩散模型的质量。为了解决这个问题，我们提出了DiffRatio，一个新的训练一步扩散模型的框架：我们直接估计作为学生和数据分布在扩散时间步长之间的学习的对数密度比的梯度的评分差，而不是独立估计教师和学生评分然后取其差。这种方法大大简化了训练流程，显著减少了梯度估计偏差，并提高了一步生成质量。此外，它还通过使用轻量级密度比网络而不是两个完整的评分网络来减少辅助网络的大小，从而提高计算和内存效率。DiffRatio在CIFAR-10和ImageNet（64x64和512x512）上实现了具有竞争力的一步生成结果，超越了大多数教师监督的蒸馏方法。此外，学习的密度比自然地作为验证器，使得一种原则性的推理时间并行扩展方案得以实现，进一步提高生成质量，而无需外部奖励或额外的顺序计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of score-based distillation methods in training one-step diffusion models, which suffer from biases due to errors in teacher supervision and student score estimation. The authors propose DiffRatio, a novel framework that directly estimates the score difference as the gradient of a learned log density ratio between the student and data distributions, thereby simplifying the training process and reducing gradient estimation bias. Experimental results demonstrate that DiffRatio achieves competitive one-step generation quality on CIFAR-10 and ImageNet datasets, outperforming most teacher-supervised distillation methods while also enhancing computational efficiency through the use of a lightweight density-ratio network.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善一阶段扩散模型的训练，这些模型在蒸馏过程中常常由于教师和学生模型的评分估计错误而受到偏差的影响。作者提出了一种新框架DiffRatio，直接将评分差异估计为学生和数据分布之间学习的对数密度比的梯度，从而简化训练过程并减少梯度估计偏差。实验结果表明，DiffRatio在CIFAR-10和ImageNet数据集上实现了竞争性的一阶段生成质量，超越了许多现有的教师监督蒸馏方法，同时通过轻量级密度比网络提高了计算效率。</div>
</details>
</div>
<div class="card">
<div class="title">FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models</div>
<div class="meta-line">Authors: Haonan Zhong, Wei Song, Tingxu Han, Maurice Pagnucco, Jingling Xue, Yang Song</div>
<div class="meta-line">First: 2026-01-28T17:29:53+00:00 · Latest: 2026-01-28T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAIRT2V：无训练去偏见的文本到视频扩散模型</div>
<div class="mono" style="margin-top:8px">文本到视频（T2V）扩散模型取得了快速进展，但其人口统计偏见，特别是性别偏见，仍然 largely 未被探索。我们提出了 FairT2V，这是一种无训练的去偏见框架，用于文本到视频生成，能够在不进行微调的情况下减轻编码器引起的偏见。我们首先分析了 T2V 模型中的人口统计偏见，并表明它主要源于预训练的文本编码器，这些编码器即使对于中性提示也会编码隐含的性别关联。我们用与生成视频中的偏见相关的性别倾向评分量化了这一影响。
基于这一见解，FairT2V 通过基于锚点的球面测地线变换中和提示嵌入，从而减轻人口统计偏见，同时保持语义。为了保持时间一致性，我们仅在早期身份形成步骤中通过动态去噪调度应用去偏见。我们进一步提出了一种视频级公平性评估协议，将基于 VideoLLM 的推理与人工验证相结合。在现代 T2V 模型 Open-Sora 上的实验表明，FairT2V 在各职业中显著减少了人口统计偏见，对视频质量的影响最小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the largely unexplored demographic biases, particularly gender bias, in text-to-video diffusion models. The authors propose FairT2V, a training-free debiasing framework that mitigates encoder-induced bias without the need for finetuning. Experimental results demonstrate that FairT2V significantly reduces demographic bias across various occupations while maintaining video quality, indicating its effectiveness in neutralizing implicit gender associations encoded by pretrained text encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到视频扩散模型中尚未充分探讨的人口统计偏见，特别是性别偏见。作者提出了FairT2V，这是一种无训练的去偏框架，可以在不进行微调的情况下减轻编码器引起的偏见。实验表明，FairT2V在生成视频中有效减少了人口统计偏见，同时保持了视频质量，特别是在不同职业之间，通过锚点基础的球面测地线变换中和提示嵌入，并在早期身份形成步骤中应用去偏。</div>
</details>
</div>
<div class="card">
<div class="title">Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models</div>
<div class="meta-line">Authors: Zhiqiang Kou, Junyang Chen, Xin-Qiang Cai, Xiaobo Xia, Ming-Kun Xie, Dong-Dong Wu, Biao Liu, Yuheng Jia, Xin Geng, Masashi Sugiyama, Tat-Seng Chua</div>
<div class="meta-line">First: 2026-01-28T15:14:50+00:00 · Latest: 2026-01-28T15:14:50+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20687v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher&#x27;s preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于本地小模型的正无标记强化学习蒸馏</div>
<div class="mono" style="margin-top:8px">由于隐私、成本和延迟的限制，本地部署小模型变得越来越普遍。然而，大多数实际流程停留在监督微调（SFT）阶段，未能达到强化学习（RL）对齐阶段。主要原因是RL对齐通常需要昂贵的人类偏好标注或对高质量奖励模型的高度依赖，这需要大规模API使用和持续的工程维护，这两者都不适合本地环境。为了解决这个问题，我们提出了一种用于本地小模型部署的正无标记（PU）RL蒸馏方法。在没有人类标注偏好或奖励模型的情况下，我们的方法将教师的偏好优化能力从黑箱生成中蒸馏到可本地训练的学生模型中。对于每个提示，我们查询教师一次以获得锚定响应，本地采样多个学生候选，并进行锚定条件自排名以引导成对或列表偏好，从而通过直接偏好优化或组相对策略优化实现完全本地的训练循环。理论分析证明我们的方法所诱导的偏好信号是顺序一致的，并集中在近似最优候选上，支持其在偏好优化中的稳定性。实验表明，我们的方法在低成本设置下实现了一致的强性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing need for on-premise deployment of small models due to privacy, cost, and latency constraints, which often limits the use of reinforcement learning (RL) alignment due to the need for expensive human annotations or high-quality reward models. The authors propose a positive-unlabeled RL distillation method that allows for effective preference optimization without requiring human-labeled preferences or external reward models. Their approach involves querying a teacher model to obtain anchor responses, sampling multiple student candidates, and using anchor-conditioned self-ranking to derive preferences, enabling a fully local training loop. Experimental results show that this method achieves strong performance in low-cost settings, demonstrating its effectiveness for on-premise small model deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决小型模型的本地部署面临的挑战，这些模型通常停留在监督微调阶段，无法进入强化学习对齐阶段，因为这需要昂贵的人类注释或高质量的奖励模型。作者提出了一种正无标记强化学习蒸馏方法，使得在没有人类标记偏好的情况下可以进行本地训练。该方法通过查询教师模型获得锚定响应，采样多个学生候选，并使用锚定条件自排名来推导偏好，从而实现完全本地的训练循环。实验结果表明，该方法在低成本设置下表现出色，证明了其在本地小型模型部署中的偏好优化有效性。</div>
</details>
</div>
<div class="card">
<div class="title">bi-modal textual prompt learning for vision-language models in remote sensing</div>
<div class="meta-line">Authors: Pankhi Kashyap, Mainak Singha, Biplab Banerjee</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-28T14:58:14+00:00 · Latest: 2026-01-28T14:58:14+00:00</div>
<div class="meta-line">Comments: Accepted in ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20675v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20675v1">PDF</a> · <a href="https://github.com/ipankhi/BiMoRS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于遥感的双模态文本提示学习的视觉-语言模型</div>
<div class="mono" style="margin-top:8px">提示学习（PL）已成为一种有效的策略，用于在有限监督下调整视觉-语言模型（VLMs），如CLIP，以适应下游任务。尽管PL在自然图像数据集上表现出强大的泛化能力，但其在遥感（RS）图像上的可迁移性仍未得到充分探索。RS数据呈现出独特的挑战，包括多标签场景、高类内变异性和多样的空间分辨率，这些都阻碍了现有PL方法的直接应用。特别是，当前基于提示的方法往往难以识别主导语义线索，并且在RS场景中无法泛化到新类别。为了解决这些挑战，我们提出了BiMoRS，一个轻量级的双模态提示学习框架，专为RS任务量身定制。BiMoRS采用一个冻结的图像字幕模型（例如BLIP-2）从RS图像中提取文本语义摘要。这些字幕使用BERT分词器进行分词，并与CLIP编码器的高层视觉特征融合。然后，一个轻量级的交叉注意力模块在融合的文本-视觉表示上条件化一个可学习的查询提示，生成上下文化的提示，而不改变CLIP主干。我们在三个领域泛化（DG）任务的四个RS数据集上评估BiMoRS，观察到一致的性能提升，平均超越强基线达2%。代码可在https://github.com/ipankhi/BiMoRS获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the adaptability of vision-language models (VLMs) for remote sensing (RS) tasks, where existing prompt learning methods struggle due to unique challenges such as multi-label scenes and high intra-class variability. The authors propose BiMoRS, a bi-modal prompt learning framework that utilizes a frozen image captioning model to extract textual semantic summaries from RS images, which are then combined with visual features from the CLIP encoder through a lightweight cross-attention module. Experimental results demonstrate that BiMoRS consistently outperforms strong baselines by up to 2% across four RS datasets in three domain generalization tasks, indicating its effectiveness in improving the transferability of VLMs to RS imagery.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视觉语言模型（VLM）在遥感（RS）图像中的适应性，而现有的提示学习（PL）方法未能解决这些独特的挑战。作者提出了一种名为BiMoRS的新框架，该框架利用冻结的图像字幕模型从RS图像中提取文本语义摘要，然后通过轻量级的交叉注意模块将这些摘要与视觉特征结合，生成上下文化的提示。实验结果表明，BiMoRS在四个RS数据集的三个领域泛化任务中，始终优于强基线，平均性能提升高达2%。</div>
</details>
</div>
<div class="card">
<div class="title">Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</div>
<div class="meta-line">Authors: Rohan Asthana, Vasileios Belagiannis</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T14:29:42+00:00 · Latest: 2026-01-28T14:29:42+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20642v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过对数概率的各向异性检测和缓解扩散模型中的记忆化</div>
<div class="mono" style="margin-top:8px">基于扩散的图像生成模型通过迭代去噪生成高保真图像，但仍然容易受到记忆化的影响，即无意中重现训练图像的精确副本或部分。最近的记忆化检测方法主要基于得分差异的范数作为记忆化的指标。我们证明，这种基于范数的度量在各向同性对数概率分布的假设下主要有效，这通常在高或中等噪声水平下成立。相反，分析各向异性状态揭示，记忆样本在低噪声环境中表现出指导向量与无条件得分之间的强角度对齐。通过这些见解，我们开发了一种通过整合各向同性范数和各向异性对齐的记忆化检测度量。我们的检测度量可以通过两个条件和无条件的前向传递直接在纯噪声输入上计算，消除了昂贵的去噪步骤的需要。在Stable Diffusion v1.4和v2上的检测实验表明，我们的度量在性能上优于现有的无去噪检测方法，同时速度至少比之前的最佳方法快5倍。最后，我们通过利用一种基于我们开发的度量适应记忆提示的缓解策略，展示了我们方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the issue of memorization in diffusion-based image generative models, which can inadvertently reproduce parts of training images. The authors propose a new detection metric that combines isotropic norm and anisotropic alignment, addressing the limitations of existing methods that rely on score difference norms, particularly in low-noise settings. Experimental results demonstrate that this new metric not only outperforms current denoising-free detection methods but also operates approximately five times faster than the best existing approach, and the effectiveness of the method is further validated through a mitigation strategy that adapts memorized prompts based on the detection metric.</div>
<div class="mono" style="margin-top:8px">本研究解决了扩散图像生成模型中的记忆化问题，这些模型可能无意中重现训练图像的部分内容。作者提出了一种新的检测指标，结合了各向同性范数和各向异性对齐，揭示了在低噪声环境下，记忆样本表现出强烈的角对齐。实验结果表明，该指标优于现有方法，速度约为之前最佳方法的五倍，并成功应用了一种基于检测指标调整记忆提示的缓解策略。</div>
</details>
</div>
<div class="card">
<div class="title">TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</div>
<div class="meta-line">Authors: Zhenglin Cheng, Peng Sun, Jianguo Li, Tao Lin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-03T07:45:46+00:00 · Latest: 2026-01-28T14:06:08+00:00</div>
<div class="meta-line">Comments: arxiv v1, accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05150v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05150v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (&lt; 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinFlow：通过自对抗流实现大模型的一步生成</div>
<div class="mono" style="margin-top:8px">最近在大型多模态生成模型方面的进展展示了其在多模态生成（包括图像和视频生成）中的令人印象深刻的能力。这些模型通常基于多步骤框架，如扩散和流匹配，这在本质上限制了它们的推理效率（需要40-100次函数评估（NFE））。虽然各种少步骤方法旨在加速推理，但现有解决方案存在明显的局限性。显著的蒸馏方法，如渐进蒸馏和一致性蒸馏，要么需要迭代蒸馏过程，要么在非常少的步骤（&lt; 4-NFE）时表现出显著退化。同时，将对抗训练集成到蒸馏中（例如，DMD/DMD2和SANA-Sprint）以增强性能，会引入训练不稳定性、增加复杂性，并由于辅助训练模型而导致高GPU内存开销。为此，我们提出了TwinFlow，一个简单而有效的框架，用于训练1步生成模型，绕过固定预训练教师模型的需求，并在训练过程中避免标准对抗网络，使其成为构建大规模高效模型的理想选择。在文本到图像任务中，我们的方法在1-NFE中达到了0.83的GenEval分数，超越了强基线如SANA-Sprint（基于GAN损失的框架）和RCGM（基于一致性的框架）。值得注意的是，我们通过在Qwen-Image-20B上进行全参数训练展示了TwinFlow的可扩展性，并将其转变为高效的少步骤生成器。仅需1-NFE，我们的方法在GenEval和DPG-Bench基准上匹配了原始100-NFE模型的性能，计算成本降低了$100\times$，质量降级很小。项目页面可访问 https://zhenglin-cheng.com/twinflow。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of large multi-modal generative models, which typically rely on multi-step frameworks that hinder inference speed. The authors propose TwinFlow, a novel framework that enables one-step generation without the need for fixed pretrained teacher models or standard adversarial networks, thereby simplifying the training process. Experimental results show that TwinFlow achieves a GenEval score of 0.83 in just 1-NFE on text-to-image tasks, outperforming existing methods like SANA-Sprint and RCGM, and demonstrates the ability to scale effectively while matching the performance of a 100-NFE model at a significantly reduced computational cost of $100\times$ with only minor quality degradation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高大型多模态生成模型的效率，这些模型通常依赖于多步骤框架，限制了推理速度。作者提出了TwinFlow，这是一种新颖的框架，能够实现一步生成，无需固定的预训练教师模型或标准对抗网络，从而简化了训练过程。实验结果表明，TwinFlow在文本到图像任务中以1-NFE达到了0.83的GenEval分数，超越了现有方法如SANA-Sprint和RCGM，并展示了可扩展性，能够在计算成本降低100倍的情况下，匹配100-NFE模型的性能，同时仅有轻微的质量损失。</div>
</details>
</div>
<div class="card">
<div class="title">WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport</div>
<div class="meta-line">Authors: Xinyu Wang, Ruoyu Wang, Qiangwei Peng, Peijie Zhou, Tiejun Li</div>
<div class="meta-line">First: 2026-01-28T13:41:52+00:00 · Latest: 2026-01-28T13:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20606v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WFR-MFM：动态不平衡最优传输的一步推断</div>
<div class="mono" style="margin-top:8px">从有限观察中重建动态演化是单细胞生物学中的一个基本挑战，其中动态不平衡最优传输提供了一个建模耦合传输和质量变化的原则框架。然而，现有方法在推断时依赖于轨迹模拟，使得推断成为可扩展应用的关键瓶颈。在这项工作中，我们提出了一种不平衡流匹配的均流框架，该框架使用均速和质量增长场总结任意时间间隔内的传输和质量增长动态，从而实现快速的一步生成，无需轨迹模拟。为了在Wasserstein-Fisher-Rao几何下解决动态不平衡最优传输，我们进一步基于该框架开发了Wasserstein-Fisher-Rao均流匹配（WFR-MFM）。在合成和真实的单细胞RNA测序数据集上，WFR-MFM实现了比一系列现有基线快几个数量级的推断，同时保持高预测准确性，并能够在具有数千个条件的大型合成数据集上有效预测扰动响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing dynamical evolution from limited observations in single-cell biology, where existing methods are hindered by slow trajectory simulations during inference. The authors propose a mean-flow framework for unbalanced flow matching that allows for fast one-step generation of transport and mass-growth dynamics without the need for trajectory simulation. Experimental results demonstrate that the Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM) method significantly outperforms existing baselines in terms of inference speed while maintaining high predictive accuracy, particularly in predicting perturbation responses on large synthetic datasets with numerous conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在单细胞生物学中从有限观察中重建动态演化的挑战，现有方法在推断过程中受到缓慢轨迹模拟的限制。作者提出了一种不平衡流匹配的均流框架，允许在不需要轨迹模拟的情况下快速生成运输和质量增长动态。实验结果表明，Wasserstein-Fisher-Rao均流匹配（WFR-MFM）方法在推断速度上比现有方法快几个数量级，同时保持高预测准确性，并有效预测具有数千个条件的大型合成数据集中的扰动响应。</div>
</details>
</div>
<div class="card">
<div class="title">IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework</div>
<div class="meta-line">Authors: Shaokun Wang, Yifan Yu, Yuhang He, Weili Guan, Yihong Gong</div>
<div class="meta-line">First: 2026-01-28T12:03:48+00:00 · Latest: 2026-01-28T12:03:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models&#x27; potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IOTA：通过黑白盒框架的纠正知识引导提示学习</div>
<div class="mono" style="margin-top:8px">最近，将预训练模型适应于下游任务引起了越来越多的关注。以前的参数高效调优（PET）方法将预训练模型视为不透明的黑盒模型，纯粹依赖数据驱动的优化，未充分利用其固有的先验知识。这一疏忽限制了模型在有效下游任务适应方面的潜力。为了解决这些问题，我们提出了一种新颖的黑白盒提示学习框架（IOTA），该框架将数据驱动的黑盒模块与知识驱动的白盒模块结合起来，以适应下游任务。具体而言，白盒模块通过对比错误预测与正确认知来推导纠正知识。这些知识被转化为可解释的人类提示，并通过纠正知识引导的提示选择策略来引导黑盒模块朝向更准确的预测。通过共同利用知识和数据驱动的学习信号，IOTA实现了有效的下游任务适应。在少样本和易到难适应设置下的12个图像分类基准上的实验结果证明了纠正知识的有效性以及我们的方法相较于最先进方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adaptation of pre-trained models to downstream tasks, as existing Parameter-Efficient-Tuning methods often overlook the inherent prior knowledge of these models. The authors propose a novel framework called IOTA, which combines a data-driven Black Box module with a knowledge-driven White Box module to enhance task adaptation. Experimental results across 12 image classification benchmarks show that IOTA effectively utilizes corrective knowledge, leading to superior performance compared to state-of-the-art methods in both few-shot and easy-to-hard adaptation scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高预训练模型在下游任务中的适应能力，因为现有的参数高效调优方法往往忽视了这些模型固有的先验知识。作者提出了一种名为IOTA的新框架，该框架结合了数据驱动的黑箱模块和知识驱动的白箱模块，以改善任务适应性。在12个图像分类基准测试中的实验结果表明，纠正知识的整合显著提高了预测准确性，证明了所提方法相较于最先进方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs</div>
<div class="meta-line">Authors: Yating Ren, Yikun Ban, Huobin Tan</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-20T20:10:49+00:00 · Latest: 2026-01-28T11:13:25+00:00</div>
<div class="meta-line">Comments: AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16778v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16778v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT outperforms state-of-the-art methods, demonstrating its effectiveness and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GCL-OT：针对异质文本属性图的最优传输图对比学习</div>
<div class="mono" style="margin-top:8px">最近，结构-文本对比学习通过利用图神经网络和语言模型的互补优势，在文本属性图上显示出良好的性能。然而，现有方法通常依赖于相似性估计中的同质性假设和严格的优化目标，这限制了它们在异质图上的适用性。尽管现有方法可以通过结构调整或邻居聚合来缓解异质性，但它们通常将文本嵌入视为静态目标，导致次优对齐。在本研究中，我们识别了文本属性图中的多粒度异质性，包括完全异质性、部分异质性和潜在同质性，这使得结构-文本对齐特别具有挑战性，因为存在混合、噪声和缺失的语义关联。为了实现灵活的双向对齐，我们提出了GCL-OT，一种具有最优传输的新型图对比学习框架，配备了针对每种异质性的定制机制。具体而言，对于部分异质性，我们设计了一种基于RealSoftMax的相似性估计器，以强调关键邻居-词交互，同时减轻背景噪声。对于完全异质性，我们引入了一种基于提示的过滤器，在最优传输对齐过程中自适应地排除无关噪声。此外，我们结合了OT引导的软监督，以发现具有相似语义的潜在邻居，增强潜在同质性的学习。理论分析表明，GCL-OT可以改善互信息界限和贝叶斯错误保证。在九个基准上的广泛实验表明，GCL-OT优于最先进的方法，证明了其有效性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing structure-text contrastive learning methods that rely on homophily assumptions, which hinder their effectiveness on heterophilic text-attributed graphs. The authors propose a novel framework called GCL-OT, which utilizes optimal transport to facilitate flexible and bidirectional alignment in the presence of various types of heterophily. Key experimental results indicate that GCL-OT significantly outperforms state-of-the-art methods across nine benchmarks, demonstrating its effectiveness in improving mutual information bounds and reducing Bayes error guarantees.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有结构-文本对比学习方法依赖同质性假设的局限性，这限制了它们在异质文本属性图上的有效性。作者提出了GCL-OT，这是一种利用最优传输的新的图对比学习框架，以促进不同类型异质性的灵活对齐。关键实验结果表明，GCL-OT在九个基准测试中显著优于最先进的方法，证明了其在增强结构-文本对齐方面的有效性，并改善了互信息界限和贝叶斯误差保证。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Unsupervised Semantic-Aware Exposure Correction</div>
<div class="meta-line">Authors: Puzhen Wu, Han Weng, Quan Zheng, Yi Zhan, Hewei Wang, Yiming Li, Jiahui Han, Rui Xu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-27T02:53:18+00:00 · Latest: 2026-01-28T11:08:04+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19129v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19129v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP引导的无监督语义感知曝光校正</div>
<div class="mono" style="margin-top:8px">不当曝光常常导致严重的细节丢失、颜色失真和对比度降低。曝光校正仍面临两个关键挑战：（1）忽视对象级区域语义信息导致颜色偏移伪影；（2）真实世界的曝光图像通常没有真实标签，其标注需要大量手动编辑。为了解决这些挑战，我们提出了一种新的无监督语义感知曝光校正网络。它包含一个自适应语义感知融合模块，有效地将从预训练的快速分割任意模型中提取的语义信息融合到共享的图像特征空间中。然后，融合的特征被我们的多尺度残差空间mamba组用于恢复细节和调整曝光。为了避免手动编辑，我们提出了一种由CLIP引导的伪真实生成器，经过微调以自动识别曝光情况并指导定制校正。此外，我们利用来自FastSAM和CLIP的丰富先验，开发了一种语义提示一致性损失，以强制无监督训练中的语义一致性和图像提示对齐。全面的实验结果表明我们的方法在校正真实世界曝光图像方面的有效性，并在数值和视觉上超越了最先进的无监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of improper exposure in images, which can lead to detail loss and color distortion, particularly due to the lack of object-wise semantic information and the absence of ground-truth labels in real-world images. The authors propose an unsupervised semantic-aware exposure correction network that integrates a semantic-aware fusion module with features from a pre-trained Fast Segment Anything Model, allowing for effective detail restoration and exposure adjustment through a multi-scale residual spatial mamba group. Experimental results demonstrate that this method successfully corrects exposure in real-world images and surpasses existing unsupervised techniques in both numerical and visual assessments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决图像曝光不当所带来的挑战，这可能导致细节丢失、颜色失真和对比度降低。作者提出了一种无监督的语义感知曝光校正网络，该网络利用自适应语义感知融合模块，将预训练的快速分割任意模型提取的语义信息整合到共享图像特征空间中。实验结果表明，该方法有效地校正了真实世界的曝光图像，在数值和视觉效果上均优于现有的无监督技术。</div>
</details>
</div>
<div class="card">
<div class="title">GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning</div>
<div class="meta-line">Authors: Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu, Chen-Wei Xie, Zhaoyu Chen, Yun Zheng, Wenqiang Zhang</div>
<div class="meta-line">First: 2026-01-26T14:49:04+00:00 · Latest: 2026-01-28T08:36:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18543v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18543v2">PDF</a> · <a href="https://github.com/deep-kaixun/GenAgent}{this">Code1</a> · <a href="https://github.com/deep-kaixun/GenAgent">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenAgent：通过代理多模态推理扩展文本到图像生成</div>
<div class="mono" style="margin-top:8px">我们介绍了GenAgent，通过代理多模态模型统一视觉理解和生成。与面临高昂训练成本和理解-生成权衡的统一模型不同，GenAgent通过代理框架解耦这些能力：理解由多模态模型本身处理，而生成则通过将图像生成模型视为可调用工具来实现。至关重要的是，与受限于静态管道的现有模块化系统不同，这种设计使得自主多轮交互成为可能，代理生成包含推理、工具调用、判断和反思的多模态思维链，以迭代地优化输出。我们采用两阶段训练策略：首先，在高质量工具调用和反思数据上进行监督微调以启动代理行为；其次，结合点奖励（最终图像质量）和对偶奖励（反思准确性）的端到端代理强化学习，并进行轨迹重采样以增强多轮探索。GenAgent显著提升了基础生成器（FLUX.1-dev）在GenEval++（+23.6%）和WISE（+14%）上的表现。除了性能提升外，我们的框架还展示了三个关键特性：1）对具有不同能力的生成器的跨工具泛化，2）在交互轮次中一致改进的测试时间扩展，3）自动调整到不同任务的任务自适应推理。我们的代码将可在\href{https://github.com/deep-kaixun/GenAgent}{此网址}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve text-to-image generation by unifying visual understanding and generation through an agentic multimodal model, addressing the limitations of existing unified models that incur high training costs and face trade-offs between understanding and generation. The authors developed GenAgent, which separates these capabilities by employing an agentic framework that allows for autonomous multi-turn interactions, utilizing a two-stage training strategy that includes supervised fine-tuning and end-to-end agentic reinforcement learning. Key experimental findings indicate that GenAgent enhances the performance of the base generator (FLUX.1-dev) on GenEval++ by 23.6% and on WISE by 14%, while also demonstrating properties such as cross-tool generalization, test-time scaling, and task-adaptive reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过集成视觉理解和生成来增强文本到图像的生成能力，采用了一种代理多模态模型。作者开发了GenAgent，该模型将理解和生成过程分开，允许通过推理和工具调用进行自主的多轮交互，从而优化输出。实验结果表明，GenAgent在GenEval++上提高了基础生成器FLUX.1-dev的性能23.6%，在WISE上提高了14%，同时展示了跨工具泛化、一致的测试时扩展和任务自适应推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</div>
<div class="meta-line">Authors: Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi</div>
<div class="meta-line">First: 2024-12-02T08:25:09+00:00 · Latest: 2026-01-28T08:33:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.01256v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.01256v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representations and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NLPrompt：用于视觉-语言模型的噪声标签提示学习</div>
<div class="mono" style="margin-top:8px">视觉-语言基础模型的出现，如CLIP，彻底改变了图像-文本表示，通过提示学习实现了广泛的应用。尽管前景广阔，现实世界的数据集往往包含噪声标签，这可能会降低提示学习的性能。本文展示了在提示学习中使用均值绝对误差（MAE）损失，称为PromptMAE，显著增强了对噪声标签的鲁棒性，同时保持高准确性。尽管MAE简单且以其鲁棒性而闻名，但由于其收敛速度慢和在提示学习场景外表现不佳，MAE在噪声标签学习中很少使用。为了阐明PromptMAE的鲁棒性，我们利用特征学习理论表明，MAE可以抑制噪声样本的影响，从而提高信噪比并增强整体鲁棒性。此外，我们引入了PromptOT，一种基于提示的最优传输数据净化方法，以进一步增强鲁棒性。PromptOT利用视觉-语言模型中的文本特征作为原型构建最优传输矩阵。该矩阵有效地将数据集划分为干净和噪声子集，允许对干净子集应用交叉熵损失，对噪声子集应用MAE损失。我们提出的噪声标签提示学习方法NLPrompt，提供了一种简单高效的方法，利用视觉-语言模型的表达能力和精确对齐能力进行鲁棒提示学习。我们通过在各种噪声设置下进行广泛实验验证了NLPrompt，展示了显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by noisy labels in real-world datasets that can hinder the performance of prompt learning in vision-language models. The authors propose a method called PromptMAE, which utilizes mean absolute error loss to enhance robustness against noisy labels while maintaining accuracy. Experimental results show that PromptMAE significantly improves performance by suppressing the influence of noisy samples, and when combined with the PromptOT method for optimal data purification, it further partitions datasets into clean and noisy subsets, leading to notable performance enhancements across various noise settings.</div>
<div class="mono" style="margin-top:8px">本研究解决了现实世界数据集中噪声标签对视觉-语言模型提示学习性能的影响。作者提出了一种名为PromptMAE的方法，利用平均绝对误差损失来增强对噪声标签的鲁棒性，同时保持准确性。实验结果表明，PromptMAE通过抑制噪声样本的影响显著提高了性能，而PromptOT的引入进一步优化了数据净化，通过有效分离干净和噪声子集，导致在各种噪声设置下显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</div>
<div class="meta-line">Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:15:00+00:00 · Latest: 2026-01-28T08:15:00+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20354v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20354v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一切归位：文本到图像模型的空间智能基准测试</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在生成高保真图像方面取得了显著成功，但在处理复杂空间关系（例如空间感知、推理或交互）时常常失败。这些关键方面在当前基准测试中大多被忽视，因为它们的提示设计短小或信息稀疏。本文介绍了SpatialGenEval，这是一个旨在系统评估T2I模型空间智能的新基准，涵盖两个关键方面：（1）SpatialGenEval涉及25个真实场景中的1,230个长且信息密集的提示。每个提示整合了10个空间子领域和相应的10对多项选择问答，涵盖从物体位置和布局到遮挡和因果关系。我们对21个最先进模型的广泛评估表明，高阶空间推理仍然是主要瓶颈。（2）为了证明我们信息密集设计的实用性超越简单评估，我们还构建了SpatialT2I数据集。它包含15,400对文本-图像对，重写提示以确保图像一致性，同时保持信息密度。在当前基础模型（即Stable Diffusion-XL、Uniworld-V1、OmniGen2）上的微调结果显示出一致的性能提升（+4.2%、+5.7%、+4.4%）和更现实的空间关系效果，突显了实现T2I模型空间智能的数据中心范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of text-to-image (T2I) models in handling complex spatial relationships, which are often neglected in existing benchmarks. The authors introduce SpatialGenEval, a new benchmark that evaluates the spatial intelligence of T2I models using 1,230 detailed prompts across 25 real-world scenes, each containing multiple spatial sub-domains and corresponding questions. The evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning is a significant challenge, and the creation of the SpatialT2I dataset demonstrates that fine-tuning with information-dense prompts leads to consistent performance improvements in spatial relations across various models, indicating a data-centric approach to enhancing spatial intelligence in T2I systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像（T2I）模型在处理复杂空间关系方面的不足，这在现有基准中常常被忽视。作者引入了SpatialGenEval，一个包含1,230个长且信息密集的提示的基准，覆盖25个真实场景，旨在通过整合多个空间子领域和相应的问题-答案对来评估空间智能。对21个最先进模型的评估表明，高阶空间推理是一个显著的限制，而在新构建的SpatialT2I数据集上进行微调，包含15,400对文本-图像对，导致了稳定扩散-XL、Uniworld-V1和OmniGen2等模型的一致性能提升和更现实的空间关系。</div>
</details>
</div>
<div class="card">
<div class="title">All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations</div>
<div class="meta-line">Authors: Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan</div>
<div class="meta-line">First: 2026-01-02T02:20:57+00:00 · Latest: 2026-01-28T07:14:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00533v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00533v2">PDF</a> · <a href="https://github.com/Friskknight/ORCANet-SEUD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在平滑演变的未知天气退化下的全能视频修复</div>
<div class="mono" style="margin-top:8px">全能图像修复旨在通过单一模型从多种未知退化中恢复干净图像。但将此任务扩展到视频面临独特挑战。现有方法主要关注逐帧退化变化，忽视了现实世界退化过程中的时间连续性。在实践中，退化类型和强度随时间平滑演变，且多种退化可能共存或逐渐过渡。本文介绍了平滑演变的未知退化（SEUD）场景，其中主动退化集和退化强度随时间连续变化。为支持该场景，我们设计了一个灵活的合成管道，生成具有单一、复合和演变退化的时间一致性视频。为应对SEUD场景中的挑战，我们提出了一种全能递归条件自适应提示网络（ORCANet）。首先，粗略强度估计去雾（CIED）模块使用物理先验估计雾霾强度，并提供粗略去雾特征作为初始化。其次，流提示生成（FPG）模块提取退化特征。FPG生成捕捉段级退化类型的静态提示和适应帧级强度变化的动态提示。此外，标签感知监督机制提高了不同退化下静态提示表示的可区分性。大量实验表明，ORCANet在修复质量、时间一致性和鲁棒性方面优于基于图像和视频的基线。代码可在https://github.com/Friskknight/ORCANet-SEUD获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video restoration by addressing the challenges posed by smoothly evolving unknown weather degradations, which are often overlooked in existing methods. The authors propose a novel approach called ORCANet, which incorporates a Coarse Intensity Estimation Dehazing module for initial haze intensity estimation and a Flow Prompt Generation module for extracting both static and dynamic degradation features. Experimental results demonstrate that ORCANet significantly outperforms traditional image and video restoration techniques in terms of restoration quality, temporal consistency, and robustness against varying degradation types and intensities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决平滑演变的未知天气退化所带来的挑战，来增强视频恢复能力，而现有方法主要关注逐帧变化，未能考虑时间连续性。作者提出了一种新场景，称为平滑演变未知退化（SEUD），并提出了ORCANet，这是一种旨在处理静态和动态退化特征的全能递归条件自适应提示网络。实验结果表明，ORCANet在恢复质量、时间一致性和对各种退化类型的鲁棒性方面显著优于现有的图像和视频恢复技术。</div>
</details>
</div>
<div class="card">
<div class="title">MAnchors: Memorization-Based Acceleration of Anchors via Rule Reuse and Transformation</div>
<div class="meta-line">Authors: Haonan Yu, Junhao Liu, Xin Zhang</div>
<div class="meta-line">First: 2025-02-16T10:30:01+00:00 · Latest: 2026-01-28T07:12:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.11068v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.11068v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a memorization-based framework that accelerates Anchors while preserving explanation fidelity and interpretability. Our approach leverages the iterative nature of Anchors&#x27; algorithm which gradually refines an explanation until it is precise enough for a given input by storing and reusing intermediate results obtained during prior explanations. Specifically, we maintain a memory of low-precision, high-coverage rules and introduce a rule transformation framework to adapt them to new inputs: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAnchors：基于记忆的锚点加速通过规则重用和转换</div>
<div class="mono" style="margin-top:8px">锚点是一种流行的局部模型无关解释技术，但其适用性受到计算效率低下的限制。为了解决这一限制，我们提出了一种基于记忆的框架，旨在加速锚点，同时保持解释的保真性和可解释性。我们的方法利用了锚点算法的迭代特性，该算法逐步细化解释，直到对给定输入足够精确，通过存储和重用先前解释中获得的中间结果。具体而言，我们维护一个低精度、高覆盖率规则的记忆，并引入一个规则转换框架以适应新输入：水平转换通过替换特征将预训练的解释适应当前输入，垂直转换则细化一般解释，直到对输入足够精确。我们在表格、文本和图像数据集上评估了我们的方法，证明它显著减少了解释生成时间，同时保持保真性和可解释性，从而使锚点在时间敏感的应用中得以实际采用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational inefficiency of the Anchors explanation technique, which limits its applicability. To overcome this challenge, the authors propose a memorization-based framework that accelerates the Anchors method while ensuring the fidelity and interpretability of the explanations. Their approach involves storing and reusing intermediate results from previous explanations and includes a rule transformation framework that adapts low-precision, high-coverage rules to new inputs. Experimental results across various datasets indicate that this method significantly reduces explanation generation time without compromising the quality of the explanations, facilitating the use of Anchors in time-sensitive scenarios.</div>
<div class="mono" style="margin-top:8px">本研究解决了Anchors解释技术的计算低效问题，这限制了其在时间敏感场景中的应用。作者提出了一种基于记忆的框架，通过存储和重用先前解释中的中间结果来加速Anchors方法，并利用规则转换框架将解释适应于新输入。跨表格、文本和图像数据集的实验结果表明，该方法显著减少了解释生成时间，同时保持了解释的保真性和可解释性，从而促进了Anchors在各种应用中的实际使用。</div>
</details>
</div>
<div class="card">
<div class="title">SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks</div>
<div class="meta-line">Authors: Xin Zhang, Zijin Yang, Kejiang Chen, Linfeng Ma, Weiming Zhang, Nenghai Yu</div>
<div class="meta-line">First: 2026-01-28T07:02:40+00:00 · Latest: 2026-01-28T07:02:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20310v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20310v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider&#x27;s model, can embed the provider&#x27;s watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SemBind：将扩散水印绑定到语义以抵御黑箱伪造攻击</div>
<div class="mono" style="margin-top:8px">基于潜变量的水印集成到潜变量扩散模型（LDMs）的生成过程中，简化了生成图像的检测和归属。然而，最近的黑箱伪造攻击中，攻击者需要至少一张带水印的图像和对提供者模型的黑箱访问，可以将提供者的水印嵌入未由提供者生成的图像中，给来源和信任带来巨大风险。我们提出了SemBind，这是第一个抵御黑箱伪造的基于潜变量水印的防御框架，通过学习的语义掩码将潜在信号绑定到图像语义上。通过对比学习训练，掩码器为相同提示生成近乎不变的代码，并为不同提示生成近正交的代码；这些代码在任何标准基于潜变量的水印之前被重塑和置换，以调节目标潜变量。SemBind与现有的基于潜变量的水印方案普遍兼容，并保持图像质量基本不变，同时简单的掩码比例参数提供了反伪造强度和鲁棒性之间的可调权衡。在四种主流的基于潜变量的水印方法中，我们的SemBind启用的反伪造变体显著降低了黑箱伪造下的误接受率，同时提供了可控的鲁棒性-安全性平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of latent-based watermarks to black-box forgery attacks, which can compromise the authenticity of generated images. The authors introduce SemBind, a defense framework that binds latent signals to image semantics using a learned semantic masker trained through contrastive learning. Experimental results demonstrate that SemBind significantly reduces false acceptance rates in four mainstream latent-based watermarking methods while maintaining image quality and allowing for a tunable balance between anti-forgery strength and robustness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决潜在水印在黑箱伪造攻击下的脆弱性，这可能会损害生成图像的完整性和信任度。作者提出了SemBind，这是一种防御框架，通过使用经过对比学习训练的语义掩码器将潜在信号绑定到图像语义上。实验结果表明，SemBind在四种主流潜在水印方法中显著降低了黑箱伪造场景下的误接受率，同时保持了图像质量，并允许在反伪造强度和鲁棒性之间进行可调平衡。</div>
</details>
</div>
<div class="card">
<div class="title">OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion</div>
<div class="meta-line">Authors: Shuoyan Wei, Feng Li, Chen Zhou, Runmin Cong, Yao Zhao, Huihui Bai</div>
<div class="meta-line">First: 2026-01-28T06:59:55+00:00 · Latest: 2026-01-28T06:59:55+00:00</div>
<div class="meta-line">Comments: 17 pages, 10 figures. Code will be released upon publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20308v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20308v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSDEnhancer：通过一步扩散驯服真实世界的时空视频超分辨率</div>
<div class="mono" style="margin-top:8px">扩散模型（DMs）在视频超分辨率（VSR）中表现出色，展现了生成细致细节的强大能力。然而，它们在时空视频超分辨率（STVSR）中的潜力仍然未被充分探索，STVSR不仅需要从低分辨率恢复真实视觉内容，还需要提高帧率并保持一致的时间动态。此外，现有的STVSR方法主要在简化退化假设下处理时空上采样，往往在具有复杂未知退化的真实场景中表现不佳。对重建保真度和时间一致性的高需求使得开发一个稳健的STVSR框架变得尤为复杂。为了解决这些挑战，我们提出了OSDEnhancer，这是一种新颖的框架，据我们所知，它是第一个通过高效的一步扩散过程实现真实世界STVSR的方法。OSDEnhancer通过线性预插值策略初始化基本的时空结构，并依赖于训练时间精炼和空间增强专家混合（TR-SE MoE），使不同的专家路径逐步学习稳健的、专业化的时间一致性和空间细节表示，并在推理过程中相互协作强化。进一步引入了双向可变形变分自编码器（VAE）解码器，以执行递归时空聚合和传播，增强跨帧重建保真度。实验表明，所提出的方法在真实场景中实现了最先进的性能，同时保持了优越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve space-time video super-resolution (STVSR) in real-world scenarios, where existing methods struggle with complex degradations. The authors propose OSDEnhancer, a novel framework that utilizes a one-step diffusion process combined with a linear pre-interpolation strategy and a training mechanism called temporal refinement and spatial enhancement mixture of experts (TR-SE MoE) to enhance both temporal coherence and spatial detail. Experimental results show that OSDEnhancer achieves state-of-the-art performance in STVSR while demonstrating superior generalization capabilities in challenging real-world conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决空间时间视频超分辨率（STVSR）中的挑战，特别是在具有复杂退化的真实场景中，来增强视频超分辨率（VSR）。作者提出了OSDEnhancer，这是一种新颖的框架，采用一步扩散过程，利用线性预插值策略和一种称为时间精炼和空间增强专家混合（TR-SE MoE）的训练方法，以改善时间一致性和空间细节。实验结果表明，OSDEnhancer在STVSR中实现了最先进的性能，同时在真实应用中展现出优越的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration</div>
<div class="meta-line">Authors: Yanjie Tu, Qingsen Yan, Axi Niu, Jiacong Tang</div>
<div class="meta-line">First: 2026-01-28T06:55:07+00:00 · Latest: 2026-01-28T06:55:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20306v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://leoyjtu.github.io/tpgdiff-project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TPGDiff：用于图像恢复的层次三重先验引导扩散</div>
<div class="mono" style="margin-top:8px">一体化图像恢复旨在使用单一统一模型解决多种退化类型。现有方法通常依赖退化先验来指导恢复，但在严重退化区域重建内容时常常遇到困难。尽管最近的工作利用语义信息促进内容生成，但将其整合到扩散模型的浅层中往往会破坏空间结构（例如，模糊伪影）。为了解决这个问题，我们提出了一种用于统一图像恢复的三重先验引导扩散（TPGDiff）网络。TPGDiff在整个扩散轨迹中结合退化先验，同时将结构先验引入浅层，将语义先验引入深层，实现层次化和互补的先验指导图像重建。具体而言，我们利用多源结构线索作为结构先验，以捕捉细粒度细节并指导浅层表示。为了补充这一设计，我们进一步开发了一种基于蒸馏的语义提取器，产生稳健的语义先验，确保在严重退化下深层的高层次指导可靠。此外，采用退化提取器学习退化感知先验，实现对所有时间步的扩散过程的阶段自适应控制。在单一和多重退化基准上的广泛实验表明，TPGDiff在多种恢复场景中实现了卓越的性能和泛化能力。我们的项目页面是：https://leoyjtu.github.io/tpgdiff-project。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve all-in-one image restoration methods that struggle with severely degraded regions. The authors propose a Triple-Prior Guided Diffusion (TPGDiff) network that integrates degradation, structural, and semantic priors throughout the diffusion process to enhance image reconstruction. Experimental results show that TPGDiff outperforms existing methods on both single- and multi-degradation benchmarks, demonstrating superior performance and generalization across various restoration scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过有效处理各种退化类型来改善一体化图像修复。作者提出了一种三重先验引导扩散（TPGDiff）网络，该网络在扩散过程中整合了退化、结构和语义先验，以增强图像重建。实验结果表明，TPGDiff在单一和多重退化场景中均优于现有方法，显示出在修复严重退化图像方面的卓越性能和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</div>
<div class="meta-line">Authors: Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</div>
<div class="meta-line">First: 2026-01-28T06:54:06+00:00 · Latest: 2026-01-28T06:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构约束语言引导的无配对低剂量计算机断层扫描血管造影重建扩散模型</div>
<div class="mono" style="margin-top:8px">碘化对比剂（ICM）的应用提高了计算机断层扫描（CT）在广泛临床指征中的敏感性和特异性。然而，ICM过量会导致肾损伤和危及生命的过敏反应等问题。深度学习方法可以从低剂量ICM生成正常剂量ICM的CT图像，减少所需剂量，同时保持诊断能力。然而，现有方法在处理不完全配对图像时难以实现准确增强，主要是因为模型识别特定结构的能力有限。为克服这一限制，我们提出了一种结构约束语言引导的扩散模型（SLDM），这是一种统一的医学生成模型，集成了结构协同和空间智能。首先，有效提取图像的结构先验信息以约束模型推理过程，从而确保增强过程中的结构一致性。随后，引入具有空间智能的语义监督策略，集成视觉感知和空间推理的功能，从而促使模型实现准确增强。最后，应用减影血管造影增强模块，以改善ICM剂量区域的对比度，适合观察。定性视觉比较分析和多个指标的定量结果证明了我们的方法在低剂量对比剂CT血管造影重建中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by the overdose of iodinated contrast media (ICM) in computed tomography (CT), which can lead to serious health issues while aiming to maintain diagnostic quality. The authors propose a Structure-constrained Language-informed Diffusion Model (SLDM) that integrates structural prior information and spatial intelligence to enhance low-dose ICM images effectively. Experimental results show that the SLDM achieves significant improvements in angiographic reconstruction, as evidenced by qualitative visual comparisons and quantitative metrics, demonstrating its effectiveness in enhancing low-dose contrast medium CT angiography images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决碘造影剂（ICM）过量使用在计算机断层扫描（CT）中带来的挑战，这可能导致严重的健康问题，同时仍需保持诊断准确性。作者提出了一种结构约束的语言信息扩散模型（SLDM），该模型利用结构先验信息和语义监督策略有效地增强低剂量ICM图像。实验结果表明，SLDM显著提高了血管造影重建的质量，通过定性视觉比较和定量指标证明了其在增强低剂量造影剂CT血管造影中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training</div>
<div class="meta-line">Authors: Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, Lu Jiang</div>
<div class="meta-line">First: 2025-06-05T17:51:05+00:00 · Latest: 2026-01-28T05:55:42+00:00</div>
<div class="meta-line">Comments: Camera Ready of ICLR2026. Project page: https://iceclear.github.io/projects/seedvr2/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05301v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.05301v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iceclear.github.io/projects/seedvr2/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeedVR2：通过扩散对抗后训练实现一步视频恢复</div>
<div class="mono" style="margin-top:8px">最近在基于扩散的视频恢复（VR）方面的进展显示出视觉质量的显著提升，但在推理过程中却产生了高昂的计算成本。尽管几种基于蒸馏的方法展示了一步图像恢复的潜力，但将现有方法扩展到VR仍然具有挑战性且未被充分探索，特别是在处理现实环境中的高分辨率视频时。在本研究中，我们提出了一种一步扩散基VR模型，称为SeedVR2，该模型针对真实数据进行对抗性VR训练。为了在单步内处理具有挑战性的高分辨率VR，我们对模型架构和训练过程进行了多项增强。具体而言，提出了一种自适应窗口注意机制，其中窗口大小动态调整以适应输出分辨率，避免在使用预定义窗口大小的窗口注意下观察到的高分辨率VR中的窗口不一致性。为了稳定和改善对VR的对抗后训练，我们进一步验证了一系列损失的有效性，包括提出的特征匹配损失，而不会显著牺牲训练效率。大量实验表明，SeedVR2在一步内可以实现与现有VR方法相当甚至更好的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the high computational costs associated with diffusion-based video restoration (VR) methods, which have shown improvements in visual quality but are challenging to implement in real-world high-resolution scenarios. The authors propose SeedVR2, a one-step diffusion-based VR model that incorporates adversarial training with real data and introduces enhancements such as an adaptive window attention mechanism to dynamically adjust window sizes for high-resolution outputs. Experimental results demonstrate that SeedVR2 achieves performance that is comparable to or better than existing VR methods while operating in a single step.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决基于扩散的视频恢复在提高视觉质量的同时所带来的高计算成本。作者提出了一种名为SeedVR2的一步视频恢复模型，该模型利用对真实数据的对抗训练，并在模型架构和训练过程中引入了改进，包括自适应窗口注意机制。实验结果表明，SeedVR2在一步恢复中能够实现与现有视频恢复方法相当或更优的性能，有效应对高分辨率视频恢复的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs</div>
<div class="meta-line">Authors: Jiacheng Yang, Jun Wu, Yaoyao Ding, Zhiying Xu, Yida Wang, Gennady Pekhimenko</div>
<div class="meta-line">First: 2026-01-28T05:42:07+00:00 · Latest: 2026-01-28T05:42:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20273v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20273v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StreamFusion：用于GPU上扩展的扩散变换器分布式推理的可扩展序列并行性</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）在高质量图像和视频生成中得到了越来越广泛的应用。随着对更高分辨率图像和更长视频的需求增加，单GPU推理因延迟增加和激活大小过大而变得低效。目前的框架采用序列并行性（SP）技术，如尤利西斯注意力和环形注意力来扩展推理。然而，这些实现存在三个主要限制：（1）现代GPU机器上网络拓扑的次优通信模式，（2）机器间通信中的全到全操作导致的延迟瓶颈，以及（3）使用双向通信库导致的GPU发送者-接收者同步和计算开销。为了解决这些问题，我们提出了StreamFusion，一个拓扑感知的高效DiT服务引擎。StreamFusion结合了三个关键创新：（1）考虑机器间和机器内带宽差异的拓扑感知序列并行性技术，（2）环面注意力，一种新颖的SP技术，能够将机器间的全到全操作与计算重叠，以及（3）一种单向通信实现，最小化GPU发送者-接收者同步和计算开销。我们的实验表明，StreamFusion的性能比最先进的方法平均提高了$1.35\times$（最高可达$1.77\times$）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of inference for Diffusion Transformers (DiTs) in generating high-quality images and videos, particularly as the demand for higher resolutions and longer durations increases. The authors propose StreamFusion, a topology-aware serving engine that enhances sequence parallelism by addressing limitations in existing frameworks, such as inefficient communication patterns and latency bottlenecks. Key experimental results indicate that StreamFusion achieves an average performance improvement of 1.35 times over state-of-the-art methods, with a maximum improvement of 1.77 times.</div>
<div class="mono" style="margin-top:8px">本研究的动机是随着对高分辨率图像和更长视频的需求增加，单GPU推理在扩展性方面变得低效，导致延迟增加和激活大小增大。作者提出了StreamFusion，这是一种拓扑感知的服务引擎，旨在增强GPU上分布式推理的序列并行性。主要创新包括一种考虑机器间和机器内带宽差异的拓扑感知序列并行技术、一种新颖的Torus Attention方法，能够将机器间的全到全操作与计算重叠，以及一种单边通信实现，以减少同步开销。实验结果表明，StreamFusion在性能上平均比现有最先进的方法提高了1.35倍，最高可达1.77倍。</div>
</details>
</div>
<div class="card">
<div class="title">SiNGER: A Clearer Voice Distills Vision Transformers Further</div>
<div class="meta-line">Authors: Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-25T10:29:47+00:00 · Latest: 2026-01-28T05:18:25+00:00</div>
<div class="meta-line">Comments: Main paper: 12 pages (including 3 pages of references), 6 figures, 6 tables. Appendix: 9 pages, 7 figures. ICLR 2026 accepted</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20986v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.20986v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher&#x27;s features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SiNGER：更清晰的声音进一步提炼视觉变换器</div>
<div class="mono" style="margin-top:8px">视觉变换器被广泛采用作为视觉基础模型的骨干，但它们已知会产生高范数伪影，降低表示质量。当知识蒸馏将这些特征转移给学生时，高范数伪影主导了目标，使学生过度拟合伪影而低估信息信号，从而减少了来自更大模型的收益。之前的工作试图去除伪影，但在伪影抑制和保留教师信息信号之间存在固有的权衡。为了解决这个问题，我们引入了奇异零空间引导的能量重新分配（SiNGER），这是一种新颖的蒸馏框架，能够在抑制伪影的同时保留信息信号。关键思想是原则性的教师特征精炼：在精炼过程中，我们利用零空间引导的扰动来保留信息，同时抑制伪影。然后，将精炼后的教师特征蒸馏到学生中。我们通过基于LoRA的适配器高效地实现这种扰动，所需的结构修改最小。大量实验表明，\oursname 一致性地改善学生模型，在多个下游任务中实现了最先进的性能，并产生了更清晰和更可解释的表示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the representation quality of Vision Transformers, which are prone to producing high-norm artifacts that hinder effective knowledge distillation. The authors propose a novel framework called Singular Nullspace-Guided Energy Reallocation (SiNGER) that aims to suppress these artifacts while retaining informative signals during the distillation process. Experimental results demonstrate that SiNGER consistently enhances student models, achieving state-of-the-art performance across various downstream tasks and yielding clearer, more interpretable representations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视觉变换器的表示质量，因为它们容易产生高范数伪影，负面影响知识蒸馏。作者提出了一种名为单一零空间引导能量重新分配（SiNGER）的新框架，有效抑制这些伪影，同时保留来自教师模型的信息信号。实验结果表明，SiNGER持续改善学生模型，在多个下游任务中实现了最先进的性能，并产生了更清晰、更易解释的表示。</div>
</details>
</div>
<div class="card">
<div class="title">ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models</div>
<div class="meta-line">Authors: Chenxi Ruan, Yu Xiao, Yihan Hou, Guosheng Hu, Wei Zeng</div>
<div class="meta-line">First: 2026-01-23T15:36:02+00:00 · Latest: 2026-01-28T05:15:48+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16836v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16836v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ColorConceptBench：文本到图像模型中概率颜色概念理解的基准</div>
<div class="mono" style="margin-top:8px">尽管文本到图像（T2I）模型已经取得了显著进展，但它们将颜色与隐含概念关联的能力仍然未得到充分探索。为了解决这一空白，我们引入了ColorConceptBench，这是一个新的人工标注基准，通过概率颜色分布的视角系统地评估颜色概念关联。ColorConceptBench超越了显式颜色名称或代码，探讨模型如何利用6,369个人工注释的基础翻译1,281个隐含颜色概念。我们对七个领先的T2I模型的评估表明，当前模型对抽象语义缺乏敏感性，且这一局限性似乎对标准干预（例如缩放和引导）具有抵抗力。这表明，实现类人颜色语义不仅需要更大的模型，还需要在模型学习和表示隐含意义的方式上进行根本性转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the underexplored capability of text-to-image models in associating colors with implicit concepts. The authors introduce ColorConceptBench, a human-annotated benchmark designed to evaluate color-concept associations through probabilistic color distributions, utilizing 6,369 annotations to assess how models interpret 1,281 implicit color concepts. The evaluation of seven leading text-to-image models reveals a significant lack of sensitivity to abstract semantics, indicating that current models struggle with this aspect even when standard interventions are applied, suggesting that improving color semantics in models requires a fundamental change in their learning and representation processes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像模型在将颜色与隐含概念关联方面的不足。作者引入了ColorConceptBench，这是一个人类注释的基准，旨在通过概率颜色分布评估颜色概念关联，利用了1,281个隐含颜色概念和6,369个人工注释。对七个领先的文本到图像模型的评估显示，这些模型对抽象语义缺乏敏感性，并且这种局限性在标准干预下仍然存在，这表明提高人类般的颜色语义需要在模型学习和隐含意义的表示上进行根本性的转变。</div>
</details>
</div>
<div class="card">
<div class="title">BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning</div>
<div class="meta-line">Authors: Jan Niklas Kolf, Ozan Tezcan, Justin Theiss, Hyung Jun Kim, Wentao Bao, Bhargav Bhushanam, Khushi Gupta, Arun Kejariwal, Naser Damer, Fadi Boutros</div>
<div class="meta-line">First: 2026-01-28T04:44:40+00:00 · Latest: 2026-01-28T04:44:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20246v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BLENDER：用于深度度量学习中类内图像合成的混合文本嵌入和扩散残差</div>
<div class="mono" style="margin-top:8px">深度生成模型（DGM）的兴起使得高质量合成数据的生成成为可能。当用于增强深度度量学习（DML）中的真实数据时，这些合成样本提高了类内多样性，并改善了下游DML任务的性能。我们介绍了BLenDeR，这是一种扩散采样方法，旨在通过利用基于集合论的并集和交集操作在去噪残差上以可控的方式增加DML的类内多样性。并集操作鼓励在多个提示中存在的任何属性，而交集则通过主成分替代提取共同方向。这些操作使得在每个类别内可控地合成多样的属性组合，解决了现有生成方法的关键局限性。在标准DML基准上的实验表明，BLenDeR在多个数据集和骨干网络上始终优于最先进的基线。具体而言，BLenDeR在CUB-200上实现了3.7%的Recall@1提升，在Cars-196上实现了1.8%的提升，相较于标准实验设置下的最先进基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Deep Metric Learning (DML) tasks by increasing intra-class diversity through the use of synthetic data generated by Deep Generative Models (DGM). The authors introduce BLenDeR, a diffusion sampling method that employs set-theory inspired union and intersection operations on denoising residuals to achieve controlled synthesis of diverse attribute combinations within each class. Experimental results on standard DML benchmarks show that BLenDeR outperforms existing state-of-the-art methods, achieving a 3.7% increase in Recall@1 on the CUB-200 dataset and a 1.8% increase on the Cars-196 dataset under standard experimental conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过合成数据生成来增强深度度量学习（DML）任务的性能，从而增加类内多样性。作者提出了BLenDeR，这是一种扩散采样方法，利用去噪残差上的集合论启发操作来控制每个类内多样属性组合的合成。标准DML基准上的实验结果表明，BLenDeR的表现优于最先进的方法，在CUB-200数据集上Recall@1提高了3.7%，在Cars-196上提高了1.8%，证明了其在提升DML性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Prompt-Agnostic Evolution</div>
<div class="meta-line">Authors: Junze Wang, Lei Fan, Dezheng Zhang, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T04:06:44+00:00 · Latest: 2026-01-28T04:06:44+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉提示无关进化</div>
<div class="mono" style="margin-top:8px">视觉提示调优（VPT）通过在每层的令牌序列中插入少量可学习的提示令牌，将冻结的视觉变换器（ViT）适应于下游任务。然而，我们观察到现有的VPT变体往往面临不稳定的训练动态，表现为梯度振荡。逐层分析表明，浅层提示往往早期停滞，而深层提示则表现出高方差振荡，导致跨层不匹配。这些问题减缓了收敛速度并降低了最终性能。为了解决这些挑战，我们提出了提示无关进化（$\mathtt{PAE}$），通过显式建模提示动态来增强视觉提示调优。从频域的角度来看，我们通过揭示和传播主干在识别中固有利用的频率快捷模式，以任务感知的方向初始化提示。为了确保跨层的一致性进化，我们采用共享的库普曼算子，施加全局线性变换，而不是不协调的层特定更新。最后，受到李雅普诺夫稳定性理论的启发，我们引入了一个正则化项，以限制进化过程中的误差放大。大量实验表明，$\mathtt{PAE}$加速收敛，平均加速比为$1.41\times$，并在25个数据集的多个下游任务中提高了1-3%的准确率。除了性能，$\mathtt{PAE}$是无提示的且轻量级，能够与多种VPT变体无缝集成，而无需修改主干或在推理时进行更改。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the stability and performance of Visual Prompt Tuning (VPT) methods, which often face issues such as unstable training dynamics and cross-layer mismatches. The authors propose a new method called Prompt-Agnostic Evolution (PAE), which enhances VPT by modeling prompt dynamics more effectively and initializing prompts in a task-aware direction using frequency shortcut patterns. Experimental results demonstrate that PAE accelerates convergence by an average of 1.41 times and improves accuracy by 1-3% across 25 datasets in various downstream tasks, while remaining prompt-agnostic and compatible with different VPT variants.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉提示调优（VPT）方法的稳定性和性能，这些方法常常面临梯度振荡和收敛停滞等问题。作者提出了一种新的方法，称为提示无关演化（PAE），通过在任务感知方向上初始化提示并采用共享的库普曼算子来有效建模提示动态，从而确保跨层的一致演化。实验结果表明，PAE平均加速收敛1.41倍，并在25个数据集的多种下游任务中提高了1-3%的准确性，同时保持轻量级并与不同的VPT变体兼容。</div>
</details>
</div>
<div class="card">
<div class="title">DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment</div>
<div class="meta-line">Authors: Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu, Changxin Gao, Nong Sang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T03:39:05+00:00 · Latest: 2026-01-28T03:39:05+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20218v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20218v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DenseGRPO：从稀疏到密集奖励的流匹配模型对齐</div>
<div class="mono" style="margin-top:8px">基于GRPO的最新方法在文本到图像生成的人类偏好对齐方面取得了显著进展。然而，它们仍然面临稀疏奖励问题：整个去噪轨迹的终端奖励应用于所有中间步骤，导致全局反馈信号与中间去噪步骤的精细贡献之间的不匹配。为了解决这个问题，我们引入了\textbf{DenseGRPO}，一个新的框架，通过密集奖励对齐人类偏好，评估每个去噪步骤的精细贡献。具体而言，我们的方法包括两个关键组件：（1）我们提出预测每个去噪步骤的逐步奖励增益作为密集奖励，通过基于ODE的方法在中间干净图像上应用奖励模型。这种方式确保了反馈信号与各个步骤的贡献之间的对齐，促进有效训练；（2）基于估计的密集奖励，揭示了现有基于GRPO的方法中均匀探索设置与时间变化噪声强度之间的不匹配缺陷，导致不适当的探索空间。因此，我们提出了一种奖励感知方案，通过自适应调整SDE采样器中的时间步特定随机性注入来校准探索空间，确保在所有时间步上都有合适的探索空间。在多个标准基准上的广泛实验证明了所提出的DenseGRPO的有效性，并强调了有效密集奖励在流匹配模型对齐中的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the sparse reward problem in GRPO-based approaches for flow matching models, which affects the alignment of human preferences in text-to-image generation. The authors introduce DenseGRPO, a framework that provides dense rewards by predicting step-wise reward gains for each denoising step using an ODE-based approach, ensuring that feedback signals align with individual contributions. Experimental results on multiple benchmarks show that DenseGRPO effectively improves model alignment and highlights the importance of valid dense rewards in enhancing performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决GRPO基础方法在流匹配模型中存在的稀疏奖励问题，该问题导致文本到图像生成中全局反馈信号与中间去噪步骤的贡献之间的不匹配。作者提出了DenseGRPO框架，通过使用基于常微分方程的方法预测每个去噪步骤的逐步奖励增益，从而提供密集奖励，确保反馈信号与各个贡献之间的更好对齐。多项基准测试的实验结果表明，DenseGRPO有效改善了模型对齐，通过揭示和纠正探索空间的不匹配，强调了有效密集奖励在提升训练效果中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">TeleStyle: Content-Preserving Style Transfer in Images and Videos</div>
<div class="meta-line">Authors: Shiwen Zhang, Xiaoyan Yang, Bojia Zi, Haibin Huang, Chi Zhang, Xuelong Li</div>
<div class="meta-line">First: 2026-01-28T02:16:03+00:00 · Latest: 2026-01-28T02:16:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20175v1">PDF</a> · <a href="https://github.com/Tele-AI/TeleStyle">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model&#x27;s robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TeleStyle：图像和视频中的内容保留风格迁移</div>
<div class="mono" style="margin-top:8px">内容保留风格迁移，即基于内容和风格参考生成风格化输出，仍然是扩散变换器（DiTs）面临的重大挑战，因为其内部表示中内容和风格特征的固有纠缠。在本技术报告中，我们提出了TeleStyle，这是一种轻量且有效的图像和视频风格化模型。TeleStyle基于Qwen-Image-Edit，利用基础模型在内容保留和风格定制方面的强大能力。为了促进有效训练，我们策划了一个高质量的数据集，包含不同的特定风格，并进一步使用数千种多样的自然风格类别合成三元组。我们引入了一种课程持续学习框架，以在这个干净（策划）和嘈杂（合成）三元组的混合数据集上训练TeleStyle。这种方法使模型能够在不妥协精确内容保真度的情况下，推广到未见过的风格。此外，我们引入了一个视频到视频的风格化模块，以增强时间一致性和视觉质量。TeleStyle在三个核心评估指标上实现了最先进的性能：风格相似性、内容一致性和美学质量。代码和预训练模型可在https://github.com/Tele-AI/TeleStyle获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of content-preserving style transfer in images and videos, particularly for Diffusion Transformers (DiTs) where content and style features are often entangled. The authors developed TeleStyle, a lightweight model that utilizes the Qwen-Image-Edit base model to maintain content fidelity while allowing for style customization. Through the creation of a high-quality dataset and the implementation of a Curriculum Continual Learning framework, TeleStyle was trained on a mix of curated and synthetic style triplets, resulting in state-of-the-art performance in style similarity, content consistency, and aesthetic quality across various evaluation metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决使用扩散变换器进行图像和视频的内容保留风格转移时，内容和风格特征常常纠缠在一起的问题。作者开发了TeleStyle，这是一种轻量级模型，基于Qwen-Image-Edit，利用高质量的特定风格数据集和合成三元组，通过课程持续学习框架进行有效训练。实验结果表明，TeleStyle在风格相似性、内容一致性和美学质量方面达到了最先进的性能，同时还引入了视频到视频的风格化模块，以提高时间一致性和视觉质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
