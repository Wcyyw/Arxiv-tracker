<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 03:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0352</div>
    <div class="row"><div class="card">
<div class="title">From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</div>
<div class="meta-line">Authors: Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins</div>
<div class="meta-line">First: 2026-02-11T18:42:05+00:00 · Latest: 2026-02-11T18:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11130v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从电路到动力学：理解和稳定3D扩散变换器中的故障</div>
<div class="mono" style="margin-top:8px">从稀疏点云中可靠的表面补全支撑着内容创作和机器人等多个应用。虽然3D扩散变换器在这一任务上取得了最先进的结果，但我们发现它们表现出一种灾难性的故障模式：对输入点云的任意小的表面扰动会将输出分裂成多个不相连的部分——我们称之为“熔毁”。通过机械可解释性的激活修补，我们将熔毁局部化到一个早期去噪交叉注意力激活。我们发现该激活的奇异值谱提供了一个标量代理：当发生碎片化时，其谱熵上升，修补后返回基线。通过扩散动力学进行解释，我们表明该代理跟踪反向过程的对称性破缺分岔。在这一见解的指导下，我们引入了PowerRemap，这是一种在测试时控制稀疏点云条件的技术。我们证明熔毁在最先进的架构（WaLa，Make-a-Shape）、数据集（GSO，SimJEB）和去噪策略（DDPM，DDIM）中持续存在，并且PowerRemap有效地对抗这种故障，稳定率高达98.3%。总体而言，这项工作是一个案例研究，展示了如何基于机械分析理解和引导扩散模型行为，将电路级交叉注意力机制与轨迹分岔的扩散动力学联系起来。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the reliability issues in surface completion from sparse point clouds, particularly the catastrophic failure mode termed Meltdown, where small perturbations can lead to significant output fragmentation. The authors employ activation-patching from mechanistic interpretability to identify the source of Meltdown in the early denoising cross-attention activation and use the spectral entropy of this activation as a proxy for tracking fragmentation. Their findings reveal that Meltdown occurs across various architectures and datasets, and they introduce PowerRemap, a control mechanism that stabilizes the process, achieving stabilization rates of up to 98.3%, thereby demonstrating a connection between circuit-level mechanisms and diffusion dynamics in model behavior analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于实现稀疏点云的可靠表面补全，这对内容创作和机器人技术等应用至关重要。作者发现3D扩散变换器存在一种显著的失败模式，称为Meltdown，即输入的小扰动会导致输出碎片化。为了解决这个问题，他们使用机械解释中的激活修补技术，将问题定位到早期去噪交叉注意力激活，发现其谱熵可以作为跟踪碎片化的代理。研究者们引入了PowerRemap，这是一种稳定过程的控制方法，证明其在各种架构和数据集上的有效性，稳定率高达98.3%。本研究展示了机械分析如何为理解和改善扩散模型行为提供指导。</div>
</details>
</div>
<div class="card">
<div class="title">HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion</div>
<div class="meta-line">Authors: Di Chang, Ji Hou, Aljaz Bozic, Assaf Neuberger, Felix Juefei-Xu, Olivier Maury, Gene Wei-Chin Lin, Tuur Stuyck, Doug Roble, Mohammad Soleymani, Stephane Grabli</div>
<div class="meta-line">First: 2026-02-11T18:31:47+00:00 · Latest: 2026-02-11T18:31:47+00:00</div>
<div class="meta-line">Comments: Website: https://boese0601.github.io/hairweaver/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://boese0601.github.io/hairweaver/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject&#x27;s photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HairWeaver：基于模拟到真实引导的视频扩散的少量真实感头发运动合成</div>
<div class="mono" style="margin-top:8px">我们提出了HairWeaver，这是一种基于扩散的管道，可以为单个人类图像动画化出逼真且富有表现力的头发动态。虽然现有方法成功控制身体姿势，但缺乏对头发的具体控制，因此未能捕捉复杂的头发运动，导致动画僵硬且不真实。HairWeaver通过两个专门模块克服了这一限制：一个是Motion-Context-LoRA，用于整合运动条件；另一个是Sim2Real-Domain-LoRA，用于在不同数据域中保持主体的真实感外观。这些轻量级组件旨在引导视频扩散主干，同时保持其核心生成能力。通过在从CG模拟器生成的动态人类运动的专门数据集上进行训练，HairWeaver实现了对头发运动的精细控制，并最终学会生成对运动自然反应的高度真实的头发。全面评估表明，我们的方法设定了新的技术标准，生成具有动态细节的栩栩如生的人类头发动画。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind HairWeaver is to address the limitations of existing methods in controlling hair dynamics during human animation, which often results in stiff and unrealistic representations. The authors developed a diffusion-based pipeline that incorporates two specialized modules: Motion-Context-LoRA for integrating motion conditions and Sim2Real-Domain-LoRA for maintaining photorealistic appearance across various data domains. Experimental results show that HairWeaver achieves fine control over hair motion, producing highly realistic animations that respond naturally to movement, setting a new state of the art in lifelike human hair dynamics.</div>
<div class="mono" style="margin-top:8px">HairWeaver的动机在于提高人类形象中头发动画的真实感和表现力，解决现有方法未能捕捉复杂头发运动的局限性。该方法采用基于扩散的管道，配备两个专门模块：Motion-Context-LoRA用于整合运动条件，Sim2Real-Domain-LoRA用于在不同数据域中保持真实外观。实验结果表明，HairWeaver在生成自然响应运动的逼真头发动画方面达到了最先进的性能，通过在CG模拟器生成的动态人类运动的专门数据集上进行训练，展示了对头发动态的精细控制。</div>
</details>
</div>
<div class="card">
<div class="title">Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates</div>
<div class="meta-line">Authors: Carlos Stein Brito</div>
<div class="meta-line">First: 2026-02-11T17:57:20+00:00 · Latest: 2026-02-11T17:57:20+00:00</div>
<div class="meta-line">Comments: 13 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11090v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经 PDE 代理的校准感知不确定性直接学习</div>
<div class="mono" style="margin-top:8px">神经 PDE 代理通常在数据有限或部分观察的情况下部署，其中下游决策除了低预测误差外，还依赖于校准的不确定性。现有方法通过集成复制、固定随机噪声（如 dropout）或事后校准来获得不确定性。交叉正则化不确定性在训练过程中通过路由到保留的正则化拆分的梯度学习不确定性参数。预测器在训练拆分上进行拟合优化，而低维不确定性控制在正则化拆分上进行优化，以减少训练-测试不匹配，从而实现无每个区域噪声调优的区域自适应不确定性。该框架可以在输出头、隐藏特征或特定于算子的组件（如谱模式）中学习连续噪声水平。我们在傅里叶神经算子中实例化该方法，并在 APEBench 上对观察比例和训练集大小进行评估。在这些评估中，学习到的预测分布在保留拆分上更好地校准，结果不确定性场集中在一步空间诊断中的高误差区域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for calibrated uncertainty in neural PDE surrogates, particularly in data-limited scenarios where accurate downstream decisions are critical. The authors propose a method called cross-regularized uncertainty, which learns uncertainty parameters during training by routing gradients through a held-out regularization split, allowing the predictor to optimize for fit while controlling uncertainty to minimize train-test mismatch. Experimental results demonstrate that this approach, when applied to Fourier Neural Operators and evaluated on APEBench, yields better-calibrated predictive distributions on held-out splits and results in uncertainty fields that focus on high-error regions in spatial diagnostics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高神经 PDE 代理模型中不确定性的校准，特别是在数据有限的情况下，准确的不确定性量化对决策至关重要。作者提出了一种名为交叉正则化不确定性的方法，该方法通过将梯度路由通过保留的正则化拆分，在训练过程中学习不确定性参数，从而实现适应不同状态的不确定性，而无需针对每个状态进行噪声调优。实验结果表明，该方法在傅里叶神经算子上应用并在 APEBench 上评估时，能够在保留的拆分上产生更好校准的预测分布，并且在一步空间诊断中，不确定性场有效集中在高误差区域。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tienie Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-11T17:42:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像进行内省视觉思维的对话</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的文本推理，这往往导致细粒度视觉信息的丢失。最近提出的“通过图像思考”试图通过外部工具或代码操控图像来缓解这一限制；然而，生成的视觉状态往往与语言语义的基础不够牢固，影响了有效的跨模态对齐——特别是在需要跨越遥远区域或多幅图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了“通过图像对话”，这是一个将视觉操控重新框定为语言引导特征调制的新框架。在富有表现力的语言提示的指导下，该模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的紧密耦合。我们在ViLaVT中实例化这一范式，ViLaVT是一种新型的LVLM，配备了专门为这种交互式视觉推理设计的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了强大且一致的改进，尤其在复杂的多图像和基于视频的空间推理任务上表现出显著的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the reasoning capabilities of large vision-language models (LVLMs), which often lose fine-grained visual information due to their reliance on text-only reasoning and single-pass visual encoding. The authors propose a new framework called &#x27;&#x27;chatting with images&#x27;&#x27;, which reframes visual manipulation as language-guided feature modulation, allowing the model to dynamically re-encode multiple image regions under the guidance of expressive language prompts. The experimental results show that ViLaVT, the LVLM developed using this framework, achieves significant improvements across eight benchmarks, particularly excelling in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强大型视觉语言模型（LVLM）的推理能力，这些模型由于依赖文本推理和单次视觉编码，往往会丧失细粒度的视觉信息。作者提出了一种新的框架，称为“与图像对话”，将视觉操作重新定义为语言引导的特征调制，允许根据表达性语言提示动态重新编码多个图像区域。实验结果表明，在这一框架下开发的LVLM ViLaVT在八个基准测试中显著提高了性能，特别是在复杂的多图像和基于视频的空间推理任务中表现出色，得益于其有效整合语言推理和视觉状态更新。</div>
</details>
</div>
<div class="card">
<div class="title">Is In-Context Learning Learning?</div>
<div class="meta-line">Authors: Adrian de Wynter</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-12T17:12:04+00:00 · Latest: 2026-02-11T17:39:46+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026 -- CR version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10414v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.10414v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model&#x27;s ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL fits the definition of learning; however, its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that, empirically, ICL is limited in its ability to learn and generalise to unseen tasks. Namely, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input&#x27;s linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies and on formally similar tasks, we conclude that autoregression&#x27;s ad-hoc encoding is not a robust mechanism for learning, and suggests limited all-purpose generalisability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文学习算不算学习？</div>
<div class="mono" style="margin-top:8px">上下文学习（ICL）允许一些自回归模型通过下一个标记预测来解决任务，而无需进一步训练。这导致了关于这些模型能够仅通过提示中的少量示例解决（学习）未见任务的说法。然而，推理并不总意味着学习，因为ICL并未明确编码给定的观察。相反，模型依赖于其先前知识和给定的示例（如果有的话）。我们认为，从数学上讲，ICL符合学习的定义；然而，其完整特征化需要实证工作。然后，我们对ICL进行大规模分析，消除或考虑记忆、预训练、分布变化和提示风格及措辞。我们发现，从实证上看，ICL在学习和推广未见任务的能力上是有限的。即在示例数量增多的极限情况下，准确性对示例分布、模型、提示风格和输入的语言特征不敏感。相反，它从提示中的规律中推导出模式，这导致了分布敏感性，特别是在链式思维等提示风格中。鉴于不同的准确性和形式上相似的任务，我们得出结论，自回归的临时编码并不是一个稳健的学习机制，并且暗示了有限的通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to investigate the claims regarding in-context learning (ICL) in autoregressive models, particularly whether these models genuinely learn to solve unseen tasks with minimal examples. The authors conducted a large-scale analysis of ICL by examining factors such as memorization, pretraining, distributional shifts, and prompting styles. The key findings reveal that ICL has limitations in learning and generalizing to new tasks, as increased exemplars do not significantly affect accuracy across different distributions, models, or prompt styles, indicating that ICL relies more on recognizing patterns in prompts rather than robust learning mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨自回归模型中上下文学习（ICL）的相关论断，特别是这些模型是否真正能够通过少量示例学习解决未见任务。作者进行了大规模的ICL分析，考察了记忆、预训练、分布变化和提示风格等因素。主要发现表明，ICL在学习和推广新任务方面存在局限性，因为增加示例并未显著提高不同分布、模型或提示风格下的准确性，这表明ICL更多依赖于从提示中推导模式，而非稳健的学习机制。</div>
</details>
</div>
<div class="card">
<div class="title">Language Model Inversion through End-to-End Differentiation</div>
<div class="meta-line">Authors: Kevin Yandoka Denamganaï, Kartic Subr</div>
<div class="meta-line">First: 2026-02-11T17:14:41+00:00 · Latest: 2026-02-11T17:14:41+00:00</div>
<div class="meta-line">Comments: 24 pages, 5 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11044v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过端到端微分实现语言模型反演</div>
<div class="mono" style="margin-top:8px">尽管关于语言模型（LM）的研究不断涌现，但很少有方法分析LM的可逆性。也就是说，给定一个LM和一个期望的目标输出序列，确定哪些输入提示可以产生目标输出仍然是一个未解决的问题。我们将这个问题表述为经典的基于梯度的优化。首先，我们提出了一种简单的算法，以实现给定（冻结）LM的端到端可微性，然后通过梯度下降找到优化的提示。我们的核心见解是将LM视为在令牌分布序列上操作的函数（而不是传统的在令牌序列上操作的函数）。我们的实验和消融实验表明，我们的DLM驱动的反演可以可靠且高效地优化长度为$10$和$80$的提示，以达到长度为$20$的目标，适用于几种白盒LM（开箱即用）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of inverting Language Models (LMs) to determine input prompts that yield specific target output sequences. The authors propose a gradient-based optimization approach, introducing an algorithm that achieves end-to-end differentiability of a frozen LM, allowing for the optimization of prompts through gradient descent. Experimental results indicate that their method can reliably and efficiently optimize prompts of lengths 10 and 80 for target outputs of length 20 across various white-box LMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从语言模型（LM）中确定输入提示以产生所需输出序列的挑战，这是一个尚未得到充分探讨的问题。作者提出了一种新方法，将LM的反演形式化为基于梯度的优化问题，介绍了一种算法，使得冻结的LM能够实现端到端的可微分性。实验结果表明，这种被称为DLM驱动反演的方法能够有效且可靠地优化不同长度的提示以获得目标输出，在多个白盒LM上取得了成功。</div>
</details>
</div>
<div class="card">
<div class="title">GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models</div>
<div class="meta-line">Authors: Peter Holderrieth, Uriel Singer, Tommi Jaakkola, Ricky T. Q. Chen, Yaron Lipman, Brian Karrer</div>
<div class="meta-line">First: 2025-09-29T17:58:36+00:00 · Latest: 2026-02-11T17:01:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25170v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.25170v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a &quot;flow matching model within a flow matching model&quot; to sample Markov transitions. As we show in this work, this &quot;inner&quot; flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLASS Flows：流动和扩散模型对齐的过渡采样</div>
<div class="mono" style="margin-top:8px">使用奖励对齐算法可以在推理时显著提高流动匹配和扩散模型的性能，但效率仍然是一个主要限制。虽然提出了几种算法，但我们证明了一个共同的瓶颈是这些算法依赖的采样方法：许多算法需要通过SDE采样来采样马尔可夫转移，这比ODE采样效率低且性能差。为了消除这个瓶颈，我们引入了GLASS Flows，一种新的采样范式，它模拟“流动匹配模型中的流动匹配模型”来采样马尔可夫转移。正如我们在这项工作中所展示的，这个“内部”流动匹配模型可以从预训练模型中检索，而无需任何重新训练，结合了ODE的效率和SDE的随机演化。在大规模文本到图像模型上，我们展示了GLASS Flows消除了随机演化与效率之间的权衡。结合Feynman-Kac Steering，GLASS Flows在文本到图像生成中提高了最先进的性能，使其成为流动和扩散模型推理时扩展的简单、即插即用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of flow matching and diffusion models during inference, as existing reward alignment algorithms face limitations due to their reliance on less efficient sampling methods. The authors introduce GLASS Flows, a novel sampling paradigm that simulates a flow matching model within another flow matching model to sample Markov transitions, leveraging the efficiency of ordinary differential equations (ODEs) while maintaining the stochastic characteristics of stochastic differential equations (SDEs). Experimental results demonstrate that GLASS Flows effectively eliminate the trade-off between stochastic evolution and efficiency, leading to improved performance in text-to-image generation tasks when combined with Feynman-Kac Steering, thus providing a straightforward solution for scaling flow and diffusion models at inference time.</div>
<div class="mono" style="margin-top:8px">本研究针对流匹配和扩散模型在推理过程中的低效率问题，特别是现有依赖于SDE采样的采样方法的局限性。作者提出了GLASS Flows，这是一种新颖的采样范式，利用从预训练模型中提取的内部流匹配模型，以比传统方法更高效地采样马尔可夫转移。实验结果表明，GLASS Flows有效消除了随机演化与效率之间的权衡，结合Feynman-Kac Steering在文本到图像生成中显著提高了性能，从而增强了流和扩散模型的能力，无需重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics</div>
<div class="meta-line">Authors: Nima Shoghi, Yuxuan Liu, Yuning Shen, Rob Brekelmans, Pan Li, Quanquan Gu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T14:13:28+00:00 · Latest: 2026-02-11T16:42:29+00:00</div>
<div class="meta-line">Comments: 49 pages, 28 figures. Accepted by ICLR 2026. Project page: https://bytedance-seed.github.io/ConfRover/starmd</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02128v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02128v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bytedance-seed.github.io/ConfRover/starmd">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD&#x27;s joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展的时空SE(3)扩散模型用于长时间尺度蛋白质动力学</div>
<div class="mono" style="margin-top:8px">分子动力学（MD）模拟仍然是研究蛋白质动力学的金标准，但其计算成本限制了对生物相关时间尺度的访问。最近的生成模型在加速模拟方面显示出希望，但由于架构限制、误差积累和时空动力学建模不足，它们在长时间生成方面面临挑战。我们提出了STAR-MD（分子动力学的时空自回归展开），这是一种可扩展的SE(3)等变扩散模型，能够在微秒时间尺度上生成物理上合理的蛋白质轨迹。我们的关键创新是具有联合时空注意力的因果扩散变换器，能够有效捕捉复杂的时空依赖关系，同时避免现有方法的内存瓶颈。在标准ATLAS基准测试中，STAR-MD在所有指标上都达到了最先进的性能——相比于之前的方法，显著提高了构象覆盖率、结构有效性和动态保真度。STAR-MD成功外推生成稳定的微秒级轨迹，而基线方法则在此失败，且在扩展展开过程中保持高结构质量。我们的全面评估揭示了当前模型在长时间生成方面的严重局限，同时证明了STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of molecular dynamics simulations in studying protein dynamics due to their high computational cost and inability to effectively model long-horizon generation. The authors introduce STAR-MD, a scalable SE(3)-equivariant diffusion model that utilizes a causal diffusion transformer with joint spatio-temporal attention to generate physically plausible protein trajectories over microsecond timescales. Experimental results on the ATLAS benchmark demonstrate that STAR-MD outperforms existing methods in conformational coverage, structural validity, and dynamic fidelity, successfully generating stable microsecond-scale trajectories while maintaining high structural quality, thus highlighting the potential for enhanced exploration of protein functions.</div>
<div class="mono" style="margin-top:8px">该研究解决了分子动力学模拟在研究蛋白质动态时由于高计算成本而面临的限制，特别是在长时间生成方面。作者提出了STAR-MD，这是一种可扩展的SE(3)等变扩散模型，利用具有联合时空注意力的因果扩散变换器生成微秒级的蛋白质轨迹。ATLAS基准测试的实验结果表明，STAR-MD在构象覆盖、结构有效性和动态保真度方面优于现有方法，成功生成稳定的微秒级轨迹，同时保持高结构质量，从而突显了该模型在生物学相关时间尺度下进行稳健动态模拟的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Localized Control in Diffusion Models via Latent Vector Prediction</div>
<div class="meta-line">Authors: Pablo Domingo-Gregorio, Javier Ruiz-Hidalgo</div>
<div class="meta-line">First: 2026-02-02T11:47:48+00:00 · Latest: 2026-02-11T16:40:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01991v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01991v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在向量预测实现扩散模型中的局部控制</div>
<div class="mono" style="margin-top:8px">扩散模型作为文本到图像生成的领先方法，从文本描述中生成高质量图像。然而，仅通过文本实现对所需图像的详细控制仍然是一项繁琐的试错工作。最近的方法引入了图像级控制与文本提示，利用先前的图像提取条件信息，如边缘、分割和深度图。尽管有效，这些方法在整个图像上均匀应用条件，限制了局部控制。在本文中，我们提出了一种新方法，使用户定义的图像区域能够实现精确的局部控制，同时将扩散模型的任务留给其自主生成根据原始提示的其余区域。我们的方法引入了一种新的训练框架，结合了掩蔽特征和额外的损失项，利用在任何扩散步骤中对初始潜在向量的预测来增强当前步骤与潜在空间中最终样本之间的对应关系。大量实验表明，我们的方法有效合成了具有受控局部条件的高质量图像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve localized control in diffusion models for text-to-image generation, as existing methods often require laborious trial-and-error to achieve desired image details. The authors propose a novel methodology that allows for precise control over specific regions of an image while enabling the diffusion model to generate the remaining areas based on the original text prompt. Their approach involves a new training framework that uses masking features and an additional loss term to enhance the relationship between the current diffusion step and the final sample in latent space. Experimental results indicate that this method successfully synthesizes high-quality images with controlled local conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善扩散模型中图像生成的控制能力，传统方法主要依赖文本提示，通常需要大量的试错才能达到预期效果。作者提出了一种新方法，使得用户能够对图像的特定区域进行精确的局部控制，同时让扩散模型根据原始提示自主生成其余区域。该方法采用了一种新的训练框架，利用掩模特征和额外的损失项来增强当前扩散步骤与潜在空间中最终样本之间的关系。实验结果表明，该方法成功合成了具有局部控制的高质量图像，解决了先前技术的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules</div>
<div class="meta-line">Authors: Ivan Vulić, Adam Grycner, Quentin de Laroussilhe, Jonas Pfeiffer</div>
<div class="meta-line">First: 2026-02-11T16:19:58+00:00 · Latest: 2026-02-11T16:19:58+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10993v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10993v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoRA-Squeeze：简单有效的LoRA模块后调优和内调优压缩</div>
<div class="mono" style="margin-top:8px">尽管有大量变体，标准低秩适应（LoRA）仍然是参数高效微调（PEFT）的主导技术。然而，它面临着持续的挑战，包括最佳秩的预选择和特定秩的超参数，以及异构秩模块和更复杂的LoRA衍生物的部署复杂性。在本研究中，我们介绍了LoRA-Squeeze，这是一种简单高效的方法，旨在通过在训练过程中动态或事后改变LoRA模块的秩来改进标准LoRA学习。我们的方法认为，首先学习一个表现力强的高秩解决方案，然后进行压缩，而不是直接学习一个受限的低秩解决方案是更好的。该方法涉及使用故意较高的源秩进行微调，重构或有效近似全权重更新矩阵的重构，然后使用随机奇异值分解（RSVD）在较低目标秩下创建一个新的压缩LoRA模块。在13个文本和10个视觉语言任务上的广泛实验表明，事后压缩通常产生的低秩适配器优于那些直接在目标秩下训练的适配器，尤其是在允许在目标秩下进行少量微调步骤的情况下。此外，LoRA-Squeeze的逐步内调秩退火变体始终实现最佳的LoRA大小-性能权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by standard Low-Rank Adaptation (LoRA) in parameter-efficient fine-tuning, particularly regarding optimal rank selection and deployment complexity. The authors propose LoRA-Squeeze, a methodology that enhances LoRA learning by allowing for dynamic rank adjustments either post-hoc or during training, advocating for an initial higher-rank learning followed by compression. Experimental results across 13 text and 10 vision-language tasks demonstrate that post-hoc compression yields lower-rank adapters that outperform those trained directly at the target rank, with a gradual in-tuning rank annealing variant showing the best trade-off between LoRA size and performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决标准低秩适应（LoRA）在参数高效微调中面临的挑战，特别是最优秩选择和部署复杂性的问题。作者提出了一种新方法，称为LoRA-Squeeze，该方法首先学习一个高秩解决方案，然后进行压缩，而不是从低秩解决方案开始。通过对13个文本任务和10个视觉语言任务的广泛实验，结果表明，后期压缩通常产生的低秩适配器优于直接在目标秩上训练的适配器，而逐渐的微调秩退火变体在LoRA大小和性能之间表现出最佳的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models</div>
<div class="meta-line">Authors: Mingyu Cao, Alvaro Correia, Christos Louizos, Shiwei Liu, Lu Yin</div>
<div class="meta-line">First: 2026-02-11T15:41:09+00:00 · Latest: 2026-02-11T15:41:09+00:00</div>
<div class="meta-line">Comments: 11 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10953v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10953v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model&#x27;s uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>搜索或加速：用于扩散语言模型的置信切换位置束搜索</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）通过迭代去噪一个被屏蔽的序列来生成文本，反复决定在每一步中承诺哪些位置。标准解码遵循贪婪规则：解锁最有信心的位置，但这种局部选择可能会使模型锁定在次优的解锁顺序，特别是在推理密集的提示上。我们提出了SOAR，一种无训练的解码算法，它根据模型的不确定性调整其行为。当置信度低时，SOAR暂时扩大对替代解锁决策的搜索，以避免过早承诺；当置信度高时，它收缩搜索并并行解码多个位置，以减少去噪迭代的次数。在Dream-7B和LLaDA-8B上的数学推理和代码生成基准（GSM8K、MBPP、HumanEval）中，SOAR提高了生成质量，同时保持了竞争性的推理速度，提供了一种在DLM解码中平衡质量和效率的实用方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the decoding process of Diffusion Language Models (DLMs), which often leads to suboptimal text generation due to a greedy unmasking strategy. The authors propose a new decoding algorithm called SOAR, which adjusts its approach based on the model&#x27;s confidence levels; it expands the search for unmasking decisions when confidence is low and accelerates the process when confidence is high. Experimental results demonstrate that SOAR enhances the quality of generated text on benchmarks such as GSM8K, MBPP, and HumanEval while maintaining competitive inference speeds, thus providing a practical solution for balancing quality and efficiency in DLM decoding.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善扩散语言模型（DLMs）的解码过程，因其贪婪的解掩策略常导致文本生成不理想。作者提出了一种新的解码算法SOAR，根据模型的置信度调整其方法；当置信度低时，扩大解掩决策的搜索范围，而当置信度高时，加速解码。实验结果表明，SOAR在GSM8K、MBPP和HumanEval等基准测试中提高了生成文本的质量，同时保持了竞争性的推理速度，从而有效地平衡了DLM解码中的质量和效率。</div>
</details>
</div>
<div class="card">
<div class="title">SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices</div>
<div class="meta-line">Authors: Dongting Hu, Aarush Gupta, Magzhan Gabidolla, Arpit Sahni, Huseyin Coskun, Yanyu Li, Yerlan Idelbayev, Ahsan Mahmood, Aleksei Lebedev, Dishani Lahiri, Anujraaj Goyal, Ju Hu, Mingming Gong, Sergey Tulyakov, Anil Kag</div>
<div class="meta-line">First: 2026-01-13T07:46:46+00:00 · Latest: 2026-02-11T15:25:31+00:00</div>
<div class="meta-line">Comments: Project page: https://snap-research.github.io/snapgenplusplus/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08303v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08303v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://snap-research.github.io/snapgenplusplus/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SnapGen++：释放扩散变换器在边缘设备上高效高保真图像生成的潜力</div>
<div class="mono" style="margin-top:8px">最近在扩散变换器（DiTs）方面的进展为图像生成设定了新标准，但由于其高计算和内存成本，仍不适合在设备上部署。在本研究中，我们提出了一种高效的DiT框架，专为移动和边缘设备量身定制，在严格的资源限制下实现变换器级别的生成质量。我们的设计结合了三个关键组件。首先，我们提出了一种紧凑的DiT架构，采用自适应的全局-局部稀疏注意机制，平衡全局上下文建模和局部细节保留。其次，我们提出了一种弹性训练框架，在统一的超网络中联合优化不同容量的子-DiTs，使单个模型能够动态调整以在不同硬件上实现高效推理。最后，我们开发了知识引导的分布匹配蒸馏，这是一种将DMD目标与来自少步教师模型的知识转移相结合的步骤蒸馏管道，生成高保真和低延迟的输出（例如，4步），适合实时设备使用。这些贡献共同使得可扩展、高效和高质量的扩散模型能够在多种硬件上部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the impracticality of deploying diffusion transformers for image generation on mobile and edge devices due to their high computational and memory requirements. The authors present an efficient diffusion transformer framework that includes a compact architecture with adaptive global-local sparse attention, an elastic training framework for optimizing varying capacities within a supernetwork, and a Knowledge-Guided Distribution Matching Distillation method for high-fidelity, low-latency image generation. Experimental results demonstrate that this approach achieves transformer-level generation quality while being suitable for real-time use on constrained hardware.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散变换器在边缘设备上进行图像生成时由于高计算和内存需求而导致的不可行性。作者提出了一种高效的扩散变换器框架，包括具有自适应全局-局部稀疏注意力机制的紧凑架构、用于优化不同容量模型的弹性训练框架，以及用于高保真、低延迟图像生成的知识引导分布匹配蒸馏方法。主要发现表明，该方法在资源受限的硬件上实现了变换器级别的生成质量，适合实时使用。</div>
</details>
</div>
<div class="card">
<div class="title">CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control</div>
<div class="meta-line">Authors: Riccardo Barbano, Alexander Denker, Zeljko Kereta, Runchang Li, Francisco Vargas</div>
<div class="meta-line">First: 2026-02-11T15:12:43+00:00 · Latest: 2026-02-11T15:12:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10933v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10933v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMAD：通过随机最优控制实现的协作多智能体扩散</div>
<div class="mono" style="margin-top:8px">连续时间生成模型在图像恢复和合成方面取得了显著成功。然而，控制多个预训练模型的组合仍然是一个未解决的挑战。目前的方法主要将组合视为概率密度的代数组合，例如通过乘积或专家混合。这种观点假设目标分布是明确已知的，但这种情况几乎从未发生。在本研究中，我们提出了一种不同的范式，将组合生成形式化为一个协作随机最优控制问题。我们不再简单地组合概率密度，而是将预训练的扩散模型视为相互作用的智能体，其扩散轨迹通过最优控制共同引导，朝向在其聚合输出上定义的共享目标。我们在条件MNIST生成上验证了我们的框架，并将其与一种简单的推理时间DPS风格基线进行比较，后者用每步梯度指导替代了学习的协作控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of controlling the composition of multiple pre-trained generative models, which is often treated as algebraic combinations of probability densities. The authors propose a novel approach that frames compositional generation as a cooperative Stochastic Optimal Control problem, where pre-trained diffusion models are treated as interacting agents whose diffusion paths are optimized towards a common goal. Experimental results demonstrate that this framework outperforms a naive baseline that relies on per-step gradient guidance in generating conditional MNIST images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决控制多个预训练生成模型组合的挑战，这对于图像恢复和合成等任务至关重要。作者提出了一种新方法，将组合生成框架视为合作随机最优控制问题，将预训练扩散模型视为相互作用的代理，其扩散路径经过优化以实现共同目标。实验结果表明，该框架在条件MNIST生成的背景下优于依赖逐步梯度引导的简单基线。</div>
</details>
</div>
<div class="card">
<div class="title">Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance</div>
<div class="meta-line">Authors: Rashid Mushkani</div>
<div class="meta-line">First: 2026-02-11T14:53:58+00:00 · Latest: 2026-02-11T14:53:58+00:00</div>
<div class="meta-line">Comments: Presented at PAIRS: Participatory AI Research &amp; Practice Symposium</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Participatory approaches are widely invoked in AI governance, yet participation rarely translates into durable influence. In public sector and civic AI systems, community contributions such as deliberations, annotations, prompts, and incident reports are often recorded informally, weakly linked to system updates, and disconnected from enforceable rights or sustained compensation. As a result, participation is frequently symbolic rather than accountable. We introduce the Participation Ledger, a machine readable and auditable framework that operationalizes participation as traceable influence, enforceable authority, and compensable labor. The ledger represents participation as an influence graph that links contributed artifacts to verified changes in AI systems, including datasets, prompts, adapters, policies, guardrails, and evaluation suites. It integrates three elements: a Participation Evidence Standard documenting consent, privacy, compensation, and reuse terms; an influence tracing mechanism that connects system updates to replayable before and after tests, enabling longitudinal monitoring of commitments; and encoded rights and incentives. Capability Vouchers allow authorized community stewards to request or constrain specific system capabilities within defined boundaries, while Participation Credits support ongoing recognition and compensation when contributed tests continue to provide value. We ground the framework in four urban AI and public space governance deployments and provide a machine readable schema, templates, and an evaluation plan for assessing traceability, enforceability, and compensation in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可追溯、可执行和可补偿的参与：以人为本的人工智能治理参与账本</div>
<div class="mono" style="margin-top:8px">参与式方法在人工智能治理中被广泛提及，但参与很少转化为持久的影响。在公共部门和公民人工智能系统中，社区贡献如讨论、注释、提示和事件报告通常以非正式方式记录，与系统更新的联系较弱，并且与可执行权利或持续补偿脱节。因此，参与往往是象征性的，而非负责任的。我们引入了参与账本，这是一个机器可读和可审计的框架，将参与操作化为可追溯的影响、可执行的权威和可补偿的劳动。该账本将参与表示为影响图，将贡献的文物与人工智能系统中的经过验证的变化链接起来，包括数据集、提示、适配器、政策、保护措施和评估套件。它整合了三个要素：记录同意、隐私、补偿和重用条款的参与证据标准；将系统更新与可重放的前后测试连接起来的影响追踪机制，使承诺的纵向监测成为可能；以及编码的权利和激励。能力凭证允许授权的社区管理者在定义的边界内请求或限制特定的系统能力，而参与积分则支持在贡献的测试继续提供价值时的持续认可和补偿。我们在四个城市人工智能和公共空间治理的部署中为该框架提供基础，并提供机器可读的模式、模板和评估计划，以评估实践中的可追溯性、可执行性和补偿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of participatory approaches in AI governance, as current methods often result in symbolic rather than accountable participation. The authors propose the Participation Ledger, a machine-readable framework that operationalizes participation by linking community contributions to verified changes in AI systems through an influence graph. Key experimental findings demonstrate the framework&#x27;s effectiveness in four urban AI governance deployments, showcasing its ability to provide traceable influence, enforceable authority, and compensable labor, while also offering a schema and evaluation plan for practical implementation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强AI治理中参与方法的有效性，因为当前的方法往往导致象征性而非负责任的参与。作者提出了参与账本，这是一个机器可读的框架，通过可追踪的影响、可执行的权威和可补偿的劳动来实现参与。该框架在四个城市AI治理部署中的实施的主要发现包括建立参与证据标准、用于监测系统更新的影响追踪机制，以及引入能力凭证和参与积分，以确保对社区贡献的持续认可和补偿。</div>
</details>
</div>
<div class="card">
<div class="title">Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation</div>
<div class="meta-line">Authors: Nadeem Nazer, Hongkuan Zhou, Lavdim Halilaj, Ylli Sadikaj, Steffen Staab</div>
<div class="meta-line">First: 2025-12-10T09:19:17+00:00 · Latest: 2026-02-11T12:13:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09446v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09446v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like &quot;hole&quot;, &quot;cut&quot;, &quot;scratch&quot; that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of &quot;abnormal&quot; with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于渐进调优的缺陷感知混合提示优化用于零-shot多类型异常检测与分割</div>
<div class="mono" style="margin-top:8px">最近的视觉语言模型（VLMs）如CLIP在显著分布变化下展示了令人印象深刻的异常检测性能，通过文本提示利用高层次的语义信息。然而，这些模型往往忽视了细粒度的细节，例如“孔”、“切口”、“划痕”等异常类型，这些细节可以提供更具体的异常性质洞察。我们认为，识别细粒度异常类型1）丰富了“异常”的结构语义表示，缩小了粗糙异常信号与细粒度缺陷类别之间的差距；2）使制造商能够快速理解异常的根本原因并实施更有针对性和适当的纠正措施。尽管纳入这些详细的语义信息至关重要，但为每种缺陷类型设计手工提示既耗时又容易受到人为偏见的影响。因此，我们提出了DAPO，一种基于渐进调优的缺陷感知提示优化的新方法，用于在分布变化下进行零-shot多类型和二元异常检测与分割。我们的方法通过学习具有固定文本锚点和可学习的标记嵌入的混合缺陷感知提示，将与异常相关的图像特征与其对应的文本语义对齐。我们在公共基准（MPDD、VisA、MVTec-AD、MAD和Real-IAD）和一个内部数据集上进行了实验。结果表明，与基线模型相比，DAPO在分布变化下在图像级别的AUROC和平均精度指标上平均提高了3.7%，在零-shot设置下定位新异常类型的平均提高了6.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance anomaly detection in vision language models by incorporating fine-grained details about different types of anomalies, which can help manufacturers identify root causes and implement corrective measures more effectively. The authors propose a novel method called Defect-aware Prompt Optimization (DAPO), which utilizes progressive tuning to create hybrid prompts that align image features with corresponding text semantics for zero-shot multi-type anomaly detection and segmentation. Experimental results on various public benchmarks and an internal dataset show that DAPO outperforms baseline models, achieving a 3.7% average improvement in AUROC and average precision metrics at the image level, and a 6.5% improvement in localizing novel anomaly types in zero-shot scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过纳入不同类型缺陷的细粒度细节来增强异常检测，这可以帮助制造商更有效地识别根本原因并实施纠正措施。作者提出了一种名为缺陷感知提示优化（DAPO）的新方法，该方法利用渐进调优创建混合提示，将图像特征与其对应的文本语义对齐，以实现零样本多类型异常检测和分割。在多个公共基准和一个内部数据集上的实验结果表明，DAPO优于基线模型，在图像级别的AUROC和平均精度指标上平均提高了3.7%，并在零样本条件下定位新异常类型的能力提高了6.5%。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-End Consistency Model</div>
<div class="meta-line">Authors: Linwei Dong, Ruoyu Guo, Ge Bai, Zehuan Yuan, Yawei Luo, Changqing Zou</div>
<div class="meta-line">First: 2026-02-11T11:51:01+00:00 · Latest: 2026-02-11T11:51:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10764v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10764v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双端一致性模型</div>
<div class="mono" style="margin-top:8px">慢迭代采样特性仍然是扩散和基于流的生成模型实际部署的主要瓶颈。尽管一致性模型（CMs）代表了一种基于蒸馏的高效生成的最先进方法，但其大规模应用仍受到两个关键问题的限制：训练不稳定性和采样不灵活性。现有方法试图通过架构调整或正则化目标来缓解这些问题，但忽视了对轨迹选择的关键依赖。在本研究中，我们首先分析了这两个限制：训练不稳定性源于不稳定的自监督项引起的损失发散，而采样不灵活性则源于误差积累。基于这些见解和分析，我们提出了双端一致性模型（DE-CM），选择重要的子轨迹集群以实现稳定和有效的训练。DE-CM分解PF-ODE轨迹，并选择三个关键子轨迹作为优化目标。具体而言，我们的方法利用连续时间CMs目标实现少步蒸馏，并利用流匹配作为边界正则化器来稳定训练过程。此外，我们提出了一种新颖的噪声到噪声（N2N）映射，可以将噪声映射到任何点，从而减轻第一步中的误差积累。大量实验结果表明我们方法的有效性：在ImageNet 256x256数据集上，它在一步生成中达到了1.70的最先进FID分数，超越了现有的基于CM的一步方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of slow iterative sampling in diffusion and flow-based generative models, which hinder their practical deployment. The authors propose the Dual-End Consistency Model (DE-CM) to improve training stability and sampling flexibility by selecting critical sub-trajectory clusters for optimization. Experimental results demonstrate that DE-CM achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, surpassing existing consistency model approaches.</div>
<div class="mono" style="margin-top:8px">本研究解决了扩散和基于流的生成模型中缓慢迭代采样的问题，这限制了它们的实际应用。作者提出了双端一致性模型（DE-CM），该模型专注于选择关键子轨迹集群，以增强训练稳定性和采样灵活性。实验结果表明，DE-CM在ImageNet 256x256数据集上的一步生成中达到了1.70的最先进FID分数，超越了现有的一致性模型方法。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning</div>
<div class="meta-line">Authors: Kian Majlessi, Amir Masoud Soltani, Mohammad Ebrahim Mahdavi, Aurelien Gourrier, Peyman Adibi</div>
<div class="meta-line">First: 2026-02-11T11:15:34+00:00 · Latest: 2026-02-11T11:15:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10744v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10744v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无内容多模型导向表示学习的自监督图像超分辨率质量评估</div>
<div class="mono" style="margin-top:8px">应用于现实世界低分辨率（LR）图像的超分辨率（SR）通常会导致复杂、不规则的退化，这源于自然场景获取的固有复杂性。与在明确定义场景下创建的合成LR图像产生的SR伪影相比，这些失真是高度不可预测的，并且在不同的现实生活环境中显著变化。因此，从现实LR获得的SR图像的质量评估（SR-IQA）仍然是一个具有挑战性且未被充分探索的问题。在这项工作中，我们提出了一种针对这种高度不适定现实环境的无参考SR-IQA方法。所提出的方法使得针对现实世界SR应用的领域自适应IQA成为可能，特别是在数据稀缺的领域。我们假设超分辨率图像中的退化在很大程度上依赖于基础SR算法，而不仅仅由图像内容决定。为此，我们引入了一种自监督学习（SSL）策略，首先在预训练阶段预训练多个SR模型导向的表示。我们的对比学习框架从由同一SR模型生成的图像中形成正样本对，从由不同方法生成的图像中形成负样本对，与图像内容无关。所提出的方法S3 RIQA进一步结合了有针对性的预处理，以提取互补的质量信息，并引入辅助任务以更好地处理与不同SR缩放因子相关的各种退化特征。为此，我们构建了一个新的数据集SRMORSS，以支持无监督的预训练；该数据集包括应用于众多真实LR图像的广泛SR算法，填补了现有数据集的空白。在真实SR-IQA基准上的实验表明，S3 RIQA始终优于大多数相关的最先进指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in assessing the quality of super-resolved images derived from real-world low-resolution images, which often exhibit unpredictable degradations. The authors propose a no-reference super-resolution image quality assessment (SR-IQA) method that utilizes a self-supervised learning strategy to pretrain multiple representations based on different super-resolution algorithms. Key experimental results indicate that the proposed method, S3 RIQA, significantly outperforms existing state-of-the-art metrics on real SR-IQA benchmarks, demonstrating its effectiveness in handling diverse degradation profiles associated with various super-resolution techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决来自真实低分辨率图像的超分辨率图像质量评估中的挑战，这些图像通常表现出不可预测的降级。作者提出了一种无参考超分辨率图像质量评估(SR-IQA)方法，利用自监督学习策略对基于不同超分辨率算法的多种表示进行预训练。实验结果表明，所提出的方法S3 RIQA在真实SR-IQA基准测试中显著优于现有的最先进指标，证明了其在处理与不同超分辨率缩放因子相关的各种降级特征方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisit Visual Prompt Tuning: The Expressiveness of Prompt Experts</div>
<div class="meta-line">Authors: Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-31T07:41:06+00:00 · Latest: 2026-02-11T11:10:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18936v6">Abs</a> · <a href="https://arxiv.org/pdf/2501.18936v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视视觉提示调优：提示专家的表现力</div>
<div class="mono" style="margin-top:8px">视觉提示调优（VPT）通过插入任务特定的可学习提示标记，已被证明在参数高效地将预训练视觉模型适应于下游任务方面有效。尽管其经验成功，VPT的全面理论理解仍然是一个活跃的研究领域。基于最近建立的专家混合（MoE）与基于提示的方法之间的联系，其中每个注意力头可以被概念化为多个MoE模型的组合，我们将VPT重新解释为将新的提示专家引入这些MoE结构。我们识别出现有VPT框架中的一个关键限制：提示专家的功能表现力受限，保持静态，因此在适应性方面有限。为了解决这个问题，我们提出了视觉自适应提示调优（VAPT），这是一种新方法，使提示专家具备增强的表现力，同时保持参数效率。在VTAB-1K和FGVC上的实证评估表明，VAPT实现了显著的性能提升，分别超过完全微调基线7.34%和1.04%。此外，VAPT在需要更少额外参数的情况下，始终优于VPT。此外，我们的理论分析表明，VAPT实现了最佳样本效率。这些结果共同强调了我们方法的理论基础和经验优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the effectiveness of Visual Prompt Tuning (VPT) for adapting pre-trained vision models to specific tasks, while addressing the limitations of existing frameworks. The authors propose a new method called Visual Adaptive Prompt Tuning (VAPT), which introduces enhanced expressiveness to prompt experts within a Mixture of Experts (MoE) structure. Experimental results on VTAB-1K and FGVC show that VAPT improves performance significantly, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively, while also demonstrating better efficiency with fewer additional parameters compared to VPT.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉提示调优（VPT）在将预训练视觉模型适应特定任务时，由于提示专家的静态特性而导致的局限性。作者提出了一种新方法，称为视觉自适应提示调优（VAPT），该方法在保持参数效率的同时增强了提示专家的表现力。VTAB-1K和FGVC上的实验结果表明，VAPT在性能上超过了现有的VPT方法和完全微调的基线，分别提高了7.34%和1.04%，同时根据理论分析实现了最佳样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving</div>
<div class="meta-line">Authors: Sining Ang, Yuguang Yang, Chenxu Dang, Canyu Chen, Cheng Chi, Haiyan Liu, Xuanyao Mao, Jason Bao, Xuliang, Bingchuan Sun, Yan Wang</div>
<div class="meta-line">First: 2026-02-11T10:25:05+00:00 · Latest: 2026-02-11T10:25:05+00:00</div>
<div class="meta-line">Comments: 22 pages (10 pages main text + 12 pages appendix), 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10719v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer&#x27;s confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从表征互补性到双系统：协同VLM和仅视觉骨干网实现端到端驾驶</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）驾驶通过语言支持的骨干网增强了端到端（E2E）规划，但在通常的准确性与成本权衡之外的变化仍不清楚。我们通过在相同的扩散Transformer规划器下，使用完整的VLM和仅视觉骨干网实例化系统，重新审视这个问题。RQ1：在骨干网层面，VLM可以在仅视觉骨干网之上引入额外的子空间。RQ2：这个独特的子空间在一些长尾场景中导致了不同的行为：VLM倾向于更激进，而ViT则更保守，每个在约2-3%的测试场景中决定性获胜；通过一个在每个场景中选择VLM和ViT分支之间更好轨迹的神谕，我们获得了93.58 PDMS的上限。RQ3：为了充分利用这一观察，我们提出了HybridDriveVLA，它同时运行ViT和VLM分支，并使用学习的评分器在它们的端点轨迹之间进行选择，将PDMS提高到92.10。最后，DualDriveVLA实现了一种实用的快-慢策略：默认运行ViT，仅在评分器的置信度低于阈值时调用VLM；在15%的场景中调用VLM实现了91.00 PDMS，同时提高了3.2倍的吞吐量。代码将发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to explore the impact of integrating Vision-Language-Action (VLA) systems with traditional vision-only backbones in end-to-end driving, particularly focusing on performance beyond the typical accuracy-cost trade-off. The study employs a 3-RQ analysis within the RecogDrive framework, utilizing both a full VLM and vision-only backbones under a consistent diffusion Transformer planner. Key findings reveal that the VLM introduces additional subspaces that lead to different behaviors in long-tail scenarios, with the VLM being more aggressive and the vision-only backbone more conservative; an oracle that selects the superior trajectory achieves a peak performance of 93.58 PDMS. Furthermore, the proposed HybridDriveVLA system enhances performance to 92.10 PDMS by leveraging both branches and a learned scorer for trajectory selection, while the DualDriveVLA policy optimizes efficiency by using the vision-only backbone primarily and activating the VLM only when necessary, resulting in a 91.00 PDMS with a 3.2x increase in throughput.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨将视觉-语言-动作（VLA）系统整合到端到端驾驶中的影响，特别是它如何影响准确性与成本的权衡。作者在RecogDrive框架内采用了3个研究问题的分析，比较了完整的VLM与仅使用视觉的骨干网络，使用一致的扩散变换器规划器。主要发现表明，VLM引入了额外的子空间，改变了长尾场景中的行为，其中VLM表现得更具攻击性，而仅视觉骨干则更为保守；在选择两者中更优的轨迹时，达到了93.58 PDMS的上限。提出的HybridDriveVLA利用两个分支和一个学习评分器进行轨迹选择，将PDMS提高到92.10，而DualDriveVLA策略主要使用仅视觉骨干，并选择性地激活VLM，实现了91.00 PDMS，并提高了3.2倍的吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale</div>
<div class="meta-line">Authors: Chi Zhang, Zehua Chen, Kaiwen Zheng, Jun Zhu</div>
<div class="meta-line">First: 2025-09-28T17:12:13+00:00 · Latest: 2026-02-11T09:21:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25275v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25275v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://VoiceBridge-demo.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bridge models have recently been explored for speech enhancement tasks such as denoising, dereverberation, and super-resolution, while these efforts are typically confined to a single task or small-scale datasets, with constrained general speech restoration (GSR) capability at scale. In this work, we introduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs), capable of reconstructing high-fidelity speech at full-band (\textit{i.e.,} 48~kHz) from various distortions. By compressing speech waveform into continuous latent representations, VoiceBridge models the~\textit{diverse LQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\textit{a single latent-to-latent generative process} backed by a scalable transformer architecture. To better inherit the advantages of bridge models from the data domain to the latent space, we present an energy-preserving variational autoencoder, enhancing the alignment between the waveform and latent space over varying energy levels. Furthermore, to address the difficulty of HQ reconstruction from distinctively different LQ priors, we propose a joint neural prior, uniformly alleviating the reconstruction burden of LBM. At last, considering the key requirement of GSR systems, human perceptual quality, a perceptually aware fine-tuning stage is designed to mitigate the cascading mismatch in generation while improving perceptual alignment. Extensive validation across in-domain and out-of-domain tasks and datasets (\textit{e.g.}, refining recent zero-shot speech and podcast generation results) demonstrates the superior performance of VoiceBridge. Demo samples can be visited at: https://VoiceBridge-demo.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VoiceBridge：为大规模通用语音恢复设计潜在桥接模型</div>
<div class="mono" style="margin-top:8px">最近，桥接模型在语音增强任务中得到了探索，如去噪、去混响和超分辨率，但这些努力通常局限于单一任务或小规模数据集，具有有限的大规模通用语音恢复（GSR）能力。在本研究中，我们介绍了VoiceBridge，一个基于潜在桥接模型（LBM）的GSR系统，能够从各种失真中重建高保真语音，覆盖全频带（即48 kHz）。通过将语音波形压缩为连续的潜在表示，VoiceBridge以单一的潜在到潜在生成过程建模GSR中的多样化低质量到高质量任务，支持可扩展的变换器架构。为了更好地将桥接模型的优势从数据域继承到潜在空间，我们提出了一种能量保持变分自编码器，增强了波形与潜在空间在不同能量水平下的对齐。此外，为了解决从明显不同的低质量先验中进行高质量重建的困难，我们提出了一种联合神经先验，均匀减轻LBM的重建负担。最后，考虑到GSR系统的关键要求——人类感知质量，我们设计了一个感知意识的微调阶段，以减轻生成中的级联不匹配，同时改善感知对齐。在领域内和领域外任务及数据集（例如，改进最近的零样本语音和播客生成结果）上的广泛验证证明了VoiceBridge的卓越性能。演示样本可访问：https://VoiceBridge-demo.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance general speech restoration (GSR) capabilities, which have been limited to single tasks or small datasets in previous studies. The authors introduce VoiceBridge, a GSR system that utilizes latent bridge models (LBMs) to reconstruct high-fidelity speech at full-band from various distortions. Key experimental findings indicate that VoiceBridge effectively models diverse low-quality to high-quality tasks through a single generative process, significantly improving perceptual quality and performance across multiple tasks and datasets, including zero-shot speech and podcast generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升通用语音恢复（GSR）能力，以前的研究通常局限于单一任务和小型数据集。作者提出了VoiceBridge，这是一个利用潜在桥接模型（LBM）从各种失真中重建高保真语音的GSR系统。关键实验结果表明，VoiceBridge通过单一生成过程有效地建模了多样的低质量到高质量任务，显著提高了感知质量和在多个任务及数据集上的性能，包括零样本语音和播客生成。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Frequency Modulation for Controllable Text-driven Image Generation</div>
<div class="meta-line">Authors: Tiandong Shi, Ling Zhao, Ji Qi, Jiayi Ma, Chengli Peng</div>
<div class="meta-line">First: 2026-02-11T09:06:44+00:00 · Latest: 2026-02-11T09:06:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10662v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可控文本驱动图像生成的动态频率调制</div>
<div class="mono" style="margin-top:8px">文本引导的扩散模型的成功建立了一种新的图像生成范式，该范式由文本提示的迭代优化驱动。然而，修改原始文本提示以实现预期的语义调整往往会导致意外的全局结构变化，从而干扰用户意图。现有方法依赖于经验特征图选择进行干预，其性能在很大程度上依赖于适当的选择，导致稳定性不佳。本文尝试从频率的角度解决上述问题，并分析噪声潜变量的频谱对生成过程中结构框架和细粒度纹理的层次性出现的影响。我们发现，低频成分主要负责在早期生成阶段建立结构框架。随着时间的推移，它们的影响减弱，转而由高频成分合成细粒度纹理。基于此，我们提出了一种无训练的频率调制方法，利用具有动态衰减的频率依赖加权函数。该方法在允许有针对性的语义修改的同时，保持结构框架的一致性。通过直接操控噪声潜变量，所提方法避免了对内部特征图的经验选择。大量实验表明，所提方法显著优于当前最先进的方法，实现了在保持结构和允许语义更新之间的有效平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of modifying text prompts in text-guided diffusion models without disrupting the intended image structure. The authors propose a training-free frequency modulation method that utilizes a frequency-dependent weighting function with dynamic decay to manipulate noisy latent variables directly. Experimental results show that this approach significantly improves the balance between maintaining structural consistency and allowing for targeted semantic modifications compared to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了文本驱动图像生成中保持用户意图的挑战，修改文本提示可能导致意想不到的结构变化。作者提出了一种无训练的频率调制方法，利用动态衰减的频率依赖加权函数，允许在保持生成图像结构框架的同时进行有针对性的语义修改。实验结果表明，该方法显著优于现有的最先进技术，有效平衡了结构的保留和语义更新的能力。</div>
</details>
</div>
<div class="card">
<div class="title">GenDR: Lighten Generative Detail Restoration</div>
<div class="meta-line">Authors: Yan Wang, Shijie Zhao, Kexin Zhang, Junlin Li, Li Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-03-09T22:02:18+00:00 · Latest: 2026-02-11T08:40:29+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06790v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.06790v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable progress, the misalignment of their targets leads to a suboptimal trade-off between inference speed and detail fidelity. Specifically, the T2I task requires multiple inference steps to synthesize images matching to prompts and reduces the latent dimension to lower generating difficulty. Contrariwise, SR can restore high-frequency details in fewer inference steps, but it necessitates a more reliable variational auto-encoder (VAE) to preserve input information. However, most diffusion-based SRs are multistep and use 4-channel VAEs, while existing models with 16-channel VAEs are overqualified diffusion transformers, e.g., FLUX (12B). To align the target, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with a larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand the latent space without increasing the model size. Regarding step distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenDR：轻量级生成细节恢复</div>
<div class="mono" style="margin-top:8px">尽管最近将文本到图像（T2I）扩散模型应用于现实世界超分辨率（SR）的研究取得了显著进展，但目标的不对齐导致推理速度与细节保真度之间的次优权衡。具体而言，T2I任务需要多个推理步骤来合成与提示匹配的图像，并减少潜在维度以降低生成难度。相反，SR可以在更少的推理步骤中恢复高频细节，但需要更可靠的变分自编码器（VAE）来保留输入信息。然而，大多数基于扩散的SR是多步骤的，并使用4通道VAE，而现有的16通道VAE模型则是过于复杂的扩散变换器，例如FLUX（12B）。为了对齐目标，我们提出了一种用于生成细节恢复的一步扩散模型GenDR，该模型从具有更大潜在空间的定制扩散模型中提炼而来。具体而言，我们通过表示对齐训练一个新的SD2.1-VAE16（0.9B），以在不增加模型大小的情况下扩展潜在空间。关于步骤蒸馏，我们提出了一致性得分身份蒸馏（CiD），将SR任务特定损失纳入得分蒸馏，以利用更多SR先验并对齐训练目标。此外，我们通过对抗学习和表示对齐扩展CiD（CiDA），以增强感知质量并加速训练。我们还优化了管道，以实现更高效的推理。实验结果表明，GenDR在定量指标和视觉保真度方面均实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the suboptimal trade-off between inference speed and detail fidelity in text-to-image diffusion models applied to super-resolution tasks. The authors propose GenDR, a one-step diffusion model for generative detail restoration, which is distilled from a tailored diffusion model with a larger latent space. Key experimental findings indicate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity, demonstrating improvements in perceptual quality and training efficiency through the incorporation of consistent score identity distillation and adversarial learning techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决应用于超分辨率任务的文本到图像扩散模型在推理速度和细节保真度之间的权衡。作者提出了一种一阶扩散模型GenDR，该模型从一个更大潜在空间的模型中提炼，以改善与超分辨率目标的对齐。实验结果表明，GenDR在定量指标和视觉保真度方面均优于现有方法，实现了最先进的性能，同时提高了训练效率和感知质量。</div>
</details>
</div>
<div class="card">
<div class="title">Eliminating VAE for Fast and High-Resolution Generative Detail Restoration</div>
<div class="meta-line">Authors: Yan Wang, Shijie Zhao, Junlin Li, Li Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-11T08:23:30+00:00 · Latest: 2026-02-11T08:23:30+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10630v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10630v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>消除VAE以实现快速高分辨率生成细节恢复</div>
<div class="mono" style="margin-top:8px">扩散模型在现实世界超分辨率(SR)任务中取得了显著突破，尽管推理速度较慢且对设备要求较高。为了加速推理，最近的工作如GenDR采用步骤蒸馏将步骤数最小化为1。然而，内存边界仍限制了最大处理大小， necessitating对高分辨率图像进行逐块恢复。通过对管道进行分析，我们确定变分自编码器(VAE)是延迟和内存的瓶颈。为了解决这个问题，我们利用像素(反)洗牌操作消除VAE，将基于潜变量的GenDR反转为像素空间GenDR-Pix。然而，x8像素洗牌的放大可能会引入重复模式的伪影。为了减轻失真，我们提出了多阶段对抗蒸馏，逐步去除编码器和解码器。具体而言，我们利用前一阶段模型的生成特征来指导对抗性判别。此外，我们提出随机填充以增强生成特征并避免判别器崩溃。我们还引入了掩蔽傅里叶空间损失，以惩罚幅度的异常值。为了提高推理性能，我们经验性地将基于填充的自集成与无分类器引导结合，以改善推理扩展。实验结果表明，GenDR-Pix相比GenDR实现了2.8倍加速和60%的内存节省，视觉退化微乎其微，超越了其他一步扩散SR。尽管面临种种挑战，GenDR-Pix仅需1秒即可恢复4K图像，且内存占用为6GB。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the slow inference speed and high memory requirements of diffusion models in real-world super-resolution tasks. The authors propose a method called GenDR-Pix, which eliminates the variational auto-encoder (VAE) by utilizing pixel-(un)shuffle operations and a multi-stage adversarial distillation process to enhance performance. Experimental results demonstrate that GenDR-Pix achieves a 2.8x acceleration and 60% memory savings compared to the previous GenDR model, enabling the restoration of 4K images in just 1 second with minimal visual degradation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散模型在实际超分辨率任务中的慢推理速度和高内存需求。作者提出了一种名为GenDR-Pix的方法，通过利用像素（反）洗牌操作和多阶段对抗蒸馏过程来消除变分自编码器（VAE），从而提高性能。实验结果表明，GenDR-Pix相比于之前的GenDR模型实现了2.8倍的加速和60%的内存节省，能够在仅1秒内恢复4K图像，且视觉退化极小。</div>
</details>
</div>
<div class="card">
<div class="title">From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models</div>
<div class="meta-line">Authors: Zefan Cai, Haoyi Qiu, Haozhe Zhao, Ke Wan, Jiachen Li, Jiuxiang Gu, Wen Xiao, Nanyun Peng, Junjie Hu</div>
<div class="meta-line">First: 2025-10-20T07:37:43+00:00 · Latest: 2026-02-11T07:47:19+00:00</div>
<div class="meta-line">Comments: TMLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17247v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17247v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video diffusion models have significantly enhanced text-to-video generation, particularly through alignment tuning using reward models trained on human preferences. While these methods improve visual quality, they can unintentionally encode and amplify social biases. To systematically trace how such biases evolve throughout the alignment pipeline, we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating social representation in video generation. Grounded in established social bias taxonomies, VideoBiasEval employs an event-based prompting strategy to disentangle semantic content (actions and contexts) from actor attributes (gender and ethnicity). It further introduces multi-granular metrics to evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity, (3) distributional shifts in social attributes across model variants, and (4) the temporal persistence of bias within videos. Using this framework, we conduct the first end-to-end analysis connecting biases in human preference datasets, their amplification in reward models, and their propagation through alignment-tuned video diffusion models. Our results reveal that alignment tuning not only strengthens representational biases but also makes them temporally stable, producing smoother yet more stereotyped portrayals. These findings highlight the need for bias-aware evaluation and mitigation throughout the alignment process to ensure fair and socially responsible video generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从偏好到偏见：对齐调优在视频扩散模型中塑造社会偏见的作用</div>
<div class="mono" style="margin-top:8px">最近视频扩散模型的进展显著提升了文本到视频的生成，特别是通过使用基于人类偏好的奖励模型进行对齐调优。虽然这些方法改善了视觉质量，但它们可能无意中编码和放大社会偏见。为了系统地追踪这些偏见在对齐流程中的演变，我们引入了VideoBiasEval，这是一个全面的诊断框架，用于评估视频生成中的社会表现。VideoBiasEval基于已建立的社会偏见分类法，采用基于事件的提示策略，将语义内容（动作和背景）与演员属性（性别和种族）分离。它进一步引入多层次指标来评估（1）整体种族偏见，（2）基于种族的性别偏见，（3）模型变体中社会属性的分布变化，以及（4）视频中偏见的时间持续性。利用该框架，我们首次进行端到端分析，连接人类偏好数据集中的偏见、在奖励模型中的放大以及通过对齐调优的视频扩散模型的传播。我们的结果揭示，对齐调优不仅加强了表现偏见，还使其在时间上保持稳定，产生了更平滑但更刻板的表现。这些发现强调了在对齐过程中进行偏见意识评估和缓解的必要性，以确保公平和社会责任的视频生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the unintended encoding and amplification of social biases in video diffusion models, which have improved text-to-video generation through alignment tuning with reward models based on human preferences. The authors introduce VideoBiasEval, a diagnostic framework that evaluates social representation in video generation by employing an event-based prompting strategy to separate semantic content from actor attributes, along with multi-granular metrics to assess various forms of bias. The key findings indicate that alignment tuning not only enhances representational biases but also stabilizes them over time, leading to smoother yet more stereotyped portrayals in generated videos, underscoring the necessity for bias-aware evaluation and mitigation in the alignment process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频扩散模型中社会偏见的无意放大，特别是通过基于人类偏好的奖励模型进行对齐调优所增强的偏见。作者提出了VideoBiasEval，一个评估视频生成中社会表现的诊断框架，通过事件驱动的提示策略将语义内容与演员属性分离，并采用多粒度指标评估各种偏见。主要发现表明，对齐调优不仅强化了现有的表现偏见，还导致其时间稳定性，从而产生更平滑但更刻板的影片表现，这凸显了在视频生成过程中进行偏见意识评估和缓解策略的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images</div>
<div class="meta-line">Authors: Hanzhe Yu, Yun Ye, Jintao Rong, Qi Xuan, Chen Ma</div>
<div class="meta-line">Venue: ACM MM 2025</div>
<div class="meta-line">First: 2026-02-11T05:38:40+00:00 · Latest: 2026-02-11T05:38:40+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 33rd ACM International Conference on Multimedia (ACM MM 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10546v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10546v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://real-hd.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RealHD：用于鲁棒检测最先进AI生成图像的高质量数据集</div>
<div class="mono" style="margin-top:8px">生成性AI的快速发展引发了对数字图像真实性的担忧，因为现在可以以低成本生成高度逼真的假图像，这可能增加社会风险。为此，建立了多个数据集以训练检测模型，旨在区分AI生成的图像和真实图像。然而，现有数据集存在泛化能力有限、图像质量低、提示过于简单和图像多样性不足等问题。为了解决这些限制，我们提出了一个高质量的大规模数据集，包含超过730,000张图像，涵盖多个类别，包括真实图像和AI生成图像。生成的图像通过最先进的方法合成，包括文本到图像生成（由超过10,000个精心设计的提示引导）、图像修复、图像精细化和人脸交换。每个生成的图像都标注了其生成方法和类别。修复图像还包括二进制掩码以指示修复区域，为分析提供丰富的元数据。与现有数据集相比，基于我们的数据集训练的检测模型表现出更强的泛化能力。我们的数据集不仅作为评估检测方法的强基准，还促进了AI生成图像检测技术的鲁棒性提升。在此基础上，我们提出了一种基于图像噪声熵的轻量级检测方法，该方法在分类之前将原始图像转换为非局部均值（NLM）噪声的熵张量。大量实验表明，基于我们的数据集训练的模型实现了强泛化，而我们的方法提供了具有竞争力的性能，为未来研究建立了坚实的基线。数据集和源代码可在https://real-hd.github.io公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the growing concerns regarding the authenticity of digital images due to the rise of generative AI, which can produce highly realistic fake images. To tackle the limitations of existing datasets, the authors developed RealHD, a high-quality dataset containing over 730,000 images, both real and AI-generated, synthesized using advanced techniques like text-to-image generation and image inpainting. Experimental results indicate that detection models trained on this dataset exhibit improved generalization capabilities compared to those trained on existing datasets, and the proposed lightweight detection method based on image noise entropy shows competitive performance, establishing a robust baseline for future research in AI-generated image detection.</div>
<div class="mono" style="margin-top:8px">生成性人工智能的快速发展引发了对数字图像真实性的担忧，这促使我们创建一个高质量的大规模数据集，以提高对AI生成图像的检测能力。所提出的数据集RealHD包含超过73万张跨多个类别的图像，这些图像采用最先进的方法生成，如文本到图像生成和图像修复，并为每张图像提供详细的注释。实验结果表明，基于该数据集训练的检测模型相比现有数据集表现出更强的泛化能力，而基于图像噪声熵的轻量级检测方法也展现出竞争力的性能，为未来的AI生成图像检测研究建立了强有力的基准。</div>
</details>
</div>
<div class="card">
<div class="title">DiCo: Disentangled Concept Representation for Text-to-image Person Re-identification</div>
<div class="meta-line">Authors: Giyeol Kim, Chanho Eom</div>
<div class="meta-line">First: 2026-01-15T04:08:53+00:00 · Latest: 2026-02-11T04:48:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10053v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10053v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiCo：用于文本到图像的人物重识别的解耦概念表示</div>
<div class="mono" style="margin-top:8px">文本到图像的人物重识别（TIReID）旨在根据自由形式的文本描述从大型图库中检索人物图像。由于视觉外观与文本表达之间存在显著的模态差距，以及需要建模细粒度的对应关系以区分具有相似属性（如服装颜色、纹理或服装风格）的人物，TIReID具有挑战性。为了解决这些问题，我们提出了DiCo（解耦概念表示），这是一个新颖的框架，实现了分层和解耦的跨模态对齐。DiCo引入了一种共享的基于槽的表示，其中每个槽作为跨模态的部分级锚点，并进一步分解为多个概念块。该设计使得互补属性（例如，颜色、纹理、形状）的解耦成为可能，同时保持图像和文本之间的一致部分级对应关系。在CUHK-PEDES、ICFG-PEDES和RSTPReid上的大量实验表明，我们的框架在与最先进的方法相比时表现出竞争力，同时通过明确的槽和块级表示增强了解释性，以获得更细粒度的检索结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve text-to-image person re-identification (TIReID), which faces challenges due to the significant modality gap between visual and textual data and the need for precise modeling of individual characteristics. The authors propose DiCo, a framework that utilizes a shared slot-based representation to achieve hierarchical and disentangled cross-modal alignment, allowing for the separation of complementary attributes like color and texture while ensuring part-level correspondence. Experimental results on datasets such as CUHK-PEDES, ICFG-PEDES, and RSTPReid show that DiCo performs competitively with state-of-the-art methods and enhances interpretability through its explicit representations, leading to more accurate retrieval outcomes.</div>
<div class="mono" style="margin-top:8px">文本到图像的人物重识别（TIReID）的研究动机在于视觉和文本数据之间显著的模态差距以及在相似个体之间建立细粒度对应关系的需求。为了解决这些挑战，作者提出了DiCo，这是一种新颖的框架，利用共享的槽位表示实现分层和解耦的跨模态对齐。在CUHK-PEDES、ICFG-PEDES和RSTPReid等数据集上的实验结果表明，DiCo不仅在性能上与最先进的方法竞争，还通过其明确的槽位和块级表示提高了解释性，从而实现更精确的检索结果。</div>
</details>
</div>
<div class="card">
<div class="title">3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars</div>
<div class="meta-line">Authors: Zhongju Wang, Zhenhong Sun, Beier Wang, Yifu Wang, Daoyi Dong, Huadong Mo, Hongdong Li</div>
<div class="meta-line">First: 2026-02-11T04:31:13+00:00 · Latest: 2026-02-11T04:31:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10516v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DXTalker：在表现力丰富的3D对话头像中统一身份、口型同步、情感和空间动态</div>
<div class="mono" style="margin-top:8px">基于音频的3D对话头像生成在虚拟通信、数字人类和互动媒体中变得越来越重要，头像必须保持身份、与语音同步口型、表达情感并展现逼真的空间动态，共同定义了表现力的更广泛目标。然而，由于训练数据不足、受限的主体身份、狭窄的音频表示和有限的显式可控性，实现这一目标仍然具有挑战性。本文提出了3DXTalker，通过数据策划的身份建模、丰富的音频表示和空间动态可控性来生成表现力丰富的3D对话头像。3DXTalker通过2D到3D的数据策划管道和解耦表示实现可扩展的身份建模，缓解数据稀缺并改善身份泛化。然后，我们引入超越标准语音嵌入的逐帧幅度和情感线索，确保优越的口型同步和细腻的表情调节。这些线索通过基于流匹配的变换器统一，以实现连贯的面部动态。此外，3DXTalker还支持自然的头部姿态运动生成，同时通过基于提示的条件支持风格化控制。大量实验表明，3DXTalker在统一框架内集成了口型同步、情感表达和头部姿态动态，在3D对话头像生成中实现了卓越的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the expressivity of 3D talking avatars in virtual communication and interactive media, addressing challenges related to identity preservation, lip synchronization, emotional expression, and spatial dynamics. The authors propose 3DXTalker, which utilizes a data-curated identity modeling approach, audio-rich representations, and controllable spatial dynamics to improve avatar generation. Experimental results demonstrate that 3DXTalker effectively integrates lip synchronization, emotional expression, and head-pose dynamics, achieving superior performance in generating expressive 3D talking avatars compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决身份保留、口型同步、情感表达和空间动态等挑战，增强虚拟交流中3D说话头像的表现力。作者提出了一种名为3DXTalker的方法，该方法利用数据策划的身份建模、丰富的音频表示和可控的空间动态来改善头像生成。实验结果表明，3DXTalker成功地将口型同步、情感表达和头部姿态动态整合在一起，在生成富有表现力的3D说话头像方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering</div>
<div class="meta-line">Authors: Dana Feng, Bhada Yun, April Yi Wang</div>
<div class="meta-line">First: 2026-01-31T03:46:20+00:00 · Latest: 2026-02-11T04:02:52+00:00</div>
<div class="meta-line">Comments: To appear in CHI&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00496v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00496v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a Delphi process with 5 seniors, an AI-assisted debugging task with 10 juniors, and blind reviews of junior prompt histories by 5 more seniors. We found that agency in software engineering is primarily constrained by organizational policies rather than individual preferences, with experienced developers maintaining control through detailed delegation while novices struggle between over-reliance and cautious avoidance. Seniors leverage pre-AI foundational instincts to steer modern tools and possess valuable perspectives for mentoring juniors in their early AI-encouraged career development. From synthesis of results, we suggest three practices that focus on preserving agency in software engineering for coding, learning, and mentorship, especially as AI grows increasingly autonomous.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从初级到高级：在代理人工智能中分配代理权和导航职业成长</div>
<div class="mono" style="margin-top:8px">初级工程师作为人工智能原住民进入，高级工程师在职业生涯中适应。人工智能不仅改变了工程师的编码方式，还重塑了在工作和职业成长中持有代理权的主体。我们通过三阶段混合方法研究贡献了初级和高级工程师在使用代理人工智能方面的案例：结合5名高级工程师的德尔菲过程的ACTA，10名初级工程师的人工智能辅助调试任务，以及5名高级工程师对初级提示历史的盲审。我们发现，软件工程中的代理权主要受到组织政策的限制，而非个人偏好，经验丰富的开发者通过详细的委派保持控制，而新手则在过度依赖和谨慎回避之间挣扎。高级工程师利用前人工智能的基础直觉来引导现代工具，并为初级工程师在早期人工智能鼓励的职业发展中提供宝贵的指导视角。根据结果的综合，我们建议三种实践，专注于在软件工程中保持编码、学习和指导的代理权，尤其是在人工智能日益自主的情况下。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates how the integration of AI in software engineering affects the agency and professional growth of junior and senior engineers. Using a three-phase mixed-methods approach, the study involved a Delphi process with five senior engineers, an AI-assisted debugging task with ten junior engineers, and blind reviews of junior prompt histories by five additional seniors. The findings reveal that organizational policies primarily limit agency in software engineering, with seniors maintaining control through delegation while juniors face challenges in balancing reliance on AI tools. The study suggests practices to preserve agency in coding, learning, and mentorship as AI continues to evolve in the field.</div>
<div class="mono" style="margin-top:8px">本研究探讨了代理AI在软件工程中的整合如何影响初级和高级工程师的自主权和职业发展。研究采用了三阶段混合方法，包括与五名高级工程师的德尔菲过程结合的ACTA、十名初级工程师的AI辅助调试任务，以及对另外五名高级工程师的初级提示历史的盲审。研究结果表明，软件工程中的自主权主要受到组织政策的限制，而非个人选择，高级开发人员通过详细的委派保持控制，而初级开发人员则面临对AI的过度依赖和谨慎回避的挑战，这突显了在AI变得更加自主的情况下，保持编码、学习和指导中的自主权的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Authenticated Workflows: A Systems Approach to Protecting Agentic AI</div>
<div class="meta-line">Authors: Mohan Rajagopalan, Vinay Rao</div>
<div class="meta-line">First: 2026-02-11T03:04:50+00:00 · Latest: 2026-02-11T03:04:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10465v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10465v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认证工作流：保护自主人工智能的系统方法</div>
<div class="mono" style="margin-top:8px">自主人工智能系统自动化企业工作流，但现有防御措施——护栏、语义过滤器——是概率性的，且常常被绕过。我们引入认证工作流，这是企业自主人工智能的第一个完整信任层。安全性归结为保护四个基本边界：提示、工具、数据和上下文。我们在每个边界交叉处强制执行意图（操作符合组织政策）和完整性（操作在密码学上是可信的），结合密码学消除攻击类别与运行时政策执行。这提供了确定性安全性——操作要么携带有效的密码学证明，要么被拒绝。我们引入MAPL，一种AI原生政策语言，动态表达自主约束，随着代理的演变和调用上下文的变化，政策规模为O(log M + N)，而不是O(M x N)规则，通过与工作流依赖的密码学证明的分层组合。我们通过集成九个领先框架（MCP、A2A、OpenAI、Claude、LangChain、CrewAI、AutoGen、LlamaIndex、Haystack）的通用安全运行时证明其实用性，采用薄适配器，要求零协议修改。形式证明建立了完整性和健全性。实证验证显示在174个测试案例中100%召回率，零假阳性，防护9个OWASP前10风险中的10个，并完全缓解两个高影响生产CVE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the security of agentic AI systems that automate enterprise workflows, as existing defenses are often probabilistic and can be bypassed. The authors propose a novel approach called authenticated workflows, which establishes a complete trust layer by enforcing intent and integrity at four critical boundaries: prompts, tools, data, and context. The experimental results demonstrate the practicality of this method, achieving 100% recall with zero false positives across 174 test cases, effectively addressing 9 of the 10 OWASP Top 10 risks, and completely mitigating two high-impact production Common Vulnerabilities and Exposures (CVEs).</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强自动化企业工作流的代理人工智能系统的安全性，这些系统通常容易受到现有概率性防御的攻击。作者提出了一种新方法，称为认证工作流，建立了一个完整的信任层，保护四个关键边界：提示、工具、数据和上下文，通过加密方法和运行时策略执行来强制意图和完整性。实验结果表明，该方法的有效性，在174个测试用例中实现了100%的召回率且没有假阳性，缓解了10个OWASP前10大风险中的9个，并完全解决了两个重要的生产漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">Found-RL: foundation model-enhanced reinforcement learning for autonomous driving</div>
<div class="meta-line">Authors: Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo, Tianyi Wang, Yiheng Feng, Samuel Labi, Sikai Chen</div>
<div class="meta-line">First: 2026-02-11T02:56:04+00:00 · Latest: 2026-02-11T02:56:04+00:00</div>
<div class="meta-line">Comments: 39 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10458v1">PDF</a> · <a href="https://github.com/ys-qu/found-rl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP&#x27;s dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Found-RL：基础模型增强的自主驾驶强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已成为端到端自主驾驶（AD）的主导范式。然而，RL在复杂场景中存在样本效率低和缺乏语义可解释性的问题。基础模型，特别是视觉-语言模型（VLM），可以通过提供丰富的上下文感知知识来缓解这一问题，但其高推理延迟阻碍了在高频RL训练循环中的部署。为了解决这一问题，我们提出了Found-RL，一个旨在高效增强AD的RL平台，利用基础模型。核心创新是异步批量推理框架，它将重型VLM推理与仿真循环解耦，有效解决延迟瓶颈以支持实时学习。我们引入了多种监督机制：价值-边际正则化（VMR）和优势加权动作指导（AWAG），以有效提炼专家级VLM动作建议到RL策略中。此外，我们采用高吞吐量的CLIP进行密集奖励塑造。我们通过条件对比动作对齐解决CLIP的动态盲点，该方法根据离散化的速度/指令对提示进行条件化，并从上下文特定的动作锚评分中产生归一化的、基于边际的奖励。Found-RL提供了一个端到端的管道，用于精细调整VLM集成，并表明轻量级RL模型在保持实时推理（约500 FPS）的同时，可以实现接近VLM的性能，与十亿参数的VLM相比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the sample inefficiency and lack of semantic interpretability in reinforcement learning (RL) for autonomous driving. The authors propose Found-RL, which utilizes an asynchronous batch inference framework to integrate foundation models, specifically Vision-Language Models (VLMs), into RL training without incurring high inference latency. Key experimental results demonstrate that Found-RL enables a lightweight RL model to achieve performance comparable to that of billion-parameter VLMs while maintaining real-time inference capabilities at approximately 500 frames per second.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决强化学习（RL）在自动驾驶中存在的样本效率低和缺乏语义可解释性的问题。作者提出了Found-RL，利用异步批量推理框架将基础模型，特别是视觉-语言模型（VLM），整合到RL中，同时克服延迟问题。实验结果表明，Found-RL使轻量级RL模型的性能可与大型VLM相媲美，并保持约500帧每秒（FPS）的实时推理。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0359.html">20260212_0359</a>
<a href="archive/20260211_0404.html">20260211_0404</a>
<a href="archive/20260210_0406.html">20260210_0406</a>
<a href="archive/20260209_0325.html">20260209_0325</a>
<a href="archive/20260208_0323.html">20260208_0323</a>
<a href="archive/20260207_0339.html">20260207_0339</a>
<a href="archive/20260206_0339.html">20260206_0339</a>
<a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
