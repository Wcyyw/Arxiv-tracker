<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 03:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0339</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：回收适配器以寻找资源高效适应和推理的主子空间</div>
<div class="mono" style="margin-top:8px">大型模型的快速增长引发了对其环境影响和可及性公平性的担忧，因为计算成本显著。低秩适配器（LoRA）为微调大型模型提供了一种轻量级解决方案，导致大量针对不同领域的公开适配器的出现。我们提出：这些预训练的适配器能否被利用以进一步简化对新任务的适应，同时解决这些挑战？我们介绍了EigenLoRAx，这是一种参数高效的微调方法，回收现有适配器以创建与其共享领域知识对齐的主子空间，并可以在低资源场景中进一步增强正交基向量。这使得通过仅在子空间的主成分上学习轻量级系数来快速适应新任务，消除了微调整个适配器的需要。EigenLoRAx所需的参数和内存显著减少，提高了训练和推理的效率。我们的方法在不同领域和任务中表现出色，为边缘应用、个性化和在资源受限环境中公平部署大型模型提供了可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing size of large models raises concerns about their environmental impact and accessibility due to high computational costs. To address these issues, the authors propose EigenLoRAx, a parameter-efficient finetuning method that utilizes existing Low-Rank Adapters (LoRA) to form a principal subspace that captures shared domain knowledge, which can be enhanced with orthogonal basis vectors in low-resource settings. Experimental results show that EigenLoRAx significantly reduces the number of parameters and memory required, leading to improved efficiency in both training and inference while maintaining strong performance across various domains and tasks.</div>
<div class="mono" style="margin-top:8px">大型模型的快速增长引发了对其环境影响和可及性的担忧，因为计算成本高昂。本研究提出了EigenLoRAx，一种利用现有低秩适配器（LoRA）创建主子空间以高效适应新任务的方法，允许在低资源场景中添加正交基向量。实验结果表明，EigenLoRAx显著减少了所需的参数和内存，提高了训练和推理的效率，同时在各种领域和任务中保持了良好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Shared LoRA Subspaces for almost Strict Continual Learning</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几乎严格持续学习的共享LoRA子空间</div>
<div class="mono" style="margin-top:8px">将大型预训练模型高效且持续地适应新任务对于实际部署至关重要，但由于灾难性遗忘和高昂的再训练成本，仍然具有挑战性。虽然像低秩适应（LoRA）这样的参数高效调优方法降低了计算需求，但它们缺乏严格持续学习和知识整合的机制，而不依赖于数据重放或多个适配器。我们提出了Share，一种新颖的参数高效持续微调方法，它学习并动态更新一个共享的低秩子空间，实现跨多个任务和模态的无缝适应。Share构建了一个基础子空间，从过去任务中提取核心知识，并通过识别重要的子空间方向逐步整合新信息。每个新任务的知识被纳入这个不断发展的子空间，促进前向知识转移，同时最小化灾难性干扰。这种方法在传统LoRA方法上实现了高达100倍的参数减少和281倍的内存节省，保持了与联合训练模型相当的性能。一个Share模型可以替代数百个特定任务的LoRA适配器，支持可扩展的异步持续学习。在图像分类、自然语言理解、3D姿态估计和文本到图像生成等实验中验证了其有效性，使Share成为大规模AI系统中终身学习的实用且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of efficiently adapting large pretrained models to new tasks while mitigating catastrophic forgetting and the high costs associated with retraining. The authors propose a novel method called Share, which utilizes a shared low-rank subspace for continual fine-tuning, allowing for the dynamic integration of knowledge from multiple tasks without the need for data replay or multiple adapters. Experimental results demonstrate that Share achieves significant reductions in parameters and memory usage, with up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods, while maintaining performance levels similar to those of jointly trained models across various tasks such as image classification and natural language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决将大型预训练模型有效适应新任务时面临的灾难性遗忘和高昂重训练成本的问题。作者提出了一种名为Share的新方法，该方法利用单一共享的低秩子空间进行持续微调，允许动态整合来自多个任务的知识。实验结果表明，Share在参数减少方面可达100倍，在内存节省方面可达281倍，相较于传统的低秩适应方法，同时在图像分类和自然语言理解等各种任务中保持与联合训练模型相似的性能水平。</div>
</details>
</div>
<div class="card">
<div class="title">SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</div>
<div class="meta-line">Authors: Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu</div>
<div class="meta-line">First: 2026-01-12T05:03:12+00:00 · Latest: 2026-02-05T18:37:54+00:00</div>
<div class="meta-line">Comments: 12 pages, 14 figures, accepted in WACVW 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07209v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07209v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIRR-LMM：通过大型多模态模型进行单图像反射去除</div>
<div class="mono" style="margin-top:8px">玻璃表面产生复杂的反射和透射光交互，使得单图像反射去除（SIRR）变得具有挑战性。现有数据集在合成数据的物理真实感上有限，或在真实捕捉中规模不足。我们引入了一种合成数据集生成框架，通过真实背景图像对3D玻璃模型进行路径追踪，以创建具有不同玻璃属性、相机设置和后处理效果的物理准确反射场景。为了利用大型多模态模型（LMM）的能力，我们将图像层连接成单一复合输入，应用联合标注，并使用任务特定的LoRA进行微调，而不是全参数训练。这使得我们的方法在反射去除和分离性能上优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of single-image reflection removal (SIRR) caused by complex interactions of light on glass surfaces, which are exacerbated by limitations in existing datasets. The authors developed a synthetic dataset generation framework that utilizes path tracing of 3D glass models against real backgrounds to create realistic reflection scenarios with diverse glass properties and camera settings. By employing a Large Multimodal Model (LMM) with a composite input approach and fine-tuning through task-specific LoRA, the study demonstrates significant improvements in reflection removal and separation performance over current state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于玻璃表面光线复杂交互而导致的单图像反射去除（SIRR）挑战，现有数据集对此表现不足。作者开发了一种合成数据集生成框架，通过在真实背景上路径追踪3D玻璃模型，创建具有多样玻璃属性和相机设置的真实反射场景。通过使用大规模多模态模型（LMM）并将图像层合并为复合输入，结合任务特定的LoRA微调，该研究在反射去除和分离性能上相较于当前最先进技术显示出显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</div>
<div class="meta-line">Authors: Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</div>
<div class="meta-line">First: 2026-02-05T18:08:20+00:00 · Latest: 2026-02-05T18:08:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://junwankimm.github.io/CSFM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://junwankimm.github.io/CSFM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更好的源，更好的流：学习条件依赖的源分布以进行流匹配</div>
<div class="mono" style="margin-top:8px">流匹配最近作为扩散生成模型的有希望的替代方案出现，特别是在文本到图像生成方面。尽管它在允许任意源分布方面具有灵活性，但大多数现有方法依赖于标准高斯分布，这一选择源自扩散模型，并且在这种情况下很少将源分布本身视为优化目标。在本研究中，我们表明，源分布的原则性设计不仅可行，而且在现代文本到图像系统的规模上是有益的。具体而言，我们提出在流匹配目标下学习条件依赖的源分布，以更好地利用丰富的条件信号。我们识别出直接将条件纳入源时出现的关键失败模式，包括分布崩溃和不稳定性，并表明适当的方差正则化和源与目标之间的方向对齐对于稳定和有效的学习至关重要。我们进一步分析目标表示空间的选择如何影响具有结构源的流匹配，揭示了这些设计最有效的范围。针对多个文本到图像基准的广泛实验表明了一致且稳健的改进，包括在FID中高达3倍的收敛速度，突显了原则性源分布设计在条件流匹配中的实际好处。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve flow matching techniques for text-to-image generation by optimizing the source distribution, which is often overlooked in existing methods that default to a standard Gaussian distribution. The authors propose a novel approach that learns a condition-dependent source distribution tailored to the flow matching objective, addressing issues like distributional collapse and instability through variance regularization and directional alignment. Experimental results across various text-to-image benchmarks show significant enhancements, including up to a threefold increase in convergence speed measured by FID, demonstrating the advantages of a well-designed source distribution in conditional flow matching.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过优化源分布来改进文本到图像生成的流匹配技术，而现有方法通常将源分布固定为标准高斯分布。作者提出了一种新方法，学习适应流匹配目标的条件依赖源分布，通过方差正则化和方向对齐来解决分布崩溃和不稳定性等问题。多个基准测试的实验结果表明，性能显著提升，包括FID收敛速度提高至三倍，证明了在条件流匹配中良好设计的源分布的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation</div>
<div class="meta-line">Authors: Lingrui Li, Yanfeng Zhou, Nan Pu, Xin Chen, Zhun Zhong</div>
<div class="meta-line">First: 2026-02-05T17:47:35+00:00 · Latest: 2026-02-05T17:47:35+00:00</div>
<div class="meta-line">Comments: 8 pages, BIBM2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05937v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05937v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于医学图像分割的多尺度全局实例提示调优以实现持续测试时适应</div>
<div class="mono" style="margin-top:8px">分布偏移是来自不同临床中心的医学图像中常见的挑战，显著阻碍了预训练语义分割模型在多个领域的实际应用。持续测试时适应（CTTA）已成为应对不断演变的目标领域中的跨域偏移的有前景的方法。大多数现有的CTTA方法依赖于逐步更新模型参数，这不可避免地会导致错误累积和灾难性遗忘，尤其是在长期适应中。最近基于提示调优的工作显示出缓解上述两个问题的潜力，通过仅更新视觉提示。尽管这些方法表现出良好的性能，但仍存在几个局限性：1）缺乏多尺度提示多样性，2）实例特定知识的整合不足，3）隐私泄露的风险。为克服这些局限性，我们提出了多尺度全局实例提示调优（MGIPT），以增强提示的尺度多样性并捕获全局和实例级知识以实现稳健的CTTA。具体而言，MGIPT由自适应尺度实例提示（AIP）和多尺度全局级提示（MGP）组成。AIP动态学习轻量级和实例特定的提示，通过自适应最优尺度选择机制来减轻错误累积。MGP捕获不同尺度的领域级知识，以确保具有抗遗忘能力的稳健适应。这些互补组件通过加权集成方法结合，实现有效的双层适应，整合全局和局部信息。在医学图像分割基准上的广泛实验表明，我们的MGIPT在不断变化的目标领域中实现了稳健的适应，超越了最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by distribution shifts in medical images from various clinical centers, which hinder the effectiveness of pre-trained semantic segmentation models in real-world applications. The authors propose a novel method called Multi-scale Global-Instance Prompt Tuning (MGIPT), which enhances prompt diversity and incorporates both global and instance-specific knowledge for continual test-time adaptation (CTTA). Experimental results show that MGIPT significantly outperforms existing state-of-the-art methods in medical image segmentation, demonstrating robust adaptation capabilities across continuously evolving target domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决来自不同临床中心的医学图像中的分布转移问题，这妨碍了预训练语义分割模型在实际应用中的有效性。作者提出了一种新方法，称为多尺度全局实例提示调优（MGIPT），该方法增强了提示的多样性，并结合了全局和实例特定知识，以实现持续的测试时适应（CTTA）。实验结果表明，MGIPT在医学图像分割中显著优于现有的最先进方法，展示了在不断变化的目标领域中强大的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression</div>
<div class="meta-line">Authors: Kangjie Zhang, Wenxuan Huang, Xin Zhou, Boxiang Zhou, Dejia Song, Yuan Xie, Baochang Zhang, Lizhuang Ma, Nemo Chen, Xu Tang, Yao Hu, Shaohui Lin</div>
<div class="meta-line">First: 2026-02-05T17:25:16+00:00 · Latest: 2026-02-05T17:25:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05909v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05909v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP-Map：参数高效CLIP压缩的结构化矩阵映射</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）在各种计算机视觉任务中得到了广泛应用，例如文本到图像生成、图像-文本检索和图像描述。然而，CLIP面临着高内存和计算成本，这限制了其在资源有限的应用场景中的使用。现有的CLIP压缩方法通常通过选择其子集作为权重继承，利用掩码优化或重要权重测量来减少预训练CLIP权重的大小。然而，这种基于选择的权重继承往往妥协了特征表示能力，尤其是在极端压缩情况下。本文提出了一种新颖的基于映射的CLIP压缩框架CLIP-Map。它利用可学习矩阵通过克罗内克分解进行全映射，以尽可能保留原始权重的信息。为了解决可学习映射带来的优化挑战，我们提出了对角继承初始化，以减少分布偏移问题，从而实现高效有效的映射学习。大量实验结果表明，所提出的CLIP-Map在各种压缩比下优于基于选择的框架，特别是在高压缩设置下观察到显著的增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high memory and computation costs associated with Contrastive Language-Image Pre-training (CLIP), which limits its application in resource-constrained environments. The authors propose a novel compression framework called CLIP-Map, which utilizes learnable matrices for mapping and combining pretrained weights through Full-Mapping with Kronecker Factorization, aiming to retain as much information as possible from the original weights. Experimental results indicate that CLIP-Map significantly outperforms existing select-based compression methods across various compression ratios, particularly showing marked improvements under high compression conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决对比语言-图像预训练（CLIP）所需的高内存和计算成本，这限制了其在资源受限场景中的应用。作者提出了一种名为CLIP-Map的新型压缩框架，该框架利用可学习矩阵通过全映射和克罗内克分解来映射和组合预训练权重，旨在保留原始权重的信息。实验结果表明，CLIP-Map在各种压缩比下显著优于现有的基于选择的压缩方法，尤其在高压缩条件下取得了显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Alignment-Aware Model Adaptation via Feedback-Guided Optimization</div>
<div class="meta-line">Authors: Gaurav Bhatt, Aditya Chinchure, Jiawei Zhou, Leonid Sigal</div>
<div class="meta-line">First: 2026-02-02T16:03:16+00:00 · Latest: 2026-02-05T17:05:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02258v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02258v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过反馈引导优化的对齐感知模型适应</div>
<div class="mono" style="margin-top:8px">微调是将基础模型适应于下游任务的主要机制；然而，标准方法在很大程度上孤立地优化任务目标，而未考虑次要但关键的对齐目标（例如，安全性和幻觉避免）。因此，下游微调可能会降低对齐性，并未能纠正先前存在的错位行为。我们提出了一种对齐感知的微调框架，通过基于策略梯度的正则化集成外部对齐信号的反馈。我们的方法引入了一种自适应门控机制，动态平衡每个样本的监督梯度和对齐驱动的梯度，优先考虑不确定或错位的案例，同时允许良好对齐的示例遵循标准的监督更新。该框架进一步学习对完全错位输入的弃权行为，将保守响应直接纳入微调模型中。在一般和特定领域的指令微调基准上的实验表明，在不牺牲下游任务性能的情况下，持续减少有害和幻觉输出。额外分析显示对对抗性微调、基于提示的攻击和不安全初始化的鲁棒性，确立了自适应门控对齐优化作为一种有效的对齐保持和对齐恢复模型适应的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the fine-tuning process of foundation models for downstream tasks by addressing the critical alignment objectives that are often overlooked, such as safety and hallucination avoidance. The authors propose an alignment-aware fine-tuning framework that utilizes feedback from an external alignment signal through policy-gradient-based regularization, incorporating an adaptive gating mechanism to balance supervised and alignment-driven gradients on a per-sample basis. Experimental results show that this approach consistently reduces harmful and hallucinated outputs while maintaining performance on downstream tasks, demonstrating its robustness against adversarial fine-tuning and unsafe initializations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决标准方法中常被忽视的重要对齐目标，来改善基础模型在下游任务中的微调过程。作者提出了一种对齐感知的微调框架，该框架利用外部对齐信号的反馈，通过基于策略梯度的正则化，结合自适应门控机制，在每个样本的基础上平衡监督和对齐驱动的梯度。实验结果表明，该方法在保持下游任务性能的同时，持续减少有害和虚假输出，并且在对抗性微调和不安全初始化方面表现出鲁棒性，突显了其在保持和恢复模型对齐方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning</div>
<div class="meta-line">Authors: Qiang Zhu, Kuan Lu, Menghao Huo, Yuxiao Li</div>
<div class="meta-line">Venue: 2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL), pp. 626-632,</div>
<div class="meta-line">First: 2025-05-21T20:37:33+00:00 · Latest: 2026-02-05T16:53:22+00:00</div>
<div class="meta-line">Comments: Published in: 2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16001v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.16001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-to-image translation aims to learn a mapping between a source and a target domain, enabling tasks such as style transfer, appearance transformation, and domain adaptation. In this work, we explore a diffusion-based framework for image-to-image translation by adapting Diffusion Transformers (DiT), which combine the denoising capabilities of diffusion models with the global modeling power of transformers. To guide the translation process, we condition the model on image embeddings extracted from a pre-trained CLIP encoder, allowing for fine-grained and structurally consistent translations without relying on text or class labels. We incorporate both a CLIP similarity loss to enforce semantic consistency and an LPIPS perceptual loss to enhance visual fidelity during training. We validate our approach on two benchmark datasets: face2comics, which translates real human faces to comic-style illustrations, and edges2shoes, which translates edge maps to realistic shoe images. Experimental results demonstrate that DiT, combined with CLIP-based conditioning and perceptual similarity objectives, achieves high-quality, semantically faithful translations, offering a promising alternative to GAN-based models for paired image-to-image translation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散变换器和CLIP图像条件的图像到图像翻译</div>
<div class="mono" style="margin-top:8px">图像到图像翻译旨在学习源域和目标域之间的映射，从而实现风格迁移、外观转换和领域适应等任务。在这项工作中，我们探索了一种基于扩散的图像到图像翻译框架，通过适应扩散变换器（DiT），将扩散模型的去噪能力与变换器的全局建模能力相结合。为了指导翻译过程，我们将模型条件化于从预训练的CLIP编码器提取的图像嵌入，允许在不依赖文本或类别标签的情况下进行细粒度和结构一致的翻译。我们结合了CLIP相似性损失以强制语义一致性，以及LPIPS感知损失以增强训练过程中的视觉保真度。我们在两个基准数据集上验证了我们的方法：face2comics，将真实人脸翻译为漫画风格插图，以及edges2shoes，将边缘图转换为真实鞋子图像。实验结果表明，结合CLIP基础条件和感知相似性目标的DiT实现了高质量、语义忠实的翻译，为成对图像到图像翻译任务提供了有前景的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need for effective image-to-image translation methods that can perform tasks like style transfer and domain adaptation without relying on text or class labels. The authors propose a diffusion-based framework utilizing Diffusion Transformers (DiT) that integrates the denoising strengths of diffusion models with the comprehensive modeling capabilities of transformers, while conditioning the translation on image embeddings from a pre-trained CLIP encoder. Experimental results on the face2comics and edges2shoes datasets show that this approach achieves high-quality and semantically consistent translations, presenting a viable alternative to traditional GAN-based models for paired image-to-image translation tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了图像到图像翻译的挑战，该过程涉及在源域和目标域之间进行映射，应用于风格转移和领域适应等任务。作者提出了一种基于扩散的框架，利用扩散变换器（DiT），将扩散模型的去噪优势与变换器的全面建模能力相结合，同时基于预训练的CLIP编码器的图像嵌入进行条件翻译。在face2comics和edges2shoes数据集上的实验结果表明，该方法实现了高质量和语义一致的翻译，为成对图像到图像翻译任务提供了一种可行的替代传统GAN方法的方案。</div>
</details>
</div>
<div class="card">
<div class="title">DARWIN: Dynamic Agentically Rewriting Self-Improving Network</div>
<div class="meta-line">Authors: Henry Jiang</div>
<div class="meta-line">First: 2026-02-05T16:35:46+00:00 · Latest: 2026-02-05T16:35:46+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05848v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05848v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARWIN：动态代理自我改进网络</div>
<div class="mono" style="margin-top:8px">DARWIN是一个进化的GPT模型，利用类似遗传算法的优化结构，多个独立的GPT代理使用独特的训练代码进行单独训练。在每次迭代中，GPT模型被提示修改彼此的训练代码，以期以突变的方式提高其性能，然后通过遗传算法对最佳GPT代理进行基准测试和选择以进行下一次迭代。出于演示目的以及预算和时间限制，使用OpenAI API提示训练代码改进，并使用nanoGPT框架作为训练代码。DARWIN还利用持久的基于JSON的内存文件跟踪先前的推理和代码更改，以与模型性能的改进相关联，并提供双向接口以便于人机交互干预，允许模型请求升级，例如额外的数据集、训练脚本和文件层次结构的重组。在实验中，DARWIN在5次训练迭代中实现了模型FLOPS利用率（MFU）提高1.26%和困惑度提高2.07%的改进，相较于基线配置，展示了作为扩展进化GPT训练基础的良好能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind DARWIN is to enhance the performance of GPT models through an evolutionary approach that mimics genetic algorithms. The method involves training multiple independent GPT agents with unique training codes, where each agent iteratively modifies the training code of others to improve performance, while a genetic algorithm selects the best agents for subsequent iterations. Experimental results indicate that DARWIN achieved a 1.26 percent improvement in model FLOPS utilization and a 2.07 percent reduction in perplexity over five training iterations compared to baseline configurations, highlighting its potential for advancing evolutionary GPT training.</div>
<div class="mono" style="margin-top:8px">DARWIN项目的动机是通过模仿遗传算法的进化方法来提高GPT模型的性能。该方法涉及训练多个具有独特训练代码的独立GPT代理，每个代理在迭代中修改其他代理的训练代码以提高性能，同时遗传算法选择最佳代理进行后续迭代。实验结果表明，DARWIN在五次训练迭代中相比基线配置实现了1.26%的模型FLOPS利用率提升和2.07%的困惑度降低，显示出其在扩展进化GPT训练方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>外部感知者：通过框架实例化将外部推理与自我中心视觉先验解开</div>
<div class="mono" style="margin-top:8px">随着对空间基础任务（如视觉-语言导航/行动）的需求上升，视觉-语言模型（VLM）中的外部感知能力受到越来越多的关注。然而，VLM在需要明确视角转换的外部空间查询上仍然脆弱，答案依赖于在目标中心框架中的推理，而不是观察到的相机视图。因此，我们引入了外部感知者，这是一种无训练策略，通过现成的几何专家从一张或多张图像中恢复度量3D状态，然后实例化与指令语义意图对齐的查询条件外部参考框架。通过将重建的几何体确定性地转换为目标框架，并用结构化的、基于几何的表示来提示主干VLM，外部感知者将心理旋转从隐式推理转移到显式计算。我们在多个主干系列上评估外部感知者在空间推理基准上的表现，观察到在外部任务上有一致且显著的提升（约10%），同时保持强大的自我中心表现，超越了空间感知微调模型以及最先进的开源和专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance allocentric perception capabilities in Vision-Language Models (VLMs) for spatially grounded tasks, which often struggle with perspective shifts. The authors propose a training-free method called Allocentric Perceiver that utilizes geometric experts to recover metric 3D states from images and creates a query-conditioned allocentric reference frame. Experimental results demonstrate that Allocentric Perceiver achieves consistent improvements of approximately 10% on allocentric tasks across various backbone models while maintaining strong performance on egocentric tasks, outperforming both fine-tuned models and state-of-the-art systems.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉语言模型（VLMs）在执行以目标为中心的空间推理时的局限性，这对于视觉语言导航等任务至关重要。作者提出了一种名为Allocentric Perceiver的方法，该方法利用现成的几何专家从图像中恢复3D状态，并创建一个与查询条件相对应的以目标为中心的参考框架。实验结果表明，Allocentric Perceiver在以目标为中心的任务上实现了约10%的提升，同时在以自我为中心的任务上保持了强劲的表现，超越了空间感知微调模型和该领域的领先模型。</div>
</details>
</div>
<div class="card">
<div class="title">One-step Latent-free Image Generation with Pixel Mean Flows</div>
<div class="meta-line">Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</div>
<div class="meta-line">First: 2026-01-29T18:59:56+00:00 · Latest: 2026-02-05T15:31:04+00:00</div>
<div class="meta-line">Comments: Tech report. Code at https://github.com/Lyy-iiis/pMF</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22158v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22158v2">PDF</a> · <a href="https://github.com/Lyy-iiis/pMF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &quot;pixel MeanFlow&quot; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步无潜变量图像生成与像素均流</div>
<div class="mono" style="margin-top:8px">现代基于扩散/流的图像生成模型通常具有两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无潜变量的一步扩散/流铺平了道路。在这项工作中，我们朝着这一目标迈出了进一步的步伐，提出了“像素均流”（pMF）。我们的核心指导方针是将网络输出空间和损失空间分别进行公式化。网络目标被设计为位于假定的低维图像流形上（即x预测），而损失则通过速度空间中的均流定义。我们引入了图像流形与平均速度场之间的简单变换。在实验中，pMF在256x256分辨率（2.22 FID）和512x512分辨率（2.48 FID）上实现了一步无潜变量生成的强大结果，填补了这一领域的一个关键缺失部分。我们希望我们的研究能进一步推动基于扩散/流的生成模型的边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to advance image generation techniques by eliminating the need for multi-step sampling and latent space, which are common in modern diffusion and flow-based models. The authors propose a novel method called &quot;pixel MeanFlow&quot; (pMF), which separates the network output space from the loss space, targeting a low-dimensional image manifold for predictions while defining the loss using MeanFlow in the velocity space. Experimental results demonstrate that pMF achieves competitive performance in one-step latent-free image generation on ImageNet, with FID scores of 2.22 at 256x256 resolution and 2.48 at 512x512 resolution, thereby contributing significantly to the field of generative models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过消除现代扩散和基于流的模型中常见的多步采样和潜在空间的需求，推动图像生成技术的发展。作者提出了一种新方法，称为“像素均流”（pMF），该方法将网络输出空间与损失空间分开，目标是针对低维图像流形，同时通过速度空间中的均流定义损失。实验结果表明，pMF在ImageNet上实现了一步无潜在图像生成的竞争性能，在256x256分辨率下的FID得分为2.22，在512x512分辨率下为2.48，从而对生成模型领域做出了重要贡献。</div>
</details>
</div>
<div class="card">
<div class="title">RocqSmith: Can Automatic Optimization Forge Better Proof Agents?</div>
<div class="meta-line">Authors: Andrei Kozyrev, Nikita Khramov, Denis Lochmelis, Valerio Morelli, Gleb Solovev, Anton Podkopaev</div>
<div class="meta-line">First: 2026-02-05T15:28:26+00:00 · Latest: 2026-02-05T15:28:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05762v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05762v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RocqSmith：自动优化能否打造更好的证明代理？</div>
<div class="mono" style="margin-top:8px">本研究探讨了自动AI代理优化方法在形式验证环境中对现实代理的适用性，重点关注Rocq中的自动定理证明作为一个具有代表性和挑战性的领域。我们评估了不同的自动代理优化器在优化Rocq证明生成代理任务中的表现，并评估了代理系统的细粒度调优部分（如提示设计、上下文知识和控制策略）是否可以自动化。我们的结果表明，尽管几种优化器带来了可测量的改进，但简单的少量引导是最 consistently 有效的；然而，所研究的方法都无法与精心设计的最先进证明代理的性能相匹配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential of automatic optimization methods for enhancing proof agents in formal verification, specifically in the context of automated theorem proving using Rocq. The researchers applied various automatic agent optimizers to improve a Rocq proof-generation agent and examined the feasibility of automating aspects of agent tuning, including prompt design and control strategies. The findings indicate that while some optimizers lead to measurable improvements, simple few-shot bootstrapping consistently outperforms others, although none of the methods achieved the performance level of a meticulously designed state-of-the-art proof agent.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自动优化技术在形式验证中增强证明代理的潜力，特别是在使用Rocq进行自动定理证明的背景下。该研究评估了各种自动代理优化器，以确定它们在改善Rocq证明生成代理方面的有效性，重点关注提示设计和控制策略等方面。研究结果表明，尽管一些优化器提供了可测量的改进，但最可靠的方法是简单的少量引导，尽管测试的所有方法都未能超越精心设计的最先进证明代理的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification</div>
<div class="meta-line">Authors: Lexiang Hu, Youze Xue, Dian Li, Gang Liu, Zhouchen Lin</div>
<div class="meta-line">First: 2026-02-05T14:52:35+00:00 · Latest: 2026-02-05T14:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05729v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性全局与细粒度感知融合用于与硬负样本增强兼容的MLLM嵌入</div>
<div class="mono" style="margin-top:8px">多模态嵌入作为对齐视觉与语言的桥梁，主要实现方式——基于CLIP和基于MLLM的嵌入模型——都仅限于捕捉全局语义信息。尽管许多研究集中于细粒度理解，我们观察到当前MLLM嵌入所针对的复杂场景通常涉及全局与细粒度元素的混合感知模式，因此需要一种兼容的融合机制。本文提出了适应性全局与细粒度感知融合（AGFF-Embed），该方法促使MLLM生成多个关注不同语义信息维度的嵌入，然后进行自适应平滑聚合。此外，我们将AGFF-Embed与显式梯度增强（EGA）技术结合，实现批次内硬负样本增强，而无需对数据集进行细粒度编辑。在MMEB和MMVP-VLM基准上的评估表明，与其他多模态嵌入模型相比，AGFF-Embed在一般和细粒度理解方面全面实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing CLIP-based and MLLM-based embedding models, which primarily capture global semantic information and often struggle with complex scenarios requiring both global and fine-grained understanding. The authors propose a novel method called Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), which generates multiple embeddings focusing on different semantic dimensions and aggregates them adaptively. Experimental results on the MMEB and MMVP-VLM benchmarks demonstrate that AGFF-Embed achieves state-of-the-art performance in both general and fine-grained understanding, outperforming other multimodal embedding models.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有的基于CLIP和MLLM的嵌入模型在多模态上下文中捕捉全局和细粒度语义信息的局限性。作者提出了一种新方法，称为适应性全局和细粒度感知融合（AGFF-Embed），该方法生成多个关注不同语义维度的嵌入，并进行自适应聚合。在MMEB和MMVP-VLM基准上的实验结果表明，AGFF-Embed在一般和细粒度理解方面相比其他多模态嵌入模型实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions</div>
<div class="meta-line">Authors: Yihao Ouyang, Shiwei Li, Haozhao Wang, Xiandi Luo, Zhuoqi Hu, Yuetong Song, Qiyu Qin, Yichen Li, Ruixuan Li</div>
<div class="meta-line">First: 2026-02-05T14:36:44+00:00 · Latest: 2026-02-05T14:36:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05709v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非线性作为秩：具有径向基函数的生成低秩适配器</div>
<div class="mono" style="margin-top:8px">低秩适配（LoRA）通过两个低秩矩阵的乘积来近似预训练权重矩阵的更新。然而，标准LoRA遵循显式秩范式，增加模型容量需要向低秩矩阵添加更多行或列（即基向量），导致参数大幅增长。本文发现这些基向量存在显著的参数冗余，可以通过轻量级非线性函数紧凑表示。因此，我们提出了生成低秩适配器（GenLoRA），用非线性基向量生成替代显式基向量存储。具体而言，GenLoRA为每个低秩矩阵维护一个潜在向量，并使用一组轻量级径向基函数（RBF）合成基向量。每个RBF所需的参数远少于显式基向量，从而提高了GenLoRA的参数效率。针对多个数据集和架构的广泛实验表明，GenLoRA在较小的参数预算下获得了更高的有效LoRA秩，从而实现了更优的微调性能。代码可在 https://anonymous.4open.science/r/GenLoRA-1519 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies in standard low-rank adaptation (LoRA), which leads to significant parameter growth when increasing model capacity. The authors propose a novel approach called Generative Low-Rank Adapter (GenLoRA), which utilizes lightweight radial basis functions (RBFs) to generate basis vectors instead of storing them explicitly. Experimental results demonstrate that GenLoRA achieves higher effective LoRA ranks with fewer parameters, resulting in improved fine-tuning performance across various datasets and architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决标准低秩适应（LoRA）中的低效问题，该方法在增加模型容量时需要显著的参数增长，因为其采用显式秩范式。作者提出了一种新方法，称为生成低秩适配器（GenLoRA），该方法利用轻量级径向基函数（RBF）生成基向量，而不是显式存储它们。实验结果表明，GenLoRA在更少的参数下实现了更高的有效LoRA秩，从而在多个数据集和架构上提高了微调性能。</div>
</details>
</div>
<div class="card">
<div class="title">ShapeUP: Scalable Image-Conditioned 3D Editing</div>
<div class="meta-line">Authors: Inbar Gat, Dana Cohen-Bar, Guy Levy, Elad Richardson, Daniel Cohen-Or</div>
<div class="meta-line">First: 2026-02-05T13:59:16+00:00 · Latest: 2026-02-05T13:59:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05676v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShapeUP：可扩展的图像条件3D编辑</div>
<div class="mono" style="margin-top:8px">最近在3D基础模型方面的进展使得高保真资产的生成成为可能，但精确的3D操作仍然是一个重大挑战。现有的3D编辑框架往往在视觉可控性、几何一致性和可扩展性之间面临困难的权衡。具体而言，基于优化的方法速度极慢，多视角2D传播技术存在视觉漂移，而无训练的潜在操作方法本质上受限于固定的先验，无法直接受益于扩展。在本研究中，我们提出了ShapeUP，一个可扩展的图像条件3D编辑框架，将编辑形式化为在本地3D表示中的监督潜在到潜在的转换。这种形式化使ShapeUP能够基于预训练的3D基础模型，利用其强大的生成先验，同时通过监督训练将其适应于编辑。在实践中，ShapeUP在由源3D形状、编辑后的2D图像和相应的编辑后3D形状组成的三元组上进行训练，并使用3D扩散变换器（DiT）学习直接映射。这种图像作为提示的方法使得对局部和全局编辑的细粒度视觉控制成为可能，并实现了隐式、无掩码的定位，同时保持与原始资产的严格结构一致性。我们的广泛评估表明，ShapeUP在身份保留和编辑保真度方面始终优于当前的训练和无训练基线，为本地3D内容创作提供了一个强大且可扩展的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in precise 3D manipulation within existing frameworks, which often struggle with trade-offs between visual controllability, geometric consistency, and scalability. The authors propose ShapeUP, a scalable, image-conditioned 3D editing framework that utilizes a supervised latent-to-latent translation approach within a native 3D representation, building on a pretrained 3D foundation model. Experimental results show that ShapeUP, trained on triplets of source 3D shapes, edited 2D images, and corresponding edited 3D shapes using a 3D Diffusion Transformer, significantly outperforms current baselines in identity preservation and edit fidelity, thus providing a robust solution for 3D content creation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有3D编辑框架在精确3D操作方面面临的挑战，这些框架在视觉可控性、几何一致性和可扩展性方面存在困难。作者提出了ShapeUP，这是一种可扩展的图像条件3D编辑框架，采用在原生3D表示中进行监督的潜在到潜在转换方法，基于预训练的3D基础模型。实验结果表明，ShapeUP通过使用3D扩散变换器对源3D形状、编辑的2D图像和相应的编辑3D形状的三元组进行训练，在身份保持和编辑保真度方面显著优于当前基线，证明了其在稳健和可扩展的3D内容创作中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers</div>
<div class="meta-line">Authors: Jiaji Zhang, Hailiang Zhao, Guoxuan Zhu, Ruichao Sun, Jiaju Wu, Xinkui Zhao, Hanlin Tang, Weiyi Lu, Kan Liu, Tao Lan, Lin Qu, Shuiguang Deng</div>
<div class="meta-line">First: 2026-02-05T12:42:22+00:00 · Latest: 2026-02-05T12:42:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05605v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Shiva-DiT：基于残差的可微分Top-$k$选择用于高效扩散变换器</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）由于自注意力的平方扩展而产生高昂的计算成本。现有的剪枝方法无法同时满足可微分性、效率和硬件开销所需的严格静态预算。为了解决这个问题，我们提出了Shiva-DiT，通过基于残差的可微分Top-$k$选择有效地调和这些相互冲突的要求。通过利用一个考虑残差的直通估计器，我们的方法在静态编译中强制执行确定的令牌计数，同时通过残差梯度估计保持端到端的可学习性。此外，我们引入了上下文感知路由器和自适应比例策略，以自主学习自适应剪枝计划。在主流模型（包括SD3.5）上的实验表明，Shiva-DiT建立了新的帕累托前沿，与现有基线相比，实现了1.54$\times$的墙钟加速，并具有更高的保真度，有效消除了不规则张量开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computational costs associated with Diffusion Transformers (DiTs) due to the quadratic scaling of self-attention, highlighting the limitations of existing pruning methods that do not meet the requirements for differentiability and efficiency. The authors propose Shiva-DiT, which utilizes Residual-Based Differentiable Top-$k$ Selection to balance these conflicting needs, incorporating a residual-aware straight-through estimator for deterministic token counts and end-to-end learnability. Experimental results on models like SD3.5 show that Shiva-DiT achieves a 1.54× speedup while maintaining superior fidelity compared to existing methods, effectively reducing ragged tensor overheads.</div>
<div class="mono" style="margin-top:8px">该研究解决了扩散变换器因自注意力的平方扩展而导致的高计算成本问题，而现有的剪枝方法在保持可微性和效率方面未能有效管理这些问题。作者提出了Shiva-DiT，利用基于残差的可微Top-$k$选择来平衡这些相互矛盾的要求，采用基于残差的直通估计器实现确定性令牌计数，并通过残差梯度估计保持可学习性。在SD3.5等模型上的实验结果表明，Shiva-DiT实现了1.54倍的速度提升，同时保持了优于现有方法的保真度，从而在效率上设定了新的标准，而不妥协性能。</div>
</details>
</div>
<div class="card">
<div class="title">MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks</div>
<div class="meta-line">Authors: Long D. Nguyen, Binh P. Nguyen</div>
<div class="meta-line">First: 2026-02-05T11:39:49+00:00 · Latest: 2026-02-05T11:39:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05567v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05567v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAGPrompt：用于图神经网络的消息自适应图提示调优</div>
<div class="mono" style="margin-top:8px">预训练的图神经网络（GNN）迁移效果良好，但由于预训练目标与任务要求之间的不匹配，将其适应于下游任务仍然具有挑战性。图提示调优提供了一种参数高效的替代方案，但大多数方法仅修改输入或表示，保持消息传递不变，限制了它们适应邻域交互的能力。我们提出了消息自适应图提示调优，通过在消息传递步骤中注入可学习的提示，重新加权传入的邻居消息，并在消息聚合过程中添加任务特定的提示向量，同时保持主干GNN不变。该方法与常见的GNN主干和预训练策略兼容，并适用于各种下游设置。在多样的节点和图级数据集上的实验表明，在少样本设置中，相较于先前的图提示方法，表现出一致的提升，同时在全样本情况下实现与微调竞争的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adaptation of pre-trained graph neural networks (GNNs) to downstream tasks, addressing the challenges posed by mismatches between pre-training objectives and task requirements. The authors introduce a method called message-adaptive graph prompt tuning, which incorporates learnable prompts into the message passing step of GNNs to reweight incoming neighbor messages and integrate task-specific prompt vectors during message aggregation, while keeping the backbone GNN unchanged. Experimental results demonstrate that this approach consistently outperforms previous graph prompting methods in few-shot scenarios and achieves performance comparable to fine-tuning in full-shot settings across various node- and graph-level datasets.</div>
<div class="mono" style="margin-top:8px">本研究解决了由于预训练目标与特定任务要求之间的不匹配，导致预训练图神经网络（GNN）适应下游任务的挑战。作者提出了一种消息自适应图提示调优的方法，该方法在GNN的消息传递阶段中引入可学习的提示，允许对传入邻居消息进行重新加权，并在消息聚合过程中添加特定任务的向量，同时保持原始GNN架构不变。实验结果表明，该方法在少样本场景中始终优于先前的图提示方法，并在全样本设置中在各种节点和图级数据集上实现了与微调相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Plug-and-play linear attention with provable guarantees for training-free image restoration</div>
<div class="meta-line">Authors: Srinivasan Kidambi, Karthik Palaniappan, Pravin Nair</div>
<div class="meta-line">First: 2025-06-10T07:37:41+00:00 · Latest: 2026-02-05T11:35:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08520v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08520v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-head self-attention (MHSA) is a key building block in modern vision Transformers, yet its quadratic complexity in the number of tokens remains a major bottleneck for real-time and resource-constrained deployment. We present PnP-Nystra, a training-free Nyström-based linear attention module designed as a plug-and-play replacement for MHSA in {pretrained} image restoration Transformers, with provable kernel approximation error guarantees. PnP-Nystra integrates directly into window-based architectures such as SwinIR, Uformer, and Dehazeformer, yielding efficient inference without finetuning. Across denoising, deblurring, dehazing, and super-resolution on images, PnP-Nystra delivers $1.8$--$3.6\times$ speedups on an NVIDIA RTX 4090 GPU and $1.8$--$7\times$ speedups on CPU inference. Compared with the strongest training-free linear-attention baselines we evaluate, our method incurs the smallest quality drop and stays closest to the original model&#x27;s outputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可证明保证的即插即用线性注意力用于无训练图像恢复</div>
<div class="mono" style="margin-top:8px">多头自注意力（MHSA）是现代视觉Transformer中的关键构建块，但其在令牌数量上的平方复杂度仍然是实时和资源受限部署的主要瓶颈。我们提出PnP-Nystra，这是一种基于Nyström的无训练线性注意力模块，旨在作为MHSA在{预训练}图像恢复Transformer中的即插即用替代品，具有可证明的核近似误差保证。PnP-Nystra可以直接集成到基于窗口的架构中，如SwinIR、Uformer和Dehazeformer，实现高效推理而无需微调。在图像去噪、去模糊、去雾和超分辨率方面，PnP-Nystra在NVIDIA RTX 4090 GPU上提供$1.8$--$3.6\times$的加速，在CPU推理上提供$1.8$--$7\times$的加速。与我们评估的最强无训练线性注意力基线相比，我们的方法产生的质量下降最小，并且与原始模型的输出最接近。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the quadratic complexity of multi-head self-attention (MHSA) in vision Transformers, which limits their real-time and resource-constrained applications. The authors propose PnP-Nystra, a training-free Nyström-based linear attention module that serves as a plug-and-play alternative to MHSA in pretrained image restoration Transformers, ensuring provable kernel approximation error guarantees. Experimental results demonstrate that PnP-Nystra achieves speedups of 1.8 to 3.6 times on an NVIDIA RTX 4090 GPU and 1.8 to 7 times on CPU inference across various tasks such as denoising, deblurring, dehazing, and super-resolution, while maintaining the smallest quality drop compared to existing training-free linear-attention methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现代视觉变换器中多头自注意力（MHSA）的二次复杂性，这限制了实时和资源受限的应用。作者提出了PnP-Nystra，这是一种基于Nyström的训练无关线性注意力模块，可作为预训练图像恢复变换器中MHSA的即插即用替代方案，并确保可证明的核近似误差保证。实验结果表明，PnP-Nystra在去噪、去模糊、去雾和超分辨率等任务中，在NVIDIA RTX 4090 GPU上实现了1.8到3.6倍的加速，在CPU推理上实现了1.8到7倍的加速，同时与现有的训练无关线性注意力方法相比，保持了最小的质量下降。</div>
</details>
</div>
<div class="card">
<div class="title">FastVMT: Eliminating Redundancy in Video Motion Transfer</div>
<div class="meta-line">Authors: Yue Ma, Zhikai Wang, Tianhao Ren, Mingzhe Zheng, Hongyu Liu, Jiayi Guo, Mark Fong, Yuxuan Xue, Zixiang Zhao, Konrad Schindler, Qifeng Chen, Linfeng Zhang</div>
<div class="meta-line">First: 2026-02-05T11:15:59+00:00 · Latest: 2026-02-05T11:15:59+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR2026, Project page: fastvmt.gitHub.io, Code: https://github.com/mayuelala/FastVMT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05551v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05551v1">PDF</a> · <a href="https://github.com/mayuelala/FastVMT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FastVMT：消除视频运动转移中的冗余</div>
<div class="mono" style="margin-top:8px">视频运动转移旨在通过根据文本提示生成视觉内容，同时转移参考视频中观察到的运动模式来合成视频。最近的方法主要使用扩散变换器（DiT）架构。为了实现令人满意的运行时间，一些方法试图加速DiT中的计算，但未能解决结构性低效源。在这项工作中，我们识别并消除了早期工作中的两种计算冗余：运动冗余是因为通用DiT架构未反映帧间运动小且平滑的事实；梯度冗余则发生在忽视梯度沿扩散轨迹缓慢变化的情况下。为了减轻运动冗余，我们将相应的注意力层掩蔽到局部邻域，以便不对不必要的远距离图像区域计算交互权重。为了利用梯度冗余，我们设计了一种优化方案，重用先前扩散步骤的梯度，并跳过不必要的梯度计算。平均而言，FastVMT实现了3.43倍的加速，而不降低生成视频的视觉保真度或时间一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of video motion transfer, which synthesizes videos based on text prompts while transferring motion patterns from reference videos. The authors propose a method called FastVMT that addresses computational inefficiencies in existing Diffusion Transformer architectures by eliminating two types of redundancy: motion redundancy and gradient redundancy. The experimental results demonstrate that FastVMT achieves an average speedup of 3.43 times without compromising the visual fidelity or temporal consistency of the generated videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高视频运动转移的效率，该技术基于文本提示合成视频，同时转移参考视频中的运动模式。作者提出了一种名为FastVMT的方法，通过消除两种冗余（运动冗余和梯度冗余）来解决扩散变换器架构中的计算低效问题。实验结果表明，FastVMT在不影响生成视频的视觉保真度或时间一致性的情况下，平均实现了3.43倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">RefAM: Attention Magnets for Zero-Shot Referral Segmentation</div>
<div class="meta-line">Authors: Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele</div>
<div class="meta-line">First: 2025-09-26T17:59:57+00:00 · Latest: 2026-02-05T10:20:31+00:00</div>
<div class="meta-line">Comments: Project Page: https://refam-diffusion.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22650v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22650v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://refam-diffusion.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach achieves strong performance and surpasses prior methods on most datasets, establishing a new state of the art without fine-tuning, additional components and complex reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RefAM：用于零-shot引用分割的注意力磁铁</div>
<div class="mono" style="margin-top:8px">大多数现有的引用分割方法仅通过微调或组合多个预训练模型来实现强大的性能，通常需要额外的训练和架构修改。同时，大规模生成扩散模型编码了丰富的语义信息，使其作为通用特征提取器具有吸引力。在这项工作中，我们引入了一种新方法，直接利用扩散变换器的特征和注意力分数用于下游任务，无需架构修改或额外训练。为了系统地评估这些特征，我们扩展了基准测试，增加了涵盖图像和视频的视觉语言基础任务。我们的关键见解是停用词充当注意力磁铁：它们积累多余的注意力，可以被过滤以减少噪声。此外，我们识别出在更深层次中出现的全局注意力汇（GAS），并表明它们可以安全地被抑制或重定向到辅助标记上，从而产生更清晰和更准确的基础图。我们进一步提出了一种注意力重分配策略，其中附加的停用词将背景激活划分为更小的簇，从而产生更清晰和更局部的热图。基于这些发现，我们开发了RefAM，一个简单的无训练基础框架，结合了交叉注意力图、GAS处理和重分配。在零-shot引用图像和视频分割基准测试中，我们的方法实现了强大的性能，并在大多数数据集上超越了先前的方法，建立了新的最先进水平，无需微调、额外组件和复杂推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve referring segmentation performance without the need for fine-tuning or complex model architectures. The authors introduce RefAM, a method that leverages attention scores from diffusion transformers as feature extractors for downstream tasks, which requires no additional training or architectural changes. Key experimental findings reveal that stop words function as attention magnets, allowing for noise reduction, and that global attention sinks can be managed to enhance grounding maps, resulting in strong performance across zero-shot referring image and video segmentation benchmarks, surpassing previous methods and establishing a new state of the art.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高引用分割的性能，而无需微调或复杂的模型架构，这在现有方法中很常见。作者提出了RefAM，这是一种新颖的方法，利用扩散变换器的注意力分数作为下游任务的特征提取器，避免了额外的训练或架构更改。关键实验结果表明，该方法有效利用停用词作为注意力磁铁来增强特征提取，识别并管理全局注意力汇以提高定位精度，并实施注意力重分配策略，从而产生更清晰和更局部化的分割热图，在零-shot引用图像和视频分割基准中取得了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter</div>
<div class="meta-line">Authors: Xukun Li, Yu Sun, Lei Zhang, Bosheng Huang, Yibo Peng, Yuan Meng, Haojun Jiang, Shaoxuan Xie, Guacai Yao, Alois Knoll, Zhenshan Bing, Xinlong Wang, Zhenguo Sun</div>
<div class="meta-line">First: 2026-02-05T10:13:34+00:00 · Latest: 2026-02-05T10:13:34+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05513v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05513v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DECO：用于双手灵巧操作的解耦多模态扩散变换器及其插件触觉适配器</div>
<div class="mono" style="margin-top:8px">DECO框架概述。DECO是一个基于DiT的策略，解耦多模态条件。图像和动作标记通过联合自注意力进行交互，而本体状态和可选条件通过自适应层归一化注入。触觉信号通过交叉注意力注入，同时使用轻量级的基于LoRA的适配器高效微调预训练策略。DECO还配有DECO-50，这是一个包含触觉感知的双手灵巧操作数据集，包含4个场景和28个子任务，覆盖超过50小时的数据，约500万帧和8000条成功轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for improved bimanual dexterous manipulation in robotic systems, particularly through the integration of multimodal sensory inputs. The proposed DECO framework utilizes a decoupled multimodal diffusion transformer that employs joint self-attention for image and action tokens, while proprioceptive states and tactile signals are incorporated through adaptive layer normalization and cross attention, respectively. Key experimental findings include the development of the DECO-50 dataset, which features over 50 hours of data across 4 scenarios and 28 sub-tasks, resulting in approximately 5 million frames and 8,000 successful manipulation trajectories.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于提高机器人双手灵巧操作的能力，特别是通过整合触觉感知。作者提出了DECO框架，该框架利用解耦的多模态扩散变换器，通过联合自注意力处理图像和动作标记，同时通过自适应层归一化和交叉注意力引入本体状态和触觉信号。主要实验结果包括开发了DECO-50数据集，该数据集涵盖超过50小时的数据和8000条成功的操作轨迹，展示了所提方法在增强机器人操作能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt State Space Model</div>
<div class="meta-line">Authors: Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao</div>
<div class="meta-line">First: 2025-04-24T16:46:32+00:00 · Latest: 2026-02-05T09:06:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.17732v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.17732v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-One image restoration aims to address multiple image degradation problems using a single model, offering a more practical and versatile solution compared to designing dedicated models for each degradation type. Existing approaches typically rely on Degradation-specific models or coarse-grained degradation prompts to guide image restoration. However, they lack fine-grained modeling of degradation information and face limitations in balancing multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a novel All-in-One image restoration framework that introduces a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM). The DP-SSM leverages the fine-grained degradation features captured by the extractor as dynamic prompts, which are then incorporated into the state space modeling process. This enhances the model&#x27;s adaptability to diverse degradation types, while a complementary High-Frequency Enhancement Block (HEB) recovers local high-frequency details. Extensive experiments on a mixed dataset containing seven degradation types show that DPMambaIR achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These results highlight the potential and superiority of DPMambaIR as a unified solution for All-in-One image restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPMambaIR：通过降级感知提示状态空间模型的全能图像修复</div>
<div class="mono" style="margin-top:8px">全能图像修复旨在通过单一模型解决多种图像降级问题，相较于为每种降级类型设计专用模型，提供了更实用和多功能的解决方案。现有方法通常依赖于特定降级模型或粗粒度降级提示来指导图像修复。然而，它们缺乏对降级信息的细粒度建模，并在平衡多任务冲突方面面临限制。为克服这些限制，我们提出了DPMambaIR，这是一种新颖的全能图像修复框架，引入了细粒度降级提取器和降级感知提示状态空间模型（DP-SSM）。DP-SSM利用提取器捕获的细粒度降级特征作为动态提示，然后将其纳入状态空间建模过程。这增强了模型对多种降级类型的适应性，同时一个互补的高频增强块（HEB）恢复局部高频细节。在包含七种降级类型的混合数据集上的大量实验表明，DPMambaIR在PSNR和SSIM中分别达到了27.69dB和0.893的最佳性能。这些结果突显了DPMambaIR作为全能图像修复统一解决方案的潜力和优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a versatile solution for All-in-One image restoration that can effectively handle multiple types of image degradation without the need for separate models. The authors introduce DPMambaIR, which employs a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM) to dynamically incorporate degradation features into the restoration process. Experimental results demonstrate that DPMambaIR outperforms existing methods, achieving a PSNR of 27.69dB and an SSIM of 0.893 across a mixed dataset with seven degradation types, indicating its effectiveness as a unified restoration framework.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个多功能的图像修复解决方案，能够有效解决多种退化问题，而无需单独的模型。作者提出了DPMambaIR，利用细粒度退化提取器和退化感知提示状态空间模型（DP-SSM），动态地将详细的退化特征纳入修复过程。实验结果表明，DPMambaIR在包含七种不同退化类型的混合数据集上表现优于现有方法，PSNR达到27.69dB，SSIM为0.893，表明其作为统一修复框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching</div>
<div class="meta-line">Authors: Chang Zou, Changlin Li, Yang Li, Patrol Li, Jianbing Wu, Xiao He, Songtao Liu, Zhao Zhong, Kailin Huang, Linfeng Zhang</div>
<div class="meta-line">First: 2026-02-05T08:45:08+00:00 · Latest: 2026-02-05T08:45:08+00:00</div>
<div class="meta-line">Comments: 17 pages, 7 figures; cvpr2026 submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05449v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisCa：通过适应蒸馏的可学习特征缓存加速视频扩散变换器</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在视频生成领域取得了巨大成功，但这一进展伴随着快速上升的计算负担。在现有的加速方法中，特征缓存因其无训练特性和显著的加速性能而受到欢迎，但在进一步压缩时不可避免地面临语义和细节的丢失。另一种广泛采用的方法是基于训练的步蒸馏，尽管在图像生成中取得了成功，但在视频生成中也面临着在少数步骤下的剧烈降级。此外，当简单地将无训练特征缓存应用于步蒸馏模型时，由于采样步骤更稀疏，质量损失变得更加严重。本文首次新颖地引入了一种适应蒸馏的可学习特征缓存机制。我们采用轻量级可学习神经预测器替代传统的无训练启发式方法，以便更准确地捕捉高维特征演变过程。此外，我们探讨了在大规模视频模型上高度压缩蒸馏的挑战，并提出了一种保守的限制均流方法，以实现更稳定和无损的蒸馏。通过这些举措，我们进一步将加速边界推向$11.8\times$，同时保持生成质量。大量实验证明了我们方法的有效性。代码在补充材料中，并将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the increasing computational burden associated with video generation using diffusion models, while maintaining quality. The authors introduce a novel distillation-compatible learnable feature caching mechanism that utilizes a lightweight neural predictor to enhance the accuracy of high-dimensional feature evolution capture. Experimental results indicate that this approach achieves an acceleration of up to 11.8 times compared to existing methods, while effectively preserving the quality of generated videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决与使用扩散模型进行视频生成相关的日益增加的计算负担，同时保持生成质量。作者提出了一种新颖的兼容蒸馏的可学习特征缓存机制，利用轻量级神经预测器来提高高维特征演变捕捉的准确性，与传统的无训练方法形成对比。实验结果表明，该方法能够在不显著降低质量的情况下，将视频生成加速至11.8倍，证明了其在克服现有加速技术局限性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TensLoRA: Tensor Alternatives for Low-Rank Adaptation</div>
<div class="meta-line">Authors: Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, François Leduc-Primeau</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-22T17:15:23+00:00 · Latest: 2026-02-05T08:34:09+00:00</div>
<div class="meta-line">Comments: Published at ICASSP 2026. 5 pages, 1 figure, 2 tables. Code can be found at https://github.com/ax-le/TensLoRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19391v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19391v3">PDF</a> · <a href="https://github.com/ax-le/TensLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TensLoRA：低秩适应的张量替代方案</div>
<div class="mono" style="margin-top:8px">低秩适应（LoRA）广泛用于通过向注意力投影添加可训练的低秩矩阵来高效适应变换器。尽管有效，这些矩阵被认为是每个注意力投影（查询、键和值）和每层的独立的。最近的扩展考虑了联合的基于张量的适应，但仅限于有限的形式且没有系统框架。我们引入了TensLoRA，一个统一框架，将LoRA更新聚合为高阶张量，并建模广泛的基于张量的低秩适应家族。我们的公式推广了现有的基于张量的方法，并实现了特定模式的压缩率，使参数预算能够根据模态和任务进行调整。在视觉和语言基准上的实验表明，张量构造直接影响性能，有时在相似参数数量下优于标准LoRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of Low-Rank Adaptation (LoRA) for Transformers by addressing the limitations of independent low-rank matrices used in attention projections. The authors propose TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors, allowing for a broader range of tensor-based low-rank adaptations with mode-specific compression rates tailored to different modalities and tasks. Experimental results on vision and language benchmarks demonstrate that the proposed tensor construction can improve performance, sometimes surpassing standard LoRA while maintaining similar parameter counts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决注意力投影中独立低秩矩阵的局限性，来提高适应变换器的效率。作者提出了TensLoRA，一个统一框架，将LoRA更新聚合为高阶张量，从而允许更广泛的基于张量的低秩适应，并根据不同的模态和任务量身定制特定模式的压缩率。在视觉和语言基准上的实验结果表明，所提出的张量构造可以提高性能，有时在保持相似参数数量的情况下超越标准LoRA。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Velocity: A Variance Perspective on Flow Matching</div>
<div class="meta-line">Authors: Donglin Yang, Yongxing Zhang, Xin Yu, Liang Hou, Xin Tao, Pengfei Wan, Xiaojuan Qi, Renjie Liao</div>
<div class="meta-line">First: 2026-02-05T08:25:05+00:00 · Latest: 2026-02-05T08:25:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05435v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05435v1">PDF</a> · <a href="https://github.com/linYDTHU/StableVelocity">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定速度：流匹配的方差视角</div>
<div class="mono" style="margin-top:8px">尽管流匹配优雅，但其依赖于单样本条件速度导致高方差训练目标，从而使优化不稳定并减慢收敛。通过明确表征这种方差，我们识别出1）在先验附近的高方差状态，优化具有挑战性，2）在数据分布附近的低方差状态，条件和边际速度几乎重合。利用这一见解，我们提出了稳定速度，一个统一框架，改善训练和采样。对于训练，我们引入了稳定速度匹配（StableVM），一种无偏方差减少目标，以及方差感知表示对齐（VA-REPA），在低方差状态下自适应增强辅助监督。对于推理，我们表明低方差状态下的动态允许闭式形式简化，从而实现稳定速度采样（StableVS），一种无需微调的加速。在ImageNet $256\times256$ 和大型预训练文本到图像及文本到视频模型（包括SD3.5、Flux、Qwen-Image和Wan2.2）上的大量实验表明，在低方差状态下训练效率持续提高，采样速度超过$2\times$，且不降低样本质量。我们的代码可在 https://github.com/linYDTHU/StableVelocity 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of high-variance training targets in flow matching, which hinder optimization and slow convergence. The authors propose a framework called Stable Velocity that characterizes variance in flow matching, identifying high-variance and low-variance regimes. They introduce Stable Velocity Matching for unbiased variance reduction and Variance-Aware Representation Alignment to enhance supervision in low-variance conditions. Experimental results on ImageNet and various pretrained models show significant improvements in training efficiency and over two times faster sampling in the low-variance regime without compromising sample quality.</div>
<div class="mono" style="margin-top:8px">本研究解决了流匹配中高方差训练目标带来的挑战，这会导致优化不稳定和收敛缓慢。作者提出了一个名为稳定速度的框架，其中包括用于无偏方差减少的稳定速度匹配（StableVM）和在低方差条件下增强监督的方差感知表示对齐（VA-REPA）。在ImageNet和多个预训练模型上的实验结果表明，该方法显著提高了训练效率，并在低方差条件下实现了超过两倍的采样速度，同时保持了样本质量。</div>
</details>
</div>
<div class="card">
<div class="title">Image inpainting for corrupted images by using the semi-super resolution GAN</div>
<div class="meta-line">Authors: Mehrshad Momen-Tayefeh, Mehrdad Momen-Tayefeh, Amir Ali Ghafourian Ghahramani</div>
<div class="meta-line">First: 2024-09-19T10:21:16+00:00 · Latest: 2026-02-05T07:25:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.12636v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.12636v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image inpainting is a valuable technique for enhancing images that have been corrupted. The primary challenge in this research revolves around the extent of corruption in the input image that the deep learning model must restore. To address this challenge, we introduce a Generative Adversarial Network (GAN) for learning and replicating the missing pixels. Additionally, we have developed a distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess the robustness and accuracy of our proposed model. Our training process involves varying levels of pixel corruption to attain optimal accuracy and generate high-quality images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用半监督分辨率GAN对损坏图像进行图像修复</div>
<div class="mono" style="margin-top:8px">图像修复是一种增强损坏图像的有价值技术。本研究的主要挑战在于输入图像的损坏程度，深度学习模型必须恢复这些损坏。为了解决这个挑战，我们引入了一种生成对抗网络（GAN）来学习和复制缺失的像素。此外，我们开发了一种独特的超分辨率GAN（SRGAN）变体，称为半SRGAN（SSRGAN）。此外，我们利用三个不同的数据集来评估我们提出的模型的鲁棒性和准确性。我们的训练过程涉及不同程度的像素损坏，以达到最佳准确性并生成高质量图像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image inpainting techniques for restoring corrupted images, which poses a significant challenge due to varying levels of corruption. The authors propose a Generative Adversarial Network (GAN) and a novel variant called Semi-SRGAN (SSRGAN) to effectively learn and reconstruct missing pixels. Experimental results demonstrate the robustness and accuracy of the SSRGAN across three diverse datasets, achieving optimal accuracy and producing high-quality restored images even with different levels of pixel corruption.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善图像修复技术，以恢复受损图像，这面临着与损坏程度相关的挑战。作者提出了一种专门设计的生成对抗网络（GAN），用于学习和复制缺失的像素，并引入了一种新变体，称为半超分辨率GAN（SSRGAN）。实验结果表明，该模型在三个不同的数据集上具有良好的鲁棒性和准确性，训练过程中在不同的像素损坏程度下进行，以实现高质量的图像修复。</div>
</details>
</div>
<div class="card">
<div class="title">UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching</div>
<div class="meta-line">Authors: Woongjib Choi, Sangmin Lee, Hyungseob Lim, Hong-Goo Kang</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-01T11:04:53+00:00 · Latest: 2026-02-05T07:24:16+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00771v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00771v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present a vocoder-free framework for audio super-resolution that employs a flow matching generative model to capture the conditional distribution of complex-valued spectral coefficients. Unlike conventional two-stage diffusion-based approaches that predict a mel-spectrogram and then rely on a pre-trained neural vocoder to synthesize waveforms, our method directly reconstructs waveforms via the inverse Short-Time Fourier Transform (iSTFT), thereby eliminating the dependence on a separate vocoder. This design not only simplifies end-to-end optimization but also overcomes a critical bottleneck of two-stage pipelines, where the final audio quality is fundamentally constrained by vocoder performance. Experiments show that our model consistently produces high-fidelity 48 kHz audio across diverse upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniverSR：通过无声码器流匹配实现统一和多功能音频超分辨率</div>
<div class="mono" style="margin-top:8px">本文提出了一种无声码器的音频超分辨率框架，采用流匹配生成模型捕捉复数谱系数的条件分布。与传统的两阶段扩散方法不同，我们的方法直接通过逆短时傅里叶变换（iSTFT）重建波形，从而消除了对单独声码器的依赖。这一设计不仅简化了端到端优化，还克服了两阶段管道的一个关键瓶颈，即最终音频质量根本上受限于声码器性能。实验表明，我们的模型在不同的上采样因子下始终能够生成高保真48 kHz音频，在语音和一般音频数据集上实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of conventional audio super-resolution methods that rely on vocoders, which can constrain audio quality. The authors propose a vocoder-free framework utilizing a flow matching generative model to directly reconstruct waveforms through the inverse Short-Time Fourier Transform (iSTFT). Experimental results demonstrate that their approach consistently generates high-fidelity 48 kHz audio across various upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统音频超分辨率方法的局限性，这些方法依赖于预训练的声码器，可能限制音频质量。作者提出了一种无声码器的框架，利用流匹配生成模型通过逆短时傅里叶变换（iSTFT）直接重建波形。实验结果表明，他们的方法在不同的上采样因子下始终生成高保真48 kHz音频，在语音和一般音频数据集上实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Safety Alignment for Text-to-Image Diffusion Models</div>
<div class="meta-line">Authors: Yu Lei, Jinbin Bai, Qingyu Shi, Aosong Feng, Hongcheng Gao, Xiao Zhang, Rex Ying</div>
<div class="meta-line">First: 2025-08-02T02:23:20+00:00 · Latest: 2026-02-05T07:15:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01151v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.01151v3">PDF</a> · <a href="https://github.com/M-E-AGI-Lab/PSAlign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models have revolutionized visual content generation, yet their deployment is hindered by a fundamental limitation: safety mechanisms enforce rigid, uniform standards that fail to reflect diverse user preferences shaped by age, culture, or personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that transitions generative safety from static filtration to user-conditioned adaptation. We introduce Sage, a large-scale dataset capturing diverse safety boundaries across 1,000 simulated user profiles, covering complex risks often missed by traditional datasets. By integrating these profiles via a parameter-efficient cross-attention adapter, PSA dynamically modulates generation to align with individual sensitivities. Extensive experiments demonstrate that PSA achieves a calibrated safety-quality trade-off: under permissive profiles, it relaxes over-cautious constraints to enhance visual fidelity, while under restrictive profiles, it enforces state-of-the-art suppression, significantly outperforming static baselines. Furthermore, PSA exhibits superior instruction adherence compared to prompt-engineering methods, establishing personalization as a vital direction for creating adaptive, user-centered, and responsible generative AI. Our code, data, and models are publicly available at https://github.com/M-E-AGI-Lab/PSAlign.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散模型的个性化安全对齐</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型彻底改变了视觉内容生成，但其部署受到一个基本限制的阻碍：安全机制强制执行僵化的统一标准，未能反映由年龄、文化或个人信仰塑造的多样化用户偏好。为了解决这个问题，我们提出了个性化安全对齐（PSA），一个将生成安全从静态过滤转变为用户条件适应的框架。我们引入了Sage，一个大规模数据集，捕捉了1,000个模拟用户档案中的多样化安全边界，涵盖了传统数据集常常忽视的复杂风险。通过使用参数高效的交叉注意力适配器整合这些档案，PSA动态调节生成以与个体敏感性对齐。大量实验表明，PSA实现了经过校准的安全与质量的权衡：在宽松的档案下，它放宽过于谨慎的约束以增强视觉保真度，而在严格的档案下，它实施最先进的抑制，显著优于静态基线。此外，PSA在遵循指令方面表现优于提示工程方法，确立了个性化作为创建适应性、以用户为中心和负责任的生成AI的重要方向。我们的代码、数据和模型可在https://github.com/M-E-AGI-Lab/PSAlign公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the deployment of text-to-image diffusion models by addressing the limitations of rigid safety mechanisms that do not accommodate diverse user preferences. The authors propose a framework called Personalized Safety Alignment (PSA), which shifts generative safety from static filtration to user-conditioned adaptation, utilizing a large-scale dataset named Sage that captures diverse safety boundaries across 1,000 simulated user profiles. Experimental results show that PSA effectively balances safety and visual quality by adjusting constraints based on user profiles, outperforming static baselines and demonstrating better adherence to user instructions compared to traditional prompt-engineering methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善文本到图像扩散模型的安全机制，这些机制目前施加的严格标准无法满足多样化用户偏好的需求。作者提出了一个名为个性化安全对齐（PSA）的框架，该框架将生成安全性从静态过滤转变为用户条件适应。研究中引入了一个大型数据集Sage，该数据集捕捉了1,000个模拟用户档案的多样化安全边界，并利用参数高效的交叉注意力适配器根据个体敏感性动态调整生成。实验结果表明，PSA有效地平衡了安全性和质量，在宽松的用户档案下提高了视觉保真度，同时在严格的用户档案下保持强有力的抑制，且在指令遵循方面优于传统静态方法和提示工程技术，突显了个性化在生成AI中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</div>
<div class="meta-line">Authors: Hao Feng, Wei Shi, Ke Zhang, Xiang Fei, Lei Liao, Dingkang Yang, Yongkun Du, Xuecheng Wu, Jingqun Tang, Yang Liu, Hong Chen, Can Huang</div>
<div class="meta-line">First: 2026-02-05T07:09:57+00:00 · Latest: 2026-02-05T07:09:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05384v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05384v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dolphin-v2：通过可扩展锚点提示的通用文档解析</div>
<div class="mono" style="margin-top:8px">文档解析随着视觉语言模型（VLM）在OCR能力上的进步而受到广泛关注。然而，该领域仍然在数十个专业模型之间分散，具有不同的优势，迫使用户在复杂的模型选择中徘徊，并限制了系统的可扩展性。此外，现有的两阶段方法依赖于轴对齐的边界框进行布局检测，未能有效处理扭曲或拍摄的文档。为此，我们提出了Dolphin-v2，这是一种两阶段文档图像解析模型，显著改进了原始Dolphin。在第一阶段，Dolphin-v2联合执行文档类型分类（数字原生与拍摄）和布局分析。对于数字原生文档，它进行更细粒度的元素检测和阅读顺序预测。在第二阶段，我们采用混合解析策略：拍摄的文档作为完整页面整体解析，以处理几何扭曲，而数字原生文档则通过检测到的布局锚点进行元素级并行解析，从而实现高效的内容提取。与原始Dolphin相比，Dolphin-v2引入了几个关键增强： (1) 通过整体页面级理解对拍摄文档进行稳健解析， (2) 更细粒度的元素检测（21个类别）及语义属性提取，如作者信息和文档元数据， (3) 代码块识别及缩进保留，而现有系统通常缺乏这些功能。我们在DocPTBench、OmniDocBench和自建的RealDoc-160基准上进行了全面评估。结果显示显著改进：在具有挑战性的OmniDocBench上整体提高14.78分，拍摄文档的错误率降低91%，同时通过并行处理保持高效推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the fragmentation in document parsing models and the limitations of existing two-stage approaches that struggle with distorted or photographed documents. The authors introduce Dolphin-v2, a two-stage document image parsing model that enhances the original Dolphin by jointly performing document type classification and layout analysis in the first stage, followed by a hybrid parsing strategy in the second stage that accommodates both photographed and digital-born documents. Key experimental findings indicate that Dolphin-v2 achieves significant improvements, including a 14.78-point increase on the OmniDocBench and a 91% reduction in errors for photographed documents, while also enabling efficient content extraction through parallel processing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由于众多专业模型导致的文档解析碎片化问题，这使得用户体验复杂且限制了系统的可扩展性。作者提出了Dolphin-v2，这是一种两阶段的文档图像解析模型，通过在第一阶段联合分类文档类型和分析布局，第二阶段采用混合解析策略来处理拍摄和数字文档。实验结果显示显著改进，包括在OmniDocBench上提高14.78分，以及对拍摄文档的错误率减少91%，同时通过并行处理确保高效的内容提取。</div>
</details>
</div>
<div class="card">
<div class="title">Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</div>
<div class="meta-line">Authors: Yongwoo Kim, Sungmin Cha, Hyunsoo Kim, Jaewon Lee, Donghyun Kim</div>
<div class="meta-line">First: 2026-02-05T06:05:24+00:00 · Latest: 2026-02-05T06:05:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05339v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过不安全-安全配对和方向性Fisher加权适应实现一致性保持的概念消除</div>
<div class="mono" style="margin-top:8px">随着文本到图像扩散模型的多样性增加，有选择地消除不良概念（例如，有害内容）的能力变得不可或缺。然而，现有的概念消除方法主要集中在去除不安全概念上，而未提供相应安全替代方案的指导，这往往导致无法保持原始生成与消除生成之间的结构和语义一致性。本文提出了一种新颖的框架，PAIRed Erasing (PAIR)，将概念消除从简单的移除重新构建为使用不安全-安全配对进行一致性保持的语义重新对齐。我们首先从不安全输入生成安全对应物，同时保持结构和语义的保真度，形成配对的不安全-安全多模态数据。利用这些配对，我们引入两个关键组件：（1）配对语义重新对齐，一种指导目标，使用不安全-安全配对将目标概念明确映射到语义对齐的安全锚点；（2）DoRA的Fisher加权初始化，使用不安全-安全配对初始化参数高效的低秩适应矩阵，鼓励生成安全替代方案，同时选择性地抑制不安全概念。这些组件共同实现了细粒度的消除，仅去除目标概念，同时保持整体语义一致性。大量实验表明，我们的方法显著优于最先进的基线，实现了有效的概念消除，同时保持结构完整性、语义连贯性和生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for effective concept erasure in text-to-image diffusion models, particularly for removing harmful content while maintaining structural and semantic consistency. The authors propose a novel framework called PAIRed Erasing (PAIR), which utilizes unsafe-safe pairs to facilitate consistency-preserving semantic realignment rather than mere removal of concepts. Key experimental findings indicate that this approach significantly outperforms existing methods, achieving targeted concept erasure while preserving the integrity and coherence of the generated images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于在文本到图像的扩散模型中有效地去除概念，特别是去除有害内容的同时保持结构和语义的一致性。作者提出了一种名为PAIRed Erasing (PAIR)的新框架，该框架利用不安全-安全对来促进保持一致性的语义重新对齐，而不是简单的去除。关键实验结果表明，该方法显著优于现有方法，实现了针对性概念的去除，同时保持了生成图像的完整性、一致性和质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0339.html">20260206_0339</a>
<a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
