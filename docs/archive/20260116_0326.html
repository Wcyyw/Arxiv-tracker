<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-16 03:26</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260116_0326</div>
    <div class="row"><div class="card">
<div class="title">Identifying Models Behind Text-to-Image Leaderboards</div>
<div class="meta-line">Authors: Ali Naseh, Yuefeng Peng, Anshuman Suri, Harsh Chaudhari, Alina Oprea, Amir Houmansadr</div>
<div class="meta-line">First: 2026-01-14T17:30:58+00:00 · Latest: 2026-01-14T17:30:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09647v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>识别文本到图像排行榜背后的模型</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型越来越受欢迎，在线生成了大量AI生成的图像。为了比较模型质量，基于投票的排行榜已成为标准，依赖于匿名模型输出以确保公平。在这项工作中，我们展示了这种匿名性可以很容易地被打破。我们发现每个T2I模型生成的图像在图像嵌入空间中形成独特的聚类，使得在没有提示控制或训练数据的情况下实现准确的去匿名化。使用22个模型和280个提示（15万张图像），我们的基于质心的方法实现了高准确性，并揭示了系统性的模型特征。我们进一步引入了提示级可区分性度量，并进行大规模分析，显示某些提示可以导致近乎完美的可区分性。我们的发现揭示了T2I排行榜中的基本安全缺陷，并激励更强的匿名防御措施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerabilities in the anonymity of text-to-image (T2I) models used in voting-based leaderboards, which are intended to fairly compare model quality. The authors employ a centroid-based method to analyze the outputs of 22 T2I models across 280 prompts, totaling 150,000 images, revealing that the outputs cluster distinctly in the image embedding space, allowing for accurate deanonymization. The study uncovers significant model-specific signatures and introduces a prompt-level distinguishability metric, demonstrating that certain prompts can achieve near-perfect distinguishability, thereby highlighting critical security flaws in T2I leaderboards and the need for improved anonymization strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到图像（T2I）模型排行榜中依赖匿名输出进行公平比较的安全漏洞。作者采用基于质心的方法分析了22个T2I模型在280个提示下的输出，总计15万张图像，揭示每个模型的生成在图像嵌入空间中形成明显的聚类。主要发现表明，模型输出的匿名性很容易被破坏，强调了由于识别出的系统性模型特征，需要改进T2I排行榜中的匿名化技术。</div>
</details>
</div>
<div class="card">
<div class="title">Training Large Neural Networks With Low-Dimensional Error Feedback</div>
<div class="meta-line">Authors: Maher Hanut, Jonathan Kadmon</div>
<div class="meta-line">First: 2025-02-27T22:45:41+00:00 · Latest: 2026-01-14T17:19:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.20580v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.20580v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training deep neural networks typically relies on backpropagating high dimensional error signals a computationally intensive process with little evidence supporting its implementation in the brain. However, since most tasks involve low-dimensional outputs, we propose that low-dimensional error signals may suffice for effective learning. To test this hypothesis, we introduce a novel local learning rule based on Feedback Alignment that leverages indirect, low-dimensional error feedback to train large networks. Our method decouples the backward pass from the forward pass, enabling precise control over error signal dimensionality while maintaining high-dimensional representations. We begin with a detailed theoretical derivation for linear networks, which forms the foundation of our learning framework, and extend our approach to nonlinear, convolutional, and transformer architectures. Remarkably, we demonstrate that even minimal error dimensionality on the order of the task dimensionality can achieve performance matching that of traditional backpropagation. Furthermore, our rule enables efficient training of convolutional networks, which have previously been resistant to Feedback Alignment methods, with minimal error. This breakthrough not only paves the way toward more biologically accurate models of learning but also challenges the conventional reliance on high-dimensional gradient signals in neural network training. Our findings suggest that low-dimensional error signals can be as effective as high-dimensional ones, prompting a reevaluation of gradient-based learning in high-dimensional systems. Ultimately, our work offers a fresh perspective on neural network optimization and contributes to understanding learning mechanisms in both artificial and biological systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用低维误差反馈训练大型神经网络</div>
<div class="mono" style="margin-top:8px">训练深度神经网络通常依赖于反向传播高维误差信号，这是一种计算密集型的过程，几乎没有证据支持其在大脑中的实现。然而，由于大多数任务涉及低维输出，我们提出低维误差信号可能足以实现有效学习。为了验证这一假设，我们引入了一种基于反馈对齐的新型局部学习规则，利用间接的低维误差反馈来训练大型网络。我们的方法将反向传播与前向传播解耦，使得在保持高维表示的同时，能够精确控制误差信号的维度。我们首先对线性网络进行了详细的理论推导，这构成了我们学习框架的基础，并将我们的方法扩展到非线性、卷积和变换器架构。值得注意的是，我们证明即使是与任务维度相当的最小误差维度也能达到与传统反向传播相匹配的性能。此外，我们的规则使得卷积网络的高效训练成为可能，而这些网络之前对反馈对齐方法一直抵抗，且误差极小。这一突破不仅为更生物学上准确的学习模型铺平了道路，还挑战了在神经网络训练中对高维梯度信号的传统依赖。我们的发现表明，低维误差信号可以与高维信号同样有效，促使对高维系统中基于梯度的学习进行重新评估。最终，我们的工作为神经网络优化提供了新的视角，并有助于理解人工和生物系统中的学习机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore an alternative to the computationally intensive process of backpropagating high-dimensional error signals in training deep neural networks, which lacks strong evidence of its biological plausibility. The authors propose a novel local learning rule based on Feedback Alignment that utilizes low-dimensional error feedback to train large networks, allowing for a decoupling of the backward and forward passes while controlling error signal dimensionality. The experimental results show that using low-dimensional error signals, even at a minimal level corresponding to task dimensionality, can achieve performance comparable to traditional backpropagation, particularly in convolutional networks that previously struggled with Feedback Alignment, thereby suggesting a reevaluation of gradient-based learning methods in neural network training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索一种替代方案，以应对深度神经网络训练中高维误差信号反向传播的计算密集型过程，该过程缺乏强有力的生物学合理性证据。作者提出了一种基于反馈对齐的新型局部学习规则，利用低维误差信号来训练大型网络，从而实现了反向传播与前向传播的解耦，同时保持高维表示。关键实验结果表明，使用低维误差信号，即使在与任务维度相对应的最小水平上，也能实现与传统反向传播相当的性能，特别是在之前对反馈对齐方法表现不佳的卷积网络中，这提示我们重新评估神经网络训练中的基于梯度的学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems</div>
<div class="meta-line">Authors: Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Zujun Yu, Yisheng Lv</div>
<div class="meta-line">First: 2026-01-14T16:36:26+00:00 · Latest: 2026-01-14T16:36:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09613v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09613v1">PDF</a> · <a href="https://github.com/Hub-Tian/CogRail">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CogRail：智能铁路运输系统中认知入侵感知的视觉语言模型基准测试</div>
<div class="mono" style="margin-top:8px">准确和及时地感知潜在入侵目标对于确保铁路运输系统的安全至关重要。然而，大多数现有系统仅关注固定视觉范围内的物体分类，并应用基于规则的启发式方法来确定入侵状态，常常忽视潜在入侵风险的目标。预见这些风险需要对感兴趣物体（OOI）的空间上下文和时间动态进行认知，这对传统视觉模型提出了挑战。为了促进深度入侵感知，我们引入了一个新基准CogRail，它整合了策划的开源数据集和认知驱动的问题-答案注释，以支持时空推理和预测。在此基准的基础上，我们对最先进的视觉语言模型（VLMs）进行了系统评估，使用多模态提示识别它们在该领域的优势和局限性。此外，我们对VLMs进行了微调以提高性能，并提出了一个联合微调框架，整合了位置感知、运动预测和威胁分析三个核心任务，促进了通用基础模型向专门针对认知入侵感知的模型的有效适应。大量实验表明，当前的大规模多模态模型在认知入侵感知任务所需的复杂时空推理方面存在困难，突显了现有基础模型在这一安全关键领域的局限性。相比之下，我们提出的联合微调框架通过使模型能够针对特定领域的推理需求进行有针对性的适应，显著提高了模型性能，突显了结构化多任务学习在提高准确性和可解释性方面的优势。代码将可在 https://github.com/Hub-Tian/CogRail 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the safety of railway transportation systems by enhancing the perception of potential intrusion targets, which is often inadequately addressed by existing systems that focus on fixed object classification. The authors introduce a benchmark called CogRail, which combines curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Experimental results indicate that current visual-language models (VLMs) struggle with the complex reasoning required for cognitive intrusion perception, but the proposed joint fine-tuning framework significantly improves model performance by adapting general-purpose models to meet domain-specific needs, demonstrating the effectiveness of structured multi-task learning in this context.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提高对潜在入侵目标的感知来增强铁路运输系统的安全性，而现有系统往往仅关注对象分类和基于规则的启发式方法，未能充分应对这一问题。作者提出了一个名为CogRail的基准，结合了策划的开源数据集和认知驱动的问题-答案注释，以支持时空推理和预测。通过对最先进的视觉-语言模型（VLMs）进行系统评估和微调，研究发现当前模型在认知入侵感知所需的复杂推理方面存在困难，而所提出的联合微调框架通过将模型适应于特定的推理需求显著提高了性能，证明了结构化多任务学习在此背景下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks</div>
<div class="meta-line">Authors: Ningzhe Shi, Yiqing Zhou, Ling Liu, Jinglin Shi, Yihao Wu, Haiwei Shi, Hanxiao Yu</div>
<div class="meta-line">First: 2025-08-16T15:29:59+00:00 · Latest: 2026-01-14T15:28:17+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TMC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12079v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12079v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrated sensing and communication (ISAC) can enhance artificial intelligence-generated content (AIGC) networks by providing efficient sensing and transmission. Existing AIGC services usually assume that the accuracy of the generated content can be ensured, given accurate input data and prompt, thus only the content generation quality (CGQ) is concerned. However, it is not applicable in ISAC-based AIGC networks, where content generation is based on inaccurate sensed data. Moreover, the AIGC model itself introduces generation errors, which depend on the number of generating steps (i.e., computing resources). To assess the quality of experience of ISAC-based AIGC services, we propose a content accuracy and quality aware service assessment metric (CAQA). Since allocating more resources to sensing and generating improves content accuracy but may reduce communication quality, and vice versa, this sensing-generating (computing)-communication three-dimensional resource tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution space that grows exponentially with the number of users. To solve the CAQA-AIGC problem with low complexity, a linear programming (LP) guided deep reinforcement learning (DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the LP-guided approach and the action filter, LPDRL-F can transform the original three-dimensional solution space to two dimensions, reducing complexity while improving the learning performance of DRL. Simulations show that compared to existing DRL and generative diffusion model (GDM) algorithms without LP, LPDRL-F converges faster and finds better resource allocation solutions, improving AvgCAQA by more than 10%. With LPDRL-F, CAQA-AIGC can achieve an improvement in AvgCAQA of more than 50% compared to existing schemes focusing solely on CGQ.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LP引导的深度强化学习的内容准确性和质量感知资源分配用于ISAC驱动的AIGC网络</div>
<div class="mono" style="margin-top:8px">集成感知与通信（ISAC）通过提供高效的感知和传输，可以增强人工智能生成内容（AIGC）网络。现有的AIGC服务通常假设生成内容的准确性可以得到保证，前提是输入数据和提示准确，因此只关注内容生成质量（CGQ）。然而，这在基于ISAC的AIGC网络中并不适用，因为内容生成是基于不准确的感知数据。此外，AIGC模型本身引入了生成误差，这取决于生成步骤的数量（即计算资源）。为了评估基于ISAC的AIGC服务的体验质量，我们提出了一种内容准确性和质量感知服务评估指标（CAQA）。由于将更多资源分配给感知和生成可以提高内容准确性，但可能降低通信质量，反之亦然，因此必须优化这种感知-生成（计算）-通信三维资源权衡，以最大化所有用户的平均CAQA（AvgCAQA）（CAQA-AIGC）。该问题是NP难题，解决方案空间随着用户数量的增加而呈指数增长。为了解决CAQA-AIGC问题并降低复杂性，提出了一种基于线性规划（LP）引导的深度强化学习（DRL）算法，带有动作过滤器（LPDRL-F）。通过LP引导的方法和动作过滤器，LPDRL-F可以将原始的三维解决方案空间转化为二维，从而降低复杂性，同时提高DRL的学习性能。仿真表明，与现有的没有LP的DRL和生成扩散模型（GDM）算法相比，LPDRL-F收敛更快，找到更好的资源分配解决方案，AvgCAQA提高超过10%。使用LPDRL-F，CAQA-AIGC的AvgCAQA相比于仅关注CGQ的现有方案提高超过50%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of integrated sensing and communication (ISAC) networks for artificial intelligence-generated content (AIGC) services, which often rely on inaccurate sensed data. The authors propose a content accuracy and quality aware service assessment metric (CAQA) and introduce a linear programming (LP) guided deep reinforcement learning (DRL) algorithm with an action filter (LPDRL-F) to optimize the resource allocation across sensing, generating, and communication. Experimental results demonstrate that LPDRL-F significantly improves the average CAQA (AvgCAQA) by over 10% compared to existing DRL and generative diffusion model algorithms, and achieves more than a 50% improvement in AvgCAQA compared to schemes that focus solely on content generation quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升集成感知与通信（ISAC）网络在人工智能生成内容（AIGC）服务中的性能，这些服务通常依赖于不准确的感知数据。作者提出了一种内容准确性和质量感知服务评估指标（CAQA），并引入了一种基于线性规划（LP）指导的深度强化学习（DRL）算法及其动作过滤器（LPDRL-F），以优化感知、生成和通信维度的资源分配。实验结果表明，与现有的DRL和生成扩散模型算法相比，LPDRL-F显著提高了平均CAQA（AvgCAQA）超过10%，并且与传统仅关注内容生成质量的方法相比，AvgCAQA的提升超过50%。</div>
</details>
</div>
<div class="card">
<div class="title">ViSTA: Visual Storytelling using Multi-modal Adapters for Text-to-Image Diffusion Models</div>
<div class="meta-line">Authors: Sibo Dong, Ismail Shaheen, Maggie Shen, Rupayan Mallick, Sarah Adel Bargal</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-13T19:57:40+00:00 · Latest: 2026-01-14T15:15:58+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12198v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12198v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models have achieved remarkable success, yet generating coherent image sequences for visual storytelling remains challenging. A key challenge is effectively leveraging all previous text-image pairs, referred to as history text-image pairs, which provide contextual information for maintaining consistency across frames. Existing auto-regressive methods condition on all past image-text pairs but require extensive training, while training-free subject-specific approaches ensure consistency but lack adaptability to narrative prompts. To address these limitations, we propose a multi-modal history adapter for text-to-image diffusion models, \textbf{ViSTA}. It consists of (1) a multi-modal history fusion module to extract relevant history features and (2) a history adapter to condition the generation on the extracted relevant features. We also introduce a salient history selection strategy during inference, where the most salient history text-image pair is selected, improving the quality of the conditioning. Furthermore, we propose to employ a Visual Question Answering-based metric TIFA to assess text-image alignment in visual storytelling, providing a more targeted and interpretable assessment of generated images. Evaluated on the StorySalon and FlintStonesSV dataset, our proposed ViSTA model is not only consistent across different frames, but also well-aligned with the narrative text descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSTA：使用多模态适配器进行文本到图像扩散模型的视觉讲故事</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型取得了显著成功，但生成连贯的图像序列以进行视觉讲故事仍然具有挑战性。一个关键挑战是有效利用所有先前的文本-图像对，称为历史文本-图像对，这些对提供了保持帧间一致性的上下文信息。现有的自回归方法对所有过去的图像-文本对进行条件化，但需要大量训练，而无训练的特定主题方法确保一致性但缺乏对叙事提示的适应性。为了解决这些限制，我们提出了一种用于文本到图像扩散模型的多模态历史适配器，\textbf{ViSTA}。它由（1）一个多模态历史融合模块来提取相关的历史特征和（2）一个历史适配器来基于提取的相关特征进行生成。我们还在推理过程中引入了一种显著历史选择策略，选择最显著的历史文本-图像对，从而提高条件化的质量。此外，我们建议采用基于视觉问答的度量TIFA来评估视觉讲故事中的文本-图像对齐，提供对生成图像更有针对性和可解释的评估。在StorySalon和FlintStonesSV数据集上的评估表明，我们提出的ViSTA模型不仅在不同帧之间保持一致，而且与叙事文本描述良好对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the generation of coherent image sequences for visual storytelling using text-to-image diffusion models, which face challenges in maintaining consistency across frames. The authors propose a multi-modal history adapter, ViSTA, which includes a multi-modal history fusion module to extract relevant features from previous text-image pairs and a history adapter to condition the image generation on these features. Experimental results on the StorySalon and FlintStonesSV datasets demonstrate that ViSTA achieves consistency across frames and aligns well with narrative text descriptions, outperforming existing methods in terms of quality and coherence.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善基于文本到图像扩散模型的视觉叙事中连贯图像序列的生成，这些模型在保持帧间一致性方面面临挑战。作者提出了一种多模态历史适配器ViSTA，其中包括一个多模态历史融合模块，用于提取先前文本-图像对的相关特征，以及一个历史适配器，用于基于这些特征进行生成。对StorySalon和FlintStonesSV数据集的实验结果表明，ViSTA在帧间保持一致性，并与叙事文本描述良好对齐，在质量和连贯性方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts</div>
<div class="meta-line">Authors: Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu</div>
<div class="meta-line">Venue: ICML 2024</div>
<div class="meta-line">First: 2023-09-12T11:19:36+00:00 · Latest: 2026-01-14T13:34:07+00:00</div>
<div class="meta-line">Comments: ICML 2024 main conference paper. The source code is available at https://github.com/zhiyichin/P4D</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2309.06135v3">Abs</a> · <a href="https://arxiv.org/pdf/2309.06135v3">PDF</a> · <a href="https://github.com/zhiyichin/P4D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered &quot;safe&quot; can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Prompting4Debugging：通过发现问题提示对文本到图像扩散模型进行红队测试</div>
<div class="mono" style="margin-top:8px">文本到图像的扩散模型，如稳定扩散（SD），最近在高质量内容生成方面表现出显著能力，成为近期变革性人工智能浪潮的代表之一。然而，这一进展伴随着对这种生成技术滥用的日益关注，特别是在生成受版权保护或不适合工作的（NSFW）图像方面。尽管已经采取措施通过模型微调过滤不当图像/提示或去除不良概念/风格，但这些安全机制对多样化问题提示的可靠性仍然在很大程度上未被探索。在这项工作中，我们提出了Prompting4Debugging（P4D）作为一种调试和红队工具，自动发现扩散模型的问题提示，以测试已部署安全机制的可靠性。我们展示了P4D工具在揭示SD模型安全机制新漏洞方面的有效性。特别是，我们的结果表明，现有安全提示基准中大约一半的提示原本被认为是“安全的”，实际上可以被操控以绕过许多已部署的安全机制，包括概念去除、负提示和安全指导。我们的发现表明，如果没有全面的测试，对有限的安全提示基准的评估可能会导致对文本到图像模型的虚假安全感。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the growing concerns regarding the misuse of text-to-image diffusion models, particularly in generating inappropriate or copyrighted content. The authors introduce Prompting4Debugging (P4D), a tool designed to automatically identify problematic prompts that can challenge the reliability of existing safety mechanisms in these models. The experimental results reveal that approximately 50% of prompts previously deemed &#x27;safe&#x27; can be manipulated to circumvent various safety measures, highlighting significant vulnerabilities in the current safety evaluations of text-to-image models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决对文本到图像扩散模型滥用的日益关注，特别是在生成不当内容方面。作者提出了Prompting4Debugging (P4D)工具，旨在自动识别可以测试这些模型安全机制可靠性的问题提示。关键发现表明，约50%之前被认为是“安全”的提示可以被操纵以绕过现有的安全措施，这表明当前的评估可能会对文本到图像模型的安全性提供误导性的安全感。</div>
</details>
</div>
<div class="card">
<div class="title">Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process</div>
<div class="meta-line">Authors: Sangjun Han, Youngmi Hur</div>
<div class="meta-line">Venue: IET Image Processing, 2025; 19:e70238</div>
<div class="meta-line">First: 2026-01-14T11:57:15+00:00 · Latest: 2026-01-14T11:57:15+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IET Image Processing. This is the authors&#x27; final accepted manuscript</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09410v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09410v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于拉普拉斯金字塔和重复放大与缩小过程的超分辨率模型中的细节损失</div>
<div class="mono" style="margin-top:8px">随着人工智能的进步，图像处理引起了显著关注。图像超分辨率是一项与现实应用密切相关的重要技术，因为它提高了现有图像的质量。由于增强细节对于超分辨率任务至关重要，因此应强调对高频信息有贡献的像素。本文提出了两种方法来增强超分辨率图像中的高频细节：基于拉普拉斯金字塔的细节损失和重复放大与缩小过程。我们的细节损失与总损失结合，引导模型分别生成和控制超分辨率图像和细节图像。这种方法使模型能够更有效地关注高频成分，从而改善超分辨率图像。此外，重复的放大和缩小通过从多个低分辨率特征中提取多样化信息，增强了细节损失的有效性。我们进行了两种类型的实验。首先，我们设计了一个结合我们方法的基于CNN的模型。该模型实现了最先进的结果，超越了所有当前可用的基于CNN的模型，甚至一些基于注意力的模型。其次，我们在小规模上将我们的方法应用于现有的基于注意力的模型。在我们所有的实验中，添加了细节损失的基于注意力的模型相比原始模型显示出改进。这些结果表明，我们的方法有效地增强了不同模型结构下的超分辨率图像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image super-resolution, a crucial technology for enhancing image quality in various applications, particularly by focusing on fine details. The authors propose two methods: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process, which together guide a model to generate and control super-resolution and detail images separately. Experimental results show that a CNN-based model incorporating these methods achieves state-of-the-art performance, outperforming existing CNN and some attention-based models, while attention-based models that integrate the detail loss also demonstrate significant improvements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过增强细节来提高超分辨率图像的质量，这对实际应用至关重要。作者提出了两种方法：基于拉普拉斯金字塔的细节损失和重复的上采样与下采样过程，以强调高频信息。实验结果表明，结合这些方法的CNN模型达到了最先进的性能，超越了现有的CNN模型和一些基于注意力的模型，同时也证明了基于注意力的模型在细节损失的帮助下，超分辨率图像得到了改善，适用于多种模型结构。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Continual Semantic Segmentation</div>
<div class="meta-line">Authors: Yifu Guo, Yuquan Lu, Wentao Zhang, Zishan Xu, Dexia Chen, Siyu Zhang, Yizhe Zhang, Ruixuan Wang</div>
<div class="meta-line">First: 2025-08-07T06:34:34+00:00 · Latest: 2026-01-14T11:28:28+00:00</div>
<div class="meta-line">Comments: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05065v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.05065v2">PDF</a> · <a href="https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦持续语义分割</div>
<div class="mono" style="margin-top:8px">持续语义分割（CSS）需要在不遗忘先前获得知识的情况下学习新类别，解决密集预测任务中灾难性遗忘的基本挑战。然而，现有的CSS方法通常采用单阶段编码器-解码器架构，其中分割掩码和类别标签紧密耦合，导致旧类别和新类别学习之间的干扰以及次优的保留-可塑性平衡。我们提出了DecoupleCSS，一种新颖的两阶段CSS框架。通过将类别感知检测与类别无关的分割解耦，DecoupleCSS实现了更有效的持续学习，在学习新类别的同时保留过去的知识。第一阶段利用经过LoRA调整的预训练文本和图像编码器，编码类别特定信息并生成位置感知提示。在第二阶段，采用Segment Anything Model（SAM）生成精确的分割掩码，确保分割知识在新旧类别之间共享。这种方法改善了CSS中保留与适应之间的平衡，在各种具有挑战性的任务中实现了最先进的性能。我们的代码公开可用： https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of catastrophic forgetting in Continual Semantic Segmentation (CSS), where new classes must be learned without losing previously acquired knowledge. The authors propose DecoupleCSS, a two-stage framework that separates class-aware detection from class-agnostic segmentation to enhance continual learning. Experimental results demonstrate that this method effectively preserves past knowledge while integrating new classes, achieving state-of-the-art performance across various challenging tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在持续语义分割（CSS）中，学习新类别时可能干扰已获得知识的灾难性遗忘问题。作者提出了DecoupleCSS，这是一种将类别感知检测与类别无关分割分开的两阶段框架，以增强持续学习。实验结果表明，该方法改善了保留与适应之间的平衡，在各种具有挑战性的任务中实现了最先进的性能，同时有效地保留了过去的知识并适应新类别。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework</div>
<div class="meta-line">Authors: Ewelina Gajewska, Katarzyna Budzynska, Jarosław A Chudziak</div>
<div class="meta-line">First: 2026-01-14T10:20:32+00:00 · Latest: 2026-01-14T10:20:32+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09342v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过社区驱动的多智能体框架改善隐性仇恨言论检测</div>
<div class="mono" style="margin-top:8px">本研究提出了一种针对隐性仇恨言论的情境检测框架，实施为一个多智能体系统，包括一个中央调解智能体和动态构建的社区智能体，代表特定的人口群体。我们的方法明确整合了来自公开知识源的社会文化背景，实现了超越最先进提示方法（零样本提示、少样本提示、思维链提示）和其他方法的身份感知调解，在具有挑战性的ToxiGen数据集上表现优异。我们通过将平衡准确率作为分类公平性的核心指标，增强了性能评估的技术严谨性，考虑了真正阳性和真正阴性率之间的权衡。我们证明了我们的社区驱动咨询框架显著提高了所有目标群体的分类准确性和公平性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the detection of implicitly hateful speech by incorporating socio-cultural context into the moderation process. The authors propose a multi-agent framework that includes a central Moderator Agent and Community Agents representing various demographic groups, allowing for identity-aware moderation. Experimental results on the ToxiGen dataset show that this community-driven approach significantly improves classification accuracy and fairness compared to existing prompting methods and other alternatives, as measured by balanced accuracy, which considers the trade-off between true positive and true negative rates.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将社会文化背景纳入审查过程来增强对隐性仇恨言论的检测。作者提出了一种多智能体框架，包括一个中央审核代理和代表不同人口群体的社区代理，利用公开可用的知识来源进行身份意识审查。在ToxiGen数据集上的实验结果表明，与现有的提示方法和替代技术相比，这种以社区为驱动的方法在分类准确性和公平性方面显著提高，平衡准确性作为衡量标准，考虑了真正阳性和真正阴性率。</div>
</details>
</div>
<div class="card">
<div class="title">Positional Embedding-Aware Activations</div>
<div class="meta-line">Authors: Kathan Shah, Chawin Sitawarin</div>
<div class="meta-line">First: 2023-06-27T06:49:40+00:00 · Latest: 2026-01-14T10:11:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2306.15242v2">Abs</a> · <a href="https://arxiv.org/pdf/2306.15242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional activation functions. Our proposed architecture, SPDER, is a simple MLP that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. Our results indicate that SPDERs speed up training by 10x and converge to losses 1,500-50,000x lower than that of the state-of-the-art for image representation. SPDER is also state-of-the-art in audio representation. The superior representation capability allows SPDER to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation. We provide intuition as to why SPDER significantly improves fitting compared to that of other INR methods while requiring no hyperparameter tuning or preprocessing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>位置嵌入感知激活</div>
<div class="mono" style="margin-top:8px">我们提出了一种神经网络架构，旨在自然地学习位置嵌入，并克服传统激活函数面临的对低频的谱偏差。我们提出的架构SPDER是一个简单的多层感知器，使用由正弦函数乘以一个称为阻尼函数的亚线性函数组成的激活函数。正弦函数使网络能够自动学习输入坐标的位置信息，而阻尼函数则通过防止坐标值被投影到有限值范围内来传递实际坐标值。我们的结果表明，SPDER的训练速度提高了10倍，收敛到的损失比最先进的图像表示方法低1,500-50,000倍。SPDER在音频表示方面也处于最先进水平。其卓越的表示能力使SPDER在多个下游任务中表现出色，如图像超分辨率和视频帧插值。我们提供了直觉，解释了为什么SPDER在拟合方面显著优于其他INR方法，同时不需要超参数调优或预处理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the spectral bias towards lower frequencies in conventional activation functions and to enhance the learning of positional embeddings in neural networks. The authors propose a new architecture called SPDER, which is a simple MLP that utilizes an activation function combining a sinusoidal function with a damping function. Experimental results demonstrate that SPDER accelerates training by 10 times and achieves losses that are 1,500 to 50,000 times lower than state-of-the-art methods for image representation, while also performing exceptionally well in audio representation and various downstream tasks such as image super-resolution and video frame interpolation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种神经网络架构，能够有效学习位置嵌入，并解决传统激活函数中固有的低频谱偏差。作者提出了SPDER，这是一种简单的多层感知器（MLP），采用一种将正弦成分与阻尼函数相结合的激活函数，使网络能够学习位置嵌入，同时保持实际坐标值。实验结果表明，SPDER的训练速度提高了10倍，损失值比当前最先进的方法低1500到50000倍，并且在音频表示方面也表现优异，同时在图像超分辨率和视频帧插值等多个下游任务中也表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</div>
<div class="meta-line">Authors: Jiachen Li, Xiaojin Gong</div>
<div class="meta-line">First: 2023-10-26T08:12:53+00:00 · Latest: 2026-01-14T09:17:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.17218v3">Abs</a> · <a href="https://arxiv.org/pdf/2310.17218v3">PDF</a> · <a href="https://github.com/RikoLi/PCL-CLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原型对比学习的CLIP微调用于物体重识别</div>
<div class="mono" style="margin-top:8px">本研究旨在将大规模预训练的视觉-语言模型（如对比语言-图像预训练（CLIP））适应于提高各种监督设置下的物体重识别（Re-ID）性能。尽管提示学习使得名为CLIP-ReID的近期工作取得了良好的性能，但由于ReID任务中缺乏语义标签，其基本机制和提示学习的必要性仍不清楚。在本研究中，我们首先分析了提示学习在CLIP-ReID中的作用，并识别其局限性。基于我们的调查，我们提出了一种简单而有效的方法来适应CLIP用于监督物体Re-ID。我们的方法直接使用原型对比学习（PCL）损失微调CLIP的图像编码器，消除了对提示学习的需求。在人和车的Re-ID数据集上的实验结果表明，我们的方法与CLIP-ReID相比具有竞争力。此外，我们将基于PCL的CLIP微调方法扩展到无监督场景，在这些场景中我们实现了最先进的性能。代码可在https://github.com/RikoLi/PCL-CLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of improving object re-identification (Re-ID) using large-scale pre-trained vision-language models like CLIP, particularly in light of unclear mechanisms surrounding prompt learning in existing methods. The authors analyze the limitations of prompt learning in CLIP-ReID and propose a novel approach that fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, thereby eliminating the need for prompt learning. Experimental results on person and vehicle Re-ID datasets show that this method is competitive with CLIP-ReID and achieves state-of-the-art performance in unsupervised scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用大规模预训练的视觉-语言模型（如CLIP）来提高物体重识别（Re-ID）性能，特别是在提示学习机制不明确的情况下。作者分析了CLIP-ReID中提示学习的局限性，并提出了一种新方法，通过原型对比学习（PCL）损失直接微调CLIP的图像编码器，从而消除了对提示学习的需求。在人员和车辆Re-ID数据集上的实验结果表明，该方法与CLIP-ReID具有竞争力，并在无监督场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data</div>
<div class="meta-line">Authors: Felix Koehler, Nils Thuerey</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-27T08:31:55+00:00 · Latest: 2026-01-14T08:02:51+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025: https://neurips.cc/virtual/2025/poster/116770 ; V2: Updated references, fix typos</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23111v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23111v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tum-pbs.github.io/emulator-superiority">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural operators or emulators for PDEs trained on data from numerical solvers are conventionally assumed to be limited by their training data&#x27;s fidelity. We challenge this assumption by identifying &quot;emulator superiority,&quot; where neural networks trained purely on low-fidelity solver data can achieve higher accuracy than those solvers when evaluated against a higher-fidelity reference. Our theoretical analysis reveals how the interplay between emulator inductive biases, training objectives, and numerical error characteristics enables superior performance during multi-step rollouts. We empirically validate this finding across different PDEs using standard neural architectures, demonstrating that emulators can implicitly learn dynamics that are more regularized or exhibit more favorable error accumulation properties than their training data, potentially surpassing training data limitations and mitigating numerical artifacts. This work prompts a re-evaluation of emulator benchmarking, suggesting neural emulators might achieve greater physical fidelity than their training source within specific operational regimes. Project Page: https://tum-pbs.github.io/emulator-superiority</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经仿真器的优越性：当机器学习用于偏微分方程时超越其训练数据</div>
<div class="mono" style="margin-top:8px">传统上，基于数值求解器数据训练的偏微分方程（PDE）神经算子或仿真器被认为受限于其训练数据的保真度。我们通过识别“仿真器优越性”来挑战这一假设，即仅基于低保真度求解器数据训练的神经网络在与高保真度参考进行评估时可以实现比这些求解器更高的准确性。我们的理论分析揭示了仿真器归纳偏差、训练目标和数值误差特征之间的相互作用如何在多步展开过程中实现优越性能。我们通过使用标准神经架构在不同的PDE上实证验证了这一发现，表明仿真器可以隐式学习比其训练数据更规则的动态或表现出更有利的误差累积特性，可能超越训练数据的限制并减轻数值伪影。这项工作促使我们重新评估仿真器基准测试，建议在特定操作范围内，神经仿真器可能实现比其训练源更高的物理保真度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the concept of &quot;emulator superiority&quot; in neural operators for partial differential equations (PDEs), challenging the traditional belief that the accuracy of machine learning models is constrained by the fidelity of their training data. The authors employ theoretical analysis and empirical validation across various PDEs to demonstrate that neural networks trained on low-fidelity data can outperform numerical solvers when assessed against higher-fidelity references. The findings indicate that these emulators can learn dynamics that are more regularized and exhibit better error accumulation properties, suggesting that they may exceed the limitations of their training data and reduce numerical artifacts.</div>
<div class="mono" style="margin-top:8px">本研究探讨了神经算子在偏微分方程（PDE）中的“仿真器优越性”概念，挑战了传统观点，即机器学习模型的准确性严格受限于其训练数据的保真度。作者通过理论分析探讨了仿真器的归纳偏差、训练目标和数值误差特征之间的关系，并通过在各种PDE上使用标准神经架构进行实证测试来验证其发现。结果表明，基于低保真数据训练的神经仿真器在与高保真参考进行评估时，可以超越传统求解器的表现，暗示这些仿真器能够学习到减轻数值伪影并提高准确性的动态，超越训练数据的限制。</div>
</details>
</div>
<div class="card">
<div class="title">Integrating Diverse Assignment Strategies into DETRs</div>
<div class="meta-line">Authors: Yiwei Zhang, Jin Gao, Hanshi Wang, Fudong Ge, Guan Luo, Weiming Hu, Zhipeng Zhang</div>
<div class="meta-line">First: 2026-01-14T07:28:54+00:00 · Latest: 2026-01-14T07:28:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09247v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09247v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of ``one-to-many&#x27;&#x27; supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse ``one-to-many&#x27;&#x27; supervision can be integrated to achieve state-of-the-art results without compromising model elegance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将多样化的分配策略整合到DETR中</div>
<div class="mono" style="margin-top:8px">标签分配是目标检测器中的关键组成部分，特别是在DETR风格的框架中，尽管一对一匹配策略具有端到端的优雅性，但由于稀疏监督而导致收敛缓慢。虽然最近的研究探索了一对多的分配以丰富监督信号，但它们通常引入复杂的、特定于架构的修改，并且通常专注于单一的辅助策略，缺乏统一和可扩展的设计。本文首先系统地研究了“一对多”监督的影响，并揭示了一个令人惊讶的见解：性能提升并不是由监督的数量驱动，而是由所采用的分配策略的多样性驱动。这个发现表明，可以实现更优雅、参数高效的方法。在此基础上，我们提出了LoRA-DETR，一个灵活且轻量的框架，可以无缝地将多样化的分配策略整合到任何DETR风格的检测器中。我们的方法在训练期间通过多个低秩适应（LoRA）分支增强主网络，每个分支实例化不同的一对多分配规则。这些分支作为辅助模块，将丰富、多样的监督梯度注入主模型，并在推理时被丢弃，从而不增加额外的计算成本。这种设计促进了稳健的联合优化，同时保持了原始检测器的架构简单性。在不同基线上的大量实验验证了我们方法的有效性。我们的工作为增强检测器提供了一种新范式，证明了多样化的“一对多”监督可以整合以实现最先进的结果，而不妥协模型的优雅性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the convergence speed of DETR-style object detectors, which struggle with sparse supervision due to their one-to-one matching strategy. The authors systematically investigate the impact of one-to-many supervision and discover that performance improvements are linked to the diversity of assignment strategies rather than the quantity of supervision. To address this, they propose LoRA-DETR, a flexible framework that incorporates multiple Low-Rank Adaptation branches during training to implement various one-to-many assignment rules, enhancing the supervisory signals without increasing computational costs during inference. Experimental results demonstrate that this approach effectively integrates diverse supervision strategies, achieving state-of-the-art performance while preserving the simplicity of the original architecture.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决DETR风格目标检测器中，由于稀疏监督导致的一对一匹配策略的收敛速度慢的问题。作者系统地研究了一对多监督的影响，发现性能提升主要源于分配策略的多样性，而非监督数量的增加。基于这一发现，他们提出了LoRA-DETR，这是一个灵活的框架，通过在训练期间添加低秩适应分支，将各种分配策略整合到DETR检测器中，这些分支在推理时不增加计算成本。实验结果表明，该方法在提高检测器性能的同时，保持了原始架构的简洁性。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</div>
<div class="meta-line">Authors: Xuanyu Hu</div>
<div class="meta-line">First: 2025-12-23T11:04:34+00:00 · Latest: 2026-01-14T07:14:01+00:00</div>
<div class="meta-line">Comments: 15 pages, 2 figures, 4 tables. Submitted to ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20249v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20249v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过跨主体软ROI融合的统一多模态脑解码</div>
<div class="mono" style="margin-top:8px">多模态脑解码旨在从fMRI等脑活动信号中重建与视觉刺激一致的语义信息，并生成可读的自然语言描述。然而，多模态脑解码在跨主体泛化和可解释性方面仍面临关键挑战。我们提出了一种BrainROI模型，并在NSD数据集的脑描述评估中取得了领先水平的结果。在跨主体设置下，与最近的最先进方法和代表性基线相比，BLEU-4和CIDEr等指标显示出明显的改善。首先，为了解决不同主体之间功能脑拓扑的异质性，我们设计了一种新的fMRI编码器。我们使用多图谱软功能分区（soft-ROI）作为共享空间。我们将MINDLLM中的离散ROI连接策略扩展为体素级门控融合机制（Voxel-gate）。我们还通过全局标签对齐确保一致的ROI映射，从而增强跨主体的可转移性。其次，为了克服手动和黑箱提示方法在稳定性和透明性方面的局限性，我们引入了一种可解释的提示优化过程。在一个小样本闭环中，我们使用本地部署的Qwen模型迭代生成和选择人类可读的提示。该过程提高了提示设计的稳定性，并保留了可审计的优化轨迹。最后，我们在推理过程中施加参数化解码约束，以进一步提高生成描述的稳定性和质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance multimodal brain decoding, which reconstructs semantic information from brain activity signals like fMRI and generates natural language descriptions, while addressing challenges in cross-subject generalization and interpretability. The authors propose a BrainROI model that utilizes a new fMRI encoder and a voxel-wise gated fusion mechanism to create a shared space using multi-atlas soft functional parcellations, achieving significant improvements in metrics such as BLEU-4 and CIDEr on the NSD dataset compared to state-of-the-art methods. Additionally, they introduce an interpretable prompt optimization process to enhance stability and transparency in prompt design, alongside parameterized decoding constraints during inference to further improve the quality of generated descriptions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强多模态脑解码，该方法从脑活动信号（如fMRI）中重建语义信息，同时解决跨个体泛化和可解释性方面的挑战。作者提出了一种BrainROI模型，利用新的fMRI编码器和体素级门控融合机制，为不同个体的功能脑拓扑创建共享空间。实验结果表明，与最新的方法相比，该模型在NSD数据集上的脑描述评估指标（如BLEU-4和CIDEr）有显著提升，同时引入了一种新的可解释提示优化过程，增强了提示设计的稳定性和透明性。</div>
</details>
</div>
<div class="card">
<div class="title">QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents</div>
<div class="meta-line">Authors: Yuchong Xie, Zesen Liu, Mingyu Luo, Zhixiang Zhang, Kaikai Zhang, Yuanyuan Yuan, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</div>
<div class="meta-line">First: 2025-10-27T07:04:08+00:00 · Latest: 2026-01-14T07:07:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23675v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23675v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding agents integrated into IDEs orchestrate powerful tools and high-privilege system access, creating a high-stakes attack surface. Prior work on Indirect Prompt Injection (IPI) is mainly query-specific, requiring particular user queries as triggers and leading to poor generalizability. We propose query-agnostic IPI, a new attack paradigm that reliably executes malicious payloads under arbitrary user queries. Our key insight is that malicious payloads should leverage the invariant prompt context (i.e., system prompt and tool descriptions) rather than variant user queries. We present QueryIPI, an automated framework that uses tool descriptions as optimizable payloads and refines them via iterative, prompt-based blackbox optimization. QueryIPI leverages system invariants for initial seed generation aligned with agent conventions, and iterative reflection to resolve instruction-following failures and safety refusals. Experiments on five simulated agents show that QueryIPI achieves up to 87% success rate, outperforming the best baseline (50%). Crucially, generated malicious descriptions transfer to real-world coding agents, highlighting a practical security risk.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueryIPI：与查询无关的编码代理间接提示注入</div>
<div class="mono" style="margin-top:8px">现代编码代理集成在IDE中，协调强大的工具和高权限系统访问，形成高风险攻击面。之前的间接提示注入（IPI）工作主要是查询特定的，需要特定用户查询作为触发器，导致泛化能力差。我们提出了与查询无关的IPI，这是一种新的攻击范式，可以在任意用户查询下可靠地执行恶意负载。我们的关键见解是，恶意负载应利用不变的提示上下文（即系统提示和工具描述），而不是可变的用户查询。我们提出了QueryIPI，一个自动化框架，使用工具描述作为可优化的负载，并通过迭代的基于提示的黑箱优化进行精炼。QueryIPI利用系统不变性生成与代理约定对齐的初始种子，并通过迭代反思解决指令遵循失败和安全拒绝。在五个模拟代理上的实验表明，QueryIPI的成功率高达87%，超越了最佳基线（50%）。重要的是，生成的恶意描述可以转移到现实世界的编码代理中，突显了实际的安全风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the security vulnerabilities of modern coding agents that have high-privilege system access. The authors introduce QueryIPI, a novel attack paradigm that enables query-agnostic Indirect Prompt Injection (IPI), which executes malicious payloads regardless of user queries by utilizing the invariant prompt context. Experimental results demonstrate that QueryIPI achieves a success rate of up to 87% in executing these payloads on five simulated agents, significantly outperforming the best baseline success rate of 50%, and the findings indicate that the generated malicious descriptions can be effectively transferred to real-world coding agents, underscoring a serious security concern.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现代编码代理的安全漏洞，这些代理具有高权限系统访问权限并集成在IDE中。作者提出了QueryIPI，这是一种新颖的攻击方法，采用查询无关的间接提示注入（IPI），使恶意负载能够在不考虑用户查询的情况下执行，利用不变的提示上下文。实验结果表明，QueryIPI在五个模拟代理上实现了高达87%的攻击成功率，显著超过最佳基线的50%成功率，并且生成的恶意描述被证明可以转移到现实世界的编码代理上，表明存在严重的安全威胁。</div>
</details>
</div>
<div class="card">
<div class="title">Universal Few-Shot Spatial Control for Diffusion Models</div>
<div class="meta-line">Authors: Kiet T. Nguyen, Chanhyuk Lee, Donggyun Kim, Dong Hoon Lee, Seunghoon Hong</div>
<div class="meta-line">First: 2025-09-09T09:08:07+00:00 · Latest: 2026-01-14T06:19:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07530v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07530v2">PDF</a> · <a href="https://github.com/kietngt00/UFC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的通用少样本空间控制</div>
<div class="mono" style="margin-top:8px">在预训练的文本到图像扩散模型中，空间条件显著改善了生成图像结构的细粒度控制。然而，现有的控制适配器在遇到与训练任务有显著不同的新空间控制条件时，适应性有限且训练成本高。为了解决这一限制，我们提出了通用少样本控制（UFC），一种能够推广到新空间条件的多功能少样本控制适配器。给定一组未见任务的图像-条件对和一个查询条件，UFC利用查询条件和支持条件之间的类比构建任务特定的控制特征，通过匹配机制和对一小组任务特定参数的更新来实现。对六个新空间控制任务的实验表明，UFC仅用30个新任务的标注示例进行微调，就能实现与空间条件一致的细粒度控制。值得注意的是，当用0.1%的完整训练数据进行微调时，UFC在各种控制任务中达到了与完全监督基线相当的性能。我们还展示了UFC在各种扩散骨干网络上的适用性，并在UNet和DiT架构上证明了其有效性。代码可在 https://github.com/kietngt00/UFC 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the adaptability and reduce the training costs of spatial control in pretrained text-to-image diffusion models, which currently struggle with novel conditions. The authors propose a method called Universal Few-Shot Control (UFC), which utilizes a few image-condition pairs to create task-specific control features through a matching mechanism and updates on a small set of parameters. Experimental results demonstrate that UFC, when fine-tuned with only 30 examples from new tasks, achieves fine-grained control that aligns with spatial conditions and performs competitively with fully supervised methods using just 0.1% of the training data across various diffusion architectures.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强预训练文本到图像扩散模型在新空间条件下的适应性，并降低训练成本，而现有的控制适配器在面对与训练任务有显著差异的新空间控制条件时表现有限。作者提出了一种名为通用少样本控制（UFC）的方法，该方法利用来自未见任务的少量图像-条件对，通过匹配机制和对一小组任务特定参数的更新来创建任务特定的控制特征。实验结果表明，UFC仅需30个标注示例即可实现细粒度控制，并且在仅用0.1%的完整训练数据进行微调时，其在各种控制任务中表现出与完全监督基线相当的竞争力，显示出其在不同扩散架构（如UNet和DiT）中的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies</div>
<div class="meta-line">Authors: Jamie Magrill, Leah Gornstein, Sandra Seekins, Barry Magrill</div>
<div class="meta-line">First: 2026-01-14T05:13:04+00:00 · Latest: 2026-01-14T05:13:04+00:00</div>
<div class="meta-line">Comments: 24 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09169v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09169v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p &lt; 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>幻影中的建筑：评估生成图像模型在建筑风格、元素和类型上的表现</div>
<div class="mono" style="margin-top:8px">生成性人工智能（GenAI）文本到图像系统越来越多地用于生成建筑图像，但它们在一个历史上受规则约束的领域中再现准确图像的能力仍然缺乏明确的特征。我们评估了五个广泛使用的GenAI图像平台（Adobe Firefly、DALL-E 3、Google Imagen 3、Microsoft Image Generator和Midjourney），使用了30个涵盖风格、类型和规范元素的建筑提示。每个提示-生成器对生成了四幅图像（共600幅图像）。两位建筑历史学家独立根据预定义标准对每幅图像的准确性进行评分，通过共识解决分歧。设置级别的表现总结为每组四幅图像中零到四幅准确图像。来自常见提示的图像输出比来自稀有提示的准确性高2.7倍（p &lt; 0.05）。在各个平台之间，整体准确性有限（最高准确性得分52%；最低32%；平均42%）。四幅图像全部正确（4/4）的结果在各个平台之间相似。相比之下，四幅图像全部错误（0/4）的结果差异显著，Imagen 3表现出最少的失败，而Microsoft Image Generator表现出最多的失败。对图像数据集的定性审查识别出重复出现的模式，包括过度装饰、中世纪风格与其后复兴之间的混淆，以及对描述性提示的误表述（例如，蛋与箭、带状柱、悬拱）。这些发现支持对GenAI合成内容的可见标签、未来训练数据集的来源标准以及谨慎使用GenAI建筑图像的教育用途的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the effectiveness of generative artificial intelligence (GenAI) in producing accurate architectural imagery, a field characterized by strict historical rules. The study evaluated five popular GenAI platforms using 30 architectural prompts, generating a total of 600 images, which were then scored for accuracy by two architectural historians. The results indicated that images generated from common prompts were significantly more accurate than those from rare prompts, with an overall accuracy ranging from 32% to 52%, and highlighted issues such as over-embellishment and misrepresentation of architectural styles, underscoring the need for improved labeling and standards in GenAI-generated content.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成性人工智能（GenAI）文本到图像系统在生成准确建筑图像方面的有效性，这一领域具有严格的历史规则。研究评估了五个流行的GenAI平台，使用30个建筑提示生成了总共600幅图像，这些图像由两位建筑历史学家独立评分以评估准确性。结果显示，来自常见提示生成的图像显著比稀有提示生成的图像更准确，各平台的整体准确性有限，范围在32%到52%之间，强调了在GenAI生成的建筑内容中改进标签和标准的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Detection Thresholds: The Impact of False Positives and Negatives on Super-Resolution Ultrasound Localization Microscopy</div>
<div class="meta-line">Authors: Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz</div>
<div class="meta-line">First: 2024-11-11T22:58:56+00:00 · Latest: 2026-01-14T05:05:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.07426v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.07426v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution ultrasound imaging with ultrasound localization microscopy (ULM) offers a high-resolution view of microvascular structures. Yet, ULM image quality heavily relies on precise microbubble (MB) detection. Despite the crucial role of localization algorithms, there has been limited focus on the practical pitfalls in MB detection tasks such as setting the detection threshold. This study examines how False Positives (FPs) and False Negatives (FNs) affect ULM image quality by systematically adding controlled detection errors to simulated data. Results indicate that while both FP and FN rates impact Peak Signal-to-Noise Ratio (PSNR) similarly, increasing FP rates from 0\% to 20\% decreases Structural Similarity Index (SSIM) by 7\%, whereas same FN rates cause a greater drop of around 45\%. Moreover, dense MB regions are more resilient to detection errors, while sparse regions show high sensitivity, showcasing the need for robust MB detection frameworks to enhance super-resolution imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估检测阈值：假阳性和假阴性对超分辨率超声定位显微镜的影响</div>
<div class="mono" style="margin-top:8px">超分辨率超声成像结合超声定位显微镜（ULM）提供了微血管结构的高分辨率视图。然而，ULM图像质量在很大程度上依赖于精确的微泡（MB）检测。尽管定位算法起着至关重要的作用，但对MB检测任务中实际陷阱的关注有限，例如设置检测阈值。本研究通过系统地向模拟数据添加受控检测错误，考察假阳性（FP）和假阴性（FN）如何影响ULM图像质量。结果表明，尽管FP和FN率对峰值信噪比（PSNR）的影响相似，但将FP率从0%提高到20%会使结构相似性指数（SSIM）下降7%，而相同的FN率则导致约45%的更大下降。此外，密集的MB区域对检测错误更具韧性，而稀疏区域则表现出高敏感性，显示出需要强大的MB检测框架以增强超分辨率成像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the critical role of microbubble detection in super-resolution ultrasound localization microscopy (ULM) and the impact of detection thresholds on image quality. The researchers systematically introduced controlled detection errors, specifically False Positives (FPs) and False Negatives (FNs), to simulated data to evaluate their effects on ULM image quality. The findings reveal that while both FP and FN rates similarly affect Peak Signal-to-Noise Ratio (PSNR), increasing FP rates from 0% to 20% results in a 7% decrease in Structural Similarity Index (SSIM), whereas the same FN rates lead to a more significant drop of approximately 45%, indicating that dense microbubble regions are more resilient to detection errors compared to sparse regions, highlighting the necessity for robust detection frameworks in super-resolution imaging.</div>
<div class="mono" style="margin-top:8px">本研究探讨了超分辨率超声定位显微镜（ULM）中微泡检测阈值的关键问题，这对微血管结构的高质量成像至关重要。研究人员系统地向模拟数据引入了控制的检测错误，特别是假阳性（FP）和假阴性（FN），以评估它们对图像质量的影响。研究结果表明，尽管FP和FN对峰值信噪比（PSNR）的影响相似，但将FP率从0%增加到20%时，结构相似性指数（SSIM）下降了7%，而相同的FN率导致约45%的更大下降，表明密集的微泡区域对检测错误更具韧性，而稀疏区域则表现出较高的敏感性，强调了在ULM中需要强大的检测框架。</div>
</details>
</div>
<div class="card">
<div class="title">SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</div>
<div class="meta-line">Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</div>
<div class="meta-line">First: 2026-01-14T04:42:19+00:00 · Latest: 2026-01-14T04:42:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model&#x27;s fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3&#x27;s multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSVP：用于工业零样本异常检测的协同语义-视觉提示</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）利用视觉-语言模型（VLMs）实现无监督的工业检测。然而，现有的ZSAD范式受限于单一视觉骨干，难以平衡全局语义泛化与细粒度结构可区分性。为了解决这一问题，我们提出了协同语义-视觉提示（SSVP），有效融合多样的视觉编码，以提升模型的细粒度感知。具体而言，SSVP引入了层次语义-视觉协同（HSVS）机制，深度整合DINOv3的多尺度结构先验到CLIP语义空间。随后，视觉条件提示生成器（VCPG）采用跨模态注意力指导动态提示生成，使语言查询能够精确锚定特定的异常模式。此外，为了解决全局评分与局部证据之间的差异，视觉-文本异常映射器（VTAM）建立了双门校准范式。在七个工业基准上的广泛评估验证了我们方法的鲁棒性；SSVP在MVTec-AD上实现了93.0\%的图像-AUROC和92.2\%的像素-AUROC，显著优于现有的零样本方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings, which currently faces limitations due to reliance on single visual backbones that do not effectively balance global semantic generalization and fine-grained structural discriminability. The authors propose a method called Synergistic Semantic-Visual Prompting (SSVP), which integrates diverse visual encodings through a Hierarchical Semantic-Visual Synergy (HSVS) mechanism that combines multi-scale structural priors from DINOv3 with the CLIP semantic space. Experimental results demonstrate that SSVP achieves state-of-the-art performance on seven industrial benchmarks, with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly surpassing existing zero-shot detection methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善工业环境中的零样本异常检测（ZSAD），目前由于依赖单一视觉骨干网络而面临局限，这些网络无法有效平衡语义泛化和结构可辨别性。作者提出了一种名为协同语义-视觉提示（SSVP）的方法，通过层次语义-视觉协同（HSVS）机制整合多样的视觉编码，增强模型对细粒度细节的感知能力。实验结果表明，SSVP在七个工业基准测试中实现了最先进的性能，在MVTec-AD上获得93.0%的图像AUROC和92.2%的像素AUROC，显著超越现有的零样本检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">Tuning-free Visual Effect Transfer across Videos</div>
<div class="meta-line">Authors: Maxwell Jones, Rameen Abdal, Or Patashnik, Ruslan Salakhutdinov, Sergey Tulyakov, Jun-Yan Zhu, Kuan-Chieh Jackson Wang</div>
<div class="meta-line">First: 2026-01-12T18:59:32+00:00 · Latest: 2026-01-14T04:31:07+00:00</div>
<div class="meta-line">Comments: Project Page: https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07833v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.07833v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video&#x27;s existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input&#x27;s motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website at https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无调优视频间视觉效果转移</div>
<div class="mono" style="margin-top:8px">我们提出了RefVFX，一个新的框架，可以以前馈方式将复杂的时间效果从参考视频转移到目标视频或图像。虽然现有方法在基于提示或关键帧条件的编辑方面表现出色，但在动态时间效果（如动态光照变化或角色变换）方面却面临挑战，这些效果难以通过文本或静态条件描述。转移视频效果具有挑战性，因为模型必须将新的时间动态与输入视频的现有运动和外观相结合。为了解决这个问题，我们引入了一个大规模的三元组数据集，每个三元组由一个参考效果视频、一个输入图像或视频以及一个对应的输出视频（展示转移效果）组成。创建这些数据并非易事，尤其是视频到视频的效果三元组，这些三元组并不自然存在。为了生成这些三元组，我们提出了一个可扩展的自动化管道，创建高质量的配对视频，旨在在基于某些固定、可重复的效果进行转换的同时，保留输入的运动和结构。然后，我们用从LoRA适配器派生的图像到视频效果和通过程序化合成生成的基于代码的时间效果增强这些数据。基于我们的新数据集，我们使用最近的文本到视频骨干网络训练我们的参考条件模型。实验结果表明，RefVFX生成了视觉一致和时间连贯的编辑，能够在未见效果类别中进行泛化，并在定量指标和人类偏好方面超越仅基于提示的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the transfer of complex temporal effects from reference videos to target videos or images, addressing limitations in existing methods that struggle with dynamic effects. The authors introduce RefVFX, a framework that utilizes a large-scale dataset of triplets consisting of reference effect videos, input images or videos, and corresponding output videos, generated through a scalable automated pipeline. Experimental results show that RefVFX achieves visually consistent and temporally coherent edits, generalizes well across unseen effect categories, and surpasses prompt-only baselines in both quantitative metrics and human preference.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善复杂时间效果从参考视频到目标视频或图像的转移，解决现有方法在动态效果方面的局限性。作者提出了RefVFX框架，利用一个包含参考效果视频、输入图像或视频及相应输出视频的三元组的大规模数据集，通过可扩展的自动化管道创建。实验结果表明，RefVFX实现了视觉一致和时间连贯的编辑，能够很好地泛化到未见过的效果类别，并在定量指标和人类偏好方面超越了仅基于提示的基线。</div>
</details>
</div>
<div class="card">
<div class="title">Distributional Machine Unlearning via Selective Data Removal</div>
<div class="meta-line">Authors: Youssef Allouah, Rachid Guerraoui, Sanmi Koyejo</div>
<div class="meta-line">First: 2025-07-20T20:21:23+00:00 · Latest: 2026-01-14T04:19:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15112v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.15112v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning systems increasingly face requirements to remove entire domains of information--such as toxic language or biases--rather than individual user data. This task presents a dilemma: full removal of the unwanted domain data is computationally expensive, while random partial removal is statistically inefficient. We find that a domain&#x27;s statistical influence is often concentrated in a small subset of its data samples, suggesting a path between ineffective partial removal and unnecessary complete removal. We formalize this as distributional unlearning: a framework to select a small subset that balances forgetting an unwanted distribution while preserving a desired one. Using Kullback-Leibler divergence constraints, we derive the exact removal-preservation Pareto frontier for Gaussian distributions and prove that models trained on the edited data achieve corresponding log-loss bounds. We propose a distance-based selection algorithm and show it is quadratically more sample-efficient than random removal in the challenging low-divergence regime. Experiments across synthetic, text, and image datasets (Jigsaw, CIFAR-10, SMS spam) show our method requires 15-82% less deletion than full removal for strong unlearning effects, e.g., halving initial forget set accuracy. Ultimately, by showing a small forget set often suffices, our framework lays the foundations for more scalable and rigorous subpopulation unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过选择性数据移除实现分布式机器遗忘</div>
<div class="mono" style="margin-top:8px">机器学习系统越来越面临移除整个信息领域的要求——例如有毒语言或偏见——而不是单个用户数据。这个任务提出了一个困境：完全移除不需要的领域数据在计算上成本高，而随机部分移除在统计上效率低。我们发现一个领域的统计影响通常集中在其数据样本的一个小子集上，这表明在无效的部分移除和不必要的完全移除之间存在一条路径。我们将其形式化为分布式遗忘：一个选择小子集的框架，平衡遗忘不需要的分布同时保留所需的分布。通过使用Kullback-Leibler散度约束，我们推导出高斯分布的确切移除-保留帕累托前沿，并证明在编辑数据上训练的模型达到了相应的对数损失界限。我们提出了一种基于距离的选择算法，并展示其在具有挑战性的低散度范围内比随机移除的样本效率高出二次。针对合成、文本和图像数据集（Jigsaw、CIFAR-10、SMS垃圾邮件）的实验表明，我们的方法在实现强遗忘效果时需要比完全移除少15-82%的删除，例如，将初始遗忘集的准确率减半。最终，通过表明一个小的遗忘集通常足够，我们的框架为更具可扩展性和严谨性的子群体遗忘奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing need for machine learning systems to remove entire domains of information, such as toxic language or biases, rather than just individual user data, which poses challenges in terms of computational efficiency. The authors propose a framework called distributional unlearning, which focuses on selecting a small subset of data samples that effectively balances the removal of unwanted distribution while preserving the desired one, using Kullback-Leibler divergence constraints. Experimental results demonstrate that their distance-based selection algorithm is quadratically more sample-efficient than random removal, requiring 15-82% less deletion than full removal to achieve significant unlearning effects, such as reducing initial forget set accuracy by half, thereby providing a foundation for more scalable subpopulation unlearning methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是机器学习系统日益需要有效地删除整个信息领域，例如有毒语言或偏见，而不产生高昂的计算成本。作者提出了一种称为分布性遗忘的方法，该方法涉及选择一小部分数据样本，以平衡删除不需要的分布和保留所需的分布。实验结果表明，他们的基于距离的选择算法在样本效率上比随机删除高出二次，要求减少15-82%的删除量即可在多个数据集（包括Jigsaw、CIFAR-10和SMS垃圾邮件）上实现显著的遗忘效果，从而为更具可扩展性的子群体遗忘奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment</div>
<div class="meta-line">Authors: Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mu-Jiang-Shan Wang</div>
<div class="meta-line">First: 2026-01-14T03:44:27+00:00 · Latest: 2026-01-14T03:44:27+00:00</div>
<div class="meta-line">Comments: 18 pages, 7 figures. Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09120v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\% cross-jurisdictional performance retention versus 76.2\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一质量评估的自适应多阶段专利权利要求生成</div>
<div class="mono" style="margin-top:8px">当前的专利权利要求生成系统面临三个基本限制：跨司法管辖区的泛化能力差、权利要求与现有技术之间的语义关系建模不足，以及质量评估不可靠。我们提出了一种新颖的三阶段框架，通过关系感知的相似性分析、领域自适应的权利要求生成和统一的质量评估来解决这些挑战。我们的方法采用多头注意力机制，具有八个专门的头用于显式关系建模，结合课程学习与动态LoRA适配器选择，涵盖五个专利领域，并在评估方面实现交叉注意力机制以进行全面的质量评估。在USPTO HUPD数据集、EPO专利集合和Patent-CE基准上的广泛实验表明显著改进：相较于GPT-4o，ROUGE-L提升7.6分，BERTScore较Llama-3.1-8B提升8.3%，与人类专家的相关性为0.847，而单独评估模型为0.623。我们的方法在跨司法管辖区的性能保持方面为89.4%，而基线为76.2%，为自动化专利起诉工作流建立了全面解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to overcome the limitations of existing patent claim generation systems, which struggle with cross-jurisdictional generalization, semantic relationship modeling, and quality assessment. The authors propose a three-stage framework that utilizes relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment, employing multi-head attention and curriculum learning across various patent domains. Experimental results on multiple datasets show significant improvements, including a 7.6-point ROUGE-L gain over GPT-4o, an 8.3% BERTScore enhancement over Llama-3.1-8B, and a correlation of 0.847 with human experts, surpassing the 0.623 correlation of separate evaluation models, while maintaining 89.4% cross-jurisdictional performance retention compared to 76.2% for baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服现有专利权利要求生成系统的局限性，这些系统在跨管辖区泛化、语义关系建模和质量评估方面存在困难。作者提出了一种三阶段框架，利用关系感知相似性分析、领域自适应权利要求生成和统一质量评估，采用多头注意力机制和跨多个专利领域的课程学习。实验结果显示，在多个数据集上取得了显著改善，包括相较于GPT-4o提高7.6点的ROUGE-L分数、相较于Llama-3.1-8B提高8.3%的BERTScore，以及与人类专家的相关性为0.847，而单独评估模型为0.623，同时保持89.4%的跨管辖区性能保留率，相较于基线的76.2%.</div>
</details>
</div>
<div class="card">
<div class="title">DyDiT++: Diffusion Transformers with Timestep and Spatial Dynamics for Efficient Visual Generation</div>
<div class="meta-line">Authors: Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You</div>
<div class="meta-line">First: 2025-04-09T11:48:37+00:00 · Latest: 2026-01-14T03:33:41+00:00</div>
<div class="meta-line">Comments: This paper was accepted to the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) on January 9, 2026. arXiv admin note: substantial text overlap with arXiv:2410.03456</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.06803v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.06803v4">PDF</a> · <a href="https://github.com/alibaba-damo-academy/DyDiT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the static inference paradigm, which inevitably introduces redundant computation in certain diffusion timesteps and spatial regions. To overcome this inefficiency, we propose Dynamic Diffusion Transformer (DyDiT), an architecture that dynamically adjusts its computation along both timestep and spatial dimensions. Building on these designs, we present an extended version, DyDiT++, with improvements in three key aspects. First, it extends the generation mechanism of DyDiT beyond diffusion to flow matching, demonstrating that our method can also accelerate flow-matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT++. Remarkably, with &lt;3% additional fine-tuning iterations, our approach reduces the FLOPs of DiT-XL by 51%, yielding 1.73x realistic speedup on hardware, and achieves a competitive FID score of 2.07 on ImageNet. The code is available at https://github.com/alibaba-damo-academy/DyDiT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyDiT++：具有时间步和空间动态的扩散变换器用于高效视觉生成</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiT）是一种新兴的视觉生成扩散模型，表现出优越的性能，但面临着巨大的计算成本。我们的研究表明，这些成本主要源于静态推理范式，这不可避免地在某些扩散时间步和空间区域引入冗余计算。为了解决这一低效问题，我们提出了动态扩散变换器（DyDiT），一种在时间步和空间维度上动态调整计算的架构。在这些设计的基础上，我们提出了扩展版本DyDiT++，在三个关键方面进行了改进。首先，它将DyDiT的生成机制扩展到流匹配，证明我们的方法也可以加速基于流匹配的生成，增强其多样性。此外，我们增强了DyDiT以应对更复杂的视觉生成任务，包括视频生成和文本到图像生成，从而拓宽其在现实世界中的应用。最后，为了解决全面微调的高成本并实现技术的普及，我们研究了以参数高效的方式训练DyDiT的可行性，并引入了基于时间步的动态LoRA（TD-LoRA）。在包括DiT、SiT、Latte和FLUX在内的多种视觉生成模型上进行的广泛实验表明，DyDiT++的有效性。值得注意的是，通过&lt;3%的额外微调迭代，我们的方法将DiT-XL的FLOPs减少了51%，在硬件上实现了1.73倍的真实加速，并在ImageNet上获得了竞争性的FID分数2.07。代码可在https://github.com/alibaba-damo-academy/DyDiT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs associated with the Diffusion Transformer (DiT) model for visual generation, which arise from its static inference paradigm. The authors propose the Dynamic Diffusion Transformer (DyDiT) architecture, which dynamically adjusts computation across both timestep and spatial dimensions, and introduce an extended version, DyDiT++, that enhances generation mechanisms and tackles complex tasks like video and text-to-image generation. Experimental results show that DyDiT++ reduces the FLOPs of DiT-XL by 51% with less than 3% additional fine-tuning iterations, achieving a 1.73x speedup on hardware and a competitive FID score of 2.07 on ImageNet.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉生成中的扩散变换器（DiT）模型所面临的高计算成本，这些成本源于其静态推理范式。作者提出了动态扩散变换器（DyDiT）架构，该架构在时间步和空间维度上动态调整计算，并提出了增强版DyDiT++，将生成机制扩展到流匹配。实验结果表明，DyDiT++在DiT-XL上实现了51%的FLOPs减少，且仅需不到3%的额外微调迭代，导致硬件上实现1.73倍的加速，同时在ImageNet上保持了竞争力的FID分数2.07。</div>
</details>
</div>
<div class="card">
<div class="title">Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</div>
<div class="meta-line">Authors: Ruofan Wang, Xin Wang, Yang Yao, Juncheng Li, Xuan Tong, Xingjun Ma</div>
<div class="meta-line">First: 2025-08-03T12:51:47+00:00 · Latest: 2026-01-14T02:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01741v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.01741v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread practice of fine-tuning open-source Vision-Language Models (VLMs) raises a critical security concern: jailbreak vulnerabilities in base models may persist in downstream variants, enabling transferable attacks across fine-tuned systems. To investigate this risk, we propose the Simulated Ensemble Attack (SEA), a grey-box jailbreak framework that assumes full access to the base VLM but no knowledge of the fine-tuned target. SEA enhances transferability via Fine-tuning Trajectory Simulation (FTS), which models bounded parameter variations in the vision encoder, and Targeted Prompt Guidance (TPG), which stabilizes adversarial optimization through auxiliary textual guidance. Experiments on the Qwen2-VL family demonstrate that SEA achieves consistently high transfer success and toxicity rates across diverse fine-tuned variants, including safety-enhanced models, while standard PGD-based image jailbreaks exhibit negligible transferability. Further analysis reveals that fine-tuning primarily induces localized parameter shifts around the base model, explaining why attacks optimized over a simulated neighborhood transfer effectively. We also show that SEA generalizes across different base generations (e.g., Qwen2.5/3-VL), indicating that its effectiveness arises from shared fine-tuning-induced behaviors rather than architecture- or initialization-specific factors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟集成攻击：跨微调视觉语言模型转移越狱</div>
<div class="mono" style="margin-top:8px">微调开源视觉语言模型（VLM）的广泛实践引发了一个关键的安全问题：基础模型中的越狱漏洞可能在下游变体中持续存在，从而使得可转移攻击在微调系统之间发生。为调查这一风险，我们提出了模拟集成攻击（SEA），这是一种灰盒越狱框架，假设对基础VLM有完全访问权限，但对微调目标没有知识。SEA通过微调轨迹模拟（FTS）增强可转移性，该方法模拟视觉编码器中的有界参数变化，以及目标提示引导（TPG），通过辅助文本指导稳定对抗优化。对Qwen2-VL系列的实验表明，SEA在各种微调变体中，包括安全增强模型，始终实现高转移成功率和毒性率，而标准的基于PGD的图像越狱则表现出微不足道的可转移性。进一步分析表明，微调主要导致基础模型周围的局部参数偏移，这解释了为什么在模拟邻域上优化的攻击能够有效转移。我们还表明，SEA在不同基础代际（例如Qwen2.5/3-VL）之间具有泛化能力，表明其有效性源于共享的微调引起的行为，而不是特定于架构或初始化的因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security risks associated with fine-tuning Vision-Language Models (VLMs), specifically the persistence of jailbreak vulnerabilities from base models in their fine-tuned counterparts. The authors introduce the Simulated Ensemble Attack (SEA), a grey-box framework that utilizes Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG) to enhance the transferability of attacks. Experimental results on the Qwen2-VL family indicate that SEA achieves high transfer success and toxicity rates across various fine-tuned models, including those with safety enhancements, while traditional PGD-based image jailbreaks show minimal transferability. The findings suggest that fine-tuning leads to localized parameter shifts, allowing effective transfer of attacks optimized in a simulated environment, and that SEA&#x27;s effectiveness is consistent across different base model generations.</div>
<div class="mono" style="margin-top:8px">本研究关注于微调视觉-语言模型（VLMs）所带来的安全风险，特别是基础模型中的越狱漏洞在其微调变体中可能持续存在。作者提出了模拟集成攻击（SEA），这是一种灰盒框架，利用微调轨迹模拟（FTS）和目标提示引导（TPG）来增强攻击的可转移性。在Qwen2-VL系列上的实验结果表明，SEA在各种微调模型中，包括安全增强模型，达到了高转移成功率和毒性率，而传统的基于PGD的图像越狱攻击则表现出微不足道的可转移性。研究结果表明，微调导致局部参数变化，从而允许在模拟环境中优化的攻击有效转移，并且SEA的有效性在不同基础模型代际之间是一致的。</div>
</details>
</div>
<div class="card">
<div class="title">MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting</div>
<div class="meta-line">Authors: Kangda Wei, Ruihong Huang</div>
<div class="meta-line">First: 2026-01-14T02:35:19+00:00 · Latest: 2026-01-14T02:35:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09085v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMR-GRPO：通过关注多样性的奖励重加权加速GRPO风格训练</div>
<div class="mono" style="margin-top:8px">群体相对策略优化（GRPO）已成为训练数学推理模型的标准方法；然而，它对每个提示的多个完成的依赖使得训练计算成本高昂。尽管最近的工作减少了达到峰值性能所需的训练步骤数量，但由于每步成本更高，整体的实际训练时间往往保持不变甚至增加。我们提出了MMR-GRPO，它结合了最大边际相关性，根据完成的多样性重新加权奖励。我们的关键见解是，语义上冗余的完成对边际学习信号的贡献有限；优先考虑多样化的解决方案可以产生更有信息量的更新并加速收敛。在三个模型规模（1.5B、7B、8B）、三种GRPO变体和五个数学推理基准上的广泛评估表明，MMR-GRPO在达到可比的峰值性能的同时，平均减少了47.9%的训练步骤和70.2%的实际时间。这些收益在模型、方法和基准之间是一致的。我们将发布我们的代码、训练模型和实验协议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational inefficiency of Group Relative Policy Optimization (GRPO) in training mathematical reasoning models, which typically requires multiple completions per prompt. The authors propose MMR-GRPO, a method that employs Maximal Marginal Relevance to adjust rewards based on the diversity of completions, aiming to enhance learning efficiency. Experimental results demonstrate that MMR-GRPO achieves similar peak performance while reducing the average number of training steps by 47.9% and wall-clock time by 70.2%, indicating significant improvements in training efficiency across various model sizes and benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在训练数学推理模型时，群体相对策略优化（GRPO）在计算效率上的不足，该方法通常需要每个提示多个完成。作者提出了MMR-GRPO，这是一种利用最大边际相关性根据完成的多样性调整奖励的方法。实验结果表明，MMR-GRPO在减少平均训练步骤47.9%和墙钟时间70.2%的同时，仍能实现相似的峰值性能，且在不同模型规模和基准测试中均表现出一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation</div>
<div class="meta-line">Authors: Wilson Y. Lee</div>
<div class="meta-line">First: 2026-01-14T02:34:58+00:00 · Latest: 2026-01-14T02:34:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09084v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类判断够多少才算足够？人类偏好评估的可行性限制</div>
<div class="mono" style="margin-top:8px">人类偏好评估广泛用于比较生成模型，但仍不清楚需要多少判断才能可靠地检测到小的改进。我们表明，当偏好信号在提示中分散时（即所有提示类型的信息量相似），比例分配是最小最大最优的：没有分配策略能显著提高可检测性。对大规模人类偏好数据集的实证分析表明，大多数比较都处于这种分散状态，表现出小的偏好边际，要求的判断数量远超通常收集的数量，即使在样本良好的比较中也是如此。这些限制在评估协议和模式中持续存在，包括聊天、图像生成和带执行反馈的代码生成。相比之下，经过策划的基准通过减少提示引起的变异性系统性地诱导更大的边际，并通过减少$1.5\times$的提示级方差来提高可检测性。我们的结果表明，模糊或负面的人工评估结果通常反映了评估能力不足，而非模型等价性，强调了明确考虑效应大小、预算和协议设计的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the adequacy of human preference evaluations in assessing generative models, particularly focusing on the number of judgments needed to detect minor improvements. The authors employed empirical analysis of extensive human preference datasets to demonstrate that most comparisons fall into a diffuse regime, where small preference margins necessitate significantly more judgments than are typically gathered. They found that curated benchmarks that minimize prompt variability lead to larger preference margins and enhance detectability, highlighting that inconclusive evaluation results often stem from insufficient evaluation power rather than true model equivalence.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在评估生成模型时，确定有效的人类判断数量的挑战，特别是在偏好信号分散的情况下。作者通过对大规模人类偏好数据集的实证分析，评估了判断分配策略对改进可检测性的影响。研究结果表明，大多数比较处于分散状态，所需的判断数量远超过通常收集的数量，而经过精心设计的基准可以通过减少提示级方差来提高可检测性，强调了在偏好评估中考虑效应大小和评估设计的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache</div>
<div class="meta-line">Authors: Chi-Chih Chang, Siqi Zhu, Zhichen Zeng, Haibin Lin, Jiaxuan You, Mohamed S. Abdelfattah, Ziheng Jiang, Xuehai Qian</div>
<div class="meta-line">First: 2026-01-14T02:34:48+00:00 · Latest: 2026-01-14T02:34:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SRT：通过树结构缓存加速强化学习的推测展开</div>
<div class="mono" style="margin-top:8px">我们提出了树结构缓存的推测展开（SRT），这是一种简单的无模型方法，用于加速语言模型的在线强化学习（RL），而不牺牲分布正确性。SRT通过在每个提示的树结构缓存中存储先前生成的续写，利用相同提示在训练步骤中的展开的经验相似性。在生成过程中，当前策略将此树作为执行推测解码的草稿模型。为了保持缓存的新鲜度并提高草稿模型质量，SRT在线更新树结构，并在空闲的GPU时间主动进行提前生成。SRT集成到标准的RL管道（例如，PPO、GRPO和DAPO）和多轮设置中，始终减少生成和步骤延迟，并降低每个标记的推理成本，在展开过程中实现高达2.08倍的实际时间加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of on-policy reinforcement learning for language models without compromising distributional correctness. The authors introduce a method called Speculative Rollout with Tree-Structured Cache (SRT), which utilizes a tree-structured cache to store previously generated continuations for the same prompt, allowing for speculative decoding during generation. Experimental results demonstrate that SRT significantly reduces generation and step latency while lowering per-token inference costs, achieving up to a 2.08x speedup in wall-clock time during rollout when integrated into standard reinforcement learning pipelines such as PPO, GRPO, and DAPO.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高语言模型的在线强化学习效率，同时不牺牲分布正确性。作者提出了一种名为树结构缓存的推测回滚（SRT）的方法，该方法利用树结构缓存存储相同提示的先前生成输出，从而在生成过程中进行推测解码。实验结果表明，SRT显著减少了生成和步骤延迟，在与标准强化学习管道（如PPO、GRPO和DAPO）集成时，回滚过程中的墙钟时间速度提升可达2.08倍。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing AI Negotiations: A Large-Scale Autonomous Negotiation Competition</div>
<div class="meta-line">Authors: Michelle Vaccaro, Michael Caosun, Harang Ju, Sinan Aral, Jared R. Curhan</div>
<div class="meta-line">First: 2025-03-09T03:25:48+00:00 · Latest: 2026-01-13T21:31:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06416v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.06416v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We conducted an International AI Negotiation Competition in which participants designed and refined prompts for AI negotiation agents. We then facilitated over 180,000 negotiations between these agents across multiple scenarios with diverse characteristics and objectives. Our findings revealed that principles from human negotiation theory remain crucial even in AI-AI contexts. Surprisingly, warmth -- a traditionally human relationship-building trait -- was consistently associated with superior outcomes across all key performance metrics. Dominant agents, meanwhile, were especially effective at claiming value. Our analysis also revealed unique dynamics in AI-AI negotiations not fully explained by existing theory, including AI-specific technical strategies like chain-of-thought reasoning and prompt injection. When we applied natural language processing (NLP) methods to the full transcripts of all negotiations, we found positivity, gratitude, and question-asking (associated with warmth) were strongly associated with reaching deals as well as objective and subjective value, whereas conversation lengths (associated with dominance) were strongly associated with impasses. The results suggest the need to establish a new theory of AI negotiation, which integrates classic negotiation theory with AI-specific negotiation theories to better understand autonomous negotiations and optimize agent performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推进人工智能谈判：大规模自主谈判竞赛</div>
<div class="mono" style="margin-top:8px">我们举办了一场国际人工智能谈判竞赛，参与者为人工智能谈判代理设计和完善提示。随后，我们在多个具有不同特征和目标的场景中促进了超过180,000次代理之间的谈判。我们的研究发现，人类谈判理论中的原则在人工智能-人工智能的背景下仍然至关重要。令人惊讶的是，温暖——一种传统的人际关系建立特质——在所有关键绩效指标中与更优的结果始终相关。与此同时，主导代理在获取价值方面尤其有效。我们的分析还揭示了人工智能-人工智能谈判中独特的动态，这些动态并未完全被现有理论解释，包括链式思维推理和提示注入等人工智能特有的技术策略。当我们将自然语言处理（NLP）方法应用于所有谈判的完整记录时，我们发现积极性、感激和提问（与温暖相关）与达成交易以及客观和主观价值密切相关，而谈话长度（与主导性相关）则与僵局密切相关。结果表明，需要建立一种新的人工智能谈判理论，将经典谈判理论与人工智能特有的谈判理论结合，以更好地理解自主谈判并优化代理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aimed to explore the dynamics of AI negotiations by organizing an International AI Negotiation Competition where participants created prompts for AI negotiation agents. Over 180,000 negotiations were conducted across various scenarios, revealing that human negotiation principles remain relevant in AI contexts, with warmth being linked to better outcomes. Additionally, dominant agents excelled at value claiming, while unique AI negotiation strategies emerged, such as chain-of-thought reasoning. Natural language processing analysis indicated that traits like positivity and gratitude correlated with successful deals, whereas longer conversations often led to impasses, highlighting the need for a new theory that combines traditional and AI-specific negotiation strategies.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过组织国际人工智能谈判竞赛，探索人工智能谈判的动态，参与者为人工智能谈判代理创建提示。进行了超过180,000次谈判，结果显示人类谈判原则，特别是温暖特质，对谈判结果有显著影响。研究发现，尽管主导型代理在价值索取方面表现出色，但积极性和感激之情等特质与成功达成交易相关，这表明有必要建立一个新的理论框架，将传统谈判理论与人工智能特定策略相结合，以增强对自主谈判的理解和表现。</div>
</details>
</div>
<div class="card">
<div class="title">SAM-pose2seg: Pose-Guided Human Instance Segmentation in Crowds</div>
<div class="meta-line">Authors: Constantin Kolomiiets, Miroslav Purkrabek, Jiri Matas</div>
<div class="meta-line">First: 2026-01-13T21:12:03+00:00 · Latest: 2026-01-13T21:12:03+00:00</div>
<div class="meta-line">Comments: GitHub: https://github.com/MiraPurkrabek/BBoxMaskPose/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08982v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08982v1">PDF</a> · <a href="https://github.com/MiraPurkrabek/BBoxMaskPose/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://mirapurkrabek.github.io/BBox-MaskPose">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segment Anything (SAM) provides an unprecedented foundation for human segmentation, but may struggle under occlusion, where keypoints may be partially or fully invisible. We adapt SAM 2.1 for pose-guided segmentation with minimal encoder modifications, retaining its strong generalization. Using a fine-tuning strategy called PoseMaskRefine, we incorporate pose keypoints with high visibility into the iterative correction process originally employed by SAM, yielding improved robustness and accuracy across multiple datasets. During inference, we simplify prompting by selecting only the three keypoints with the highest visibility. This strategy reduces sensitivity to common errors, such as missing body parts or misclassified clothing, and allows accurate mask prediction from as few as a single keypoint. Our results demonstrate that pose-guided fine-tuning of SAM enables effective, occlusion-aware human segmentation while preserving the generalization capabilities of the original model. The code and pretrained models will be available at https://mirapurkrabek.github.io/BBox-MaskPose.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM-pose2seg：人群中的姿态引导人类实例分割</div>
<div class="mono" style="margin-top:8px">Segment Anything (SAM) 为人类分割提供了前所未有的基础，但在遮挡情况下可能会遇到困难，此时关键点可能部分或完全不可见。我们对 SAM 2.1 进行了适应，以实现姿态引导分割，最小化编码器修改，保留其强大的泛化能力。通过一种称为 PoseMaskRefine 的微调策略，我们将高可见性的姿态关键点纳入 SAM 原本采用的迭代修正过程中，从而在多个数据集上提高了鲁棒性和准确性。在推理过程中，我们通过仅选择三个可见性最高的关键点来简化提示。这一策略减少了对常见错误的敏感性，例如缺失身体部位或错误分类的衣物，并允许从少至一个关键点进行准确的掩膜预测。我们的结果表明，SAM 的姿态引导微调能够有效地进行考虑遮挡的人类分割，同时保留原始模型的泛化能力。代码和预训练模型将可在 https://mirapurkrabek.github.io/BBox-MaskPose 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of the Segment Anything Model (SAM) in human segmentation, particularly in scenarios involving occlusion where keypoints may not be fully visible. The authors adapt SAM 2.1 for pose-guided segmentation by implementing a fine-tuning strategy called PoseMaskRefine, which integrates visible pose keypoints into the model&#x27;s iterative correction process. Experimental results indicate that this approach enhances the model&#x27;s robustness and accuracy across various datasets, allowing for effective segmentation even with minimal keypoint input, thus maintaining the generalization capabilities of the original SAM model.</div>
<div class="mono" style="margin-top:8px">本研究解决了Segment Anything Model（SAM）在人类分割中的局限性，特别是在关键点可能不完全可见的遮挡场景中。作者通过实施一种称为PoseMaskRefine的微调策略，将可见的姿态关键点整合到模型的迭代修正过程中，从而对SAM 2.1进行了姿态引导的分割适配。实验结果表明，该方法在多个数据集上增强了人类实例分割的鲁棒性和准确性，允许仅使用一个关键点进行有效的掩膜预测，同时保持了原始SAM模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Integration Matters for Learning PDEs with Backward SDEs</div>
<div class="meta-line">Authors: Sungje Park, Stephen Tu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-02T07:36:27+00:00 · Latest: 2026-01-13T20:06:31+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.01078v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.01078v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>集成方法在学习带有反向随机微分方程的偏微分方程中的重要性</div>
<div class="mono" style="margin-top:8px">基于反向随机微分方程（BSDE）的深度学习方法为解决高维偏微分方程（PDE）提供了替代物，作为物理信息神经网络（PINNs）的替代方案，在随机最优控制等场景中具有潜在的算法优势，其中感兴趣的PDE与基础动态系统相关。然而，文献中标准的BSDE求解器相较于PINNs的表现被实证证明较差。本文中，我们将这一性能差距的根本原因确定为标准Euler-Maruyama（EM）积分方案在一阶自洽BSDE损失中引入的离散化偏差，这使得优化景观偏离目标。我们发现，这一偏差无法通过更细的步长或多步自洽损失得到令人满意的解决。为妥善处理此问题，我们提出了一种基于Stratonovich的BSDE公式，并通过随机Heun积分进行实现。我们展示了所提方法完全消除了EM积分所面临的偏差问题。此外，我们的实证结果表明，我们的基于Heun的BSDE方法在多个高维基准测试中始终优于基于EM的变体，并与PINNs取得了竞争性结果。我们的发现强调了积分方案在基于BSDE的PDE求解器中的关键作用，这是文献中迄今为止鲜有关注的算法细节。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve the performance of backward stochastic differential equation (BSDE)-based deep learning methods for solving high-dimensional partial differential equations (PDEs), particularly in stochastic optimal control scenarios where traditional methods like Physics-Informed Neural Networks (PINNs) have shown better results. The authors identify that the performance gap is primarily due to a discretization bias from the standard Euler-Maruyama integration scheme used in BSDE losses, which misaligns the optimization process. To address this issue, they propose a Stratonovich-based BSDE formulation implemented with stochastic Heun integration, which effectively eliminates the bias associated with Euler-Maruyama integration. Experimental results demonstrate that the Heun-based BSDE method consistently outperforms Euler-Maruyama variants and achieves competitive performance compared to PINNs across various high-dimensional benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究解决了在求解高维偏微分方程（PDEs）时，基于反向随机微分方程（BSDE）的深度学习方法与物理信息神经网络（PINNs）之间的性能差距，特别是在随机最优控制的背景下。作者发现，标准的欧拉-马鲁雅马积分方案引入的离散化偏差对优化产生了负面影响。为了解决这个问题，他们提出了一种基于斯特拉托诺维奇的BSDE公式，并结合随机海因积分，有效消除了偏差。实验结果表明，基于海因的BSDE方法在多个高维基准测试中始终优于基于EM的变体，并与PINNs的性能相当。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
