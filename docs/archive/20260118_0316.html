<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 03:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_0316</div>
    <div class="row"><div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：编辑图像中物体的内在属性</div>
<div class="mono" style="margin-top:8px">我们介绍了Alterbute，一种基于扩散的方法，用于编辑图像中物体的内在属性。我们允许改变物体的颜色、纹理、材料，甚至形状，同时保持其感知身份和场景上下文。现有方法要么依赖于常常无法保持身份的无监督先验，要么使用过于严格的监督，阻碍有意义的内在变化。我们的方法依赖于：（i）一个放宽的训练目标，允许模型在身份参考图像、描述目标内在属性的文本提示以及定义外部上下文的背景图像和物体掩码的条件下，改变内在和外在属性。在推理时，我们通过重用原始背景和物体掩码来限制外在变化，从而确保仅改变所需的内在属性；（ii）视觉命名实体（VNEs）- 细粒度视觉身份类别（例如，“保时捷911 Carrera”），将共享身份定义特征的物体分组，同时允许内在属性的变化。我们使用视觉-语言模型从大型公共图像数据集中自动提取VNE标签和内在属性描述，实现可扩展的、保持身份的监督。Alterbute在保持身份的物体内在属性编辑方面优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation behind Alterbute is to improve the editing of intrinsic attributes of objects in images while maintaining their identity and scene context, addressing limitations in existing methods that either fail to preserve identity or impose restrictive supervision. The main method involves a diffusion-based approach that utilizes a relaxed training objective, allowing changes to intrinsic and extrinsic attributes based on an identity reference image, a textual prompt, and a background image with an object mask. Key experimental findings indicate that Alterbute significantly outperforms existing methods in preserving identity during the editing of intrinsic attributes such as color, texture, and shape.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种方法，以在保持物体感知身份和场景上下文的同时编辑图像中物体的内在属性，解决现有方法的局限性。作者提出了Alterbute，这是一种基于扩散的方法，采用放宽的训练目标和视觉命名实体（VNEs），以便根据身份参考图像和文本提示修改颜色、纹理和形状等属性。实验结果表明，Alterbute在保持物体身份的内在属性编辑方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</div>
<div class="meta-line">Authors: Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu</div>
<div class="meta-line">First: 2026-01-15T18:52:59+00:00 · Latest: 2026-01-15T18:52:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&#x27; prior expertise and interaction strategies through prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成性人工智能对建筑概念设计的影响：绩效、创造性自我效能和认知负荷</div>
<div class="mono" style="margin-top:8px">我们的研究考察了生成性人工智能（GenAI）如何影响建筑概念设计任务中的绩效、创造性自我效能和认知负荷。来自建筑工程及其他学科的三十六名学生参与者完成了一个两阶段的建筑设计任务，首先独立完成，然后使用外部工具（GenAI辅助条件和使用现有建筑项目在线库的对照条件）。设计结果由专家评估，而自我效能和认知负荷在每个阶段后由参与者自我报告。差异中的差异分析显示，参与者之间GenAI没有整体绩效优势；然而，子组分析表明，GenAI显著提高了新手设计师的设计绩效。相反，使用GenAI的学生的总体创造性自我效能下降。不同条件下的认知负荷没有显著差异，尽管提示使用模式显示，迭代的创意生成和视觉反馈提示与认知负荷的更大减少相关。这些发现表明，GenAI的有效性取决于用户的先前专业知识和通过提示的互动策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the influence of generative AI (GenAI) on performance, creative self-efficacy, and cognitive load in architectural conceptual design. Thirty-six student participants engaged in a two-phase design task, first independently and then with GenAI assistance or a control condition using an online repository. The results indicated no overall performance advantage of GenAI, but novice designers showed significant improvement in design performance. However, the use of GenAI led to a decline in general creative self-efficacy among students, and while cognitive load remained similar across conditions, specific prompting strategies were associated with reduced cognitive load.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成性人工智能（GenAI）对建筑概念设计任务中表现、创造性自我效能和认知负荷的影响，动机在于AI工具在设计教育中的日益普及。三十六名学生参与者完成了一个两阶段的设计任务，首先独立完成，然后使用GenAI或在线现有建筑项目库的对照条件。结果显示，GenAI没有整体性能优势，但亚组分析表明，初学者在使用GenAI时表现显著提高，而他们的创造性自我效能下降。认知负荷在不同条件下基本保持不变，尽管特定的提示策略与较低的认知负荷相关联。</div>
</details>
</div>
<div class="card">
<div class="title">Moonworks Lunara Aesthetic Dataset</div>
<div class="meta-line">Authors: Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan</div>
<div class="meta-line">First: 2026-01-12T19:11:41+00:00 · Latest: 2026-01-15T18:27:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07941v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07941v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Moonworks Lunara 美学数据集</div>
<div class="mono" style="margin-top:8px">该数据集涵盖多种艺术风格，包括来自中东、北欧、东亚和南亚的区域性美学，以及素描和油画等一般类别。所有图像均使用 Moonworks Lunara 模型生成，旨在体现独特的高质量美学风格，形成首个具有显著更高美学评分的数据集，超越了以美学为重点的数据集和通用数据集。每幅图像都附有经过人工精炼的提示和结构化注释，联合描述显著对象、属性、关系和风格线索。与强调广度而非精确度的大规模网络衍生数据集不同，Lunara 美学数据集优先考虑美学质量、风格多样性和许可透明度，并在 Apache 2.0 许可下发布，以支持研究和无限制的学术及商业使用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a high-quality dataset that captures diverse artistic styles and aesthetics. The authors developed the Moonworks Lunara Aesthetic Dataset, which includes images generated by the Lunara model, focusing on distinct artistic styles from various regions and general categories like sketch and oil painting. The key findings indicate that this dataset achieves significantly higher aesthetic scores compared to existing aesthetics-focused and general-purpose datasets, while also providing detailed annotations and a commitment to licensing transparency for academic and commercial use.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要一个高质量的数据集，以捕捉多样的艺术风格和美学特征。作者开发了Moonworks Lunara美学数据集，其中包含由Lunara模型生成的图像，重点关注独特的区域美学和一般艺术类别。实验结果表明，该数据集的美学评分显著高于现有的美学专注和通用数据集，同时为每个图像提供详细的注释，从而增强了其在艺术和设计研究及应用中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</div>
<div class="meta-line">Authors: Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong</div>
<div class="meta-line">First: 2026-01-15T16:16:34+00:00 · Latest: 2026-01-15T16:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10551v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放大型视觉语言模型在智能感知路边基础设施中的能力</div>
<div class="mono" style="margin-top:8px">城市路边基础设施的自动感知对智慧城市管理至关重要，但通用模型往往难以捕捉必要的细粒度属性和领域规则。虽然大型视觉语言模型（VLMs）在开放世界识别中表现出色，但它们在准确解释符合工程标准的复杂设施状态方面常常面临挑战，导致在实际应用中的性能不可靠。为了解决这个问题，我们提出了一种领域适应框架，将VLMs转变为智能基础设施分析的专业代理。我们的方法结合了数据高效的微调策略和基于知识的推理机制。具体而言，我们利用开放词汇微调Grounding DINO，以最小的监督强健地定位多样资产，随后在Qwen-VL上进行基于LoRA的适应，以进行深层语义属性推理。为了减轻幻觉并强制执行专业合规性，我们引入了一个双模态检索增强生成（RAG）模块，在推理过程中动态检索权威行业标准和视觉示例。在一个全面的新城市路边场景数据集上评估，我们的框架实现了58.9 mAP的检测性能和95.5%的属性识别准确率，展示了智能基础设施监测的强大解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the automated perception of urban roadside infrastructure, which is essential for effective smart city management, as existing general-purpose models often fail to accurately interpret complex facility states. The authors propose a domain-adapted framework that enhances Large Vision Language Models (VLMs) for intelligent infrastructure analysis by employing a data-efficient fine-tuning strategy combined with a knowledge-grounded reasoning mechanism. Their experimental results, evaluated on a new dataset of urban roadside scenes, show that the framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, indicating a significant advancement in reliable infrastructure monitoring capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升城市路边基础设施的自动感知能力，这对有效的智慧城市管理至关重要，因为现有的通用模型往往无法准确解读所需的复杂属性和标准。作者提出了一种领域适应框架，将大型视觉语言模型（VLM）转变为专门的基础设施分析代理，利用数据高效的微调策略与知识驱动的推理机制相结合。实验结果在新的城市路边场景数据集上评估，显示该框架实现了58.9 mAP的检测性能和95.5%的属性识别准确率，表明在智能基础设施监测能力上有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">CoGen: Creation of Reusable UI Components in Figma via Textual Commands</div>
<div class="meta-line">Authors: Ishani Kanapathipillai, Obhasha Priyankara</div>
<div class="meta-line">First: 2026-01-15T15:57:59+00:00 · Latest: 2026-01-15T15:57:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10536v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10536v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.
  The project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoGen：通过文本命令在Figma中创建可重用的UI组件</div>
<div class="mono" style="margin-top:8px">用户界面设计的发展强调了高效、可重用和可编辑组件的需求，以确保高效的设计过程。本研究介绍了CoGen，一个使用机器学习技术直接在Figma中生成可重用UI组件的系统，Figma是最流行的UI设计工具之一。CoGen解决了当前系统中的空白，专注于使用结构化JSON和自然语言提示创建原子组件，如按钮、标签和输入字段。
该项目集成了Figma API数据提取、Seq2Seq模型和微调的T5变换器用于组件生成。关键结果表明，T5模型在提示生成中的效率，准确率为98%，BLEU分数为0.2668，确保了JSON与描述性提示的映射。对于JSON创建，CoGen在生成指定组件类型的简单JSON输出方面的成功率高达100%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for efficient and reusable UI components in the design process, introducing CoGen, a system that generates these components in Figma using machine learning techniques. The method involves extracting data from the Figma API and employing Seq2Seq models along with fine-tuned T5 transformers to create atomic components like buttons and labels from structured JSON and natural language prompts. The findings indicate that the T5 model achieves a prompt generation accuracy of 98% and a BLEU score of 0.2668, while CoGen successfully generates simple JSON outputs for specified component types with a 100% success rate.</div>
<div class="mono" style="margin-top:8px">本研究通过引入CoGen系统，解决了设计过程中对高效和可重用UI组件的需求，该系统利用机器学习技术在Figma中生成这些组件。该方法结合了Figma API数据提取、Seq2Seq模型和微调的T5变换器，从结构化JSON和自然语言提示中创建原子组件，如按钮和标签。实验结果表明，T5模型在提示生成方面的准确率达到98%，BLEU分数为0.2668，而CoGen在为指定组件类型生成简单JSON输出时的成功率高达100%。</div>
</details>
</div>
<div class="card">
<div class="title">SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery</div>
<div class="meta-line">Authors: Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang</div>
<div class="meta-line">First: 2026-01-15T15:57:18+00:00 · Latest: 2026-01-15T15:57:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVII-3D：利用分米级3D定位和稀疏街景图像推进路边基础设施清单</div>
<div class="mono" style="margin-top:8px">数字双胞胎和精确资产清单的自动创建是智慧城市建设和设施生命周期管理中的关键任务。然而，由于鲁棒性有限、定位不准确以及缺乏细粒度状态理解，利用成本效益高的稀疏图像仍然具有挑战性。为了解决这些局限性，提出了SVII-3D，一个用于整体资产数字化的统一框架。首先，将经过LoRA微调的开放集检测与空间注意力匹配网络融合，以稳健地关联稀疏视图中的观察。其次，引入几何引导的精细化机制以解决结构错误，实现精确的分米级3D定位。第三，超越静态几何映射，结合多模态提示的视觉-语言模型代理被纳入，以自动诊断细粒度操作状态。实验表明，SVII-3D显著提高了识别准确性并最小化了定位误差。因此，该框架提供了一种可扩展、成本效益高的高保真基础设施数字化解决方案，有效弥合了稀疏感知与自动智能维护之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the automated creation of digital twins and precise asset inventories for smart city development, addressing challenges associated with sparse imagery. The authors propose SVII-3D, a unified framework that combines LoRA fine-tuned open-set detection with a spatial-attention matching network for robust observation association, and introduces a geometry-guided refinement mechanism for achieving decimeter-level 3D localization. Experimental results indicate that SVII-3D significantly improves identification accuracy and reduces localization errors, providing a scalable and cost-effective solution for high-fidelity infrastructure digitization and facilitating automated intelligent maintenance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强智能城市发展中数字双胞胎和精确资产清单的自动创建，解决稀疏图像相关的挑战。作者提出了SVII-3D，一个统一框架，将LoRA微调的开放集检测与空间注意力匹配网络结合，以实现稳健的观察关联，并引入几何引导的精炼机制，以实现分米级的3D定位。实验结果表明，SVII-3D显著提高了识别准确性并减少了定位误差，为高保真基础设施数字化提供了一种可扩展且具有成本效益的解决方案，促进了自动智能维护。</div>
</details>
</div>
<div class="card">
<div class="title">Curvature Tuning: Provable Training-free Model Steering From a Single Parameter</div>
<div class="meta-line">Authors: Leyang Hu, Matteo Gamba, Randall Balestriero</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-11T18:59:57+00:00 · Latest: 2026-01-15T15:36:28+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.07783v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.07783v5">PDF</a> · <a href="https://github.com/Leon-Leyang/curvature-tuning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model&#x27;s decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 8.59%/8.34% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>曲率调节：从单一参数可证明的无训练模型引导</div>
<div class="mono" style="margin-top:8px">模型和数据规模的扩大重塑了人工智能领域，使得微调预训练模型成为解决下游任务的标准范式。然而，主流的微调方法通常依赖于权重适应，缺乏可解释性，并依赖于启发式选择的超参数。本文从不同的角度出发，将重点从权重转向激活函数，通过样条算子的视角进行观察。我们提出了曲率调节（CT），这是一种可解释且有原则的引导方法，通过将单一超参数注入激活函数来调节模型的决策边界。我们证明了CT可以调整模型决策边界的曲率，并且更根本地将模型投影到光滑函数的空间，从而补充当前主要在特征适应方面发挥作用的微调方法。使该超参数可训练产生了一种新颖且高效的微调方法。实证结果表明，CT提高了泛化能力和鲁棒性。例如，它在12个数据集上分别提高了ResNet-50/152的下游准确率8.59%/8.34%（相较于线性探测）和4.64%/1.70%（相较于LoRA），并在RobustBench的$\ell_\infty$基准上提高了鲁棒准确率1032.64%/1494.46%。我们的代码可在https://github.com/Leon-Leyang/curvature-tuning获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional finetuning methods that rely on weight adaptation, which often lack interpretability and depend on heuristically chosen hyperparameters. The authors propose a novel approach called Curvature Tuning (CT), which modifies a model&#x27;s decision boundary by introducing a single hyperparameter into its activation functions, viewing them as spline operators. Experimental results demonstrate that CT significantly enhances generalization and robustness, achieving improvements in downstream accuracy of ResNet-50/152 by 8.59%/8.34% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, along with a remarkable increase in robust accuracy on the ℓ∞ benchmark from RobustBench by 1032.64%/1494.46%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决依赖权重调整的主流微调方法的局限性，这些方法通常缺乏可解释性，并依赖于启发式选择的超参数。作者提出了一种新方法，称为曲率调节（CT），该方法将重点从权重转向激活函数，通过引入一个单一的超参数来调节模型的决策边界。实验结果表明，CT显著提高了模型的泛化能力和鲁棒性，在12个数据集上，ResNet-50/152的下游准确率分别比线性探测提高了8.59%/8.34%，比LoRA提高了4.64%/1.70%，并且在RobustBench的$\ell_\infty$基准测试中，鲁棒准确率分别提高了1032.64%/1494.46%。</div>
</details>
</div>
<div class="card">
<div class="title">mergetune: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-15T15:15:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v1">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mergetune：视觉-语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉-语言模型（VLMs），如CLIP，往往会导致预训练知识的灾难性遗忘。之前的工作主要旨在减轻适应过程中的遗忘；然而，在此过程中，遗忘往往是不可避免的。我们引入了一种新范式，\emph{持续微调（CFT）}，旨在在零-shot模型已经适应后恢复预训练知识。我们提出了一种简单的、与模型无关的CFT策略（称为MERGETUNE），由线性模式连接（LMC）指导，可以在现有微调模型上事后应用，而无需架构更改。给定一个微调模型，我们继续微调其可训练参数（例如，软提示或线性头），以寻找一个具有两个低损失路径的持续模型，分别指向零-shot（例如，CLIP）和微调（例如，CoOp）解决方案。通过利用损失景观的几何特性，持续模型隐式地合并了这两种解决方案，恢复了在微调对应模型中丢失的预训练知识。一个挑战是，普通的LMC约束需要从预训练任务中重放数据。我们通过二阶代理近似这一约束，消除了对大规模数据重放的需求。实验表明，MERGETUNE在不增加参数的情况下，提高了CoOp在基础-新颖泛化上的调和平均值+5.6\%。我们首次在跨数据集迁移中显示出在DTD和EuroSAT上优于CLIP的表现。在稳健微调评估中，来自MERGETUNE的LMC合并模型以更低的推理成本超越了集成基线，与零-shot模型集成时实现了进一步的增益和最先进的结果。我们的代码可在\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of catastrophic forgetting in vision-language models (VLMs) like CLIP during fine-tuning. The authors introduce a novel approach called continued fine-tuning (CFT) through a method named MERGETUNE, which allows for the recovery of pretrained knowledge after a model has been adapted. Experimental results demonstrate that MERGETUNE enhances the harmonic mean of CoOp by 5.6% on base-novel generalization without increasing parameters, achieves superior performance compared to CLIP on DTD and EuroSAT datasets, and outperforms ensemble baselines in robust fine-tuning evaluations while maintaining lower inference costs.</div>
<div class="mono" style="margin-top:8px">本研究解决了视觉语言模型（VLMs）在微调过程中出现的灾难性遗忘问题，这通常导致预训练知识的丧失。作者提出了一种称为持续微调（CFT）的方法，通过一种名为MERGETUNE的模型无关策略，利用线性模式连通性（LMC）来恢复失去的知识，而无需更改架构。实验结果表明，MERGETUNE在基础-新颖泛化上提高了CoOp的调和平均值5.6%，在DTD和EuroSAT数据集上表现优于CLIP，并且在保持较低推理成本的同时超越了集成基线，与零-shot模型结合时实现了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous Diffusion for Mixed-Type Tabular Data</div>
<div class="meta-line">Authors: Markus Mueller, Kathrin Gruber, Dennis Fok</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2023-12-16T12:21:03+00:00 · Latest: 2026-01-15T15:08:41+00:00</div>
<div class="meta-line">Comments: published at ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.10431v6">Abs</a> · <a href="https://arxiv.org/pdf/2312.10431v6">PDF</a> · <a href="https://github.com/muellermarkus/cdtd">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes. To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at https://github.com/muellermarkus/cdtd.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合类型表格数据的连续扩散</div>
<div class="mono" style="margin-top:8px">基于评分的生成模型，通常称为扩散模型，已被证明在生成文本和图像数据方面成功。然而，它们在混合类型表格数据上的适应性仍然未被充分探索。在本研究中，我们提出了CDTD，一种用于混合类型表格数据的连续扩散模型。CDTD基于评分匹配和评分插值的新颖组合，以强制对连续和分类特征施加统一的连续噪声分布。我们明确承认通过依赖于特定模型的损失校准和初始化方案来统一不同数据类型的必要性。为了进一步解决混合类型表格数据中的高异质性，我们引入了自适应特征或类型特定的噪声调度。这些确保了特征之间的生成性能平衡，并优化了模型容量在特征和扩散时间之间的分配。我们的实验结果表明，CDTD始终优于最先进的基准模型，异常良好地捕捉特征相关性，并且噪声调度设计中的异质性提高了样本质量。复现代码可在https://github.com/muellermarkus/cdtd获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the generation of mixed-type tabular data using diffusion models, which have been successful in other domains but remain underexplored for this data type. The authors propose a Continuous Diffusion model for mixed-type Tabular Data (CDTD), employing a combination of score matching and score interpolation to create a unified noise distribution for continuous and categorical features. Experimental results demonstrate that CDTD outperforms existing benchmark models, effectively captures feature correlations, and improves sample quality through a tailored noise schedule that addresses the heterogeneity of mixed-type data.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于分数的生成模型，特别是扩散模型在生成混合类型表格数据时的局限性，这一领域尚未得到充分探索。作者提出了一种用于混合类型表格数据的连续扩散模型（CDTD），该模型结合了分数匹配和分数插值，以创建一个统一的连续噪声分布，适用于连续和分类特征。实验结果表明，CDTD在性能上优于现有基准模型，能够有效捕捉特征相关性，并通过针对混合类型数据的异质性设计的噪声调度提高样本质量。</div>
</details>
</div>
<div class="card">
<div class="title">Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer</div>
<div class="meta-line">Authors: Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan</div>
<div class="meta-line">First: 2024-04-10T08:54:00+00:00 · Latest: 2026-01-15T14:23:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.06835v2">Abs</a> · <a href="https://arxiv.org/pdf/2404.06835v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无调优自适应风格融合用于结构一致的文本驱动风格迁移</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们针对文本到图像（T2I）扩散模型中的文本驱动风格迁移任务。主要挑战是保持结构一致性，同时实现有效的风格迁移效果。过去在该领域的方法直接将内容和风格提示连接在一起进行提示级风格注入，导致不可避免的结构扭曲。在这项工作中，我们提出了一种新颖的解决方案，即自适应风格融合（ASI），以实现细粒度特征级风格融合。它由西阿摩斯交叉注意力（SiCA）组成，将单轨交叉注意力解耦为双轨结构，以获取独立的内容和风格特征，以及自适应内容-风格混合（AdaBlending）模块，以结构一致的方式耦合内容和风格信息。实验表明，我们的方法在结构保持和风格化效果方面表现出更好的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of text-driven style transfer in text-to-image diffusion models, focusing on maintaining consistent structure while achieving effective style transfer. The authors introduce a novel method called Adaptive Style Incorporation (ASI), which utilizes a Siamese Cross-Attention mechanism to separate content and style features and an Adaptive Content-Style Blending module to merge these features in a structure-consistent way. Experimental results demonstrate that ASI significantly improves both structure preservation and the quality of stylized effects compared to previous approaches that relied on prompt-level style injection.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善文本驱动的风格迁移，特别是在文本到图像的扩散模型中，同时保持结构的一致性。作者提出了一种新方法，称为自适应风格融合（ASI），该方法利用双胞胎交叉注意力（SiCA）机制将内容和风格特征分离，并通过自适应内容-风格混合（AdaBlending）模块以结构一致的方式整合这些特征。实验结果表明，与依赖于提示级风格注入的先前方法相比，该方法在结构保持和风格效果的有效性方面显著提高。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</div>
<div class="meta-line">Authors: Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</div>
<div class="meta-line">First: 2026-01-06T11:00:17+00:00 · Latest: 2026-01-15T14:19:47+00:00</div>
<div class="meta-line">Comments: Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02918v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02918v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ethanliang99.github.io/ZOOMIQA-Projectpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Zoom-IQA：具有可靠区域感知推理的图像质量评估</div>
<div class="mono" style="margin-top:8px">图像质量评估（IQA）是计算机视觉中的一个长期问题。以往的方法通常侧重于预测数值评分而没有解释，或提供缺乏精确评分的低级描述。最近基于推理的视觉语言模型（VLMs）在IQA中显示出强大的潜力，通过共同生成质量描述和评分。然而，现有的基于VLM的IQA方法往往由于整合视觉和文本线索的能力有限而遭受不可靠推理的困扰。在本研究中，我们介绍了Zoom-IQA，一个基于VLM的IQA模型，明确模拟关键的认知行为：不确定性意识、区域推理和迭代精炼。具体而言，我们提出了一个两阶段的训练流程：1）在我们的基础推理IQA（GR-IQA）数据集上进行监督微调（SFT），以教会模型将其评估与关键区域相结合；2）通过我们的KL覆盖正则化器进行动态策略探索的强化学习（RL），以防止推理和评分多样性崩溃，并采用渐进重采样策略以减轻注释偏差。大量实验表明，Zoom-IQA在鲁棒性、可解释性和泛化能力上都有所提升。对下游任务（如图像修复）的应用进一步证明了Zoom-IQA的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional Image Quality Assessment (IQA) methods, which often lack reliable reasoning and precise scoring. The authors introduce Zoom-IQA, a vision language model (VLM)-based approach that incorporates cognitive behaviors such as uncertainty awareness and region reasoning through a two-stage training process. The first stage involves supervised fine-tuning on the Grounded-Rationale-IQA dataset, while the second stage employs reinforcement learning with a KL-Coverage regularizer to enhance reasoning stability. Experimental results indicate that Zoom-IQA demonstrates improved robustness, explainability, and generalization, and its application to tasks like image restoration further validates its effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统图像质量评估（IQA）方法的局限性，这些方法往往缺乏可靠的推理和精确的评分。作者提出了Zoom-IQA，这是一种基于视觉语言模型（VLM）的方法，通过两阶段训练过程结合了不确定性意识和区域推理等认知行为。第一阶段在Grounded-Rationale-IQA数据集上进行监督微调，第二阶段则采用强化学习和KL覆盖正则化器来增强推理的稳定性。实验结果表明，Zoom-IQA在鲁棒性、可解释性和泛化能力方面有所提升，其在图像修复等任务中的应用进一步验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SPATIALGEN: Layout-guided 3D Indoor Scene Generation</div>
<div class="meta-line">Authors: Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan</div>
<div class="meta-line">First: 2025-09-18T14:12:32+00:00 · Latest: 2026-01-15T13:38:16+00:00</div>
<div class="meta-line">Comments: 3D scene generation; diffusion model; Scene reconstruction and understanding</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14981v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.14981v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,431 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPATIALGEN：基于布局的3D室内场景生成</div>
<div class="mono" style="margin-top:8px">创建高保真度的室内环境3D模型对于设计、虚拟现实和机器人等应用至关重要。然而，手动3D建模仍然耗时且劳动密集。尽管最近生成性人工智能的进展使得自动场景合成成为可能，但现有方法在平衡视觉质量、多样性、语义一致性和用户控制方面常常面临挑战。一个主要瓶颈是缺乏针对该任务的大规模高质量数据集。为了解决这一问题，我们引入了一个全面的合成数据集，包含12,328个结构化注释场景、57,431个房间和470万张照片级真实感的2D渲染图。利用该数据集，我们提出了SpatialGen，一种新颖的多视角多模态扩散模型，能够生成真实且语义一致的3D室内场景。给定一个3D布局和一张参考图像（源自文本提示），我们的模型从任意视角合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性。在我们的实验中，SpatialGen始终生成优于以前方法的结果。我们将开源我们的数据和模型，以赋能社区并推动室内场景理解和生成领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to streamline the creation of high-fidelity 3D models of indoor environments, which is crucial for various applications but is hindered by the labor-intensive nature of manual modeling. To overcome this challenge, the authors developed a comprehensive synthetic dataset comprising 12,328 structured annotated scenes, 57,431 rooms, and 4.7 million photorealistic 2D renderings. Utilizing this dataset, they introduced SpatialGen, a multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes from a given 3D layout and reference image. Experimental results demonstrate that SpatialGen outperforms existing methods in terms of visual quality and consistency across modalities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高创建高保真室内环境3D模型的效率，这对于设计、虚拟现实和机器人等应用至关重要。作者介绍了一个包含12,328个结构化注释场景和470万张照片级真实感2D渲染图的综合合成数据集，并利用该数据集开发了SpatialGen，这是一种多视角多模态扩散模型，可以根据给定的布局和参考图像生成逼真且语义一致的3D室内场景。实验结果表明，SpatialGen在生成高质量3D场景方面优于以前的方法，同时在不同模态之间保持空间一致性。</div>
</details>
</div>
<div class="card">
<div class="title">RS2-SAM2: Customized SAM2 for Referring Remote Sensing Image Segmentation</div>
<div class="meta-line">Authors: Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-03-10T12:48:29+00:00 · Latest: 2026-01-15T13:25:48+00:00</div>
<div class="meta-line">Comments: AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.07266v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.07266v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text. To address these issues, we propose \textbf{RS2-SAM2}, a novel framework that adapts SAM2 to RRSIS by aligning the adapted RS features and textual features while providing pseudo-mask-based dense prompts. Specifically, we employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. A bidirectional hierarchical fusion module is introduced to adapt SAM2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the model&#x27;s interpretation of text-described RS scenes. To provide precise target cues for SAM2, we design a mask prompt generator, which takes the visual embeddings and class tokens as input and produces a pseudo-mask as the dense prompt of SAM2. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM2 achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RS2-SAM2：用于参考遥感图像分割的定制SAM2</div>
<div class="mono" style="margin-top:8px">参考遥感图像分割（RRSIS）旨在根据文本描述对遥感（RS）图像中的目标对象进行分割。尽管Segment Anything Model 2（SAM2）在各种分割任务中表现出色，但其在RRSIS中的应用面临多重挑战，包括理解文本描述的RS场景和从文本生成有效提示。为了解决这些问题，我们提出了\textbf{RS2-SAM2}，这是一个新颖的框架，通过对齐适应的RS特征和文本特征，同时提供基于伪掩码的密集提示，将SAM2适应于RRSIS。具体而言，我们采用联合编码器共同编码视觉和文本输入，生成对齐的视觉和文本嵌入以及多模态类别标记。引入了双向层次融合模块，以适应SAM2于RS场景，并将适应的视觉特征与视觉增强的文本嵌入对齐，从而提高模型对文本描述的RS场景的理解。为了为SAM2提供精确的目标线索，我们设计了一个掩码提示生成器，该生成器以视觉嵌入和类别标记为输入，生成伪掩码作为SAM2的密集提示。在多个RRSIS基准上的实验结果表明，RS2-SAM2达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the performance of Referring Remote Sensing Image Segmentation (RRSIS), which involves segmenting objects in remote sensing images based on textual descriptions. The authors propose RS2-SAM2, a novel framework that adapts the Segment Anything Model 2 (SAM2) to better handle the challenges of RRSIS by aligning remote sensing features with textual features and generating effective prompts. The method employs a union encoder for joint encoding of visual and textual inputs, along with a bidirectional hierarchical fusion module to improve the model&#x27;s understanding of text-described scenes. Experimental results indicate that RS2-SAM2 achieves state-of-the-art performance on several RRSIS benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善基于文本描述的遥感图像中目标对象的分割，即所谓的参考遥感图像分割（RRSIS）。作者提出了一种名为RS2-SAM2的新框架，该框架将Segment Anything Model 2（SAM2）进行调整，以更好地处理RRSIS的挑战，通过对齐遥感特征与文本描述并生成有效的提示。关键实验结果表明，RS2-SAM2在多个RRSIS基准测试中实现了最先进的性能，证明了其在解释文本描述的遥感场景方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models</div>
<div class="meta-line">Authors: Leah Bar, Liron Mor Yosef, Shai Zucker, Neta Shoham, Inbar Seroussi, Nir Sochen</div>
<div class="meta-line">First: 2025-10-01T08:50:30+00:00 · Latest: 2026-01-15T13:08:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00666v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00666v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most models of generative AI for images assume that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold&#x27;s coordinate space is considered uninteresting and is predefined or considered uniform. In this study, we address the problem of Blind Image Denoising (BID), and to some extent, the problem of generating images from noise by unifying geometric and probabilistic perspectives. We introduce a novel framework that improves upon existing probabilistic approaches by incorporating geometric assumptions that enable the effective use of kernel-based probabilistic methods. Furthermore, the proposed framework extends prior geometric approaches by combining explicit and implicit manifold descriptions through the introduction of a distance function. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images&#x27;&#x27;. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成性人工智能与流形-概率投影模型的几何统一</div>
<div class="mono" style="margin-top:8px">大多数生成性人工智能图像模型假设图像本质上是嵌入在高维空间中的低维对象。此外，通常隐含假设主题图像数据集形成光滑或分段光滑的流形。常见的方法忽视几何结构，仅专注于概率方法，通过核方法等通用逼近技术来近似概率分布。在某些生成模型中，数据的低维特性通过引入低维潜在空间表现出来。然而，潜在空间或流形坐标空间中的概率分布被认为不重要，通常是预定义或均匀的。在本研究中，我们通过统一几何和概率视角来解决盲图像去噪（BID）问题，并在一定程度上解决从噪声生成图像的问题。我们引入了一种新框架，通过结合几何假设来改进现有的概率方法，从而有效利用基于核的概率方法。此外，所提出的框架通过引入距离函数，结合显式和隐式流形描述，扩展了先前的几何方法。最终框架通过将扩散模型解释为对“好图像”流形的投影机制，揭示了其本质。这一解释导致构建了一种新的确定性模型，即流形-概率投影模型（MPPM），该模型在表示（像素）空间和潜在空间中均可操作。我们证明潜在MPPM（LMPPM）在各种数据集上优于潜在扩散模型（LDM），在图像恢复和生成方面取得了更好的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing generative AI models for images, which often neglect the geometric structure of data and rely solely on probabilistic methods. The authors propose a novel framework that integrates geometric assumptions with probabilistic approaches, specifically targeting the problem of Blind Image Denoising (BID) and image generation from noise. Experimental results show that the Latent Manifold-Probabilistic Projection Model (LMPPM) significantly outperforms the Latent Diffusion Model (LDM) in image restoration and generation across various datasets, demonstrating the effectiveness of the proposed geometric and probabilistic unification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有图像生成AI模型的局限性，这些模型往往忽视数据的几何结构，过于依赖概率方法。作者提出了一种新颖的框架，将几何假设与基于核的概率技术相结合，特别针对盲图像去噪（BID）和从噪声中生成图像的问题。实验结果表明，新开发的流形-概率投影模型（MPPM），尤其是其潜在变体（LMPPM），在各种数据集的图像恢复和生成任务中显著优于潜在扩散模型（LDM）。</div>
</details>
</div>
<div class="card">
<div class="title">SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping</div>
<div class="meta-line">Authors: Thomas Boudras, Martin Schwartz, Rasmus Fensholt, Martin Brandt, Ibrahim Fayad, Jean-Pierre Wigneron, Gabriel Belouze, Fajwel Fogel, Philippe Ciais</div>
<div class="meta-line">First: 2025-12-19T23:23:14+00:00 · Latest: 2026-01-15T11:19:00+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18128v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.18128v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors&#x27; native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA-H：超越本地哨兵空间限制的高分辨率冠层高度映射</div>
<div class="mono" style="margin-top:8px">高分辨率的冠层高度映射对森林管理和生物多样性监测至关重要。尽管最近的研究引入了使用卫星影像预测高度图的深度学习方法，但这些方法通常面临数据可获取性与空间分辨率之间的权衡。为克服这些限制，我们提出了SERA-H，一个结合超分辨率模块（EDSR）和时间注意力编码（UTAE）的端到端模型。在高密度LiDAR数据（ALS）的监督下训练，我们的模型从免费提供的哨兵-1和哨兵-2（10米）时间序列数据生成2.5米分辨率的高度图。在法国的一个开源基准数据集上评估，SERA-H的平均绝对误差为2.6米，决定系数为0.82，不仅优于标准的哨兵-1/2基线，还实现了与依赖商业超高分辨率影像（SPOT-6/7、PlanetScope、Maxar）的方法相当或更好的性能。这些结果表明，将高分辨率监督与时间序列中嵌入的时空信息相结合，可以重建超出输入传感器本地分辨率的细节。SERA-H为以高重访频率自由映射森林开辟了可能性，达到与昂贵商业影像相当的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for high-resolution canopy height mapping to support forest management and biodiversity monitoring, while addressing the limitations of existing deep learning methods that struggle with data accessibility and spatial resolution. The authors introduce SERA-H, an end-to-end model that integrates a super-resolution module and temporal attention encoding, trained using high-density LiDAR data to produce 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 time series data. Experimental results demonstrate that SERA-H achieves a mean absolute error of 2.6 m and a coefficient of determination of 0.82, outperforming standard baselines and matching or exceeding the performance of methods that utilize expensive commercial high-resolution imagery.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于高分辨率树冠高度映射的需求，以支持森林管理和生物多样性监测，解决现有深度学习方法在数据可获取性和空间分辨率方面的局限。作者提出了SERA-H，这是一种端到端模型，结合了超分辨率模块和时间注意编码，利用高密度LiDAR数据进行训练，从自由可用的Sentinel-1和Sentinel-2时间序列数据中生成2.5米分辨率的高度图。实验结果表明，SERA-H的平均绝对误差为2.6米，决定系数为0.82，超越了标准基线，并与使用昂贵商业影像的方法相当或更优，从而以更低的成本和更高的频率实现详细的森林映射。</div>
</details>
</div>
<div class="card">
<div class="title">Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images</div>
<div class="meta-line">Authors: Aditya Kumar, Tom Blanchard, Adam Dziedzic, Franziska Boenisch</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-07T16:39:39+00:00 · Latest: 2026-01-15T10:58:27+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 (AI Alignment Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.05066v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.05066v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art Diffusion Models (DMs) produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, SDXL, Flux, DeepFloyd IF) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we introduce a novel fine-tuning strategy that targets only the text-generation layers in DMs. Therefore, we construct a safety fine-tuning dataset by pairing each NSFW prompt with two images: one with the NSFW term, and another where that term is replaced with a carefully crafted benign alternative while leaving the image unchanged otherwise. By training on this dataset, the model learns to avoid generating harmful text while preserving benign content and overall image quality. Finally, to advance research in the area, we release ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. It includes our curated fine-tuning dataset, a set of harmful prompts, new evaluation metrics, and a pipeline that assesses both NSFW-ness and text and image quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models, thereby contributing to their safe deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美丽的图像，毒性的文字：理解和应对生成图像中的冒犯性文本</div>
<div class="mono" style="margin-top:8px">最先进的扩散模型（DMs）生成高度逼真的图像。尽管之前的工作成功减轻了视觉领域中的不适合工作（NSFW）内容，但我们识别出一种新威胁：在图像中嵌入生成的NSFW文本。这包括冒犯性语言，如侮辱、种族歧视和性露骨的术语，给用户带来重大风险。我们展示了所有最先进的DM（例如，SD3、SDXL、Flux、DeepFloyd IF）都容易受到此问题的影响。通过广泛的实验，我们证明了现有的减轻技术在视觉内容上有效，但未能防止有害文本生成，同时显著降低了良性文本生成。作为应对这一威胁的初步步骤，我们引入了一种新颖的微调策略，仅针对DM中的文本生成层。因此，我们通过将每个NSFW提示与两幅图像配对来构建安全微调数据集：一幅包含NSFW术语，另一幅将该术语替换为精心制作的良性替代品，同时保持图像其他部分不变。通过在此数据集上训练，模型学习避免生成有害文本，同时保留良性内容和整体图像质量。最后，为了推动该领域的研究，我们发布了ToxicBench，这是一个用于评估图像中NSFW文本生成的开源基准。它包括我们策划的微调数据集、一组有害提示、新的评估指标，以及一个评估NSFW程度和文本与图像质量的管道。我们的基准旨在指导未来在文本到图像模型中减轻NSFW文本生成的努力，从而促进其安全部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the increasing use of state-of-the-art Diffusion Models (DMs) that generate highly realistic images, which inadvertently produce offensive text embedded within these images, posing risks to users. The authors conducted extensive experiments to demonstrate that existing mitigation techniques for visual content do not effectively prevent harmful text generation and often degrade benign text generation. To address this issue, they introduced a novel fine-tuning strategy that focuses on the text-generation layers of DMs, utilizing a safety fine-tuning dataset that pairs NSFW prompts with images containing both harmful and benign text. This approach allows the model to learn to avoid generating harmful text while maintaining the quality of benign content and images. Additionally, they released ToxicBench, an open-source benchmark for evaluating NSFW text generation in images, which includes their fine-tuning dataset and new evaluation metrics to support future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由最先进的扩散模型（DMs）生成的图像中出现的冒犯性文本，这些文本尽管在减轻不安全视觉内容方面取得了进展，但仍对用户构成风险。作者进行了广泛的实验，发现现有技术无法防止有害文本的生成，同时降低了良性文本的质量，突显了当前方法的不足。为了解决这个问题，他们提出了一种新的微调策略，专注于DMs的文本生成层，使用安全微调数据集将NSFW提示与良性替代品配对，使模型能够学习避免生成有害文本，同时保持图像质量。此外，他们发布了ToxicBench，这是一个开源基准，用于评估图像中的NSFW文本生成，包含他们的数据集、有害提示、新的评估指标以及评估NSFW性和内容质量的管道。</div>
</details>
</div>
<div class="card">
<div class="title">GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</div>
<div class="meta-line">Authors: Yeonjoon Jung, Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T06:48:20+00:00 · Latest: 2026-01-15T10:43:27+00:00</div>
<div class="meta-line">Comments: 39th Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20355v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20355v2">PDF</a> · <a href="https://github.com/SqueezeBits/GraLoRA.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA&#x27;s structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA&#x27;s limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraLoRA：用于参数高效微调的粒状低秩适应</div>
<div class="mono" style="margin-top:8px">低秩适应（LoRA）是一种流行的生成模型参数高效微调（PEFT）方法，以其简单性和有效性而受到重视。尽管最近有所改进，LoRA仍然存在一个根本性限制：当瓶颈扩大时会过拟合。它在秩32-64时表现最佳，但在更高秩时准确性停滞或下降，仍然无法达到完全微调（FFT）的性能。我们确定根本原因是LoRA的结构瓶颈，它将梯度纠缠引入无关的输入通道并扭曲梯度传播。为了解决这个问题，我们引入了一种新结构，粒状低秩适应（GraLoRA），将权重矩阵划分为子块，每个子块都有自己的低秩适配器。GraLoRA以微不足道的计算或存储成本克服了LoRA的局限性，有效地增加了表示能力，更接近FFT行为。在代码生成和常识推理基准上的实验表明，GraLoRA始终优于LoRA和其他基线，在HumanEval+上实现了高达+8.5%的绝对增益。这些改进在模型大小和秩设置中均有效，使GraLoRA成为PEFT的可扩展和稳健的解决方案。代码、数据和脚本可在https://github.com/SqueezeBits/GraLoRA.git获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the overfitting issue in Low-Rank Adaptation (LoRA) during parameter-efficient fine-tuning of generative models, particularly when the bottleneck is widened. The authors propose a new method called Granular Low-Rank Adaptation (GraLoRA), which partitions weight matrices into sub-blocks with individual low-rank adapters to mitigate the structural bottleneck and improve gradient propagation. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other baseline methods, achieving up to an 8.5% absolute gain in Pass@1 on HumanEval+, indicating its effectiveness across various model sizes and rank settings for parameter-efficient fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决低秩适应（LoRA）在生成模型的参数高效微调中，当秩超过最佳水平时出现的过拟合问题。作者提出了一种新方法，称为粒状低秩适应（GraLoRA），该方法将权重矩阵划分为具有独立低秩适配器的子块，以减轻梯度纠缠并改善梯度传播。实验结果表明，GraLoRA显著优于LoRA和其他基线方法，在HumanEval+上实现了高达8.5%的绝对增益，表明其在各种模型规模和秩设置下的参数高效微调中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation</div>
<div class="meta-line">Authors: Dong-Yu Chen, Yixin Guo, Shuojin Yang, Tai-Jiang Mu, Shi-Min Hu</div>
<div class="meta-line">First: 2026-01-15T09:26:45+00:00 · Latest: 2026-01-15T09:26:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10214v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10214v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越修复：释放3D理解以实现精确的相机控制视频生成</div>
<div class="mono" style="margin-top:8px">相机控制在条件视频生成中得到了广泛研究；然而，在忠实保留视频内容的同时精确改变相机轨迹仍然是一项具有挑战性的任务。实现精确相机控制的主流方法是根据目标轨迹扭曲3D表示。然而，这些方法未能充分利用视频扩散模型（VDMs）的3D先验，常常陷入修复陷阱，导致主题不一致和生成质量下降。为了解决这个问题，我们提出了DepthDirector，一个具有精确相机可控性的视频重新渲染框架。通过利用来自显式3D表示的深度视频作为相机控制指导，我们的方法可以在新相机轨迹下忠实再现输入视频的动态场景。具体而言，我们设计了一种视图-内容双流条件机制，将源视频和在目标视点下渲染的扭曲深度序列注入预训练的视频生成模型中。这种几何指导信号使VDMs能够理解相机运动并利用其3D理解能力，从而促进精确的相机控制和一致的内容生成。接下来，我们引入了一种基于轻量级LoRA的视频扩散适配器来训练我们的框架，充分保留VDMs的知识先验。此外，我们使用虚幻引擎5构建了一个名为MultiCam-WarpData的大规模多相机同步数据集，包含1000个动态场景的8K视频。大量实验表明，DepthDirector在相机可控性和视觉质量方面优于现有方法。我们的代码和数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve camera control in video generation while maintaining content fidelity, as existing methods often struggle with subject inconsistency and quality degradation. The authors propose DepthDirector, a video re-rendering framework that utilizes depth video from explicit 3D representations to guide camera control, employing a View-Content Dual-Stream Condition mechanism to integrate source videos and warped depth sequences into a pretrained video generation model. Experimental results demonstrate that DepthDirector significantly enhances both camera controllability and visual quality compared to current methods, supported by a large-scale dataset called MultiCam-WarpData containing 8K videos across 1K dynamic scenes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视频生成中的相机控制，同时保持内容的真实性，因为现有方法往往在主题一致性和质量下降方面存在困难。作者提出了DepthDirector，这是一种视频重渲染框架，利用来自显式3D表示的深度视频来指导相机控制，采用视图-内容双流条件机制，将源视频和在目标视角下变形的深度序列整合到预训练的视频生成模型中。实验结果表明，与当前技术相比，DepthDirector在相机可控性和视觉质量方面显著提升，并通过使用虚幻引擎5创建的大规模数据集MultiCam-WarpData提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">From Physical Degradation Models to Task-Aware All-in-One Image Restoration</div>
<div class="meta-line">Authors: Hu Gao, Xiaoning Lei, Xichen Xu, Xingjian Wang, Lizhuang Ma</div>
<div class="meta-line">First: 2026-01-15T08:47:10+00:00 · Latest: 2026-01-15T08:47:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从物理退化模型到任务感知的一体化图像恢复</div>
<div class="mono" style="margin-top:8px">一体化图像恢复旨在通过单一训练模型自适应处理多种恢复任务。尽管现有方法通过引入提示信息或利用大型模型取得了令人满意的结果，但增加的学习模块提高了系统复杂性，阻碍了实时应用。在本文中，我们采用物理退化建模的视角，预测任务感知的逆退化算子，以实现高效的一体化图像恢复。该框架分为两个阶段。在第一阶段，预测的逆算子生成初始恢复图像，并提供一个不确定性感知图，突出难以重建的区域，确保恢复的可靠性。在第二阶段，恢复在该不确定性感知图的指导下进一步精细化。两个阶段均使用相同的逆算子预测网络，在算子预测后引入任务感知参数以适应不同的退化任务。此外，通过加速逆算子的卷积，所提出的方法实现了高效的一体化图像恢复。最终紧密集成的架构称为OPIR，通过实验进行了广泛验证，展示了优越的一体化恢复性能，同时在任务对齐恢复方面保持高度竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve all-in-one image restoration by addressing the complexity and real-time applicability issues of existing methods. The authors propose a two-stage framework that utilizes a physical degradation modeling approach to predict a task-aware inverse degradation operator, which is employed to generate an initial restored image and an uncertainty perception map in the first stage, followed by refinement in the second stage. Experimental results show that the proposed method, OPIR, achieves superior performance in all-in-one image restoration while maintaining competitiveness in task-specific restoration, demonstrating its efficiency and reliability in handling multiple restoration tasks with a single model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高一体化图像修复方法的效率和有效性，这些方法通常由于额外的学习模块而面临复杂性增加和实时应用能力下降的问题。作者提出了一种两阶段框架，利用物理降解建模方法预测任务感知逆降解算子，在第一阶段生成初步修复图像和不确定性感知图，在第二阶段进行精细化修复。实验结果表明，所提出的方法OPIR在一体化图像修复中表现优越，同时在任务特定修复场景中保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Debiased Orthogonal Boundary-Driven Efficient Noise Mitigation</div>
<div class="meta-line">Authors: Hao Li, Jiayang Gu, Jingkuan Song, An Zhang, Lianli Gao</div>
<div class="meta-line">First: 2024-10-02T18:42:56+00:00 · Latest: 2026-01-15T07:49:21+00:00</div>
<div class="meta-line">Comments: 20 pages, 4 figures, 11 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.01944v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.01944v2">PDF</a> · <a href="https://github.com/leolee99/OSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mitigating the detrimental effects of noisy labels on the training process has become increasingly critical, as obtaining entirely clean or human-annotated samples for large-scale pre-training tasks is often impractical. Nonetheless, existing noise mitigation methods often encounter limitations in practical applications due to their task-specific design, model dependency, and significant computational overhead. In this work, we exploit the properties of high-dimensional orthogonality to identify a robust and effective boundary in cone space for separating clean and noisy samples. Building on this, we propose One-Step Anti-noise (OSA), a model-agnostic noisy label mitigation paradigm that employs an estimator model and a scoring function to assess the noise level of input pairs through just one-step inference. We empirically validate the superiority of OSA, demonstrating its enhanced training robustness, improved task transferability, streamlined deployment, and reduced computational overhead across diverse benchmarks, models, and tasks. Our code is released at https://github.com/leolee99/OSA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>去偏正交边界驱动的高效噪声缓解</div>
<div class="mono" style="margin-top:8px">缓解噪声标签对训练过程的有害影响变得越来越重要，因为在大规模预训练任务中获得完全干净或人工标注的样本往往不切实际。然而，现有的噪声缓解方法在实际应用中常常受到任务特定设计、模型依赖性和显著计算开销的限制。在本研究中，我们利用高维正交性的特性，在锥空间中识别出一个稳健有效的边界，以分离干净样本和噪声样本。在此基础上，我们提出了一种模型无关的噪声标签缓解范式——一步反噪声（OSA），该范式通过仅一步推理，利用估计模型和评分函数评估输入对的噪声水平。我们通过实验证明了OSA的优越性，展示了其增强的训练鲁棒性、改善的任务可迁移性、简化的部署和在不同基准、模型和任务中减少的计算开销。我们的代码已发布在 https://github.com/leolee99/OSA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges posed by noisy labels in training machine learning models, as obtaining clean samples is often impractical. The authors propose a novel method called One-Step Anti-noise (OSA), which utilizes high-dimensional orthogonality to establish a boundary in cone space for distinguishing between clean and noisy samples, allowing for a model-agnostic approach to noise mitigation. Experimental results demonstrate that OSA significantly enhances training robustness, improves task transferability, simplifies deployment, and reduces computational overhead across various benchmarks and models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决训练过程中噪声标签带来的挑战，因为获取完全干净的样本往往是不切实际的。作者提出了一种名为One-Step Anti-noise (OSA) 的模型无关方法，该方法利用高维正交性在锥空间中创建边界，以区分干净样本和噪声样本。实验结果表明，OSA显著增强了训练的鲁棒性，提高了任务的可迁移性，简化了部署，并减少了在各种基准和模型中的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL</div>
<div class="meta-line">Authors: Wenwen Liao, Jianbo Yu, Yuansong Wang, Shifu Yan, Xiaofeng Yang</div>
<div class="meta-line">First: 2026-01-15T06:53:59+00:00 · Latest: 2026-01-15T06:53:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10117v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单一提示：VICL的协同融合与排列</div>
<div class="mono" style="margin-top:8px">视觉上下文学习（VICL）使得修复模型能够仅通过少量提示快速适应新的视觉任务。然而，现有方法存在两个关键问题：（1）仅选择最相似的提示会丢弃来自其他高质量提示的互补线索；（2）未能利用不同提示排列所隐含的结构信息。我们提出了一种端到端的VICL框架以克服这些限制。首先，自适应融合模块聚合来自多个提示的关键模式和注释，以形成更精确的上下文提示。其次，我们引入特定排列的轻量级MLP，以将布局先验与核心模型解耦，同时对整体模型的影响最小。此外，双向微调机制交换查询和提示的角色，鼓励模型从融合上下文重建原始提示，从而增强融合模块与修复模型之间的协作。在前景分割、单对象检测和图像着色的实验中，我们的方法展示了优越的结果和强大的跨任务泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing Vision In-Context Learning (VICL) methods, which either overlook complementary information from multiple prompts or fail to utilize structured prompt arrangements. The authors propose an end-to-end VICL framework that includes an adaptive Fusion Module to combine critical patterns from various prompts and arrangement-specific lightweight MLPs to manage layout priors without significantly impacting the core model. Experimental results on tasks such as foreground segmentation, single-object detection, and image colorization show that the proposed method achieves superior performance and strong generalization across different tasks.</div>
<div class="mono" style="margin-top:8px">该研究解决了现有视觉上下文学习（VICL）方法的局限性，这些方法要么丢弃来自多个提示的有价值信息，要么未能利用不同提示排列的结构信息。作者提出了一种端到端的VICL框架，包括一个自适应融合模块，用于结合来自多个提示的关键模式，以及特定排列的轻量级MLP，以在不显著影响核心模型的情况下管理布局先验。实验结果表明，该方法在前景分割、单对象检测和图像着色等任务上表现优越，并在不同任务之间具有强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning Diffusion Models via Intermediate Distribution Shaping</div>
<div class="meta-line">Authors: Gautham Govind Anil, Shaan Ul Haque, Nithish Kannen, Dheeraj Nagaraj, Sanjay Shakkottai, Karthikeyan Shanmugam</div>
<div class="meta-line">First: 2025-10-03T03:18:47+00:00 · Latest: 2026-01-15T06:46:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02692v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02692v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs KL regularized reward maximization with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过中间分布塑形微调扩散模型</div>
<div class="mono" style="margin-top:8px">扩散模型广泛用于各领域的生成任务。虽然预训练的扩散模型有效捕捉训练数据分布，但通常希望使用奖励函数来塑形这些分布，以便与下游应用对齐。策略梯度方法，如近端策略优化（PPO），在自回归生成的背景下被广泛使用。然而，这些方法所需的边际似然对于扩散模型是不可处理的，导致了替代提案和放松。在此背景下，我们将基于拒绝采样的微调变体统一为GRAFT，并表明这隐式执行了带有重塑奖励的KL正则化奖励最大化。然后，我们引入P-GRAFT以在中间噪声水平塑形分布，并通过实验证明这可以导致更有效的微调。我们通过偏差-方差权衡从数学上解释这一点。基于此，我们提出逆噪声校正，以在不利用显式奖励的情况下改善流模型。我们在文本到图像（T2I）生成、布局生成、分子生成和无条件图像生成上对我们的方法进行了实证评估。值得注意的是，我们的框架应用于Stable Diffusion 2，在流行的T2I基准测试中，在VQAScore方面优于策略梯度方法，并显示出相对于基础模型的$8.81\%$的相对改善。对于无条件图像生成，逆噪声校正在较低的FLOPs/图像下改善了生成图像的FID。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the performance of pre-trained diffusion models in generative tasks by shaping their output distributions to align better with specific applications. The authors propose a method called P-GRAFT, which unifies various approaches to fine-tuning diffusion models through intermediate distribution shaping and employs a bias-variance tradeoff to explain its effectiveness. Experimental results demonstrate that P-GRAFT significantly improves fine-tuning outcomes, achieving an 8.81% relative improvement in VQAScore on text-to-image generation benchmarks compared to existing policy gradient methods and enhancing the FID of generated images in unconditional generation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使用奖励函数来调整预训练扩散模型的输出分布，以便与特定的下游应用对齐。作者提出了一种名为P-GRAFT的方法，该方法通过在中间噪声水平上执行KL正则化奖励最大化来统一多种扩散模型微调的方法。实验结果表明，该方法在文本到图像生成等任务中显著提高了性能，在流行基准上相较于基础模型实现了8.81%的相对提升，同时通过逆噪声校正改善了无条件图像生成的质量。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Visual In-Context Learning by Multi-Faceted Fusion</div>
<div class="meta-line">Authors: Wenwen Liao, Jianbo Yu, Yuansong Wang, Qingchao Jiang, Xiaofeng Yang</div>
<div class="meta-line">First: 2026-01-15T06:25:09+00:00 · Latest: 2026-01-15T06:25:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant &quot;retrieve-then-prompt&quot; approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model&#x27;s reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多面融合增强视觉上下文学习</div>
<div class="mono" style="margin-top:8px">视觉上下文学习（VICL）已成为一种强大的范式，使模型能够通过学习上下文示例来执行新颖的视觉任务。主流的“检索-再提示”方法通常依赖于选择单一最佳视觉提示，这种做法往往会丢弃其他合适候选者中的有价值上下文信息。尽管最近的工作探索了将前K个提示融合为单一增强表示，但这仍然只是将多个丰富信号压缩为一个，限制了模型的推理能力。我们认为，需要一种更具多面性和协作性的融合，以释放这些多样化上下文的全部潜力。为了解决这一局限性，我们引入了一种新颖的框架，超越单一提示融合，朝向多组合协作融合。我们的方法生成三个上下文表示分支，每个分支通过整合来自不同高质量提示的组合信息形成。这些互补的指导信号随后被输入到提出的MULTI-VQGAN架构中，该架构旨在共同解释和利用来自多个来源的协作信息。在包括前景分割、单对象检测和图像着色等多样任务上的广泛实验，突显了其强大的跨任务泛化能力、有效的上下文融合能力，以及比现有方法产生更稳健和准确预测的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of the dominant &quot;retrieve-then-prompt&quot; approach in Visual In-Context Learning (VICL), which often overlooks valuable contextual information by focusing on a single visual prompt. To address this, the authors propose a novel framework that implements multi-combination collaborative fusion, generating three contextual representation branches from different combinations of top-quality prompts instead of collapsing them into one. Experimental results demonstrate that this method enhances cross-task generalization and produces more robust and accurate predictions across various tasks, including foreground segmentation, single-object detection, and image colorization, compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于传统的“检索-再提示”方法在视觉上下文学习（VICL）中的局限性，该方法通常通过关注单一最佳视觉提示而忽视了有价值的上下文信息。为了解决这个问题，作者提出了一种新框架，实施多组合协作融合，从不同的高质量提示组合中生成三个上下文表示分支，而不是将它们合并为一个。实验结果表明，该方法增强了跨任务的泛化能力，改善了上下文融合，并在前景分割、单对象检测和图像着色等任务中相比现有技术产生了更强健和准确的预测。</div>
</details>
</div>
<div class="card">
<div class="title">Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text</div>
<div class="meta-line">Authors: Piyush Singh Pasi</div>
<div class="meta-line">First: 2026-01-15T05:56:37+00:00 · Latest: 2026-01-15T05:56:37+00:00</div>
<div class="meta-line">Comments: EACL 2026 Findings accepted. Initial Draft of Camera-ready</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10096v1">PDF</a> · <a href="https://github.com/m2m-codebase/M2M">Code1</a> · <a href="https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k">Code2</a> · <a href="https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多语言到多模态 (M2M)：通过单语文本解锁新语言</div>
<div class="mono" style="margin-top:8px">多模态模型在英语中表现出色，得益于丰富的图像-文本和音频-文本数据，但由于多语言多模态资源有限，其他语言的性能急剧下降。现有解决方案严重依赖机器翻译，而多语言文本建模的进展仍未得到充分利用。我们介绍了METAL，这是一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间。尽管其简单性，METAL在英语中匹配基线性能（94.9%的召回率@10），并在XTD文本到图像检索中实现强大的零样本迁移（11种语言中10种未见语言的平均召回率为89.5%）。定性t-SNE可视化显示多语言嵌入与多模态表示紧密对齐，而权重分析表明，变换重塑了嵌入几何形状，而不是进行简单的旋转。除了图像-文本检索，METAL还推广到音频-文本检索和跨语言文本到图像生成。我们在https://github.com/m2m-codebase/M2M发布代码和检查点，以及包括MSCOCO多语言30K（https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k）、AudioCaps多语言（https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual）和Clotho多语言（https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual）在内的多语言评估数据集，以促进进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance gap in multimodal models for languages other than English, which suffer from limited multilingual multimodal resources. The authors introduce METAL, a lightweight alignment method that utilizes only English text to map multilingual text embeddings into a multimodal space, employing a few linear layers. Experimental results demonstrate that METAL achieves competitive performance in English with a 94.9 percent Recall at 10 and strong zero-shot transfer capabilities, averaging 89.5 percent Recall at 10 across 11 languages, including 10 unseen languages, in text-to-image retrieval tasks, while also generalizing effectively to audio-text retrieval and cross-lingual text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善多模态模型在英语以外语言中的表现，因为这些语言缺乏多语言多模态资源。作者提出了METAL，这是一种轻量级对齐方法，仅利用英语文本将多语言文本嵌入映射到多模态空间，采用少量线性层。实验结果表明，METAL在英语中达到了94.9%的基线表现（Recall@10），并在XTD文本到图像检索任务中，在11种语言（包括10种未见语言）上实现了89.5%的强零样本迁移能力（Recall@10），同时也能够推广到音频文本检索和跨语言文本到图像生成。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation</div>
<div class="meta-line">Authors: Jyun-Ping Kao, Shinyeong Rho, Shahar Lazarev, Hyun-Hae Cho, Fangxu Xing, Taehoon Shin, C. -C. Jay Kuo, Jonghye Woo</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2025-11-08T23:29:28+00:00 · Latest: 2026-01-15T05:18:46+00:00</div>
<div class="meta-line">Comments: Accepted for presentation at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06163v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06163v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in children plays a crucial role in improving outcomes in education and mental health. Diagnosing ADHD using neuroimaging data, however, remains challenging due to heterogeneous presentations and overlapping symptoms with other conditions. To address this, we propose a novel parameter-efficient transfer learning approach that adapts a large-scale 3D convolutional foundation model, pre-trained on CT images, to an MRI-based ADHD classification task. Our method introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional kernels into 2D low-rank updates, dramatically reducing trainable parameters while achieving superior performance. In a five-fold cross-validated evaluation on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved state-of-the-art results, with one model variant reaching 71.9% accuracy and another attaining an AUC of 0.716. Both variants use only 1.64 million trainable parameters (over 113x fewer than a fully fine-tuned foundation model). Our results represent one of the first successful cross-modal (CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a new benchmark for ADHD classification while greatly improving efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于低秩适应的3D卷积基础模型的跨模态微调用于ADHD分类</div>
<div class="mono" style="margin-top:8px">儿童注意缺陷/多动障碍（ADHD）的早期诊断在改善教育和心理健康结果中起着至关重要的作用。然而，由于表现异质性和与其他疾病症状重叠，使用神经影像数据诊断ADHD仍然具有挑战性。为此，我们提出了一种新颖的参数高效迁移学习方法，将在CT图像上预训练的大规模3D卷积基础模型适应于基于MRI的ADHD分类任务。我们的方法通过将3D卷积核分解为2D低秩更新，引入了3D低秩适应（LoRA），显著减少了可训练参数，同时实现了卓越的性能。在对公共扩散MRI数据库进行的五折交叉验证评估中，我们的3D LoRA微调策略达到了最先进的结果，其中一个模型变体的准确率达到71.9%，另一个的AUC为0.716。两个变体仅使用了164万可训练参数（比完全微调的基础模型少超过113倍）。我们的结果代表了神经影像学中基础模型的首次成功跨模态（CT到MRI）适应，为ADHD分类建立了新的基准，同时大大提高了效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in children using neuroimaging data, which is complicated by heterogeneous presentations and symptom overlap with other conditions. The authors propose a novel transfer learning approach that employs Low-Rank Adaptation (LoRA) to fine-tune a large-scale 3D convolutional foundation model, originally trained on CT images, for an MRI-based ADHD classification task. Experimental results demonstrate that their 3D LoRA fine-tuning method achieved state-of-the-art performance in a five-fold cross-validation on a public diffusion MRI database, with one model variant reaching 71.9% accuracy and another achieving an AUC of 0.716, while utilizing only 1.64 million trainable parameters, significantly fewer than a fully fine-tuned model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善儿童注意缺陷多动障碍（ADHD）的早期诊断，这一过程因症状的异质性及其与其他疾病的重叠而变得复杂。作者提出了一种新颖的迁移学习方法，利用在CT图像上预训练的大规模3D卷积基础模型，将其适应于基于MRI的ADHD分类，采用低秩适应（LoRA）显著减少可训练参数的数量。在对公共扩散MRI数据库进行五折交叉验证评估中，该方法取得了最先进的结果，其中一个模型变体的准确率达到71.9%，另一个的AUC值为0.716，同时仅使用了164万个可训练参数，显示出相较于传统微调方法的显著效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation</div>
<div class="meta-line">Authors: Chengzhuo Tong, Mingkun Chang, Shenglong Zhang, Yuran Wang, Cheng Liang, Zhizheng Zhao, Ruichuan An, Bohan Zeng, Yang Shi, Yifan Dai, Ziming Zhao, Guanbin Li, Pengfei Wan, Yuanxing Zhang, Wentao Zhang</div>
<div class="meta-line">First: 2026-01-15T04:33:06+00:00 · Latest: 2026-01-15T04:33:06+00:00</div>
<div class="meta-line">Comments: 16 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10061v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10061v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoF-T2I：视频模型作为文本到图像生成的纯视觉推理器</div>
<div class="mono" style="margin-top:8px">最近的视频生成模型揭示了帧链（CoF）推理的出现，使得逐帧视觉推理成为可能。凭借这一能力，视频模型已成功应用于各种视觉任务（例如，迷宫求解、视觉难题）。然而，由于缺乏明确的视觉推理起点和可解释的中间状态，其在增强文本到图像（T2I）生成方面的潜力仍然未被充分探索。为填补这一空白，我们提出了CoF-T2I模型，通过逐步视觉细化将CoF推理整合到T2I生成中，其中中间帧作为明确的推理步骤，最终帧作为输出。为了建立这样的明确生成过程，我们策划了CoF-Evol-Instruct，一个CoF轨迹数据集，模拟从语义到美学的生成过程。为了进一步提高质量并避免运动伪影，我们为每帧启用独立编码操作。实验表明，CoF-T2I显著优于基础视频模型，并在具有挑战性的基准测试中取得了竞争力的表现，在GenEval上达到0.86，在Imagine-Bench上达到7.468。这些结果表明视频模型在推动高质量文本到图像生成方面具有巨大的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the potential of video generation models to enhance text-to-image (T2I) generation through Chain-of-Frame (CoF) reasoning, which has not been fully explored. The authors propose CoF-T2I, a model that incorporates CoF reasoning into T2I generation by using progressive visual refinement, where intermediate frames serve as explicit reasoning steps leading to the final output. Experimental results demonstrate that CoF-T2I significantly outperforms the baseline video model and achieves competitive scores on challenging benchmarks, with a score of 0.86 on GenEval and 7.468 on Imagine-Bench, highlighting the effectiveness of video models in improving T2I generation quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索视频生成模型在增强文本到图像（T2I）生成中的潜力，由于缺乏明确的视觉推理框架，这一领域尚未得到充分研究。作者提出了CoF-T2I模型，该模型通过逐步视觉细化将帧链（CoF）推理整合到T2I生成中，利用中间帧作为明确的推理步骤。实验结果表明，CoF-T2I显著优于基线视频模型，并在具有挑战性的基准测试中取得了竞争性结果，在GenEval上得分为0.86，在Imagine-Bench上得分为7.468，突显了视频模型在提高T2I生成质量方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Disentangled Concept Representation for Text-to-image Person Re-identification</div>
<div class="meta-line">Authors: Giyeol Kim, Chanho Eom</div>
<div class="meta-line">First: 2026-01-15T04:08:53+00:00 · Latest: 2026-01-15T04:08:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10053v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10053v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像的人物重识别的解耦概念表示</div>
<div class="mono" style="margin-top:8px">文本到图像的人物重识别（TIReID）旨在根据自由形式的文本描述从大型图库中检索人物图像。由于视觉外观与文本表达之间存在显著的模态差距，以及需要建模细粒度的对应关系以区分具有相似属性（如服装颜色、纹理或风格）的人，TIReID具有挑战性。为了解决这些问题，我们提出了DiCo（解耦概念表示），这是一个新颖的框架，实现了分层和解耦的跨模态对齐。DiCo引入了一种基于共享槽的表示，其中每个槽作为跨模态的部分级锚点，并进一步分解为多个概念块。该设计使得互补属性（例如，颜色、纹理、形状）的解耦成为可能，同时保持图像和文本之间的一致部分级对应关系。在CUHK-PEDES、ICFG-PEDES和RSTPReid上的大量实验表明，我们的框架在与最先进的方法相比时表现出竞争力，同时通过明确的槽和块级表示增强了解释性，以获得更细粒度的检索结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve text-to-image person re-identification (TIReID), which faces challenges due to the significant modality gap between visual and textual data and the need for precise correspondences among individuals with similar attributes. The authors propose a novel framework called DiCo (Disentangled Concept Representation) that employs a shared slot-based representation to achieve hierarchical and disentangled cross-modal alignment. Experimental results on datasets such as CUHK-PEDES, ICFG-PEDES, and RSTPReid show that DiCo not only competes effectively with state-of-the-art methods but also enhances interpretability through its explicit slot- and block-level representations, leading to improved fine-grained retrieval outcomes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善文本到图像的人物重识别（TIReID），该领域面临着视觉和文本模态之间显著差距的挑战，以及需要在具有相似属性的个体之间建立细粒度对应关系。作者提出了一种名为DiCo（解耦概念表示）的新框架，该框架利用共享的基于插槽的表示来实现分层和解耦的跨模态对齐，从而允许对颜色、纹理和形状等属性进行分解。在CUHK-PEDES、ICFG-PEDES和RSTPReid等数据集上的实验结果表明，DiCo不仅与最先进的方法竞争，而且通过其明确的表示增强了解释性，从而改善了检索结果。</div>
</details>
</div>
<div class="card">
<div class="title">How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation</div>
<div class="meta-line">Authors: Wilson Y. Lee</div>
<div class="meta-line">First: 2026-01-14T02:34:58+00:00 · Latest: 2026-01-15T03:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09084v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09084v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类判断够多少才算足够？人类偏好评估的可行性限制</div>
<div class="mono" style="margin-top:8px">人类偏好评估广泛用于比较生成模型，但仍不清楚需要多少判断才能可靠地检测到小的改进。我们表明，当偏好信号在提示中分散时（即所有提示类型的信息量相似），比例分配是最小最大最优的：没有分配策略能显著提高可检测性。对大规模人类偏好数据集的实证分析表明，大多数比较都处于这种分散状态，表现出小的偏好边际，需要比通常收集的更多的判断，即使在样本良好的比较中也是如此。这些限制在评估协议和模式中持续存在，包括聊天、图像生成和带执行反馈的代码生成。相比之下，减少提示引起的变异性的精心设计基准系统地诱导更大的边际，并通过减少$1.5\times$的提示级方差来提高可检测性。我们的结果表明，模糊或负面的人工评估结果通常反映的是评估能力不足，而非模型等价性，强调了明确考虑效应大小、预算和协议设计的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of determining the adequate number of human judgments needed for reliable detection of improvements in generative models, as current practices often yield inconclusive results. The authors employed empirical analysis of large-scale human preference datasets to investigate the effects of preference signal diffusion across various prompts. Their findings reveal that most comparisons fall into a diffuse regime, necessitating significantly more judgments than typically collected, while curated benchmarks that minimize prompt variability can enhance detectability by reducing prompt-level variance by 1.5 times, highlighting the importance of considering effect size and evaluation design in human assessments.</div>
<div class="mono" style="margin-top:8px">本研究探讨了人类偏好评估在评估生成模型时的充分性，特别关注检测微小改进所需的判断数量。作者通过对大量人类偏好数据集的实证分析，研究了偏好信号在不同提示中的扩散效应。研究结果表明，大多数比较处于扩散状态，所需的判断数量远超通常收集的数量，并强调经过策划的基准可以通过减少提示级方差来提高可检测性，从而突显在偏好评估中考虑效应大小和评估设计的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation</div>
<div class="meta-line">Authors: Xinyu Hou, Xiaoming Li, Chen Change Loy</div>
<div class="meta-line">First: 2024-06-18T17:22:23+00:00 · Latest: 2026-01-15T03:44:38+00:00</div>
<div class="meta-line">Comments: Accepted by IJCV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.12805v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.12805v3">PDF</a> · <a href="https://github.com/itsmag11/AITTI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the high-quality results of text-to-image generation, stereotypical biases have been spotted in their generated contents, compromising the fairness of generative models. In this work, we propose to learn adaptive inclusive tokens to shift the attribute distribution of the final generative outputs. Unlike existing de-biasing approaches, our method requires neither explicit attribute specification nor prior knowledge of the bias distribution. Specifically, the core of our method is a lightweight adaptive mapping network, which can customize the inclusive tokens for the concepts to be de-biased, making the tokens generalizable to unseen concepts regardless of their original bias distributions. This is achieved by tuning the adaptive mapping network with a handful of balanced and inclusive samples using an anchor loss. Experimental results demonstrate that our method outperforms previous bias mitigation methods without attribute specification while preserving the alignment between generative results and text descriptions. Moreover, our method achieves comparable performance to models that require specific attributes or editing directions for generation. Extensive experiments showcase the effectiveness of our adaptive inclusive tokens in mitigating stereotypical bias in text-to-image generation. The code will be available at https://github.com/itsmag11/AITTI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AITTI：学习自适应包容性标记用于文本到图像生成</div>
<div class="mono" style="margin-top:8px">尽管文本到图像生成的结果质量很高，但其生成内容中发现了刻板偏见，影响了生成模型的公平性。在这项工作中，我们提出学习自适应包容性标记，以改变最终生成输出的属性分布。与现有的去偏见方法不同，我们的方法不需要显式的属性规范或偏见分布的先验知识。具体而言，我们方法的核心是一个轻量级自适应映射网络，可以为需要去偏见的概念定制包容性标记，使得这些标记能够推广到未见过的概念，无论其原始偏见分布如何。这是通过使用锚损失对自适应映射网络进行调优，利用少量平衡和包容性样本实现的。实验结果表明，我们的方法在不需要属性规范的情况下优于之前的偏见缓解方法，同时保持生成结果与文本描述之间的对齐。此外，我们的方法在性能上与需要特定属性或编辑方向进行生成的模型相当。大量实验展示了我们自适应包容性标记在减轻文本到图像生成中的刻板偏见方面的有效性。代码将发布在 https://github.com/itsmag11/AITTI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the stereotypical biases present in text-to-image generation, which undermine the fairness of generative models. The authors propose a method that learns adaptive inclusive tokens through a lightweight adaptive mapping network, which customizes these tokens without requiring explicit attribute specification or prior knowledge of bias distributions. Experimental results indicate that this approach outperforms existing bias mitigation techniques while maintaining alignment between generated images and text descriptions, and it achieves performance comparable to models that depend on specific attributes for generation, demonstrating the effectiveness of adaptive inclusive tokens in reducing bias in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到图像生成中存在的刻板偏见，这些偏见削弱了生成模型的公平性。作者提出了一种利用自适应包容性标记的方法，通过轻量级自适应映射网络来调整生成输出的属性分布，而无需明确的属性规范或对偏见分布的先验知识。实验结果表明，该方法在保持生成图像与文本描述之间的一致性的同时，超越了之前的偏见缓解技术，并且与需要特定属性的模型表现相当，展示了自适应包容性标记在减少文本到图像生成中的偏见方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data</div>
<div class="meta-line">Authors: Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu</div>
<div class="meta-line">Venue: KDD</div>
<div class="meta-line">First: 2026-01-15T03:22:03+00:00 · Latest: 2026-01-15T03:22:03+00:00</div>
<div class="meta-line">Comments: Accepted in Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10031v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FilDeep：基于多保真数据学习弹塑性固体的大变形</div>
<div class="mono" style="margin-top:8px">弹塑性固体的大变形的科学计算在各种制造应用中至关重要。传统数值方法存在多种固有局限性，促使深度学习（DL）成为一种有前景的替代方案。目前DL技术的有效性通常依赖于高数量和高精度数据集的可用性，而在大变形问题中，这些数据集仍然难以获得。在数据集构建过程中，数据数量与数据精度之间存在困境，导致DL模型的性能不佳。为了解决这一挑战，我们专注于大变形的代表性应用——拉伸弯曲问题，并提出FilDeep，一个基于保真的深度学习框架，用于弹塑性固体的大变形。我们的FilDeep旨在通过同时使用低保真和高保真数据进行训练来解决数量与精度的困境，前者提供更大的数量但精度较低，而后者提供更高的精度但数量较少。在FilDeep中，我们为实际的大变形问题提供了细致的设计。特别地，我们提出了启用注意力的跨保真模块，以有效捕捉MF数据之间的长程物理交互。据我们所知，FilDeep是第一个使用MF数据解决大变形问题的DL框架。大量实验表明，FilDeep始终实现了最先进的性能，并可以高效地应用于制造。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the scientific computation of large deformations in elastic-plastic solids, which is essential for various manufacturing applications, as traditional numerical methods have significant limitations. The authors propose FilDeep, a Fidelity-based Deep Learning framework that addresses the challenge of balancing data quantity and accuracy by training on both low-fidelity and high-fidelity datasets. Experimental results show that FilDeep consistently achieves state-of-the-art performance in modeling large deformations, demonstrating its effectiveness and potential for efficient deployment in manufacturing contexts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善弹塑性固体大变形的计算建模，这对各种制造应用至关重要，但传统数值方法存在局限性。作者提出了FilDeep，这是一种基于保真度的深度学习框架，旨在通过同时训练低保真度和高保真度数据集来解决数据数量和准确性之间的平衡问题。实验结果表明，FilDeep在大变形建模中实现了最先进的性能，证明了其有效性和在制造环境中实际应用的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
