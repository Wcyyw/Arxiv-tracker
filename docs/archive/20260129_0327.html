<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 03:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0327</div>
    <div class="row"><div class="card">
<div class="title">LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning</div>
<div class="meta-line">Authors: Obed Junias, Maria Leonor Pacheco</div>
<div class="meta-line">First: 2026-01-23T07:07:19+00:00 · Latest: 2026-01-27T18:33:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16504v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16504v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑常识问答：逻辑常识推理的基准</div>
<div class="mono" style="margin-top:8px">常识推理通常涉及评估多个合理的解释，而不是选择单一的原子答案，但大多数基准依赖于单标签评估，模糊了陈述是否共同合理、相互排斥或共同不合理。我们引入了LOGICAL-COMMONSENSEQA，一个将常识推理重新框定为对原子陈述对的逻辑组合的基准，使用合理性级别运算符（与、或、既不/也不）。在零-shot、少-shot和思维链提示下评估指令调优、推理专用和微调模型，我们发现模型在合取推理上表现合理，在析取推理上表现中等，但在基于否定的问题上性能急剧下降。LOGICAL-COMMONSENSEQA揭示了基本推理的局限性，并提供了一个受控框架，以推动组合常识推理的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing commonsense reasoning benchmarks that typically focus on single-label evaluations, which do not adequately capture the complexity of evaluating multiple plausible interpretations. The authors introduce LOGICAL-COMMONSENSEQA, a benchmark that reformulates commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators such as AND, OR, and NEITHER/NOR. Experimental results show that while models perform reasonably well on conjunctive reasoning and moderately on disjunctive reasoning, their performance significantly declines on negation-based questions, highlighting fundamental reasoning limitations and providing a framework for improving compositional commonsense reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有常识推理基准的局限性，这些基准通常依赖于单标签评估，无法捕捉评估多个合理解释的复杂性。作者提出了LOGICAL-COMMONSENSEQA，这是一个基准，将常识推理重新构建为对原子语句对的逻辑组合，使用诸如AND、OR和NEITHER/NOR等合理性级别运算符。实验结果表明，尽管模型在合取推理上表现良好，在析取推理上表现中等，但在基于否定的问题上性能显著下降，这突显了基本推理的局限性，并提供了改进组合常识推理的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</div>
<div class="meta-line">Authors: Cong Cao, Yujie Xu, Xiaodong Xu</div>
<div class="meta-line">First: 2025-11-14T12:40:21+00:00 · Latest: 2026-01-27T18:27:31+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11236v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.11236v3">PDF</a> · <a href="https://github.com/cao-cong/FSMSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters. Our code and dataset are available at https://github.com/cao-cong/FSMSE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参数高效的 MoE LoRA 用于少样本多风格编辑</div>
<div class="mono" style="margin-top:8px">近年来，图像编辑受到了越来越多的关注。然而，通用图像编辑模型在面对新风格时往往无法产生令人满意的结果。挑战在于如何仅使用有限的配对数据有效地微调通用图像编辑模型以适应新风格。为了解决这个问题，本文提出了一种新颖的少样本风格编辑框架。为此，我们构建了一个包含五种不同风格的基准数据集。相应地，我们提出了一种参数高效的多风格混合专家低秩适应（MoE LoRA），具有特定风格和共享风格的路由机制，以共同微调多种风格。特定风格的路由确保不同风格之间不相互干扰，而共享风格的路由自适应地分配共享的 MoE LoRA 以学习共同模式。我们的 MoE LoRA 可以通过一种新颖的度量引导方法自动确定每层的最佳秩，该方法估计每个单秩组件的重要性分数。此外，我们探索了在变换器中的扩散模型（DiT）中插入 LoRA 的最佳位置，并结合对抗学习和流匹配来指导扩散训练过程。实验结果表明，我们提出的方法在显著减少 LoRA 参数的情况下优于现有的最先进方法。我们的代码和数据集可在 https://github.com/cao-cong/FSMSE 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image editing models&#x27; performance when adapting to new styles with limited paired data. The authors propose a few-shot style editing framework that includes a benchmark dataset with five distinct styles and introduces a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) method. Key experimental findings indicate that this approach, which utilizes style-specific and style-shared routing mechanisms, significantly outperforms existing state-of-the-art methods while using fewer LoRA parameters, demonstrating effective fine-tuning across multiple styles.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高图像编辑模型在有限配对数据下适应新风格的性能。作者提出了一种少样本风格编辑框架，包括一个包含五种不同风格的基准数据集，并引入了一种参数高效的多风格混合专家低秩适应（MoE LoRA）方法。关键实验结果表明，该方法在使用显著更少的LoRA参数的情况下，优于现有的最先进方法，展示了有效的风格特定和风格共享路由机制，以同时微调多种风格。</div>
</details>
</div>
<div class="card">
<div class="title">MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents</div>
<div class="meta-line">Authors: Lukas Aichberger, Alasdair Paren, Guohao Li, Philip Torr, Yarin Gal, Adel Bibi</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-13T18:59:12+00:00 · Latest: 2026-01-27T18:10:17+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10809v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10809v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in operating system (OS) agents have enabled vision-language models (VLMs) to directly control a user&#x27;s computer. Unlike conventional VLMs that passively output text, OS agents autonomously perform computer-based tasks in response to a single user prompt. OS agents do so by capturing, parsing, and analysing screenshots and executing low-level actions via application programming interfaces (APIs), such as mouse clicks and keyboard inputs. This direct interaction with the OS significantly raises the stakes, as failures or manipulations can have immediate and tangible consequences. In this work, we uncover a novel attack vector against these OS agents: Malicious Image Patches (MIPs), adversarially perturbed screen regions that, when captured by an OS agent, induce it to perform harmful actions by exploiting specific APIs. For instance, a MIP can be embedded in a desktop wallpaper or shared on social media to cause an OS agent to exfiltrate sensitive user data. We show that MIPs generalise across user prompts and screen configurations, and that they can hijack multiple OS agents even during the execution of benign instructions. These findings expose critical security vulnerabilities in OS agents that have to be carefully addressed before their widespread deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对代理的恶意图像补丁劫持多模态操作系统代理</div>
<div class="mono" style="margin-top:8px">最近操作系统（OS）代理的进展使得视觉-语言模型（VLMs）能够直接控制用户的计算机。与被动输出文本的传统VLMs不同，OS代理能够自主执行计算机任务，以响应单一用户提示。OS代理通过捕获、解析和分析屏幕截图，并通过应用程序编程接口（APIs）执行低级操作，如鼠标点击和键盘输入，来实现这一点。这种与操作系统的直接交互显著提高了风险，因为失败或操控可能会产生即时和切实的后果。在这项工作中，我们揭示了一种针对这些OS代理的新攻击向量：恶意图像补丁（MIPs），这些对屏幕区域进行对抗性扰动的图像，当被OS代理捕获时，会利用特定的APIs诱使其执行有害操作。例如，MIP可以嵌入桌面壁纸或在社交媒体上分享，以导致OS代理泄露敏感用户数据。我们展示了MIPs在用户提示和屏幕配置之间的泛化能力，并且它们可以在执行良性指令时劫持多个OS代理。这些发现暴露了OS代理中的关键安全漏洞，必须在其广泛部署之前仔细解决。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the security vulnerabilities posed by operating system (OS) agents that utilize vision-language models (VLMs) to autonomously perform tasks on a user&#x27;s computer. The authors introduce a novel attack vector called Malicious Image Patches (MIPs), which are adversarially altered screen regions that can manipulate OS agents into executing harmful actions. Through experimental results, the study demonstrates that MIPs can effectively generalize across different user prompts and screen configurations, allowing them to hijack multiple OS agents even when benign instructions are being executed, highlighting significant security risks that need to be mitigated before the widespread use of OS agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决操作系统（OS）代理利用视觉语言模型（VLM）在用户计算机上自主执行任务所带来的安全漏洞。作者提出了一种新的攻击向量，称为恶意图像补丁（MIPs），这些补丁是经过对抗性修改的屏幕区域，能够操纵OS代理通过特定的应用程序接口（API）执行有害操作。实验表明，MIPs能够有效地跨不同用户提示和屏幕配置进行泛化，成功劫持多个OS代理，即使在执行良性指令时也能实现，突显出在OS代理安全部署之前需要缓解的重大安全风险。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living</div>
<div class="meta-line">Authors: Huy Trinh</div>
<div class="meta-line">First: 2026-01-27T18:06:51+00:00 · Latest: 2026-01-27T18:06:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19853v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to &quot;empty room&quot; and &quot;person present&quot;, and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the &quot;person present&quot; class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas &quot;empty room&quot; samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于可解释的基于雷达的占用检测的生成潜在对齐</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们研究如何使毫米波雷达的存在检测在环境辅助生活（AAL）设置中更具可解释性，因为基于摄像头的感知引发隐私问题。我们提出了一种生成潜在对齐（GLA）框架，该框架结合了轻量级卷积变分自编码器和冻结的CLIP文本编码器，以学习雷达范围-角度（RA）热图的低维潜在表示。潜在空间与对应于“空房间”和“有人在场”的两个语义锚点软对齐，并在该对齐的潜在空间中应用Grad-CAM，以可视化支持每个存在决策的空间区域。在我们的毫米波雷达数据集中，我们定性观察到“有人在场”类别产生的紧凑Grad-CAM斑块与强RA返回重合，而“空房间”样本则产生模糊或没有证据。我们还使用不相关的文本提示进行了消融研究，这降低了重建和定位的效果，表明雷达特定的锚点在此设置中对于有意义的解释是重要的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for interpretable presence detection using mmWave radar in Ambient Assisted Living environments, where privacy concerns limit the use of cameras. The authors introduce a Generative Latent Alignment (GLA) framework that integrates a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to create a low-dimensional latent representation of radar Range-Angle heatmaps. Experimental results show that the &#x27;person present&#x27; class generates compact Grad-CAM visualizations aligned with strong radar returns, while &#x27;empty room&#x27; samples exhibit diffuse or absent evidence, highlighting the importance of radar-specific semantic anchors for effective interpretation.</div>
<div class="mono" style="margin-top:8px">本研究解决了在环境辅助生活中使用毫米波雷达进行可解释性存在检测的需求，因为隐私问题限制了摄像头的使用。作者提出了一种生成潜在对齐（GLA）框架，该框架将轻量级卷积变分自编码器与冻结的CLIP文本编码器结合，以创建雷达范围-角度热图的低维潜在表示。实验结果表明，&#x27;有人在场&#x27;类别生成的Grad-CAM可视化与强雷达回波对齐，而&#x27;空房间&#x27;样本则显示出模糊的证据，这表明雷达特定语义锚点在增强可解释性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-27T18:04:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>它会零-shot吗？：预测任意查询的零-shot分类性能</div>
<div class="mono" style="margin-top:8px">视觉-语言模型如CLIP为文本和图像创建了对齐的嵌入空间，使任何人都可以通过简单命名他们想要区分的类别来构建视觉分类器。然而，在一个领域表现良好的模型在另一个领域可能会失败，非专家用户没有简单的方法来评估他们选择的VLM是否适用于他们的问题。我们基于先前的工作，使用仅文本的比较来评估模型在给定自然语言任务中的表现，并探索生成与该任务相关的合成图像的方法，以评估和改进零-shot准确性的预测。我们展示了生成的图像对基线文本-only分数的显著提升了这些预测的质量。此外，它为用户提供了关于用于评估的图像类型的反馈。在标准CLIP基准数据集上的实验表明，基于图像的方法帮助用户在没有任何标记示例的情况下预测VLM是否对他们的应用有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to assist non-expert users in evaluating the effectiveness of Vision-Language Models (VLMs) for zero-shot classification tasks, as performance can vary significantly across different domains. The authors build on previous methods that utilize text-only comparisons and introduce an approach that generates synthetic images relevant to the classification task to enhance the prediction of zero-shot accuracy. Experimental results on standard CLIP benchmark datasets indicate that incorporating generated imagery significantly improves the accuracy of predictions compared to text-only assessments, providing users with valuable feedback on the types of images influencing their evaluations.</div>
<div class="mono" style="margin-top:8px">本研究解决了非专家用户在预测视觉语言模型（VLM）在不同领域的零样本分类任务有效性时面临的挑战。作者在先前使用仅文本比较的方法基础上，引入了一种结合与分类任务相关的合成图像的方法，以增强零样本准确性的预测。标准CLIP基准数据集上的实验结果表明，与仅文本评估相比，生成图像的加入显著提高了预测的准确性，为用户提供了关于影响其评估的图像类型的有价值反馈。</div>
</details>
</div>
<div class="card">
<div class="title">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</div>
<div class="meta-line">Authors: Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:58:12+00:00 · Latest: 2026-01-27T17:58:12+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026, Codebase: https://github.com/Nicous20/EgoHandICL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19850v1">PDF</a> · <a href="https://github.com/Nicous20/EgoHandICL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoHandICL：基于上下文学习的自我中心3D手重建</div>
<div class="mono" style="margin-top:8px">在自我中心视觉中，稳健的3D手重建面临深度模糊、自遮挡和复杂的手物体交互等挑战。先前的方法通过扩展训练数据或添加辅助线索来缓解这些问题，但在未见的上下文中往往表现不佳。我们提出了EgoHandICL，这是第一个用于3D手重建的上下文学习（ICL）框架，能够在具有挑战性的自我中心条件下改善语义对齐、视觉一致性和稳健性。EgoHandICL引入了由视觉-语言模型（VLMs）指导的互补示例检索、为多模态上下文量身定制的ICL分词器，以及基于掩码自编码器（MAE）的架构，使用手引导的几何和感知目标进行训练。在ARCTIC和EgoExo4D上的实验显示出相对于最先进方法的一致性提升。我们还展示了现实世界的泛化能力，并通过使用重建的手作为视觉提示来改善EgoVLM手物体交互推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of robust 3D hand reconstruction in egocentric vision, which include depth ambiguity, self-occlusion, and complex hand-object interactions. The authors propose EgoHandICL, an in-context learning framework that enhances semantic alignment, visual consistency, and robustness in difficult conditions. Key experimental results demonstrate that EgoHandICL achieves consistent improvements over state-of-the-art methods on the ARCTIC and EgoExo4D datasets, and it also shows enhanced real-world generalization and improved hand-object interaction reasoning by utilizing reconstructed hands as visual prompts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在自我中心视觉中进行稳健的3D手重建所面临的挑战，包括深度模糊、自我遮挡和复杂的手-物体交互。作者提出了EgoHandICL，一个增强语义对齐、视觉一致性和在困难场景下稳健性的上下文学习框架。关键实验结果表明，EgoHandICL在ARCTIC和EgoExo4D数据集上相较于现有的最先进方法取得了一致的改进，并且在使用重建的手作为视觉提示时，展示了现实世界的泛化能力，同时增强了手-物体交互推理。</div>
</details>
</div>
<div class="card">
<div class="title">APEX-Agents</div>
<div class="meta-line">Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</div>
<div class="meta-line">First: 2026-01-20T18:53:44+00:00 · Latest: 2026-01-27T17:31:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14242v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-Agents</div>
<div class="mono" style="margin-top:8px">我们介绍了代理的人工智能生产力指数（APEX-Agents），这是一个评估人工智能代理是否能够执行由投资银行分析师、管理顾问和企业律师创建的长期跨应用任务的基准。APEX-Agents要求代理在具有文件和工具的现实工作环境中导航。我们使用Pass@1测试了八个代理以进行排行榜。Gemini 3 Flash（思维=高）获得最高分24.0%，其次是GPT-5.2（思维=高）、Claude Opus 4.5（思维=高）和Gemini 3 Pro（思维=高）。我们开源了APEX-Agents基准（n=480），包括所有提示、评分标准、金标准输出、文件和元数据。我们还开源了Archipelago，这是我们用于代理执行和评估的基础设施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the APEX-Agents study is to establish a benchmark for evaluating the capabilities of AI agents in performing complex, long-term tasks typical of investment banking, management consulting, and corporate law. The researchers developed the AI Productivity Index for Agents (APEX-Agents) and tested eight different agents using a metric called Pass@1 in realistic work environments. The findings revealed that Gemini 3 Flash (Thinking=High) achieved the highest performance score of 24.0%, followed by GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro, with the benchmark and its associated resources being made available for public use.</div>
<div class="mono" style="margin-top:8px">APEX-Agents研究的动机是创建一个基准，以评估AI代理在专业环境中执行复杂长期任务的能力，这些任务通常出现在投资银行分析师、管理顾问和企业律师的工作中。研究人员开发了AI生产力指数（APEX-Agents），并使用Pass@1指标测试了八种不同的代理。结果显示，Gemini 3 Flash（Thinking=High）获得了最高分24.0%，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro，它们也被评为Thinking=High。该基准包括480个任务，以及所有相关材料，已开源以供进一步研究和开发。</div>
</details>
</div>
<div class="card">
<div class="title">Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering</div>
<div class="meta-line">Authors: Kun Li, Michael Ying Yang, Sami Sebastian Brandt</div>
<div class="meta-line">First: 2026-01-27T17:24:32+00:00 · Latest: 2026-01-27T17:24:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于查询引导的时空频率交互用于音乐音频视觉问答</div>
<div class="mono" style="margin-top:8px">音频-视觉问答（AVQA）是一项具有挑战性的多模态任务，需要在给定视频中共同推理音频、视觉和文本信息，以回答自然语言问题。受视频问答（Video QA）最新进展的启发，许多现有的AVQA方法主要集中在视觉信息处理上，利用预训练模型提取对象级和运动级表示。然而，在这些方法中，音频输入主要被视为视频分析的补充，文本问题信息对音频-视觉理解的贡献很小，因为它通常仅在推理的最后阶段整合。为了解决这些局限性，我们提出了一种新颖的查询引导时空频率（QSTar）交互方法，有效地结合了查询引导线索，并利用音频信号的独特频域特征，以及空间和时间感知，以增强音频-视觉理解。此外，我们引入了一个受提示启发的查询上下文推理（QCR）模块，指导模型更精确地关注语义相关的音频和视觉特征。在多个AVQA基准上进行的广泛实验表明，我们提出的方法有效，显著提高了现有音频问答、视觉问答、视频问答和AVQA方法的性能。代码和预训练模型将在发表后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Audio-Visual Question Answering (AVQA) by addressing the limitations of existing methods that inadequately integrate audio and textual information. The authors propose a novel Query-guided Spatial-Temporal-Frequency (QSTar) interaction method that enhances audio-visual understanding by incorporating question-guided clues and leveraging the unique frequency-domain characteristics of audio signals. Experimental results on various AVQA benchmarks show that this approach significantly outperforms existing methods in Audio QA, Visual QA, Video QA, and AVQA tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有音视频问答（AVQA）方法的局限性，这些方法往往未充分利用音频和文本信息。作者提出了一种新颖的查询引导空间-时间-频率（QSTar）交互方法，该方法将查询引导线索与音频信号的独特频率特征相结合，从而增强对音视频内容的理解。在多个AVQA基准上的实验结果表明，该方法显著优于传统的音频问答、视觉问答、视频问答和AVQA方法。</div>
</details>
</div>
<div class="card">
<div class="title">WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</div>
<div class="meta-line">Authors: Xinrui Zhang, Yufeng Wang, Shuangkang Fang, Zesheng Wang, Dacheng Qi, Wenrui Ding</div>
<div class="meta-line">First: 2026-01-27T16:14:34+00:00 · Latest: 2026-01-27T16:14:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19753v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19753v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://buaaxrzhang.github.io/WaterClear-GS/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaterClear-GS：用于水下重建和修复的光学感知高斯点云</div>
<div class="mono" style="margin-top:8px">水下3D重建和外观修复受到水的复杂光学特性的阻碍，如波长依赖的衰减和散射。现有的基于神经辐射场（NeRF）的方法在渲染速度和颜色修复方面表现不佳，而3D高斯点云（3DGS）本质上缺乏建模复杂体积散射效应的能力。为了解决这些问题，我们提出了WaterClear-GS，这是第一个纯3DGS基础框架，明确将局部衰减和散射的水下光学特性集成到高斯原语中，消除了对辅助介质网络的需求。我们的方法采用双分支优化策略，以确保水下光度一致性，同时自然恢复无水外观。该策略通过深度引导的几何正则化和感知驱动的图像损失增强，并结合曝光约束、空间自适应正则化和物理引导的光谱正则化，共同强制执行局部3D一致性并保持自然视觉感知。在标准基准和我们新收集的数据集上的实验表明，WaterClear-GS在新视图合成（NVS）和水下图像修复（UIR）任务上表现出色，同时保持实时渲染。代码将发布在 https://buaaxrzhang.github.io/WaterClear-GS/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve underwater 3D reconstruction and appearance restoration, which are challenged by the complex optical properties of water. The authors propose WaterClear-GS, a novel framework that integrates underwater optical properties into Gaussian primitives, using a dual-branch optimization strategy to ensure photometric consistency and recover water-free appearances. Experimental results show that WaterClear-GS significantly outperforms existing methods in novel view synthesis and underwater image restoration tasks while achieving real-time rendering speeds.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善水下三维重建和外观恢复，这些任务受到水的复杂光学特性的挑战。作者提出了WaterClear-GS，这是一种新颖的框架，将水下光学特性集成到高斯原语中，采用三维高斯喷溅方法，避免了对辅助介质网络的需求。实验结果表明，WaterClear-GS在新视图合成和水下图像恢复任务中显著提高了性能，同时实现了实时渲染速度。</div>
</details>
</div>
<div class="card">
<div class="title">SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation</div>
<div class="meta-line">Authors: Helin Wang, Bowen Shi, Andros Tjandra, John Hoffman, Yi-Chiao Wu, Apoorv Vyas, Najim Dehak, Ann Lee, Wei-Ning Hsu</div>
<div class="meta-line">First: 2026-01-27T15:29:02+00:00 · Latest: 2026-01-27T15:29:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19702v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19702v1">PDF</a> · <a href="https://github.com/facebookresearch/sam-audio">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM音频评估器：用于音频分离感知评估的统一多模态框架</div>
<div class="mono" style="margin-top:8px">音频分离的性能评估仍然是一个复杂的挑战，现有的评估指标往往与人类感知不一致，粗糙且依赖于真实信号。另一方面，主观听力测试仍然是现实评估的金标准，但它们成本高、耗时且难以扩展。本文解决了对能够在没有人工干预的情况下评估音频分离的自动化系统日益增长的需求。所提出的评估指标SAM音频评估器（SAJ）是一种多模态细粒度无参考客观指标，与人类感知高度一致。SAJ支持三个音频领域（语音、音乐和一般声音事件）和三种提示输入（文本、视觉和跨度），涵盖四个不同的评估维度（召回率、精确度、忠实度和整体）。SAM音频评估器还显示出在数据过滤、伪标记大型数据集和音频分离模型中的重新排序等潜在应用。我们在以下网址发布了我们的代码和预训练模型：https://github.com/facebookresearch/sam-audio。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of performance evaluation in audio separation, where existing metrics often fail to align with human perception and rely heavily on ground truth signals. The authors propose the SAM Audio Judge (SAJ), a unified multimodal framework that serves as a fine-grained, reference-free objective metric for evaluating audio separation across three domains: speech, music, and general sound events. Experimental results demonstrate that SAJ aligns closely with human evaluations and can be applied in various contexts, including data filtering and pseudo-labeling, thus providing a scalable alternative to traditional subjective listening tests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善音频分离的评估，现有评估指标往往与人类感知不一致，并且过于依赖真实信号。作者提出了一种新的评估指标，称为SAM Audio Judge (SAJ)，这是一个统一的多模态框架，能够在没有人工干预的情况下运行，并与人类感知高度一致。实验结果表明，SAJ能够有效评估三个领域的音频分离——语音、音乐和一般声音事件，使用各种输入提示，并涵盖回忆、精确度、真实性和整体质量等维度，表明其在数据过滤和模型重新排序等应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Problem Space Transformations for Out-of-Distribution Generalisation in Behavioural Cloning</div>
<div class="meta-line">Authors: Kiran Doshi, Marco Bagatella, Stelian Coros</div>
<div class="meta-line">First: 2024-11-06T17:05:58+00:00 · Latest: 2026-01-27T14:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.04056v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.04056v3">PDF</a> · <a href="https://github.com/kirandoshi/pst_ood_gen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combination of behavioural cloning and neural networks has driven significant progress in robotic manipulation. As these algorithms may require a large number of demonstrations for each task of interest, they remain fundamentally inefficient in complex scenarios, in which finite datasets can hardly cover the state space. One of the remaining challenges is thus out-of-distribution (OOD) generalisation, i.e. the ability to predict correct actions for states with a low likelihood with respect to the state occupancy induced by the dataset. This issue is aggravated when the system to control is treated as a black-box, ignoring its physical properties. This work highlights widespread properties of robotic manipulation, specifically pose equivariance and locality. We investigate the effect of the choice of problem space on OOD performance of BC policies and how transformations arising from characteristic properties of manipulation can be employed for its improvement. Through controlled, simulated and real-world experiments, we empirically demonstrate that these transformations allow behaviour cloning policies, using either standard MLP-based one-step action prediction or diffusion-based action-sequence prediction, to generalise better to certain OOD problem instances. Code is available at https://github.com/kirandoshi/pst_ood_gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为克隆中的分布外泛化问题空间变换</div>
<div class="mono" style="margin-top:8px">行为克隆与神经网络的结合推动了机器人操作的显著进展。由于这些算法可能需要大量示例来完成每个感兴趣的任务，因此在复杂场景中，它们在根本上仍然效率低下，因为有限的数据集几乎无法覆盖状态空间。因此，剩下的一个挑战是分布外（OOD）泛化，即预测在数据集引起的状态占用下，低概率状态的正确动作的能力。当控制的系统被视为黑箱，忽略其物理特性时，这一问题更加严重。本研究强调了机器人操作的广泛特性，特别是姿态等变性和局部性。我们研究了问题空间的选择对BC策略的OOD性能的影响，以及如何利用操作的特征属性所产生的变换来改善其性能。通过受控的模拟和现实世界实验，我们实证证明这些变换使得行为克隆策略，无论是使用标准的基于MLP的一步动作预测还是基于扩散的动作序列预测，都能更好地泛化到某些OOD问题实例。代码可在 https://github.com/kirandoshi/pst_ood_gen 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of out-of-distribution (OOD) generalisation in behavioural cloning for robotic manipulation, which is hindered by the need for extensive demonstrations and the limitations of finite datasets. The authors explore how the choice of problem space and the inherent properties of robotic manipulation, such as pose equivariance and locality, can enhance OOD performance. Through a series of controlled experiments in both simulated and real-world settings, they demonstrate that applying these transformations improves the generalisation capabilities of behaviour cloning policies, whether using standard MLP-based action prediction or diffusion-based action-sequence prediction.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决行为克隆算法在机器人操作中的低效性，特别是由于数据集有限而导致的分布外（OOD）泛化问题。作者研究了问题空间变换对行为克隆策略OOD性能的影响，重点关注姿态等变性和局部性等特性。通过在模拟和真实环境中进行的一系列控制实验，研究发现这些变换显著增强了行为克隆策略的泛化能力，无论是使用标准的多层感知器（MLP）动作预测还是基于扩散的动作序列预测。</div>
</details>
</div>
<div class="card">
<div class="title">KeepLoRA: Continual Learning with Residual Gradient Adaptation</div>
<div class="meta-line">Authors: Mao-Lin Luo, Zi-Hao Zhou, Yi-Lin Zhang, Yuanyu Wan, Tong Wei, Min-Ling Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T14:38:57+00:00 · Latest: 2026-01-27T14:38:57+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19659v1">PDF</a> · <a href="https://github.com/MaolinLuo/KeepLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KeepLoRA：带残差梯度适应的持续学习</div>
<div class="mono" style="margin-top:8px">针对预训练视觉-语言模型的持续学习需要平衡三个相互竞争的目标：保留预训练知识、保存一系列学习任务的知识，以及保持获取新知识的可塑性。本文提出了一种简单但有效的方法，称为KeepLoRA，以有效平衡这些目标。我们首先分析模型参数空间中的知识保留机制，发现一般知识主要编码在主子空间中，而任务特定知识则编码在残差子空间中。基于这一发现，KeepLoRA通过限制残差子空间中的LoRA参数更新来学习新任务，以防干扰先前学习的能力。具体而言，我们通过将新任务的梯度投影到与预训练模型的主子空间和先前任务特征的主导方向正交的子空间中来注入新任务的知识。我们的理论和实证分析确认了KeepLoRA平衡这三个目标并实现了最先进的性能。实现代码可在https://github.com/MaolinLuo/KeepLoRA获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of continual learning in pre-trained vision-language models, specifically the need to retain pre-trained knowledge while also accommodating new tasks. The authors propose a method called KeepLoRA, which restricts updates to the LoRA parameters in the residual subspace to avoid disrupting previously acquired knowledge. Experimental results demonstrate that KeepLoRA effectively balances the competing objectives of knowledge retention, task preservation, and plasticity, achieving state-of-the-art performance in continual learning scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决预训练视觉-语言模型在持续学习中的挑战，特别是需要在适应新任务的同时保留预训练知识。作者提出了一种名为KeepLoRA的方法，该方法限制对残差子空间中LoRA参数的更新，以避免干扰先前学习的任务。实验结果表明，KeepLoRA有效地平衡了对先前任务知识的保留和新知识的获取，在持续学习场景中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework</div>
<div class="meta-line">Authors: Ewelina Gajewska, Katarzyna Budzynska, Jarosław A Chudziak</div>
<div class="meta-line">First: 2026-01-14T10:20:32+00:00 · Latest: 2026-01-27T13:48:59+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09342v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09342v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过社区驱动的多智能体框架改善隐性仇恨言论检测</div>
<div class="mono" style="margin-top:8px">本研究提出了一种针对隐性仇恨言论的情境检测框架，实施为一个多智能体系统，包括一个中央调解智能体和动态构建的社区智能体，代表特定的人口群体。我们的方法明确整合了来自公开知识源的社会文化背景，实现了超越最先进提示方法（零样本提示、少样本提示、思维链提示）和其他方法的身份感知调解，在具有挑战性的ToxiGen数据集上表现优异。我们通过将平衡准确率作为分类公平性的核心指标，增强了性能评估的技术严谨性，考虑了真正阳性和真正阴性率之间的权衡。我们证明了我们的社区驱动咨询框架显著提高了所有目标群体的分类准确性和公平性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of detecting implicitly hateful speech by proposing a multi-agent framework that includes a central Moderator Agent and Community Agents representing various demographic groups. The method integrates socio-cultural context from publicly available knowledge sources to enable identity-aware moderation, outperforming existing prompting methods and alternative approaches on the ToxiGen dataset. The findings indicate that this community-driven framework significantly enhances both classification accuracy and fairness across all target groups, with balanced accuracy being emphasized as a key metric for evaluating classification fairness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过将社会文化背景融入审查过程来增强对隐性仇恨言论的检测。作者提出了一个多代理框架，包括一个中央审核代理和代表不同人口群体的社区代理，从而实现身份意识的审查。在ToxiGen数据集上的实验结果表明，与现有的提示方法和其他替代方案相比，这种以社区为驱动的方法在分类准确性和公平性方面显著提高，平衡准确性作为评估标准，考虑了真正例和假负例率。</div>
</details>
</div>
<div class="card">
<div class="title">DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation</div>
<div class="meta-line">Authors: Wei Pan, Huiguo He, Hiuyi Cheng, Yilin Shi, Lianwen Jin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-28T03:58:15+00:00 · Latest: 2026-01-27T09:19:26+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23624v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23624v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep generative models have advanced text-to-online handwriting generation (TOHG), which aims to synthesize realistic pen trajectories conditioned on textual input and style references. However, most existing methods still primarily focus on character- or word-level generation, resulting in inefficiency and a lack of holistic structural modeling when applied to full text lines. To address these issues, we propose DiffInk, the first latent diffusion Transformer framework for full-line handwriting generation. We first introduce InkVAE, a novel sequential variational autoencoder enhanced with two complementary latent-space regularization losses: (1) an OCR-based loss enforcing glyph-level accuracy, and (2) a style-classification loss preserving writing style. This dual regularization yields a semantically structured latent space where character content and writer styles are effectively disentangled. We then introduce InkDiT, a novel latent diffusion Transformer that integrates target text and reference styles to generate coherent pen trajectories. Experimental results demonstrate that DiffInk outperforms existing state-of-the-art methods in both glyph accuracy and style fidelity, while significantly improving generation efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffInk：一种基于字形和风格的潜在扩散变换器，用于文本到在线手写生成</div>
<div class="mono" style="margin-top:8px">深度生成模型推动了文本到在线手写生成（TOHG）的发展，旨在合成基于文本输入和风格参考的真实笔迹轨迹。然而，大多数现有方法仍主要集中在字符或单词级别的生成，导致在应用于完整文本行时效率低下且缺乏整体结构建模。为了解决这些问题，我们提出了DiffInk，这是第一个用于完整行手写生成的潜在扩散变换器框架。我们首先介绍了InkVAE，这是一种新颖的序列变分自编码器，增强了两种互补的潜在空间正则化损失：（1）基于OCR的损失，强制执行字形级别的准确性，以及（2）保持书写风格的风格分类损失。这种双重正则化产生了一个语义结构化的潜在空间，在该空间中，字符内容和书写者风格得以有效解耦。然后，我们介绍了InkDiT，这是一种新颖的潜在扩散变换器，集成目标文本和参考风格以生成连贯的笔迹轨迹。实验结果表明，DiffInk在字形准确性和风格保真度方面均优于现有的最先进方法，同时显著提高了生成效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and structural modeling of text-to-online handwriting generation, which has been limited by existing methods focusing on character- or word-level generation. The authors propose DiffInk, a latent diffusion Transformer framework that utilizes a novel sequential variational autoencoder called InkVAE, which incorporates two complementary latent-space regularization losses for glyph-level accuracy and style preservation. Experimental results indicate that DiffInk surpasses current state-of-the-art methods in both glyph accuracy and style fidelity, while also enhancing generation efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高文本到在线手写生成的效率和结构建模能力，而现有方法主要集中在字符或单词级别的生成上，存在局限性。作者提出了DiffInk，这是一种潜在扩散变换器框架，采用了一种名为InkVAE的新型序列变分自编码器，结合了用于字形准确性和风格保留的双重正则化损失。实验结果表明，DiffInk在字形准确性和风格保真度方面超越了当前的最先进方法，同时显著提高了生成效率。</div>
</details>
</div>
<div class="card">
<div class="title">Token Caching for Diffusion Transformer Acceleration</div>
<div class="meta-line">Authors: Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Yuming Li, Chenguang Ma</div>
<div class="meta-line">First: 2024-09-27T08:05:34+00:00 · Latest: 2026-01-27T08:13:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.18523v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.18523v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion transformers have gained substantial interest in diffusion generative modeling due to their outstanding performance. However, their computational demands, particularly the quadratic complexity of attention mechanisms and multi-step inference processes, present substantial bottlenecks that limit their practical applications. To address these challenges, we propose TokenCache, a novel acceleration method that leverages the token-based multi-block architecture of transformers to reduce redundant computations. TokenCache tackles three critical questions: (1) Which tokens should be pruned and reused by the caching mechanism to eliminate redundancy? (2) Which blocks should be targeted for efficient caching? (3) At which time steps should caching be applied to balance speed and quality? In response to these challenges, TokenCache introduces a Cache Predictor that hierarchically addresses these issues by (1) Token pruning: assigning importance scores to each token to determine which tokens to prune and reuse; (2) Block selection: allocating pruning ratio to each block to adaptively select blocks for caching; (3) Temporal Scheduling: deciding at which time steps to apply caching strategies. Experimental results across various models demonstrate that TokenCache achieves an effective trade-off between generation quality and inference speed for diffusion transformers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器加速的令牌缓存</div>
<div class="mono" style="margin-top:8px">扩散变换器因其卓越的性能在扩散生成建模中引起了广泛关注。然而，它们的计算需求，特别是注意机制的平方复杂性和多步推理过程，构成了限制其实际应用的重大瓶颈。为了解决这些挑战，我们提出了TokenCache，这是一种新颖的加速方法，利用变换器的基于令牌的多块架构来减少冗余计算。TokenCache解决了三个关键问题：（1）缓存机制应修剪和重用哪些令牌以消除冗余？（2）应针对哪些块进行高效缓存？（3）在何时应用缓存以平衡速度和质量？为应对这些挑战，TokenCache引入了一个缓存预测器，分层解决这些问题：（1）令牌修剪：为每个令牌分配重要性分数，以确定修剪和重用哪些令牌；（2）块选择：为每个块分配修剪比例，以自适应选择缓存块；（3）时间调度：决定在何时应用缓存策略。各种模型的实验结果表明，TokenCache在生成质量和推理速度之间实现了有效的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the high computational demands of diffusion transformers, particularly due to the quadratic complexity of their attention mechanisms and multi-step inference, which hinder their practical use. To mitigate these issues, the authors propose TokenCache, an acceleration method that optimizes the token-based multi-block architecture of transformers by reducing redundant computations. Experimental results indicate that TokenCache effectively balances generation quality and inference speed across various models by implementing a Cache Predictor that addresses token pruning, block selection, and temporal scheduling for caching strategies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散变换器的高计算需求，特别是由于其注意力机制的平方复杂性和多步推理过程，这限制了其实际应用。作者提出了一种名为TokenCache的新型加速方法，利用基于令牌的多块架构来减少冗余计算。实验结果表明，TokenCache通过采用缓存预测器有效地平衡了生成质量和推理速度，该预测器确定了应修剪哪些令牌、应缓存哪些块以及何时应用这些缓存策略。</div>
</details>
</div>
<div class="card">
<div class="title">Astra: General Interactive World Model with Autoregressive Denoising</div>
<div class="meta-line">Authors: Yixuan Zhu, Jiaqi Feng, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-09T18:59:57+00:00 · Latest: 2026-01-27T07:42:06+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026. Code is available at: https://github.com/EternalEvan/Astra</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08931v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.08931v3">PDF</a> · <a href="https://github.com/EternalEvan/Astra">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Astra：具有自回归去噪的通用互动世界模型</div>
<div class="mono" style="margin-top:8px">最近在扩散变换器方面的进展使视频生成模型能够从文本或图像生成高质量的视频片段。然而，能够从过去的观察和行动中预测长期未来的世界模型仍然未被充分探索，特别是在通用场景和各种形式的行动中。为了解决这一问题，我们引入了Astra，一个互动通用世界模型，能够为多样化场景（例如，自动驾驶、机器人抓取）生成真实世界的未来，并实现精确的行动交互（例如，相机运动、机器人动作）。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察并支持流式输出。我们使用噪声增强的历史记忆，以避免过度依赖过去的帧，从而平衡响应性与时间一致性。为了实现精确的行动控制，我们引入了一种行动感知适配器，直接将行动信号注入去噪过程。我们进一步开发了一种行动专家混合体，动态路由异构行动模式，增强在探索、操作和相机控制等多样化现实任务中的通用性。Astra实现了互动、一致和通用的长期视频预测，并支持各种形式的交互。多个数据集的实验表明，Astra在保真度、长期预测和行动对齐方面优于现有的最先进世界模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance world models capable of predicting long-term futures from past observations and actions, particularly in general-purpose scenarios. The authors introduce Astra, an interactive general world model that employs an autoregressive denoising architecture and temporal causal attention to effectively generate real-world futures for diverse applications such as autonomous driving and robot grasping. Key experimental results indicate that Astra significantly improves fidelity, long-range prediction, and action alignment compared to existing state-of-the-art models, demonstrating its effectiveness in handling various forms of interactions and enhancing versatility in real-world tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强能够从过去观察和动作中预测长期未来的世界模型，特别是在通用场景中。作者提出了Astra，这是一种交互式通用世界模型，采用自回归去噪架构和时间因果注意力相结合，生成适用于自主驾驶和机器人抓取等多种应用的真实世界未来。关键实验结果表明，Astra在保真度、长期预测和动作对齐方面显著优于现有的最先进世界模型，证明了其在处理各种交互和任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment</div>
<div class="meta-line">Authors: Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma, Sheng Yang</div>
<div class="meta-line">First: 2026-01-27T06:30:32+00:00 · Latest: 2026-01-27T06:30:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19247v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19247v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TIGaussian：解耦高斯分布以实现空间感知的文本-图像-3D对齐</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型在文本和图像之间建立了深刻的特征联系，但结合3D模态数据（如点云和3D高斯分布）进一步增强了3D相关任务的预训练能力，例如跨模态检索、零样本分类和场景识别。由于在提取3D模态特征和弥合不同模态之间的差距方面仍然存在挑战，我们提出了TIGaussian，一个利用3D高斯点云（3DGS）特性通过多分支3DGS标记器和特定模态的3D特征对齐策略来增强跨模态对齐的框架。具体而言，我们的多分支3DGS标记器将3DGS结构的内在属性解耦为紧凑的潜在表示，从而实现更具通用性的特征提取。为了进一步弥合模态差距，我们开发了双向跨模态对齐策略：一种多视角特征融合机制，利用扩散先验解决图像-3D对齐中的视角歧义，而文本-3D投影模块自适应地将3D特征映射到文本嵌入空间，以实现更好的文本-3D对齐。在各种数据集上的广泛实验表明，TIGaussian在多个任务中表现出最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the alignment between text, images, and 3D data, addressing the challenges in extracting 3D modal features for tasks like cross-modal retrieval and scene recognition. The authors propose TIGaussian, a framework that utilizes 3D Gaussian Splatting characteristics, incorporating a multi-branch 3DGS tokenizer and modality-specific alignment strategies to improve cross-modality alignment. Experimental results show that TIGaussian achieves state-of-the-art performance across various datasets in multiple tasks, demonstrating its effectiveness in bridging the gap between different modalities.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于增强3D模态数据与视觉语言模型的整合，以提高跨模态检索和场景识别等任务的性能。作者提出了TIGaussian框架，利用3D高斯喷溅特性，采用多分支3DGS标记器创建紧凑的潜在表示，并实施特定模态的对齐策略。实验结果表明，TIGaussian在多个数据集的多项任务中实现了最先进的性能，有效地弥合了不同模态之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics</div>
<div class="meta-line">Authors: Zhiyu Yin, Zhipeng Liu, Kehai Chen, Lemao Liu, Jin Liu, Hong-Dong Li, Yang Xiang, Min Zhang</div>
<div class="meta-line">First: 2026-01-27T06:15:12+00:00 · Latest: 2026-01-27T06:15:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19236v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VC-Bench：开创视频连接基准的 dataset 和评估指标</div>
<div class="mono" style="margin-top:8px">当前视频生成主要关注文本或图像条件，而实际应用如视频编辑和视频博客常常需要无缝连接不同的片段。在我们的工作中，我们引入了视频连接这一创新任务，旨在生成给定起始和结束片段之间的平滑中间视频内容。然而，缺乏标准化的评估基准阻碍了该任务的发展。为了解决这一问题，我们提出了 VC-Bench，这是一个专门为视频连接设计的新基准。它包括从公共平台收集的 1,579 个高质量视频，涵盖 15 个主要类别和 72 个子类别，以确保多样性和结构性。VC-Bench 关注三个核心方面：视频质量评分 VQS、起始-结束一致性评分 SECS 和过渡平滑度评分 TSS。它们共同构成了一个超越传统仅关注质量的综合框架。我们在 VC-Bench 上评估了多种最先进的视频生成模型。实验结果揭示了在保持起始-结束一致性和过渡平滑度方面的显著局限性，导致整体连贯性和流畅性降低。我们期望 VC-Bench 能作为一个开创性的基准，激励和指导未来的视频连接研究。评估指标和数据集可在以下网址公开获取：https://anonymous.4open.science/r/VC-Bench-1B67/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the need for seamless video connections in practical applications such as video editing and vlogging, which current video generation methods do not adequately support. The authors introduce VC-Bench, a benchmark specifically designed for the new task of Video Connecting, which includes a dataset of 1,579 high-quality videos across various categories and subcategories. Experimental evaluations of multiple state-of-the-art video generation models on VC-Bench demonstrate significant shortcomings in maintaining start-end consistency and transition smoothness, resulting in lower coherence and fluidity in the generated videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视频编辑和视频博客等实际应用中对无缝视频连接的需求，而当前的视频生成方法未能充分满足这一需求。作者提出了VC-Bench，这是一个专为视频连接任务设计的新基准，包含1579个来自不同类别和子类别的高质量视频数据集，并根据视频质量评分、起止一致性评分和过渡平滑度评分来评估视频生成模型。实验结果表明，现有的最先进视频生成模型在保持起止一致性和过渡平滑度方面存在显著局限，导致生成视频的连贯性和流畅性较低，突显了该领域改进方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models</div>
<div class="meta-line">Authors: Nikita Kuzmin, Songting Liu, Kong Aik Lee, Eng Siong Chng</div>
<div class="meta-line">First: 2026-01-20T13:23:44+00:00 · Latest: 2026-01-27T05:25:34+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13948v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Stream-Voice-Anon：通过神经音频编解码器和语言模型增强实时说话者匿名化的实用性</div>
<div class="mono" style="margin-top:8px">保护说话者身份对在线语音应用至关重要，但流式说话者匿名化（SA）仍然未被充分探索。最近的研究表明，神经音频编解码器（NAC）提供了优越的说话者特征解耦和语言保真度。NAC还可以与因果语言模型（LM）结合使用，以增强语言保真度和流式任务的提示控制。然而，现有的基于NAC的在线LM系统是为语音转换（VC）而设计的，缺乏隐私保护所需的技术。在这些进展的基础上，我们提出了Stream-Voice-Anon，它通过整合匿名化技术，专门为流式SA调整现代基于因果LM的NAC架构。我们的匿名化方法结合了伪说话者表示采样、说话者嵌入混合和多样的提示选择策略，以利用量化内容代码的解耦特性，防止说话者信息泄露。此外，我们比较了动态和固定延迟配置，以探索实时场景中的延迟-隐私权衡。在VoicePrivacy 2024挑战协议下，Stream-Voice-Anon在可懂度（相对WER减少高达46%）和情感保留（相对UAR高达28%）方面相比于之前的最先进流式方法DarkStream取得了显著改善，同时保持了可比的延迟（180ms对比200ms）和对懒惰知情攻击者的隐私保护，尽管在对半知情攻击者的情况下显示出15%的相对降级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance speaker anonymization in real-time voice applications, which is crucial for protecting speaker identity but remains underexplored. The authors propose Stream-Voice-Anon, which adapts neural audio codec (NAC) architectures combined with causal language models (LM) to improve streaming speaker anonymization by integrating techniques that prevent speaker information leakage. Experimental results demonstrate that Stream-Voice-Anon significantly improves intelligibility with up to a 46% relative reduction in word error rate and emotion preservation with a 28% relative increase in unweighted average recall, while maintaining comparable latency and effective privacy protection against lazy-informed attackers, although it shows a 15% relative degradation against semi-informed attackers.</div>
<div class="mono" style="margin-top:8px">本研究的动机是增强在线语音应用中的说话人匿名化，以保护说话人的身份。作者开发了Stream-Voice-Anon系统，该系统通过整合伪说话人表示采样和说话人嵌入混合等技术，适应了实时说话人匿名化的神经音频编解码器（NAC）架构。实验结果表明，Stream-Voice-Anon在可懂度方面显著提高，词错误率相对降低46%，情感保留方面相对提高28%的无加权准确率，相较于之前的方法DarkStream，同时保持了相似的延迟和对懒惰知情攻击者的有效隐私保护。</div>
</details>
</div>
<div class="card">
<div class="title">Text2Grad: Reinforcement Learning from Natural Language Feedback</div>
<div class="meta-line">Authors: Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-28T13:23:49+00:00 · Latest: 2026-01-27T04:31:51+00:00</div>
<div class="meta-line">Comments: The code for our method is available at https://github.com/microsoft/Text2Grad</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22338v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.22338v2">PDF</a> · <a href="https://github.com/microsoft/Text2Grad">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model&#x27;s policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answers while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results suggest that natural-language feedback can serve not only as explanations, but also as actionable training signals for fine-grained alignment. The code for our method is available at https://github.com/microsoft/Text2Grad.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Text2Grad：来自自然语言反馈的强化学习</div>
<div class="mono" style="margin-top:8px">传统的RLHF使用粗糙的标量奖励来优化语言模型，这掩盖了成功或失败背后的细微原因，导致学习缓慢且不透明。最近的工作通过提示或反思增强了RL与文本批评的结合，提高了可解释性，但未触及模型参数。我们提出了Text2Grad，这是一种强化学习范式，将自由形式的文本反馈转化为跨度级梯度。给定人类（或程序化）批评，Text2Grad将每个反馈短语与相关的标记跨度对齐，将这些对齐转换为可微分的奖励信号，并执行梯度更新，直接优化模型策略中有问题的部分。这产生了精确的、基于反馈的调整，而不是全局的微调。Text2Grad通过三个组件实现：（1）高质量的反馈注释管道，将批评与标记跨度配对；（2）细粒度的奖励模型，在生成解释性批评的同时预测答案的跨度级奖励；（3）一个跨度级策略优化器，反向传播自然语言梯度。在摘要、代码生成和问答任务中，Text2Grad始终超越标量奖励RL和仅提示基线，提供更高的任务指标和更丰富的可解释性。我们的结果表明，自然语言反馈不仅可以作为解释，还可以作为细粒度对齐的可操作训练信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reinforcement learning from human feedback (RLHF) process, which traditionally relies on coarse rewards that obscure the reasons for model performance. The authors introduce Text2Grad, a novel approach that transforms free-form textual feedback into span-level gradients, allowing for more precise updates to model parameters. Experimental results demonstrate that Text2Grad outperforms traditional scalar-reward RL and prompt-only methods across various tasks, including summarization, code generation, and question answering, leading to improved task metrics and greater interpretability of the model&#x27;s adjustments based on natural language feedback.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善传统的基于人类反馈的强化学习（RLHF）过程，该过程通常依赖于模糊的奖励，掩盖了模型性能的原因。作者提出了Text2Grad，这是一种新颖的强化学习方法，将自由形式的文本反馈转化为跨度级梯度，从而实现更精确的模型更新。实验结果表明，Text2Grad在摘要生成、代码生成和问答等多项任务中均优于传统的标量奖励RL和仅基于提示的方法，提升了任务性能和模型调整的可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Gulfs in UI Generation through Semantic Guidance</div>
<div class="meta-line">Authors: Seokhyeon Park, Soohyun Lee, Eugene Choi, Hyunwoo Kim, Minkyu Kweon, Yumin Song, Jinwook Seo</div>
<div class="meta-line">First: 2026-01-27T04:01:53+00:00 · Latest: 2026-01-27T04:01:53+00:00</div>
<div class="meta-line">Comments: In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While generative AI enables high-fidelity UI generation from text prompts, users struggle to articulate design intent and evaluate or refine results-creating gulfs of execution and evaluation. To understand the information needed for UI generation, we conducted a thematic analysis of UI prompting guidelines, identifying key design semantics and discovering that they are hierarchical and interdependent. Leveraging these findings, we developed a system that enables users to specify semantics, visualize relationships, and extract how semantics are reflected in generated UIs. By making semantics serve as an intermediate representation between human intent and AI output, our system bridges both gulfs by making requirements explicit and outcomes interpretable. A comparative user study suggests that our approach enhances users&#x27; perceived control over intent expression, outcome interpretation, and facilitates more predictable, iterative refinement. Our work demonstrates how explicit semantic representation enables systematic and explainable exploration of design possibilities in AI-driven UI design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义指导弥合UI生成中的鸿沟</div>
<div class="mono" style="margin-top:8px">尽管生成性AI能够从文本提示中生成高保真UI，但用户在表达设计意图和评估或完善结果时面临困难，从而产生执行和评估的鸿沟。为了理解UI生成所需的信息，我们对UI提示指南进行了主题分析，识别出关键设计语义，并发现它们是层次化和相互依赖的。利用这些发现，我们开发了一个系统，使用户能够指定语义、可视化关系，并提取语义在生成的UI中如何体现。通过使语义作为人类意图与AI输出之间的中介表示，我们的系统弥合了这两个鸿沟，使需求明确，结果可解释。比较用户研究表明，我们的方法增强了用户对意图表达、结果解释的感知控制，并促进了更可预测的迭代完善。我们的工作展示了明确的语义表示如何在AI驱动的UI设计中实现系统化和可解释的设计可能性探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges users face in articulating design intent and evaluating AI-generated user interfaces (UIs), which creates gaps in execution and evaluation. The authors conducted a thematic analysis of UI prompting guidelines to identify hierarchical and interdependent design semantics, which informed the development of a system that allows users to specify semantics, visualize relationships, and understand how these semantics are represented in generated UIs. The results from a comparative user study indicate that this approach improves users&#x27; perceived control over expressing intent and interpreting outcomes, leading to more predictable and iterative refinement in AI-driven UI design.</div>
<div class="mono" style="margin-top:8px">本研究解决了用户在表达设计意图和评估AI生成用户界面（UI）时面临的挑战，这导致了执行和评估之间的显著差距。作者通过对UI提示指南进行主题分析，识别出层次性和相互依赖的设计语义，并据此开发了一个系统，使用户能够指定语义、可视化关系，并理解这些语义如何在生成的UI中体现。比较用户研究表明，这种方法提高了用户在表达意图和解释结果方面的控制感，从而在AI驱动的UI设计中实现了更可预测和迭代的改进。</div>
</details>
</div>
<div class="card">
<div class="title">GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution</div>
<div class="meta-line">Authors: Yongsong Huang, Tzu-Hsuan Peng, Tomo Miyazaki, Xiaofeng Liu, Chun-Ting Chou, Ai-Chun Pang, Shinichiro Omachi</div>
<div class="meta-line">First: 2026-01-27T03:43:39+00:00 · Latest: 2026-01-27T03:43:39+00:00</div>
<div class="meta-line">Comments: \c{opyright} 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTFMN：用于低光照图像增强和超分辨率的引导纹理和特征调制网络</div>
<div class="mono" style="margin-top:8px">低光照图像超分辨率（LLSR）是一项具有挑战性的任务，因为低分辨率和光照不足的耦合退化。为了解决这个问题，我们提出了引导纹理和特征调制网络（GTFMN），这是一个新颖的框架，将LLSR任务解耦为两个子问题：光照估计和纹理恢复。首先，我们的网络采用专用的光照流，其目的是预测一个空间变化的光照图，该图准确捕捉光照分布。此外，该图作为我们新颖的光照引导调制块（IGM块）中的显式指导，用于动态调制纹理流中的特征。该机制实现了空间自适应恢复，使网络能够在光照不足的区域增强效果，同时保留光照良好的区域的细节。大量实验表明，GTFMN在OmniNormal5和OmniNormal15数据集上实现了最佳性能，在定量指标和视觉质量上均优于竞争方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenges of low-light image super-resolution (LLSR), which suffers from both low resolution and poor illumination. The authors propose the Guided Texture and Feature Modulation Network (GTFMN), which separates the LLSR task into illumination estimation and texture restoration. The network features an Illumination Stream that predicts a spatially varying illumination map, which is then used in the Illumination Guided Modulation Block to adaptively enhance poorly lit areas while preserving details in well-exposed regions. Experimental results show that GTFMN outperforms existing methods on the OmniNormal5 and OmniNormal15 datasets in both quantitative metrics and visual quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决低光照图像超分辨率（LLSR）面临的低分辨率和光照不足的综合问题。作者提出了引导纹理和特征调制网络（GTFMN），该框架将LLSR任务分为光照估计和纹理恢复两个子问题。该网络包括一个光照流，用于预测空间变化的光照图，该图通过光照引导调制块引导纹理流，从而在暗区实现自适应增强，同时保持光照良好的区域的细节。实验结果表明，GTFMN在OmniNormal5和OmniNormal15数据集上在定量指标和视觉质量方面均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction</div>
<div class="meta-line">Authors: Zhicheng Zhang, Zhaocheng Du, Jieming Zhu, Jiwei Tang, Fengyuan Lu, Wang Jiaheng, Song-Li Wu, Qianhui Zhu, Jingyu Li, Hai-Tao Zheng, Zhenhua Dong</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-27T03:14:20+00:00 · Latest: 2026-01-27T03:14:20+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19142v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models paradoxically degrades performance for short-sequence users due to attention polarization and length imbalance in training data. To address this, we propose LAIN(Length-Adaptive Interest Network), a plug-and-play framework that explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling. LAIN consists of three lightweight components: a Spectral Length Encoder that maps length into continuous representations, Length-Conditioned Prompting that injects global contextual cues into both long- and short-term behavior branches, and Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length. Extensive experiments on three real-world benchmarks across five strong CTR backbones show that LAIN consistently improves overall performance, achieving up to 1.15% AUC gain and 2.25% log loss reduction. Notably, our method significantly improves accuracy for short-sequence users without sacrificing longsequence effectiveness. Our work offers a general, efficient, and deployable solution to mitigate length-induced bias in sequential recommendation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长度自适应兴趣网络用于平衡CTR预测中的长短序列建模</div>
<div class="mono" style="margin-top:8px">现代推荐系统中的用户行为序列表现出显著的长度异质性，从稀疏的短期交互到丰富的长期历史。虽然较长的序列提供了更多的上下文，但我们观察到，在现有的CTR模型中，增加最大输入序列长度反而会因注意力极化和训练数据中的长度不平衡而降低短序列用户的性能。为了解决这个问题，我们提出了LAIN（长度自适应兴趣网络），这是一个即插即用的框架，明确将序列长度作为条件信号，以平衡长短序列建模。LAIN由三个轻量级组件组成：一个将长度映射为连续表示的谱长度编码器，注入全局上下文线索到长短期行为分支的长度条件提示，以及根据序列长度自适应调整注意力锐度的长度调制注意力。在五个强大的CTR骨干网络上对三个真实世界基准的广泛实验表明，LAIN始终提高整体性能，AUC提升高达1.15%，对数损失减少2.25%。值得注意的是，我们的方法显著提高了短序列用户的准确性，而不牺牲长序列的有效性。我们的工作提供了一种通用、高效且可部署的解决方案，以减轻序列推荐中的长度引起的偏差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the performance degradation in click-through rate (CTR) prediction models caused by the length heterogeneity of user behavior sequences, particularly affecting short-sequence users. The authors propose the Length-Adaptive Interest Network (LAIN), a framework that incorporates sequence length as a conditioning signal to balance the modeling of both long and short sequences. Experimental results on three real-world benchmarks demonstrate that LAIN improves overall performance, achieving up to a 1.15% increase in AUC and a 2.25% reduction in log loss, while significantly enhancing accuracy for short-sequence users without compromising the effectiveness for long sequences.</div>
<div class="mono" style="margin-top:8px">本研究解决了推荐系统中用户行为序列长度异质性的问题，较长的序列可能因注意力极化而对短序列用户的性能产生负面影响。为此，作者提出了长度自适应兴趣网络（LAIN），该网络将序列长度作为条件信号，增强对长短序列的建模。通过在三个真实世界基准上的实验结果表明，LAIN提高了整体性能，AUC提升了1.15%，对数损失降低了2.25%，特别是在不影响长序列效果的情况下，显著改善了短序列用户的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Unsupervised Semantic-Aware Exposure Correction</div>
<div class="meta-line">Authors: Puzhen Wu, Han Weng, Quan Zheng, Yi Zhan, Hewei Wang, Yiming Li, Jiahui Han, Rui Xu</div>
<div class="meta-line">First: 2026-01-27T02:53:18+00:00 · Latest: 2026-01-27T02:53:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19129v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP引导的无监督语义感知曝光校正</div>
<div class="mono" style="margin-top:8px">不当曝光常导致严重的细节丢失、颜色失真和对比度降低。曝光校正仍面临两个关键挑战：（1）忽视对象区域的语义信息导致颜色偏移伪影；（2）现实世界的曝光图像通常没有真实标签，其标注需要大量手动编辑。为了解决这些挑战，我们提出了一种新的无监督语义感知曝光校正网络。它包含一个自适应语义感知融合模块，有效地将从预训练的快速分割任意模型中提取的语义信息融合到共享的图像特征空间中。然后，融合的特征被我们的多尺度残差空间mamba组用于恢复细节和调整曝光。为了避免手动编辑，我们提出了一个由CLIP引导的伪真实生成器，经过微调以自动识别曝光情况并指导定制校正。此外，我们利用来自FastSAM和CLIP的丰富先验，开发了一种语义提示一致性损失，以强制无监督训练中的语义一致性和图像提示对齐。全面的实验结果表明，我们的方法在校正现实世界的曝光图像方面有效，并在数值和视觉上超越了最先进的无监督方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of improper exposure in images, which can lead to detail loss, color distortion, and reduced contrast, particularly in the absence of ground-truth labels for real-world exposure images. The authors propose an unsupervised semantic-aware exposure correction network that integrates an adaptive semantic-aware fusion module to combine semantic information from a pre-trained Fast Segment Anything Model with image features. Key experimental findings demonstrate that this method effectively corrects exposure in real-world images and surpasses existing unsupervised techniques in both numerical and visual quality assessments.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决图像曝光不当所带来的严重细节损失和色彩失真问题。作者提出了一种无监督的语义感知曝光校正网络，该网络利用自适应语义融合模块，将来自预训练的快速分割任意模型的语义信息整合到共享图像特征空间中。主要实验结果表明，该方法能够有效恢复图像细节并调整曝光，无需人工编辑，并在对真实曝光图像的数值和视觉评估中优于现有的无监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</div>
<div class="meta-line">Authors: Chenyu Mu, Xin He, Qu Yang, Wanshun Chen, Jiadi Yao, Huang Liu, Zihao Yi, Bo Zhao, Xingyu Chen, Ruotian Ma, Fanghua Ye, Erkun Yang, Cheng Deng, Zhaopeng Tu, Xiaolong Li, Linus</div>
<div class="meta-line">First: 2026-01-25T08:10:28+00:00 · Latest: 2026-01-27T02:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17737v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17737v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap&#x27;&#x27; between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>剧本就是你所需的一切：一种用于长时段对话到电影视频生成的代理框架</div>
<div class="mono" style="margin-top:8px">最近的视频生成进展产生了能够从简单文本提示合成惊人视觉内容的模型。然而，这些模型在从对话等高层次概念生成长篇连贯叙事方面存在困难，揭示了创意想法与其电影执行之间的“语义差距”。为了解决这个问题，我们引入了一种新颖的端到端代理框架，用于对话到电影视频生成。我们框架的核心是ScripterAgent，一个训练用于将粗略对话翻译为细致可执行电影剧本的模型。为此，我们构建了ScriptBench，一个新的大规模基准，具有丰富的多模态上下文，通过专家指导的流程进行注释。生成的剧本随后指导DirectorAgent，后者使用跨场景连续生成策略协调最先进的视频模型，以确保长时段的一致性。我们的综合评估，结合了AI驱动的CriticAgent和新的视觉-剧本对齐（VSA）指标，显示我们的框架显著提高了剧本的忠实度和时间保真度，适用于所有测试的视频模型。此外，我们的分析揭示了当前SOTA模型在视觉壮观与严格剧本遵循之间的关键权衡，为未来的自动化电影制作提供了宝贵的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by existing video generation models in creating coherent long-form narratives from high-level dialogue concepts. The authors propose an end-to-end agentic framework for dialogue-to-cinematic video generation, which includes a model called ScripterAgent that translates coarse dialogue into detailed cinematic scripts. The framework is evaluated using a new benchmark, ScriptBench, and incorporates a DirectorAgent that utilizes a continuous generation strategy to maintain coherence across scenes. Experimental results demonstrate that this framework significantly enhances script faithfulness and temporal fidelity in video generation, while also revealing a trade-off between visual appeal and adherence to the script in current state-of-the-art models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有视频生成模型在从高层次对话概念创建连贯的长篇叙事时所面临的挑战。作者提出了一种端到端的代理框架，包括将粗略对话翻译为详细电影剧本的ScripterAgent，以及利用这些剧本指导视频生成并关注长时间一致性的DirectorAgent。实验结果表明，该框架显著提高了各种视频模型的剧本忠实度和时间一致性，同时揭示了当前最先进模型在视觉吸引力和剧本遵循之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">A Systemic Evaluation of Multimodal RAG Privacy</div>
<div class="meta-line">Authors: Ali Al-Lawati, Suhang Wang</div>
<div class="meta-line">First: 2026-01-25T01:37:01+00:00 · Latest: 2026-01-27T02:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17644v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态RAG隐私的系统评估</div>
<div class="mono" style="margin-top:8px">多模态检索增强生成（mRAG）管道在视觉中心任务（例如视觉问答）中的日益采用带来了重要的隐私挑战。特别是，虽然mRAG提供了将私有数据集连接以提高模型性能的实用能力，但在推理过程中，它有泄露这些数据集中的私有信息的风险。本文通过标准模型提示进行实证研究，分析mRAG管道中固有的隐私风险。具体而言，我们实施了一个案例研究，试图推断mRAG中是否包含视觉资产，例如图像，并在存在的情况下泄露与之相关的元数据，例如标题。我们的研究结果强调了隐私保护机制的必要性，并激励未来对mRAG隐私的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of multimodal Retrieval-Augmented Generation (mRAG) systems for tasks like visual question answering raises significant privacy concerns due to the potential leakage of private information during inference. This study employs an empirical approach to evaluate the privacy risks associated with mRAG pipelines by analyzing standard model prompting. The results reveal that it is possible to infer the presence of visual assets and extract related metadata, underscoring the necessity for developing privacy-preserving mechanisms in future mRAG research.</div>
<div class="mono" style="margin-top:8px">随着多模态增强检索生成（mRAG）系统在视觉相关任务中的广泛应用，隐私问题变得尤为重要，特别是在推理过程中可能泄露私人信息的风险。本研究采用实证方法，调查与mRAG管道相关的隐私风险，通过实施案例研究，旨在确定视觉资产（如图像）是否包含在mRAG中，并识别任何相关的元数据（如标题）。结果强调了在mRAG系统中实施隐私保护机制的必要性，并突显了该领域进一步研究的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation</div>
<div class="meta-line">Authors: Xiang Gao, Yunpeng Jia</div>
<div class="meta-line">First: 2026-01-27T02:39:20+00:00 · Latest: 2026-01-27T02:39:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19115v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FBSDiff++：改进的频带替代扩散特征用于高效且高度可控的文本驱动图像到图像翻译</div>
<div class="mono" style="margin-top:8px">随着大规模文本到图像（T2I）扩散模型在开放领域图像创作中取得显著进展，越来越多的关注集中在其自然扩展到文本驱动的图像到图像（I2I）翻译领域，其中源图像作为视觉指导，辅助文本提示提供的文本指导。我们提出了FBSDiff，一个新颖的框架，从全新的频域视角将现成的T2I扩散模型适配到I2I范式。通过动态频带替代扩散特征，FBSDiff以即插即用的方式实现多功能且高度可控的文本驱动I2I（无需模型训练、微调或在线优化），允许通过逐步替代潜在扩散特征的低频带、中频带和高频带，实现外观引导、布局引导和轮廓引导的I2I翻译。此外，FBSDiff灵活地通过调节替代频带的带宽，持续控制I2I相关强度。为了进一步提升图像翻译的效率、灵活性和功能性，我们提出了FBSDiff++，主要在三个方面改进FBSDiff：（1）通过优化模型架构大幅加速推理速度（推理速度提升8.9倍）；（2）改进频带替代模块，允许任意分辨率和纵横比的输入源图像；（3）扩展模型功能，仅需对核心方法进行微调，即可实现局部图像操作和特定风格内容创作。大量定性和定量实验验证了FBSDiff++在I2I翻译视觉质量、效率、多功能性和可控性方面优于相关先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the advancements in text-to-image diffusion models and their potential application in text-driven image-to-image translation, where a source image guides the output. The authors propose FBSDiff, a framework that adapts existing text-to-image models for image-to-image tasks using a frequency-domain approach, allowing for dynamic frequency band substitution of diffusion features. The key findings demonstrate that FBSDiff++ significantly enhances inference speed by 8.9 times, accommodates arbitrary input resolutions, and supports localized image manipulation, with experimental results showing improved visual quality, efficiency, versatility, and controllability over existing methods.</div>
<div class="mono" style="margin-top:8px">该研究的动机是基于文本到图像扩散模型的进展及其在文本驱动的图像到图像翻译中的潜在应用。作者提出了FBSDiff框架，该框架采用频域方法将现有的文本到图像模型适应于图像到图像翻译，允许动态频带替换扩散特征。主要发现表明，FBSDiff++显著提高了推理速度，提升了8.9倍，能够处理任意分辨率和纵横比的源图像，并支持局部图像操作和特定风格内容创作，同时在图像翻译任务中提高了视觉质量、效率、多功能性和可控性。</div>
</details>
</div>
<div class="card">
<div class="title">Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling</div>
<div class="meta-line">Authors: Feihong Yan, Peiru Wang, Yao Zhu, Kaiyu Pang, Qingyan Wei, Huiqi Li, Linfeng Zhang</div>
<div class="meta-line">First: 2025-10-20T05:22:10+00:00 · Latest: 2026-01-27T02:28:22+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17171v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17171v2">PDF</a> · <a href="https://github.com/feihongyan1/GtR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked Autoregressive (MAR) models promise better efficiency in visual generation than autoregressive (AR) models for the ability of parallel generation, yet their acceleration potential remains constrained by the modeling complexity of spatially correlated visual tokens in a single step. To address this limitation, we introduce Generation then Reconstruction (GtR), a training-free hierarchical sampling strategy that decomposes generation into two stages: structure generation establishing global semantic scaffolding, followed by detail reconstruction efficiently completing remaining tokens. Assuming that it is more difficult to create an image from scratch than to complement images based on a basic image framework, GtR is designed to achieve acceleration by computing the reconstruction stage quickly while maintaining the generation quality by computing the generation stage slowly. Moreover, observing that tokens on the details of an image often carry more semantic information than tokens in the salient regions, we further propose Frequency-Weighted Token Selection (FTS) to offer more computation budget to tokens on image details, which are localized based on the energy of high frequency information. Extensive experiments on ImageNet class-conditional and text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), substantially outperforming existing acceleration methods across various model scales and generation tasks. Our codes will be released in https://github.com/feihongyan1/GtR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成再重建：通过两阶段采样加速掩蔽自回归模型</div>
<div class="mono" style="margin-top:8px">掩蔽自回归（MAR）模型在视觉生成中承诺比自回归（AR）模型更好的效率，因为其具备并行生成的能力，但其加速潜力仍受限于单步中空间相关视觉标记的建模复杂性。为了解决这一限制，我们提出了生成再重建（GtR），这是一种无训练的分层采样策略，将生成过程分解为两个阶段：结构生成建立全局语义框架，随后细节重建高效完成剩余标记。假设从头创建图像比基于基本图像框架补充图像更困难，GtR旨在通过快速计算重建阶段来实现加速，同时通过缓慢计算生成阶段来保持生成质量。此外，观察到图像细节上的标记通常携带比显著区域的标记更多的语义信息，我们进一步提出了频率加权标记选择（FTS），为基于高频信息能量本地化的图像细节标记提供更多计算预算。在ImageNet类条件和文本到图像生成的广泛实验中，MAR-H实现了3.72倍的加速，同时保持了可比的质量（例如，FID: 1.59，IS: 304.4 vs. 原始1.59，299.1），在各种模型规模和生成任务中显著优于现有加速方法。我们的代码将发布在https://github.com/feihongyan1/GtR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the efficiency of Masked Autoregressive (MAR) models in visual generation, which are limited by the complexity of modeling spatially correlated visual tokens. The authors propose a novel two-stage sampling strategy called Generation then Reconstruction (GtR), which separates the generation process into structure generation and detail reconstruction. Experimental results show that GtR achieves a 3.72x speedup on MAR-H while maintaining comparable quality metrics, such as FID and IS, outperforming existing acceleration methods across various model scales and generation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高掩蔽自回归（MAR）模型在视觉生成中的效率，而这一效率受到空间相关视觉标记建模复杂性的限制。作者提出了一种名为生成再构（GtR）的两阶段采样策略，将生成过程分为结构生成和细节重建。实验结果表明，GtR在MAR-H上实现了3.72倍的加速，同时保持了相似的质量指标，如FID和IS，且在不同模型规模和生成任务中优于现有的加速方法。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Memorization: Selective Learning for Copyright-Safe Diffusion Model Training</div>
<div class="meta-line">Authors: Divya Kothandaraman, Jaclyn Pytlarz</div>
<div class="meta-line">First: 2025-12-12T00:50:38+00:00 · Latest: 2026-01-26T23:42:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11194v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11194v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective learning at the concept level.
  We introduce a gradient projection method designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature&#x27;s embedding space, thereby zeroing out its influence on the model&#x27;s weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越记忆：版权安全扩散模型训练的选择性学习</div>
<div class="mono" style="margin-top:8px">大规模文本到图像扩散模型中的记忆化带来了显著的安全和知识产权风险，使得对抗性属性提取和敏感或专有特征的未经授权复制成为可能。虽然常规的去记忆化技术，如正则化和数据过滤，限制了对特定训练示例的过拟合，但它们未能系统性地防止禁止概念级特征的内化。简单地丢弃所有包含敏感特征的图像浪费了宝贵的训练数据，因此需要一种在概念级别进行选择性学习的方法。
  我们提出了一种梯度投影方法，旨在强制执行概念级特征排除的严格要求。我们的防御在反向传播过程中运行，通过系统性地识别和切除与禁止属性的嵌入对齐的训练信号。具体而言，我们将每个梯度更新投影到敏感特征嵌入空间的正交补空间，从而消除其对模型权重的影响。我们的方法无缝集成到标准扩散模型训练管道中，并补充现有的防御措施。我们分析了我们的方法对抗旨在进行特征提取的对手。在广泛的实验中，我们证明了我们的框架在严格保持生成质量和语义保真度的同时，显著减少了记忆化。通过将记忆控制重新构建为选择性学习，我们的方法为知识产权安全和隐私保护的生成性人工智能建立了一个新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the security and intellectual property risks associated with memorization in large-scale text-to-image diffusion models, which can lead to unauthorized reproduction of sensitive features. The authors propose a gradient projection method that selectively enforces the exclusion of concept-level features during the model training process. Experimental results show that this method significantly reduces memorization while maintaining high generation quality and semantic fidelity, thereby establishing a new paradigm for copyright-safe generative AI.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大规模文本到图像扩散模型中记忆化带来的安全和知识产权风险，这可能导致敏感特征的未经授权复制。作者提出了一种梯度投影方法，通过在反向传播过程中识别并去除与禁止属性相关的训练信号，选择性地排除概念级特征。实验结果表明，该方法显著减少了记忆化，同时保持了高生成质量和语义保真度，从而为版权安全和隐私保护的生成式人工智能建立了新的范式。</div>
</details>
</div>
<div class="card">
<div class="title">SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning</div>
<div class="meta-line">Authors: Momin Ahmad Khan, Yasra Chandio, Fatima Muhammad Anwar</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-25T23:15:20+00:00 · Latest: 2026-01-26T23:29:46+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.22506v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.22506v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SABRE-FL：联邦提示学习中的选择性和准确的后门拒绝</div>
<div class="mono" style="margin-top:8px">联邦提示学习已成为一种通信高效且保护隐私的范式，用于在去中心化客户端之间适应大型视觉-语言模型，如CLIP。然而，这种设置的安全性影响仍未得到充分探讨。在本研究中，我们首次研究了联邦提示学习中的后门攻击。我们表明，当恶意客户端将视觉上不可察觉的、可学习的噪声触发器注入输入图像时，全球提示学习者会变得易受针对性错误分类的影响，同时在干净输入上仍保持高准确性。基于这一脆弱性，我们提出了SABRE-FL，这是一种轻量级、模块化的防御机制，使用在离线分布外数据上训练的嵌入空间异常检测器过滤被污染的提示更新。SABRE-FL不需要访问原始客户端数据或标签，并且能够在多样化的数据集上进行泛化。我们理论和实证地表明，恶意客户端可以通过基于嵌入的检测器可靠识别和过滤。在五个不同的数据集和四个基线防御中，SABRE-FL通过显著降低后门准确性而保持干净准确性，超越了所有基线，展示了强大的实证性能，并强调了未来联邦系统中对强健提示学习的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the security vulnerabilities in Federated Prompt Learning, particularly concerning backdoor attacks where malicious clients can manipulate input images to cause misclassification. The authors introduce SABRE-FL, a lightweight defense mechanism that utilizes an embedding-space anomaly detector trained on out-of-distribution data to filter out poisoned prompt updates without needing access to raw client data or labels. Experimental results demonstrate that SABRE-FL effectively identifies and filters malicious clients across five diverse datasets, significantly reducing backdoor accuracy while maintaining high accuracy on clean inputs, thus highlighting the importance of robust defenses in federated learning systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决联邦提示学习中的安全漏洞，特别是针对可能危害模型训练完整性的后门攻击。作者提出了SABRE-FL，这是一种轻量级的防御机制，利用嵌入空间异常检测器过滤掉被污染的提示更新，而无需访问原始客户端数据或标签。实验结果表明，SABRE-FL能够有效识别并减轻恶意客户端的影响，在五个不同的数据集上显著降低后门准确率，同时保持对干净输入的高准确率，从而强调了在联邦学习系统中建立强大防御的重要性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
