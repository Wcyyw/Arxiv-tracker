<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-13 03:22</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260113_0322</div>
    <div class="row"><div class="card">
<div class="title">Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems</div>
<div class="meta-line">Authors: Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra, Konstantinos C. Zygalakis</div>
<div class="meta-line">First: 2025-03-20T15:17:05+00:00 · Latest: 2026-01-09T17:33:02+00:00</div>
<div class="meta-line">Comments: 35 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16222v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16222v2">PDF</a> · <a href="https://github.com/freyyia/pnp-langevin-poisson">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies plug-and-play (PnP) Langevin sampling strategies for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling offers a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. Herein, we leverage and adapt recent developments in this framework to tackle challenging imaging problems involving weakly informative Poisson data. Existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we explore two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is evaluated and contrasted through extensive numerical experiments and comparisons with state-of-the-art methods. The source code accompanying this paper is available at https://github.com/freyyia/pnp-langevin-poisson.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用即插即用先验的高效贝叶斯计算用于泊松逆问题</div>
<div class="mono" style="margin-top:8px">本文研究了即插即用（PnP）Langevin采样策略在低光子泊松成像问题中的贝叶斯推断，这是一类具有重要应用的挑战性问题，涉及天文学、医学和生物学。PnP Langevin采样为贝叶斯图像恢复提供了强大的框架，能够实现准确的点估计以及高级推断任务，包括不确定性量化和可视化分析，以及用于自动模型参数调优的经验贝叶斯推断。在此，我们利用并调整该框架中的最新发展，以解决涉及弱信息泊松数据的挑战性成像问题。现有的PnP Langevin算法不适合低光子泊松成像，因为其解决方案的不确定性高且正则性差，例如梯度爆炸和非负性约束。为了解决这些挑战，我们探索了两种将Langevin PnP采样扩展到泊松成像模型的策略：（i）一种加速的PnP Langevin方法，结合了边界反射和泊松似然近似；（ii）一种镜像采样算法，利用黎曼几何处理约束和似然的差正则性而无需近似。通过广泛的数值实验和与最先进方法的比较，评估和对比了这些方法的有效性。本文附带的源代码可在https://github.com/freyyia/pnp-langevin-poisson获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of Bayesian inference in low-photon Poisson imaging, which is crucial for fields like astronomy and medicine. The authors propose two enhanced plug-and-play Langevin sampling strategies to improve the performance of existing methods, specifically targeting issues such as high solution uncertainty and non-negativity constraints. Through extensive numerical experiments, the study demonstrates that these new approaches significantly outperform state-of-the-art methods in terms of accuracy and reliability in image restoration tasks.</div>
<div class="mono" style="margin-top:8px">本论文解决了低光子泊松成像中贝叶斯推断的挑战，这对天文学和医学等领域的应用至关重要。作者提出了两种先进的插拔式朗之万采样策略，以改善现有方法的性能，特别是针对高解的不确定性和非负约束等问题。通过大量数值实验，他们证明了加速PnP朗之万方法和镜面采样算法在图像恢复精度和不确定性量化方面显著优于最先进的技术。</div>
</details>
</div>
<div class="card">
<div class="title">WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation</div>
<div class="meta-line">Authors: Chanchan Wang, Yuanfang Wang, Qing Xu, Guanxin Chen</div>
<div class="meta-line">First: 2026-01-09T16:58:29+00:00 · Latest: 2026-01-09T16:58:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05942v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05942v1">PDF</a> · <a href="https://github.com/Chanchan-Wang/WaveRNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM&#x27;s direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM&#x27;s upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaveRNet：基于小波引导的多源领域泛化视网膜血管分割频率学习</div>
<div class="mono" style="margin-top:8px">领域泛化视网膜血管分割对自动化眼科诊断至关重要，但面临由非均匀照明和变化对比度引起的领域转移带来的重大挑战，同时保持细微血管结构的难度也在加大。尽管Segment Anything Model（SAM）展现了显著的零样本能力，但现有基于SAM的方法依赖于简单的适配器微调，忽视了编码领域不变特征的频域信息，导致在照明和对比度变化下的泛化性能下降。此外，SAM的直接上采样不可避免地丢失细微血管细节。为了解决这些局限性，我们提出了WaveRNet，一个基于小波引导的频率学习框架，用于稳健的多源领域泛化视网膜血管分割。具体而言，我们设计了一个光谱引导的领域调制器（SDM），将小波分解与可学习的领域标记相结合，使得能够将抗照明的低频结构与高频血管边界分离，同时促进领域特定特征的生成。此外，我们引入了一个频率自适应领域融合（FADF）模块，通过基于小波的频率相似性和软加权融合进行智能测试时领域选择。最后，我们提出了一个分层掩膜提示细化器（HMPR），通过粗到细的细化和长程依赖建模克服了SAM的上采样限制。在四个公共视网膜数据集上进行的Leave-One-Domain-Out协议下的广泛实验表明，WaveRNet实现了最先进的泛化性能。源代码可在https://github.com/Chanchan-Wang/WaveRNet获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve domain-generalized retinal vessel segmentation, which is essential for automated ophthalmic diagnosis but is hindered by challenges such as domain shifts from varying illumination and contrast. The authors propose WaveRNet, a wavelet-guided frequency learning framework that incorporates a Spectral-guided Domain Modulator to separate low-frequency structures from high-frequency vessel boundaries, and a Frequency-Adaptive Domain Fusion module for intelligent domain selection. Experimental results demonstrate that WaveRNet achieves state-of-the-art generalization performance across four public retinal datasets under the Leave-One-Domain-Out protocol.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善领域泛化的视网膜血管分割，这对自动眼科诊断至关重要，但受到照明和对比度变化引起的领域转移等挑战的阻碍。作者提出了WaveRNet，这是一种波导引导的频率学习框架，结合了光谱引导的领域调制器（SDM），以将低频结构与高频血管边界分离，并引入频率自适应领域融合（FADF）模块进行智能领域选择。实验结果表明，WaveRNet在四个公共视网膜数据集上使用留一领域法协议实现了最先进的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility</div>
<div class="meta-line">Authors: G M Shahariar, Zabir Al Nazi, Md Olid Hasan Bhuiyan, Zhouxing Shi</div>
<div class="meta-line">First: 2026-01-09T11:40:56+00:00 · Latest: 2026-01-09T11:40:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05739v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05739v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject&#x27;s online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PII-VisBench：评估视觉语言模型中个人可识别信息安全性的可见性连续体</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）越来越多地应用于隐私关键领域，但现有的个人可识别信息（PII）泄露评估主要将隐私视为静态提取任务，忽视了主体的在线存在——其在线数据量——如何影响隐私对齐。我们引入了PII-VisBench，这是一个新颖的基准，包含4000个独特探针，旨在通过在线存在的连续体评估VLM的安全性。该基准将200个主体分为四个可见性类别：高、中、低和零——基于其在线信息的程度和性质。我们基于两个关键指标评估18个开源VLM（0.3B-32B）：拒绝的PII探测查询的百分比（拒绝率）和标记为包含PII的非拒绝响应的比例（条件PII泄露率）。在模型之间，我们观察到一个一致的模式：随着主体可见性的降低，拒绝率增加，PII泄露率减少（从9.10%高降至5.34%低）。我们发现模型更可能对高可见性主体泄露PII，同时存在显著的模型家族异质性和PII类型差异。最后，改写和越狱式提示暴露了攻击和模型依赖的失败，促使我们进行关注可见性的安全评估和训练干预。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the safety of Vision Language Models (VLMs) in privacy-sensitive applications, focusing on how the online presence of individuals affects the leakage of personally identifiable information (PII). The authors introduce PII-VisBench, a benchmark with 4000 probes that categorizes subjects into visibility levels based on their online data availability and evaluates 18 open-source VLMs using metrics such as Refusal Rate and Conditional PII Disclosure Rate. The findings reveal that as the visibility of subjects decreases, the refusal of PII probing queries increases while PII disclosures decrease, indicating that models are more prone to disclose PII for high-visibility subjects, highlighting the need for visibility-aware safety evaluations and training methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是由于视觉语言模型（VLM）在隐私敏感领域的日益应用，以及现有的个人可识别信息（PII）泄露评估不足，未考虑受试者在线存在对隐私对齐的影响。作者引入了PII-VisBench，这是一个包含4000个独特探针的基准，用于评估VLM在不同在线可见性水平下的安全性，将受试者分为高、中、低和零可见性。对18个开源VLM的评估显示，随着受试者可见性的降低，PII探测查询的拒绝率增加，而条件PII泄露率降低，强调模型在高可见性受试者中更容易泄露PII，并且不同模型家族和PII类型之间存在显著差异，从而突显了对可见性敏感的安全评估和训练方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment</div>
<div class="meta-line">Authors: Jin Wang, Jianxiang Lu, Guangzheng Xu, Comi Chen, Haoyu Yang, Linqing Wang, Peng Chen, Mingtao Chen, Zhichao Hu, Longhuang Wu, Shuai Shao, Qinglin Lu, Ping Luo</div>
<div class="meta-line">First: 2026-01-09T11:15:27+00:00 · Latest: 2026-01-09T11:15:27+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TAGRPO：通过直接轨迹对齐提升图像到视频生成中的GRPO</div>
<div class="mono" style="margin-top:8px">最近的研究表明，将群体相对策略优化（GRPO）整合到流匹配模型中，特别是在文本到图像和文本到视频生成方面，具有良好的效果。然而，我们发现直接将这些技术应用于图像到视频（I2V）模型往往无法带来一致的奖励提升。为了解决这一局限性，我们提出了TAGRPO，这是一种受对比学习启发的I2V模型后训练框架。我们的方法基于一个观察，即从相同初始噪声生成的回放视频为优化提供了更好的指导。利用这一见解，我们提出了一种新颖的GRPO损失，应用于中间潜变量，鼓励与高奖励轨迹的直接对齐，同时最大化与低奖励轨迹的距离。此外，我们引入了一个回放视频的记忆库，以增强多样性并减少计算开销。尽管其简单性，TAGRPO在I2V生成中相较于DanceGRPO取得了显著的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inconsistent reward improvements observed when applying Group Relative Policy Optimization (GRPO) to image-to-video generation models. To overcome this challenge, the authors introduce TAGRPO, a post-training framework that utilizes insights from contrastive learning, specifically focusing on optimizing rollout videos generated from identical initial noise. The key experimental findings demonstrate that TAGRPO significantly enhances performance in image-to-video generation compared to the previous DanceGRPO method by encouraging direct alignment with high-reward trajectories and improving diversity through a memory bank for rollout videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高图像到视频（I2V）生成中群体相对策略优化（GRPO）的有效性，因为之前的应用并未持续改善奖励。作者提出了TAGRPO，这是一种受对比学习启发的后训练框架，利用从相同初始噪声生成的回放视频来提供更好的优化指导。关键实验结果表明，TAGRPO在I2V生成中显著优于DanceGRPO，通过将新颖的GRPO损失应用于中间潜变量，并结合回放视频的记忆库来提高多样性和降低计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Neural-Driven Image Editing</div>
<div class="meta-line">Authors: Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, Kai Wang, Qian Zheng, Hao Jin, Xiaojun Chang, Gang Pan, Shurong Dong, Kaipeng Zhang, Yang You</div>
<div class="meta-line">First: 2025-07-07T18:31:50+00:00 · Latest: 2026-01-09T10:06:25+00:00</div>
<div class="meta-line">Comments: 22 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05397v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05397v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://loongx1.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. The code and dataset are released on the project website: https://loongx1.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经驱动的图像编辑</div>
<div class="mono" style="margin-top:8px">传统的图像编辑通常依赖手动提示，劳动强度大且对运动控制或语言能力有限的个体不够友好。利用脑-计算机接口（BCI）和生成模型的最新进展，我们提出了LoongX，一种基于多模态神经生理信号的免手动图像编辑方法。LoongX利用在23,928对图像编辑配对上训练的最先进的扩散模型，每对配对都与同步的脑电图（EEG）、功能近红外光谱（fNIRS）、光电容积描记（PPG）和捕捉用户意图的头部运动信号相结合。为有效应对这些信号的异质性，LoongX集成了两个关键模块。跨尺度状态空间（CS3）模块编码信息丰富的模态特征。动态门控融合（DGF）模块进一步将这些特征聚合到统一的潜在空间中，然后通过在扩散变换器（DiT）上的微调与编辑语义对齐。此外，我们使用对比学习预训练编码器，以将认知状态与嵌入自然语言的语义意图对齐。大量实验表明，LoongX的性能与文本驱动的方法相当（CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636），并且在神经信号与语音结合时表现更佳（CLIP-T: 0.2588 vs. 0.2549）。这些结果突显了神经驱动生成模型在实现可访问、直观的图像编辑方面的潜力，并为认知驱动的创意技术开辟了新方向。代码和数据集已在项目网站发布：https://loongx1.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to make image editing more accessible, particularly for individuals with limited motor control or language abilities, by utilizing brain-computer interfaces and generative models. The proposed method, LoongX, employs a hands-free image editing approach that processes multimodal neurophysiological signals, integrating a cross-scale state space module and a dynamic gated fusion module to effectively handle the diverse data types. Experimental results indicate that LoongX achieves performance comparable to traditional text-driven methods and surpasses them when combining neural signals with speech, demonstrating its potential for intuitive image editing and advancing cognitive-driven creative technologies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为运动能力或语言能力有限的个体创造一种更易于访问的图像编辑方法，摆脱传统的手动提示。作者提出了LoongX，这是一种利用多模态神经生理信号的免手动图像编辑方法，并使用在23,928对图像编辑配对上训练的最先进扩散模型，这些配对与各种生理信号同步，以捕捉用户意图。实验结果表明，LoongX的表现与现有的基于文本的方法相当，并且在结合神经信号与语音时表现更佳，展示了其在直观图像编辑和推动认知驱动创意技术方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">GenCtrl -- A Formal Controllability Toolkit for Generative Models</div>
<div class="meta-line">Authors: Emily Cheng, Carmen Amo Alonso, Federico Danieli, Arno Blaas, Luca Zappella, Pau Rodriguez, Xavier Suau</div>
<div class="meta-line">First: 2026-01-09T08:50:02+00:00 · Latest: 2026-01-09T08:50:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05637v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenCtrl -- 生成模型的形式可控性工具包</div>
<div class="mono" style="margin-top:8px">随着生成模型的普及，对生成过程的细粒度控制变得至关重要。然而，尽管从提示到微调的受控生成方法层出不穷，一个基本问题仍未得到解答：这些模型在本质上真的可控吗？在这项工作中，我们提供了一个理论框架来正式回答这个问题。将人机交互框架视为一个控制过程，我们提出了一种新算法来估计对话环境中模型的可控集。值得注意的是，我们提供了关于估计误差的形式保证，作为样本复杂度的函数：我们推导了可控集估计的概率近似正确界限，这些界限是无分布假设的，除了输出有界性外不采用任何假设，并适用于任何黑箱非线性控制系统（即任何生成模型）。我们在控制对话过程的不同任务中实证展示了理论框架，适用于语言模型和文本到图像生成。我们的结果表明，模型的可控性出乎意料地脆弱，并且高度依赖于实验设置。这突显了对可控性进行严格分析的必要性，将重点从简单尝试控制转向首先理解其基本限制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing demand for fine-grained control over generative models, alongside the unresolved question of their true controllability. The authors develop a theoretical framework to address this issue, proposing a novel algorithm that estimates the controllable sets of models in dialogue settings, with formal guarantees on estimation error based on sample complexity. Experimental results reveal that model controllability is fragile and varies significantly with different experimental conditions, emphasizing the necessity for thorough controllability analysis to understand the inherent limitations of these models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于对生成模型进行精细控制的需求日益增加，这引发了对其真实可控性的质疑。作者提出了一个理论框架，将人机交互视为控制过程，并引入了一种新算法来估计对话环境中模型的可控集合。他们的实验证明，模型的可控性脆弱，并受到实验环境的显著影响，这强调了在尝试控制之前，首先理解可控性极限的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection</div>
<div class="meta-line">Authors: Bin-Bin Gao, Chengjie Wang</div>
<div class="meta-line">First: 2026-01-09T06:05:18+00:00 · Latest: 2026-01-09T06:05:18+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures, 34 tabels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05552v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05552v1">PDF</a> · <a href="https://github.com/gaobb/UniADet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个无语言基础模型足以实现通用视觉异常检测</div>
<div class="mono" style="margin-top:8px">通用视觉异常检测（AD）旨在识别异常图像并分割异常区域，以应对开放和动态场景，遵循零样本和少样本范式，而无需任何特定数据集的微调。我们见证了视觉语言基础模型在近期方法中的广泛应用取得了显著进展。然而，当前方法常常在复杂的提示工程、精细的适应模块和具有挑战性的训练策略上遇到困难，最终限制了它们的灵活性和通用性。为了解决这些问题，本文重新思考了视觉语言模型在AD背后的基本机制，并提出了一个令人尴尬的简单、通用且有效的通用视觉异常检测框架（UniADet）。具体而言，我们首先发现语言编码器用于推导异常分类和分割的决策权重，然后证明对于通用AD来说这是不必要的。其次，我们提出了一种令人尴尬的简单方法，完全解耦分类和分割，并解耦跨层特征，即为不同任务和层次特征学习独立权重。UniADet高度简单（仅学习解耦权重）、参数高效（仅0.002M可学习参数）、通用（适应多种基础模型）且有效（在14个涵盖工业和医疗领域的真实世界AD基准上首次大幅超越最先进的零样本/少样本和全样本AD方法）。我们将把UniADet的代码和模型发布在https://github.com/gaobb/UniADet。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve universal visual anomaly detection (AD) by addressing the limitations of current methods that rely heavily on complex prompt engineering and adaptation modules. The authors propose a new framework called UniADet, which simplifies the process by decoupling classification and segmentation tasks and using a language encoder to derive decision weights for anomaly detection without the need for dataset-specific fine-tuning. Experimental results demonstrate that UniADet is parameter-efficient, with only 0.002M learnable parameters, and significantly outperforms state-of-the-art methods in zero-/few-shot and full-shot AD across 14 real-world benchmarks in both industrial and medical domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在复杂提示工程和适应模块方面的局限性来改善通用视觉异常检测（AD）。作者提出了一种名为UniADet的新框架，通过解耦分类和分割任务，并仅使用语言编码器来推导决策权重，从而简化了过程。实验结果表明，UniADet具有很高的有效性，在工业和医疗领域的14个真实世界基准测试中，显著超越了最先进的零样本和少样本方法以及全样本AD方法。</div>
</details>
</div>
<div class="card">
<div class="title">MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation</div>
<div class="meta-line">Authors: Yanfeng Li, Yue Sun, Keren Fu, Sio-Kei Im, Xiaoming Liu, Guangtao Zhai, Xiaohong Liu, Tao Tan</div>
<div class="meta-line">First: 2026-01-09T05:57:48+00:00 · Latest: 2026-01-09T05:57:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05546v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05546v1">PDF</a> · <a href="https://github.com/Tear-kitty/MoGen/tree/master">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoGen：可控多对象图像生成的统一协作框架</div>
<div class="mono" style="margin-top:8px">现有的多对象图像生成方法在实现局部图像生成区域与其对应语义之间的精确对齐方面面临困难，常常导致对象数量不一致和属性混淆。为了解决这一限制，主流方法通常依赖外部控制信号来明确约束图像的空间布局、局部语义和视觉属性。然而，这种强依赖性使得输入格式变得僵化，无法与用户的异构资源条件和多样化约束需求兼容。为应对这些挑战，我们提出了MoGen，一种用户友好的多对象图像生成方法。首先，我们设计了一个区域语义锚点（RSA）模块，在生成过程中精确地将语言描述中的短语单元锚定到其对应的图像区域，从而实现符合多个对象数量规范的文本到图像生成。在此基础上，我们进一步引入了自适应多模态引导（AMG）模块，该模块自适应解析和整合多源控制信号的各种组合，以制定相应的结构化意图。该意图随后指导场景布局和对象属性的选择性约束，实现动态细粒度控制。实验结果表明，MoGen在生成质量、数量一致性和细粒度控制方面显著优于现有方法，同时展现出更好的可访问性和控制灵活性。代码可在：https://github.com/Tear-kitty/MoGen/tree/master获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment between localized image generation regions and their corresponding semantics based on language descriptions, which is a common challenge in existing multi-object image generation methods. The authors propose MoGen, a method that incorporates a Regional Semantic Anchor (RSA) module to accurately link language phrases to image regions, and an Adaptive Multi-modal Guidance (AMG) module that integrates various control signals for dynamic scene layout and object attribute management. Experimental results indicate that MoGen outperforms existing techniques in terms of generation quality, consistency in object quantity, and fine-grained control, while also providing greater accessibility and flexibility for users.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善现有多对象图像生成方法中局部图像生成区域与其对应语义之间的对齐，解决不一致的对象数量和属性混淆等问题。作者提出了MoGen，其中包括一个区域语义锚定（RSA）模块，准确地将语言短语与图像区域链接，以及一个自适应多模态引导（AMG）模块，整合各种控制信号以实现动态场景布局和对象属性管理。实验结果表明，MoGen在生成质量、对象数量一致性和细粒度控制方面优于现有方法，同时为用户提供了更好的可访问性和灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception</div>
<div class="meta-line">Authors: Yuankun Xie, Ruibo Fu, Zhiyong Wang, Xiaopeng Wang, Songjun Cao, Long Ma, Haonan Cheng, Long Ye</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-04-09T10:18:45+00:00 · Latest: 2026-01-09T05:15:55+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.06753v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.06753v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of audio generation technologies has escalated the risks of malicious deepfake audio across speech, sound, singing voice, and music, threatening multimedia security and trust. While existing countermeasures (CMs) perform well in single-type audio deepfake detection (ADD), their performance declines in cross-type scenarios. This paper is dedicated to studying the all-type ADD task. We are the first to comprehensively establish an all-type ADD benchmark to evaluate current CMs, incorporating cross-type deepfake detection across speech, sound, singing voice, and music. Then, we introduce the prompt tuning self-supervised learning (PT-SSL) training paradigm, which optimizes SSL front-end by learning specialized prompt tokens for ADD, requiring 458x fewer trainable parameters than fine-tuning (FT). Considering the auditory perception of different audio types, we propose the wavelet prompt tuning (WPT)-SSL method to capture type-invariant auditory deepfake information from the frequency domain without requiring additional training parameters, thereby enhancing performance over FT in the all-type ADD task. To achieve an universally CM, we utilize all types of deepfake audio for co-training. Experimental results demonstrate that WPT-XLSR-AASIST achieved the best performance, with an average EER of 3.58% across all evaluation sets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检测所有类型的深度伪音频：小波提示调优以增强听觉感知</div>
<div class="mono" style="margin-top:8px">音频生成技术的快速发展加剧了恶意深度伪音频在语音、声音、歌声和音乐中的风险，威胁多媒体安全和信任。虽然现有的对策在单一类型音频深度伪造检测（ADD）中表现良好，但在跨类型场景中的表现下降。本文致力于研究所有类型的ADD任务。我们首次全面建立了一个所有类型的ADD基准，以评估当前的对策，涵盖语音、声音、歌声和音乐的跨类型深度伪造检测。然后，我们引入了提示调优自监督学习（PT-SSL）训练范式，通过学习专门的提示标记来优化SSL前端，所需的可训练参数比微调（FT）少458倍。考虑到不同音频类型的听觉感知，我们提出了小波提示调优（WPT）-SSL方法，从频域捕捉类型不变的听觉深度伪造信息，而无需额外的训练参数，从而在所有类型的ADD任务中提升了性能。为了实现一个通用的对策，我们利用所有类型的深度伪音频进行共同训练。实验结果表明，WPT-XLSR-AASIST在所有评估集上实现了最佳性能，平均EER为3.58%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the increasing threat posed by deepfake audio technologies, which compromise multimedia security and trust. The authors establish a comprehensive benchmark for all-type audio deepfake detection (ADD) and introduce a novel training paradigm called prompt tuning self-supervised learning (PT-SSL), which utilizes specialized prompt tokens and significantly reduces the number of trainable parameters compared to traditional fine-tuning methods. The proposed wavelet prompt tuning (WPT)-SSL method effectively captures type-invariant auditory deepfake information, resulting in improved performance, with the WPT-XLSR-AASIST model achieving an average equal error rate of 3.58% across various audio types in the all-type ADD task.</div>
<div class="mono" style="margin-top:8px">本研究的动机是深fake音频技术的快速发展对多媒体安全和信任构成的威胁，涉及多种音频类型。作者建立了一个全面的所有类型音频深fake检测（ADD）基准，并引入了一种新颖的训练范式，称为提示调优自监督学习（PT-SSL），该方法显著减少了与传统微调方法相比的可训练参数数量。关键实验结果表明，他们提出的波let提示调优（WPT）-SSL方法有效捕捉类型不变的听觉深fake信息，在所有评估集上实现了平均错误率（EER）为3.58%的最佳性能，超越了现有的对策。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian BiLO: Bilevel Local Operator Learning for Efficient Uncertainty Quantification of Bayesian PDE Inverse Problems with Low-Rank Adaptation</div>
<div class="meta-line">Authors: Ray Zirui Zhang, Christopher E. Miles, Xiaohui Xie, John S. Lowengrub</div>
<div class="meta-line">First: 2025-07-22T21:20:20+00:00 · Latest: 2026-01-09T05:11:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.17019v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.17019v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainty quantification in PDE inverse problems is essential in many applications. Scientific machine learning and AI enable data-driven learning of model components while preserving physical structure, and provide the scalability and adaptability needed for emerging imaging technologies and clinical insights. We develop a Bilevel Local Operator Learning framework for Bayesian inference in PDEs (B-BiLO). At the upper level, we sample parameters from the posterior via Hamiltonian Monte Carlo, while at the lower level we fine-tune a neural network via low-rank adaptation (LoRA) to approximate the solution operator locally. B-BiLO enables efficient gradient-based sampling without synthetic data or adjoint equations and avoids sampling in high-dimensional weight space, as in Bayesian neural networks, by optimizing weights deterministically. We analyze errors from approximate lower-level optimization and establish their impact on posterior accuracy. Numerical experiments across PDE models, including tumor growth, demonstrate that B-BiLO achieves accurate and efficient uncertainty quantification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯BiLO：用于贝叶斯PDE逆问题的高效不确定性量化的双层局部算子学习</div>
<div class="mono" style="margin-top:8px">在PDE逆问题中，不确定性量化在许多应用中至关重要。科学机器学习和人工智能使得在保留物理结构的同时进行数据驱动的模型组件学习成为可能，并为新兴成像技术和临床洞察提供了所需的可扩展性和适应性。我们开发了一种用于PDE贝叶斯推断的双层局部算子学习框架（B-BiLO）。在上层，我们通过哈密顿蒙特卡洛从后验中采样参数，而在下层，我们通过低秩适应（LoRA）微调神经网络，以局部近似解算子。B-BiLO实现了高效的基于梯度的采样，无需合成数据或伴随方程，并通过确定性优化权重，避免了在高维权重空间中的采样，如贝叶斯神经网络。我们分析了来自近似下层优化的误差，并建立了它们对后验准确性的影响。针对包括肿瘤生长在内的PDE模型的数值实验表明，B-BiLO实现了准确和高效的不确定性量化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve uncertainty quantification in PDE inverse problems, which is crucial for various applications. The authors propose a Bilevel Local Operator Learning framework for Bayesian inference in PDEs (B-BiLO), utilizing Hamiltonian Monte Carlo for parameter sampling at the upper level and fine-tuning a neural network with low-rank adaptation at the lower level to locally approximate the solution operator. The key experimental findings indicate that B-BiLO provides efficient gradient-based sampling without the need for synthetic data or adjoint equations, achieving accurate uncertainty quantification across different PDE models, including tumor growth scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高偏微分方程逆问题中的不确定性量化，这对各种应用至关重要。作者提出了一种双层局部算子学习框架，用于偏微分方程的贝叶斯推断，在上层利用哈密顿蒙特卡洛进行参数采样，在下层通过低秩适应微调神经网络。实验结果表明，B-BiLO提供了高效的基于梯度的采样，并在包括肿瘤生长在内的不同偏微分方程模型中实现了准确的不确定性量化，无需合成数据或伴随方程。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</div>
<div class="meta-line">Authors: Samuel E. Johnny, Bernes L. Atabonfack, Israel Alagbe, Assane Gueye</div>
<div class="meta-line">First: 2026-01-09T03:02:41+00:00 · Latest: 2026-01-09T03:02:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05498v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05498v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无提示的基于SAM的多任务框架用于乳腺超声病变分割与分类</div>
<div class="mono" style="margin-top:8px">由于低对比度、斑点噪声和多样的病变形态，乳腺超声（BUS）成像中的肿瘤分割和分类仍然具有挑战性。本研究提出了一种多任务深度学习框架，利用Segment Anything Model (SAM)视觉编码器的嵌入共同执行病变分割和诊断分类。与基于提示的SAM变体不同，我们的方法采用无提示的完全监督适应，通过轻量级卷积头或UNet启发的解码器解码高维SAM特征以进行像素级分割。分类分支通过掩码引导注意力得到增强，使模型能够专注于与病变相关的特征，同时抑制背景伪影。在PRECISE 2025乳腺超声数据集上进行的实验中，按类别分为80%的训练和20%的测试，所提方法实现了0.887的Dice相似系数（DSC）和92.3%的准确率，排名PRECISE挑战赛排行榜的前列。这些结果表明，当SAM基础表示与分割引导学习相结合时，显著改善了乳腺超声成像中的病变描绘和诊断预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the challenges of accurate tumor segmentation and classification in breast ultrasound imaging, which are hindered by low contrast, speckle noise, and diverse lesion morphology. The authors propose a multi-task deep learning framework that utilizes embeddings from the Segment Anything Model (SAM) in a prompt-free, fully supervised manner, employing either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation, while enhancing classification through mask-guided attention. Experimental results on the PRECISE 2025 breast ultrasound dataset reveal that the method achieves a Dice Similarity Coefficient of 0.887 and an accuracy of 92.3 percent, positioning it among the top entries on the PRECISE challenge leaderboard, thus demonstrating the effectiveness of SAM-based representations in improving lesion delineation and diagnostic prediction in breast ultrasound imaging.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决乳腺超声成像中肿瘤分割和分类的准确性问题，这些问题受到低对比度、斑点噪声和多样化病变形态的影响。作者提出了一种多任务深度学习框架，该框架利用Segment Anything Model (SAM)视觉编码器的嵌入，进行无提示的联合病变分割和分类。对PRECISE 2025乳腺超声数据集的实验结果表明，该方法实现了0.887的Dice相似系数和92.3%的准确率，使其在PRECISE挑战排行榜中名列前茅，从而证明了基于SAM的表示在增强病变描绘和诊断预测方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots</div>
<div class="meta-line">Authors: Shubham Agarwal, Ofek Nourian, Michael Sidorov, Sharon Chemweno, Ofer Hadar, Naftali Lazarovitch, Jhonathan E. Ephrath</div>
<div class="meta-line">First: 2026-01-09T02:30:48+00:00 · Latest: 2026-01-09T02:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05482v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于植物根系检测与分析的多图像超分辨率框架</div>
<div class="mono" style="margin-top:8px">理解植物根系对于推进土壤-植物相互作用、养分吸收和整体植物健康的研究至关重要。然而，由于遮挡、土壤湿度变化和固有的低对比度等不利条件，准确成像地下根系仍然是一个持续的挑战，这限制了传统基于视觉的方法的有效性。在本研究中，我们提出了一种新型地下成像系统，捕捉植物根系的多个重叠视图，并整合了基于深度学习的多图像超分辨率（MISR）框架，旨在增强根系的可见性和细节。为了训练和评估我们的方法，我们构建了一个合成数据集，模拟现实的地下成像场景，结合影响图像质量的关键环境因素。我们提出的MISR算法利用视图之间的空间冗余重建高分辨率图像，提高了结构保真度和视觉清晰度。定量评估表明，我们的方法优于最先进的超分辨率基线，BRISQUE降低了2.3%，表明在相同的CLIP-IQA评分下图像质量得到了改善，从而增强了根系的表型分析。这反过来又促进了对关键根系特征的准确估计，包括根毛数量和根毛密度。所提出的框架为农业和生态研究中的自动地下植物根系成像和特征量化提供了一个有前景的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of accurately imaging plant root systems in subterranean environments, which is essential for understanding soil-plant interactions and plant health. The authors developed a novel underground imaging system that captures multiple overlapping views of roots and employs a deep learning-based Multi-Image Super Resolution (MISR) framework to enhance root visibility. Experimental results demonstrate that the MISR algorithm significantly improves image quality, achieving a 2.3 percent reduction in BRISQUE scores compared to state-of-the-art methods, thus enabling better phenotypic analysis of root traits such as root hair count and density.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善植物根系的成像，这对于理解土壤-植物相互作用和植物健康至关重要，尤其是在具有挑战性的地下环境中。作者开发了一种新型地下成像系统，并结合基于深度学习的多图像超分辨率（MISR）框架，以增强根系的可见性和细节。实验结果表明，MISR算法显著提高了图像质量，与最先进的方法相比，BRISQUE评分降低了2.3%，从而使根毛计数和密度等根系性状的表型分析得以更好地进行。</div>
</details>
</div>
<div class="card">
<div class="title">Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers</div>
<div class="meta-line">Authors: Dogyun Park, Moayed Haji-Ali, Yanyu Li, Willi Menapace, Sergey Tulyakov, Hyunwoo J. Kim, Aliaksandr Siarohin, Anil Kag</div>
<div class="meta-line">First: 2025-10-24T19:29:55+00:00 · Latest: 2026-01-09T01:58:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21986v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21986v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet naïve strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sprint：稀疏-密集残差融合用于高效扩散变换器</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）提供了最先进的生成性能，但其序列长度的二次训练成本使得大规模预训练变得极其昂贵。丢弃标记可以降低训练成本，但简单策略会降低表示效果，现有方法要么参数过重，要么在高丢弃比率下失败。我们提出了SPRINT，稀疏-密集残差融合用于高效扩散变换器，这是一种简单的方法，能够在保持质量的同时实现激进的标记丢弃（高达75%）。SPRINT利用浅层和深层的互补作用：早期层处理所有标记以捕捉局部细节，深层在稀疏子集上操作以减少计算，其输出通过残差连接融合。训练遵循两阶段计划：长时间的掩蔽预训练以提高效率，随后是短时间的全标记微调以缩小训练-推理差距。在ImageNet-1K 256x256上，SPRINT实现了9.8倍的训练节省，同时保持了可比的FID/FDD，在推理时，其路径丢弃引导（PDG）几乎将FLOPs减半，同时提高了质量。这些结果确立了SPRINT作为高效DiT训练的简单、有效和通用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high training costs associated with Diffusion Transformers (DiTs) due to their quadratic complexity with sequence length, which makes large-scale pretraining impractical. The authors introduce SPRINT, a method that allows for aggressive token dropping of up to 75% while maintaining representation quality by utilizing both shallow and deep layers for processing tokens. Experimental results on ImageNet-1K 256x256 demonstrate that SPRINT achieves a 9.8x reduction in training costs while maintaining comparable FID/FDD scores, and during inference, its Path-Drop Guidance significantly reduces FLOPs by nearly half while enhancing quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散变换器（DiTs）在序列长度上具有平方复杂度所导致的高训练成本，使得大规模预训练变得具有挑战性。作者提出了SPRINT，这是一种允许高达75%的激进令牌丢弃的方法，同时保持表示质量，通过利用浅层和深层进行处理。实验结果表明，SPRINT在ImageNet-1K 256x256上实现了9.8倍的训练成本降低，同时保持了可比的FID/FDD分数，并且在推理过程中，其路径丢弃引导显著减少了近一半的FLOPs，同时提高了质量。</div>
</details>
</div>
<div class="card">
<div class="title">Infrared-Assisted Single-Stage Framework for Joint Restoration and Fusion of Visible and Infrared Images under Hazy Conditions</div>
<div class="meta-line">Authors: Huafeng Li, Jiaqi Fang, Yafei Zhang, Yu Liu</div>
<div class="meta-line">First: 2024-11-16T02:57:12+00:00 · Latest: 2026-01-08T21:55:16+00:00</div>
<div class="meta-line">Comments: Accepted by Pattern Recognition</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.12586v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.12586v3">PDF</a> · <a href="https://github.com/fangjiaqi0909/IASSF}{\textcolor{blue}{https://github.com/fangjiaqi0909/IASSF">Code1</a> · <a href="https://github.com/fangjiaqi0909/IASSF">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infrared and visible (IR-VIS) image fusion has gained significant attention for its broad application value. However, existing methods often neglect the complementary role of infrared image in restoring visible image features under hazy conditions. To address this, we propose a joint learning framework that utilizes infrared image for the restoration and fusion of hazy IR-VIS images. To mitigate the adverse effects of feature diversity between IR-VIS images, we introduce a prompt generation mechanism that regulates modality-specific feature incompatibility. This creates a prompt selection matrix from non-shared image information, followed by prompt embeddings generated from a prompt pool. These embeddings help generate candidate features for dehazing. We further design an infrared-assisted feature restoration mechanism that selects candidate features based on haze density, enabling simultaneous restoration and fusion within a single-stage framework. To enhance fusion quality, we construct a multi-stage prompt embedding fusion module that leverages feature supplementation from the prompt generation module. Our method effectively fuses IR-VIS images while removing haze, yielding clear, haze-free fusion results. In contrast to two-stage methods that dehaze and then fuse, our approach enables collaborative training in a single-stage framework, making the model relatively lightweight and suitable for practical deployment. Experimental results validate its effectiveness and demonstrate advantages over existing methods. The source code of the paper is available at \href{https://github.com/fangjiaqi0909/IASSF}{\textcolor{blue}{https://github.com/fangjiaqi0909/IASSF</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在雾霾条件下可见光与红外图像联合恢复与融合的红外辅助单阶段框架</div>
<div class="mono" style="margin-top:8px">红外与可见光（IR-VIS）图像融合因其广泛的应用价值而受到广泛关注。然而，现有方法往往忽视了红外图像在雾霾条件下恢复可见光图像特征的互补作用。为此，我们提出了一种联合学习框架，利用红外图像进行雾霾IR-VIS图像的恢复与融合。为减轻IR-VIS图像特征多样性带来的不利影响，我们引入了一种提示生成机制，调节模态特定特征的不兼容性。这从非共享图像信息中创建了一个提示选择矩阵，随后从提示池生成提示嵌入。这些嵌入有助于生成去雾的候选特征。我们进一步设计了一种红外辅助特征恢复机制，根据雾霾密度选择候选特征，实现单阶段框架内的同时恢复与融合。为了提高融合质量，我们构建了一个多阶段提示嵌入融合模块，利用提示生成模块的特征补充。我们的方法有效地融合IR-VIS图像，同时去除雾霾，产生清晰、无雾霾的融合结果。与先去雾再融合的两阶段方法相比，我们的方法在单阶段框架中实现了协同训练，使模型相对轻量，适合实际部署。实验结果验证了其有效性，并展示了相较于现有方法的优势。论文的源代码可在\href{https://github.com/fangjiaqi0909/IASSF}{\textcolor{blue}{https://github.com/fangjiaqi0909/IASSF}获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the restoration and fusion of visible and infrared images under hazy conditions, addressing the limitations of existing methods that overlook the complementary role of infrared images. The authors propose a joint learning framework that incorporates a prompt generation mechanism to manage feature incompatibility and an infrared-assisted feature restoration mechanism that selects features based on haze density, all within a single-stage framework. Experimental results demonstrate that this method effectively produces clear, haze-free fused images and shows significant advantages over traditional two-stage approaches, validating its effectiveness for practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善在雾霾条件下可见光和红外图像的恢复与融合，因为现有方法往往忽视了红外图像在增强可见特征方面的优势。作者提出了一种联合学习框架，结合红外图像来恢复和融合雾霾的红外-可见图像，利用提示生成机制解决特征不兼容问题，并基于雾霾密度的特征恢复机制。实验结果表明，该单阶段框架有效地生成清晰、无雾的融合图像，相较于传统的将去雾和融合过程分开的两阶段方法，具有更高的效率，适用于实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-task Cross-modal Learning for Chest X-ray Image Retrieval</div>
<div class="meta-line">Authors: Zhaohui Liang, Sivaramakrishnan Rajaraman, Niccolo Marini, Zhiyun Xue, Sameer Antani</div>
<div class="meta-line">First: 2026-01-08T21:44:00+00:00 · Latest: 2026-01-08T21:44:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05399v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05399v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model&#x27;s enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>胸部X光图像检索的多任务跨模态学习</div>
<div class="mono" style="margin-top:8px">CLIP和BiomedCLIP是视觉-语言基础模型的例子，提供强大的跨模态嵌入；然而，它们并未针对细粒度的医学检索任务进行优化，例如使用胸部X光（CXR）图像查询检索临床相关的放射学报告。为了解决这一不足，我们提出了一种多任务学习框架，以微调BiomedCLIP并评估CXR图像-文本检索的改进。以BiomedCLIP为基础，我们结合了一个轻量级的MLP投影头，该头使用多任务复合损失函数进行训练，包括：（1）用于区分正常与异常CXR研究的二元交叉熵损失，（2）用于增强类内一致性的监督对比损失，以及（3）用于保持跨模态对齐的CLIP损失。实验结果表明，微调后的模型在图像到文本和文本到图像检索任务中，相较于预训练的BiomedCLIP和通用CLIP模型，表现出更平衡和临床相关的性能。此外，t-SNE可视化显示正常和异常案例的语义聚类更清晰，展示了模型增强的诊断敏感性。这些发现突显了领域自适应的多任务学习在推进生物医学应用中的跨模态检索的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve the retrieval of clinically relevant radiology reports using chest X-ray images, as existing vision-language models like CLIP and BiomedCLIP are not optimized for fine-grained medical retrieval tasks. The authors propose a multi-task learning framework that fine-tunes BiomedCLIP by incorporating a lightweight MLP projector head trained with a composite loss function, which includes binary cross-entropy loss, supervised contrastive loss, and CLIP loss. Experimental results indicate that the fine-tuned model outperforms both the pretrained BiomedCLIP and general-purpose CLIP models in terms of balanced and clinically meaningful performance in image-to-text and text-to-image retrieval tasks, with t-SNE visualizations showing improved semantic clustering of normal and abnormal cases, thus enhancing diagnostic sensitivity.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有视觉-语言模型（如CLIP和BiomedCLIP）在进行细粒度医学检索任务中的局限性，特别是在使用胸部X光图像检索相关放射学报告方面。为了提高性能，作者提出了一种多任务学习框架，通过添加轻量级MLP投影头并采用包括二元交叉熵、监督对比损失和CLIP损失的复合损失函数来微调BiomedCLIP。实验结果表明，微调后的模型在图像到文本和文本到图像的检索任务中优于预训练的BiomedCLIP和通用CLIP模型，t-SNE可视化显示正常和异常案例的语义聚类得到了改善，从而增强了诊断敏感性。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: Protocol Refinement through Intelligent Simulation Modeling</div>
<div class="meta-line">Authors: Brian Hsu, Priyanka V Setty, Rory M Butler, Ryan Lewis, Casey Stone, Rebecca Weinberg, Thomas Brettin, Rick Stevens, Ian Foster, Arvind Ramanathan</div>
<div class="meta-line">First: 2026-01-08T20:15:28+00:00 · Latest: 2026-01-08T20:15:28+00:00</div>
<div class="meta-line">Comments: 43 pages, 8 figures, submitted to RSC Digital Discovery. Equal contribution: B. Hsu, P.V. Setty, R.M. Butler. Corresponding author: A. Ramanathan</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05356v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：通过智能仿真建模进行协议细化</div>
<div class="mono" style="margin-top:8px">自动化实验协议设计和执行仍然是实现自驾实验室的一个基本瓶颈。我们介绍了PRISM（通过智能仿真建模进行协议细化），这是一个自动化设计、验证和执行实验协议的框架，基于现成的机器人仪器构建实验室平台。PRISM使用一组基于语言模型的代理协同工作，以生成和细化实验步骤。该过程首先自动收集来自网络来源的相关程序，描述实验工作流程。这些程序通过规划、批评和验证循环转换为结构化的实验步骤（例如，液体处理步骤、甲板布局和其他相关操作）。最终步骤被转换为阿贡MADSci协议格式，提供了一个统一的接口，用于协调多个机器人仪器（Opentrons OT-2液体处理器、PF400臂、Azenta板封口机和剥离器），无需在步骤之间进行人工干预。为了评估协议生成性能，我们在受限和开放式提示范式下对单一推理模型和多代理工作流进行了基准测试。生成的协议在NVIDIA Omniverse构建的数字双胞胎环境中进行了验证，以在执行前检测物理或排序错误。通过Luna qPCR扩增和细胞绘画作为案例研究，我们展示了PRISM作为一个实用的端到端工作流程，连接了基于语言的协议生成、基于仿真的验证和自动化机器人执行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in automating experimental protocol design and execution, which is crucial for the advancement of self-driving laboratories. The authors developed PRISM, a framework that utilizes language-model-based agents to automate the design, validation, and execution of experimental protocols using off-the-shelf robotic instruments. Key findings indicate that PRISM effectively generates structured experimental steps from web-based sources, validates them in a digital-twin environment, and successfully executes protocols with robotic instruments, demonstrating its capability through case studies involving Luna qPCR amplification and Cell Painting.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决实验方案设计和执行自动化中的挑战，这对于自驾实验室的发展至关重要。作者提出了PRISM，一个利用基于语言模型的代理来自动化实验方案设计、验证和执行的框架，使用现成的机器人仪器。主要实验结果表明，PRISM有效地生成和完善方案，并在数字双胞胎环境中验证，以在执行前识别错误，通过涉及Luna qPCR扩增和细胞绘画的案例研究展示了其能力。</div>
</details>
</div>
<div class="card">
<div class="title">Pixel-Perfect Visual Geometry Estimation</div>
<div class="meta-line">Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</div>
<div class="meta-line">First: 2026-01-08T18:59:49+00:00 · Latest: 2026-01-08T18:59:49+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/gangweix/pixel-perfect-depth</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05246v1">PDF</a> · <a href="https://github.com/gangweix/pixel-perfect-depth">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素完美视觉几何估计</div>
<div class="mono" style="margin-top:8px">从图像中恢复干净且准确的几何形状对机器人技术和增强现实至关重要。然而，现有的几何基础模型仍然严重受飞行像素和细节丢失的影响。本文提出了像素完美视觉几何模型，利用像素空间中的生成建模预测高质量、无飞行像素的点云。我们首先介绍了像素完美深度（PPD），这是一个基于像素空间扩散变换器（DiT）的单目深度基础模型。为了解决与像素空间扩散相关的高计算复杂性，我们提出了两个关键设计：1）语义提示的DiT，它结合了来自视觉基础模型的语义表示来提示扩散过程，保留全局语义，同时增强细粒度视觉细节；2）级联DiT架构，逐步增加图像标记的数量，提高效率和准确性。为了进一步将PPD扩展到视频（PPVD），我们引入了一种新的语义一致DiT，它从多视角几何基础模型中提取时间一致的语义。然后，我们在DiT中执行参考引导的标记传播，以在最小的计算和内存开销下保持时间一致性。我们的模型在所有生成的单目和视频深度估计模型中表现最佳，并产生比其他所有模型显著更干净的点云。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy and cleanliness of geometry recovery from images, which is crucial for applications in robotics and augmented reality, as existing models struggle with issues like flying pixels and loss of detail. The authors propose pixel-perfect visual geometry models, specifically introducing the Pixel-Perfect Depth (PPD) model that utilizes pixel-space diffusion transformers to generate high-quality point clouds. Key innovations include the Semantics-Prompted DiT for enhancing visual details while maintaining global semantics, and a Cascade DiT architecture that optimizes computational efficiency. Additionally, the extension to video through the Semantics-Consistent DiT allows for temporally coherent depth estimation with minimal overhead. Experimental results show that these models outperform existing generative monocular and video depth estimation methods, yielding significantly cleaner point clouds.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高从图像中恢复几何形状的准确性和质量，这对机器人技术和增强现实应用至关重要，旨在解决现有模型中的飞行像素和细节丢失问题。作者提出了一种新方法，称为像素完美深度（PPD），利用像素空间扩散变换器生成高质量的点云，并实施了两个关键创新：语义提示的DiT以增强细节保留，以及级联DiT架构以提高效率。实验结果表明，他们的模型在生成单目和视频深度估计模型中表现优于现有模型，产生了显著更干净的点云。</div>
</details>
</div>
<div class="card">
<div class="title">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</div>
<div class="meta-line">Authors: Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-01-08T18:59:22+00:00 · Latest: 2026-01-08T18:59:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboVIP：通过视觉身份提示增强机器人操作的多视角视频生成</div>
<div class="mono" style="margin-top:8px">操作数据的多样性、数量和质量对训练有效的机器人策略至关重要。然而，由于硬件和物理设置的限制，收集大规模的真实世界操作数据在不同环境中仍然难以扩展。最近的研究使用文本提示条件的图像扩散模型，通过改变视觉观察中的背景和桌面物体来增强操作数据。然而，这些方法往往忽视了最先进策略模型所需的多视角和时间一致观察的实际需求。此外，仅靠文本提示无法可靠地指定场景设置。为了为扩散模型提供明确的视觉指导，我们引入了视觉身份提示，提供示例图像作为条件输入，以指导所需场景设置的生成。为此，我们还建立了一个可扩展的管道，从大型机器人数据集中策划视觉身份池。使用我们增强的操作数据训练下游视觉-语言-动作和视觉运动策略模型，在模拟和真实机器人环境中均获得了一致的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of collecting large-scale real-world manipulation data for training effective robot policies, which is hindered by hardware and environmental constraints. The authors propose a method called visual identity prompting that enhances existing text-prompt conditioned image diffusion models by providing exemplar images to guide the generation of multi-view and temporally coherent observations. Experimental results demonstrate that using the augmented manipulation data significantly improves the performance of vision-language-action and visuomotor policy models in both simulation and real-robot environments.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决由于硬件和物理设置限制而导致的收集大规模真实世界操作数据的挑战，这对训练有效的机器人策略至关重要。作者提出了一种称为视觉身份提示的方法，通过提供示例图像作为明确的视觉指导，增强现有的文本提示条件图像扩散模型，从而生成先进策略模型所需的多视角和时间一致的观察。实验结果表明，使用增强的操作数据训练视觉-语言-动作和视觉运动策略模型在模拟和真实机器人环境中均带来了持续的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</div>
<div class="meta-line">Authors: William Rudman, Michal Golovanevsky, Dana Arad, Yonatan Belinkov, Ritambhara Singh, Carsten Eickhoff, Kyle Mahowald</div>
<div class="meta-line">First: 2026-01-08T18:23:03+00:00 · Latest: 2026-01-08T18:23:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中提示引发幻觉的机制</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）能力强大，但常常偏向文本提示而忽视视觉证据，从而产生幻觉。我们在一个受控的物体计数环境中研究这种失效模式，其中提示夸大了图像中的物体数量（例如，要求模型描述四朵睡莲，而实际上只有三朵）。在物体数量较少时，模型通常会纠正这种过高估计，但随着物体数量的增加，它们越来越倾向于遵循提示，而不考虑差异。通过对三种VLM的机制分析，我们识别出一小组注意力头，其消融显著减少了提示引发的幻觉（PIH），减少幅度至少为40%，且无需额外训练。在不同模型中，PIH头以特定于模型的方式介导提示复制。我们描述了这些差异，并表明PIH消融增加了对视觉证据的纠正。我们的发现为提示引发幻觉的内部机制提供了见解，揭示了这些行为实现中的模型特异性差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the phenomenon of prompt-induced hallucination in vision-language models (VLMs), where models tend to prioritize textual prompts over visual evidence, particularly in object-counting tasks. The study employs a controlled experimental setup to analyze how VLMs respond to prompts that exaggerate the number of objects present in images. Key findings indicate that while models can correct overestimations at low object counts, they increasingly conform to prompts as object counts rise, with specific attention heads identified as critical in mediating this behavior; ablation of these heads reduces hallucinations by at least 40% without further training, enhancing alignment with visual evidence.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型视觉语言模型（VLMs）中的提示诱导幻觉现象，这些模型在对象计数任务中倾向于优先考虑文本提示而非视觉证据。研究采用受控实验设置，分析模型如何响应夸大图像中物体数量的提示。研究结果表明，尽管模型在低物体计数时能够纠正过高估计，但随着物体数量的增加，它们越来越倾向于遵循提示，从而导致显著的幻觉。对三种VLMs的机制分析识别出特定的注意力头，其去除可将这些幻觉减少至少40%，突显了影响提示处理和对视觉证据依赖程度的模型特异性机制。</div>
</details>
</div>
<div class="card">
<div class="title">Improving and Evaluating Open Deep Research Agents</div>
<div class="meta-line">Authors: Doaa Allabadi, Kyle Bradbury, Jordan M. Malof</div>
<div class="meta-line">First: 2025-08-13T19:32:01+00:00 · Latest: 2026-01-08T17:54:58+00:00</div>
<div class="meta-line">Comments: 8 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10152v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10152v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We focus here on Deep Research Agents (DRAs), which are systems that can take a natural language prompt from a user, and then autonomously search for, and utilize, internet-based content to address the prompt. Recent DRAs have demonstrated impressive capabilities on public benchmarks however, recent research largely involves proprietary closed-source systems. At the time of this work, we only found one open-source DRA, termed Open Deep Research (ODR). In this work we adapt the challenging recent BrowseComp benchmark to compare ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small), comprising a subset of BrowseComp, as a more computationally-tractable DRA benchmark for academic labs. We benchmark ODR and two other proprietary systems on BC-Small: one system from Anthropic and one system from Google. We find that all three systems achieve 0% accuracy on the test set of 60 questions. We introduce three strategic improvements to ODR, resulting in the ODR+ model, which achieves a state-of-the-art 10% success rate on BC-Small among both closed-source and open-source systems. We report ablation studies indicating that all three of our improvements contributed to the success of ODR+.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进和评估开放深度研究代理</div>
<div class="mono" style="margin-top:8px">我们在这里关注深度研究代理（DRA），这是一种能够接受用户自然语言提示并自主搜索和利用基于互联网的内容来解决提示的系统。最近的DRA在公共基准测试中展示了令人印象深刻的能力，但最近的研究主要涉及专有的闭源系统。在本研究时，我们只找到一个开源DRA，称为开放深度研究（ODR）。在这项工作中，我们调整了具有挑战性的最近BrowseComp基准，以比较ODR与现有的专有系统。我们提出了BrowseComp-Small（BC-Small），它是BrowseComp的一个子集，作为一个更具计算可行性的DRA基准，供学术实验室使用。我们在BC-Small上对ODR和另外两个专有系统进行了基准测试：一个来自Anthropic，另一个来自Google。我们发现这三个系统在60个问题的测试集上均达到0%的准确率。我们为ODR引入了三项战略改进，形成了ODR+模型，该模型在BC-Small上在闭源和开源系统中都达到了10%的最新成功率。我们报告了消融研究，表明我们的三项改进均对ODR+的成功做出了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of open-source Deep Research Agents (DRAs), which autonomously search for and utilize internet content based on user prompts, in contrast to the proprietary systems that dominate the field. The authors adapted the BrowseComp benchmark to evaluate the performance of the Open Deep Research (ODR) system against two proprietary systems from Anthropic and Google, using a more manageable subset called BrowseComp-Small (BC-Small). The findings revealed that all three systems initially achieved 0% accuracy on a test set of 60 questions, but after implementing three strategic improvements to ODR, the enhanced model ODR+ achieved a state-of-the-art success rate of 10% on BC-Small, with ablation studies confirming the contributions of each improvement to this performance boost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估和改进开放式深度研究代理（DRA），这些代理能够自主利用互联网内容来响应用户提示，而这一领域主要被专有系统主导。作者将BrowseComp基准适配，创建了一个更易于管理的子集BrowseComp-Small（BC-Small），用于对开放深度研究（ODR）系统与Anthropic和Google的专有系统进行性能基准测试。研究结果显示，三种系统在60个问题的测试集上最初均达到了0%的准确率；然而，在对ODR实施三项战略性改进后，生成的ODR+模型在BC-Small上达到了显著的10%成功率，标志着在开放源代码和闭源系统中性能的显著提升，消融研究确认了每项改进的贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</div>
<div class="meta-line">Authors: Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai</div>
<div class="meta-line">First: 2026-01-08T17:13:00+00:00 · Latest: 2026-01-08T17:13:00+00:00</div>
<div class="meta-line">Comments: 13 pages, 9 figures, project page: https://github.com/hrz2000/realign</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05124v1">PDF</a> · <a href="https://github.com/hrz2000/realign">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model&#x27;s overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Re-Align：基于结构化推理的上下文图像生成与编辑对齐</div>
<div class="mono" style="margin-top:8px">上下文图像生成与编辑（ICGE）使用户能够通过交错的图像-文本提示指定视觉概念，要求对用户意图进行精确理解和忠实执行。尽管最近的统一多模态模型展现出良好的理解能力，但这些优势往往无法有效转移到图像生成上。我们提出了Re-Align，一个统一框架，通过结构化推理引导的对齐弥合理解与生成之间的差距。其核心是上下文思维链（IC-CoT），一种结构化推理范式，解耦语义指导和参考关联，提供清晰的文本目标并减轻参考图像之间的混淆。此外，Re-Align引入了一种有效的强化学习训练方案，利用替代奖励来衡量结构化推理文本与生成图像之间的对齐，从而提高模型在ICGE任务上的整体表现。大量实验验证了Re-Align在上下文图像生成和编辑任务上超越了可比模型规模和资源的竞争方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance in-context image generation and editing (ICGE) by improving the alignment between user intent and generated images. The authors propose a unified framework called Re-Align, which incorporates a structured reasoning-guided alignment method through the In-Context Chain-of-Thought (IC-CoT) paradigm, allowing for better semantic guidance and reference association. Experimental results demonstrate that Re-Align significantly outperforms existing methods of similar scale and resources in both image generation and editing tasks, indicating its effectiveness in bridging the gap between understanding and generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改善用户意图的理解和执行，增强上下文图像生成和编辑（ICGE）。作者提出了一种名为Re-Align的统一框架，该框架采用结构化推理引导的对齐方法，特别是上下文思维链（IC-CoT）范式，以将语义指导与参考关联分开。实验结果表明，Re-Align在图像生成和编辑任务中显著优于现有的同类方法，表明其在理解与生成之间架起了桥梁的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models</div>
<div class="meta-line">Authors: Arghyadeep Das, Sai Sreenivas Chintha, Rishiraj Girmal, Kinjal Pandey, Sharvi Endait</div>
<div class="meta-line">First: 2026-01-08T16:19:43+00:00 · Latest: 2026-01-08T16:19:43+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05076v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清洗思维链：堵塞大型推理模型中的个人身份信息泄露</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过生成明确的思维链（CoT）推理来提高性能、可靠性和可解释性，但这种透明性带来了严重的隐私风险：即使最终答案经过清洗，中间推理也常常泄露个人身份信息（PII）。我们研究如何引导隐私优先的推理，使模型在不暴露敏感信息的情况下进行推理，采用可部署的干预措施而非事后修订。我们引入了PII-CoT-Bench，这是一个带有隐私意识的CoT注释的监督数据集，以及一个涵盖现实和对抗性泄露场景的类别平衡评估基准。我们的结果揭示了一种能力依赖的趋势：最先进的模型最能从基于提示的控制中受益，而较弱的模型则需要微调以实现有意义的泄露减少。在模型和类别之间，这两种方法都显著减少了PII暴露，且效用损失最小，证明了可以在不牺牲性能的情况下实现私密推理。总体而言，我们表明，私密的CoT推理可以在效用损失最小的情况下实现，为构建隐私保护的推理系统提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address privacy risks associated with Large Reasoning Models (LRMs), which can leak personally identifiable information (PII) during their chain-of-thought (CoT) reasoning processes. The authors propose a method that emphasizes privacy-first reasoning through deployable interventions and introduce PII-CoT-Bench, a dataset with privacy-aware CoT annotations and a benchmark for evaluating leakage scenarios. Experimental results indicate that while state-of-the-art models benefit from prompt-based controls, weaker models require fine-tuning to effectively reduce PII exposure, with both approaches achieving significant reductions in leakage while maintaining utility.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型推理模型（LRMs）在生成链式思维（CoT）推理时可能泄露个人身份信息（PII）的隐私风险。作者提出了一种通过可部署干预措施诱导隐私优先推理的方法，并引入了PII-CoT-Bench，一个具有隐私意识的CoT注释数据集和一个平衡的泄漏场景评估基准。实验结果表明，尽管最先进的模型受益于基于提示的控制，但较弱的模型需要微调才能有效减少PII泄漏，两种方法都显著降低了PII暴露，同时保持了效用，从而证明了在不损失性能的情况下实现私密推理的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</div>
<div class="meta-line">Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar</div>
<div class="meta-line">First: 2026-01-08T16:02:56+00:00 · Latest: 2026-01-08T16:02:56+00:00</div>
<div class="meta-line">Comments: Contributed original research to top tier conference in VLM; currently undergoing peer review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05059v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05059v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut &amp; Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从理解到参与：通过视觉语言模型（VLMs）个性化药学视频剪辑</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）有望通过实现智能、可扩展和自动化的多模态内容处理，彻底改变制药行业的数字化转型。传统的异构数据模态（文本、图像、视频、音频和网页链接）的手动标注容易出现不一致、质量下降和内容利用效率低下的问题。长视频和音频数据的庞大体量进一步加剧了这些挑战（例如，长时间的临床试验访谈和教育研讨会）。在此，我们介绍了一种领域适应的视频到视频剪辑生成框架，该框架集成了音频语言模型（ALMs）和视觉语言模型（VLMs）以生成精彩剪辑。我们的贡献有三方面：（i）可重复的剪切与合并算法，具有淡入/淡出和时间戳归一化，确保平滑过渡和音视频对齐；（ii）基于角色定义和提示注入的个性化机制，以实现定制输出（市场营销、培训、合规）；（iii）一种成本高效的端到端管道策略，平衡ALM/VLM增强处理。在视频MME基准（900）和我们在14个疾病领域的16,159个药学视频的专有数据集上的评估表明，速度提升3到4倍，成本降低4倍，剪辑质量具有竞争力。除了效率提升外，我们还报告了我们的方法在剪辑连贯性评分（0.348）和信息量评分（0.721）上优于最先进的VLM基线（例如，Gemini 2.5 Pro），突显了透明、定制提取和支持合规的视频摘要在生命科学中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the inefficiencies and inconsistencies in the traditional manual annotation of diverse data modalities in the pharmaceutical industry, particularly in processing long video and audio content. The authors propose a Video to Video Clip Generation framework that combines Audio Language Models and Vision Language Models to create highlight clips, employing a reproducible Cut &amp; Merge algorithm for smooth transitions, a personalization mechanism for tailored outputs, and a cost-efficient end-to-end pipeline. Experimental results on the Video MME benchmark and a proprietary dataset of pharmacy videos show a 3 to 4 times speedup, a 4 times reduction in costs, and improved clip coherence and informativeness scores compared to existing VLM baselines, indicating significant advancements in video summarization for life sciences.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决制药行业中传统手动标注多样数据模态的低效和不一致性，特别是在处理长视频和音频内容时。作者提出了一种视频到视频剪辑生成框架，结合了音频语言模型和视觉语言模型来创建高亮剪辑，采用可重复的剪切与合并算法实现平滑过渡，个性化机制提供定制输出，以及成本高效的端到端管道。对900个视频的基准测试和16,159个制药视频的专有数据集的实验结果显示，处理速度提高了3到4倍，成本降低了4倍，剪辑的连贯性和信息量评分也优于现有的最先进模型，表明在生命科学领域的视频摘要方面取得了显著进展。</div>
</details>
</div>
<div class="card">
<div class="title">Guiding diffusion models to reconstruct flow fields from sparse data</div>
<div class="meta-line">Authors: Marc Amorós-Trepat, Luis Medrano-Navarro, Qiang Liu, Luca Guastoni, Nils Thuerey</div>
<div class="meta-line">Venue: Physics of Fluids 1 January 2026; 38 (1): 015112</div>
<div class="meta-line">First: 2025-10-22T19:01:50+00:00 · Latest: 2026-01-08T15:36:06+00:00</div>
<div class="meta-line">Comments: Published on Physics of Fluids, code and data can be found at https://github.com/tum-pbs/sparse-reconstruction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19971v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19971v2">PDF</a> · <a href="https://github.com/tum-pbs/sparse-reconstruction">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reconstruction of unsteady flow fields from limited measurements is a challenging and crucial task for many engineering applications. Machine learning models are gaining popularity for solving this problem due to their ability to learn complex patterns from data and to generalize across diverse conditions. Among these, diffusion models have emerged as being particularly powerful for generative tasks, producing high-quality samples by iteratively refining noisy inputs. In contrast to other methods, these generative models are capable of reconstructing the smallest scales of the fluid spectrum. In this work, we introduce a novel sampling method for diffusion models that enables the reconstruction of high-fidelity samples by guiding the reverse process using the available sparse data. Moreover, we enhance the reconstructions with available physics knowledge using a conflict-free update method during training. To evaluate the effectiveness of our method, we conduct experiments on 2 and 3-dimensional turbulent flow data. Our method consistently outperforms other diffusion-based methods in predicting the fluid&#x27;s structure and in pixel-wise accuracy. This study underscores the remarkable potential of diffusion models in reconstructing flow field data, paving the way for leveraging them in fluid dynamics research and applications ranging from super-resolution to reconstructions of experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>引导扩散模型从稀疏数据重建流场</div>
<div class="mono" style="margin-top:8px">从有限测量中重建非稳态流场是许多工程应用中的一项具有挑战性和关键性的任务。由于机器学习模型能够从数据中学习复杂模式并在不同条件下进行泛化，因此它们在解决此问题上越来越受欢迎。在这些模型中，扩散模型在生成任务中表现出特别强大的能力，通过迭代精炼噪声输入生成高质量样本。与其他方法相比，这些生成模型能够重建流体光谱的最小尺度。在本研究中，我们介绍了一种新颖的扩散模型采样方法，通过利用可用的稀疏数据引导反向过程，从而实现高保真样本的重建。此外，我们在训练过程中使用无冲突更新方法增强重建效果，结合可用的物理知识。为了评估我们方法的有效性，我们在二维和三维湍流数据上进行了实验。我们的算法在预测流体结构和像素级准确性方面始终优于其他基于扩散的方法。这项研究强调了扩散模型在重建流场数据方面的显著潜力，为在流体动力学研究和从超分辨率到实验重建等应用中利用它们铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The reconstruction of unsteady flow fields from limited measurements is essential for various engineering applications, motivating the exploration of machine learning models for this task. This study introduces a novel sampling method for diffusion models that enhances the reconstruction of high-fidelity flow field samples by guiding the reverse process with sparse data and incorporating physics knowledge during training. Experimental results demonstrate that this method consistently outperforms other diffusion-based approaches in predicting fluid structure and achieving higher pixel-wise accuracy in both 2D and 3D turbulent flow data.</div>
<div class="mono" style="margin-top:8px">从有限测量中重建非稳态流场对于各种工程应用至关重要，这促使研究人员探索机器学习模型，特别是扩散模型。该研究提出了一种新颖的采样方法，通过稀疏数据引导扩散模型的反向过程，结合物理知识的无冲突更新方法，提高重建质量。对二维和三维湍流数据的实验结果表明，所提方法在准确预测流体结构和实现更高的像素级准确性方面，始终优于其他基于扩散的方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling</div>
<div class="meta-line">Authors: Jin Gao, Saichandu Juluri</div>
<div class="meta-line">First: 2026-01-08T15:18:12+00:00 · Latest: 2026-01-08T15:18:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05016v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05016v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从创意到共创：一种用于增强代理3D建模的规划者-演员-评论家框架</div>
<div class="mono" style="margin-top:8px">我们提出了一个框架，将演员-评论家架构扩展到通过多代理自我反思和人类参与监督的创意3D建模。现有方法依赖于单提示代理，直接通过Blender MCP等工具执行建模命令，而我们的方法引入了规划者-演员-评论家架构。在该设计中，规划者协调建模步骤，演员执行这些步骤，评论家提供迭代反馈，而人类用户在整个过程中充当监督者和顾问。通过对单提示建模和我们的反思多代理方法进行系统比较，我们展示了在多种3D建模场景中几何准确性、美学质量和任务完成率的改善。我们的评估表明，与直接单提示执行相比，评论家引导的反思结合人类监督输入减少了建模错误，并提高了结果的复杂性和质量。这项工作确立了结构化代理自我反思在增强人类监督和顾问指导下，能够在保持高效工作流程集成的同时，产生更高质量的3D模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the quality and efficiency of 3D modeling through improved agent collaboration and human supervision. The authors propose a Planner-Actor-Critic framework that incorporates multi-agent self-reflection and human-in-the-loop supervision, contrasting it with traditional single-prompt modeling methods. Experimental results indicate that this new approach significantly improves geometric accuracy, aesthetic quality, and task completion rates, demonstrating that the integration of critic-guided reflection and human oversight leads to reduced modeling errors and enhanced complexity and quality of 3D models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过改进代理协作和人类监督来提升3D建模的质量。作者提出了一种规划者-演员-评论家框架，该框架结合了多代理自我反思和人类参与的监督，与传统的单提示建模方法形成对比。实验结果表明，这种新方法显著提高了几何准确性、美学质量和任务完成率，证明了评论家引导的反思与人类输入的结合能够减少建模错误，并产生更复杂、更高质量的输出，相较于直接执行方法效果更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification</div>
<div class="meta-line">Authors: Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq</div>
<div class="meta-line">First: 2026-01-08T15:11:04+00:00 · Latest: 2026-01-08T15:11:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05011v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05011v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用预测熵进行零-shot音频语言分类中的自动提示加权</div>
<div class="mono" style="margin-top:8px">音频语言模型最近通过利用自然语言监督展示了强大的零-shot能力，可以在没有标记训练数据的情况下对音频事件进行分类。然而，它们的性能对文本提示的措辞非常敏感，细微的变化会导致准确性的大幅波动。之前的工作通过提示学习或提示集成来缓解这个问题。然而，这些策略要么需要注释数据，要么未能考虑到某些提示可能对性能产生负面影响。在本研究中，我们提出了一种基于熵的提示加权方法，旨在找到提示贡献的稳健组合，以最大化预测置信度。为此，我们制定了一个量身定制的目标函数，最小化预测熵以产生新的提示权重，利用低熵作为高置信度的代理。我们的方法可以应用于单个样本或一批音频样本，无需额外标签，并且计算开销微乎其微。在涵盖环境、城市和人声的五个音频分类数据集上的实验表明，与经典的提示集成方法相比，在零-shot设置中取得了一致的增益，准确性提升在整个基准测试中大约高出5倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of audio-language models in zero-shot audio classification, which is often hindered by the sensitivity of these models to the wording of text prompts. The authors propose an entropy-guided prompt weighting method that seeks to optimize the combination of prompt contributions by minimizing prediction entropy, thereby enhancing prediction confidence without the need for labeled data. Experimental results across five audio classification datasets show that this approach consistently outperforms traditional prompt ensembling methods in a zero-shot context, achieving accuracy improvements that are five times greater across the benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高音频语言模型在零样本音频分类中的性能，该性能对文本提示的措辞非常敏感。作者提出了一种基于熵的提示加权方法，通过最小化预测熵来增强预测信心，而无需额外的标注数据。跨五个音频分类数据集的实验结果表明，该方法在零样本环境中始终优于传统的提示集成方法，准确性提升幅度达到五倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype</div>
<div class="meta-line">Authors: Haihua Luo, Xuming Ran, Zhengji Li, Huiyan Xue, Tingting Jiang, Jiangrong Shen, Tommi Kärkkäinen, Qi Xu, Fengyu Cong</div>
<div class="meta-line">First: 2026-01-08T11:59:35+00:00 · Latest: 2026-01-08T11:59:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无键值对的持续学习者通过任务特定的提示原型</div>
<div class="mono" style="margin-top:8px">持续学习旨在使模型在保留先前学习的信息的同时获取新知识。基于提示的方法在这一领域表现出色；然而，它们通常依赖于键值配对，这可能引入任务间干扰并阻碍可扩展性。为克服这些限制，我们提出了一种新方法，采用任务特定的提示原型（ProP），从而消除了对键值对的需求。在我们的方法中，任务特定的提示促进了当前任务的更有效特征学习，而相应的原型则捕捉输入的代表性特征。在推理过程中，通过将每个任务特定的提示与其相关的原型绑定来生成预测。此外，我们在提示初始化期间引入正则化约束，以惩罚过大的值，从而增强稳定性。在多个广泛使用的数据集上的实验表明了所提方法的有效性。与主流的基于提示的方法相比，我们的框架消除了对键值对的依赖，为未来的持续学习研究提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance continual learning by enabling models to acquire new knowledge without losing previously learned information, while addressing the limitations of key-value pairing in prompt-based methods. The authors propose a novel approach called task-specific Prompt-Prototype (ProP), which eliminates the need for key-value pairs by using task-specific prompts for effective feature learning and prototypes to capture representative input features. Experimental results on several widely used datasets indicate that this method improves stability and performance compared to traditional prompt-based approaches, providing a new direction for continual learning research.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过使模型在获取新知识的同时保留先前学习的信息来改善持续学习，解决了现有依赖键值对的提示方法的局限性。作者提出了一种新方法，称为任务特定的提示原型（ProP），通过使用任务特定的提示进行有效特征学习，并使用原型捕捉代表性特征，从而消除了对键值对的需求。对多个广泛使用的数据集的实验结果表明，该方法在稳定性和性能上优于传统的基于提示的方法，为持续学习研究提供了新的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Single Image Reflection Separation via Dual Prior Interaction Transformer</div>
<div class="meta-line">Authors: Yue Huang, Zi&#x27;ang Li, Tianle Hu, Jie Wen, Guanbin Li, Jinglin Zhang, Guoxu Zhou, Xiaozhao Fang</div>
<div class="meta-line">First: 2025-05-19T02:50:15+00:00 · Latest: 2026-01-08T11:53:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12641v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12641v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single image reflection separation aims to separate the transmission and reflection layers from a mixed image. Existing methods typically combine general priors from pre-trained models with task-specific priors such as text prompts and reflection detection. However, the transmission prior, as the most direct task-specific prior for the target transmission layer, has not been effectively modeled or fully utilized, limiting performance in complex scenarios. To address this issue, we propose a dual-prior interaction framework based on lightweight transmission prior generation and effective prior fusion. First, we design a Local Linear Correction Network (LLCN) that finetunes pre-trained models based on the physical constraint T=SI+B, where S and B represent pixel-wise and channel-wise scaling and bias transformations. LLCN efficiently generates high-quality transmission priors with minimal parameters. Second, we construct a Dual-Prior Interaction Transformer (DPIT) that employs a dual-stream channel reorganization attention mechanism. By reorganizing features from general and transmission priors for attention computation, DPIT achieves deep fusion of both priors, fully exploiting their complementary information. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双先验交互变换器进行单幅图像反射分离</div>
<div class="mono" style="margin-top:8px">单幅图像反射分离旨在从混合图像中分离传输层和反射层。现有方法通常将预训练模型的通用先验与任务特定的先验（如文本提示和反射检测）结合。然而，作为目标传输层最直接的任务特定先验，传输先验尚未得到有效建模或充分利用，限制了在复杂场景中的性能。为了解决这个问题，我们提出了一种基于轻量级传输先验生成和有效先验融合的双先验交互框架。首先，我们设计了一个局部线性校正网络（LLCN），该网络基于物理约束T=SI+B微调预训练模型，其中S和B分别表示逐像素和逐通道的缩放和偏置变换。LLCN以最小的参数高效生成高质量的传输先验。其次，我们构建了一个双先验交互变换器（DPIT），该变换器采用双流通道重组注意力机制。通过重组来自通用和传输先验的特征进行注意力计算，DPIT实现了两种先验的深度融合，充分利用了它们的互补信息。在多个基准数据集上的实验结果表明，所提出的方法达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve single image reflection separation by effectively modeling the transmission prior, which has been underutilized in existing methods. The authors propose a dual-prior interaction framework that includes a Local Linear Correction Network (LLCN) for generating high-quality transmission priors and a Dual-Prior Interaction Transformer (DPIT) for fusing general and transmission priors through a dual-stream attention mechanism. Experimental results on various benchmark datasets indicate that this approach achieves state-of-the-art performance in separating transmission and reflection layers from mixed images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效建模传输先验来改善单幅图像反射分离，而现有方法对其利用不足。作者提出了一种双先验交互框架，包括一个局部线性校正网络（LLCN）用于生成高质量的传输先验，以及一个双先验交互变换器（DPIT）通过双流注意机制融合一般先验和传输先验。多个基准数据集上的实验结果表明，该方法在从混合图像中分离传输层和反射层方面达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation</div>
<div class="meta-line">Authors: Ayush Pande</div>
<div class="meta-line">First: 2026-01-08T11:53:04+00:00 · Latest: 2026-01-08T11:53:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04860v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04860v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DivAS：通过深度加权体素聚合进行NeRF的交互式3D分割</div>
<div class="mono" style="margin-top:8px">现有的神经辐射场（NeRF）分割方法通常基于优化，需要缓慢的逐场景训练，牺牲了2D基础模型的零-shot能力。我们提出了DivAS（深度交互体素聚合分割），这是一种无优化、完全交互的框架，解决了这些局限性。我们的方法通过快速的基于GUI的工作流程运行，用户点提示生成的2D SAM掩膜使用NeRF衍生的深度先验进行精细化，以提高几何精度和前景-背景分离。我们贡献的核心是一个自定义的CUDA内核，它在200毫秒内将这些精细化的多视图掩膜聚合成一个统一的3D体素网格，实现实时视觉反馈。这种无优化设计消除了逐场景训练的需要。在Mip-NeRF 360°和LLFF上的实验表明，DivAS实现的分割质量可与基于优化的方法相媲美，同时端到端速度快2-2.5倍，排除用户提示时间时速度快达一个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the segmentation of Neural Radiance Fields (NeRFs) without the slow per-scene training typical of existing optimization-based methods. The authors present DivAS, an interactive framework that utilizes a GUI-based workflow to refine 2D SAM masks with NeRF-derived depth priors, enhancing geometric accuracy and foreground-background separation. Key experimental results demonstrate that DivAS achieves segmentation quality comparable to traditional methods while being 2-2.5 times faster overall and up to ten times faster when excluding user prompting time.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善神经辐射场（NeRFs）的分割，避免现有优化方法中典型的慢场景训练。作者提出了DivAS，这是一种无优化且完全交互的框架，利用基于GUI的工作流程来细化由用户提示生成的2D SAM掩码，并利用NeRF衍生的深度先验来增强几何精度。实验结果表明，DivAS在分割质量上可与传统方法相媲美，同时整体速度提高了2-2.5倍，排除用户提示时间后速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</div>
<div class="meta-line">Authors: Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</div>
<div class="meta-line">First: 2026-01-06T17:15:50+00:00 · Latest: 2026-01-08T11:08:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03193v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03193v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniCorn：通过自生成监督实现自我改进的统一多模态模型</div>
<div class="mono" style="margin-top:8px">尽管统一多模态模型（UMMs）在跨模态理解方面取得了显著成功，但它们在利用内部知识进行高质量生成方面仍存在显著差距。我们将这种差距形式化为传导性失语症，这是一种模型能够准确解释多模态输入但难以将这种理解转化为真实且可控的合成的现象。为了解决这个问题，我们提出了UniCorn，一个简单而优雅的自我改进框架，消除了对外部数据或教师监督的需求。通过将单个UMM划分为三个协作角色：提议者、解决者和评判者，UniCorn通过自我对弈生成高质量的交互，并采用认知模式重构将潜在理解提炼为明确的生成信号。为了验证多模态一致性的恢复，我们引入了UniCycle，这是一个基于文本到图像再到文本重构循环的循环一致性基准。大量实验表明，UniCorn在六个通用图像生成基准上相较于基础模型实现了全面且显著的改进。值得注意的是，它在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle上达到了SOTA性能，同时在WISE上进一步实现了+5.0的显著提升，在OneIG上实现了+6.5的提升。这些结果突显了我们的方法显著增强了T2I生成，同时保持了强大的理解能力，展示了完全自我监督精炼在统一多模态智能中的可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of Unified Multimodal Models (UMMs) in generating high-quality outputs despite their success in cross-modal comprehension, a gap termed Conduction Aphasia. The authors propose UniCorn, a self-improvement framework that operates without external data by dividing a UMM into three roles: Proposer, Solver, and Judge, facilitating high-quality interactions through self-play and cognitive pattern reconstruction. Experimental results show that UniCorn significantly enhances performance across six image generation benchmarks, achieving state-of-the-art results on several metrics and demonstrating its effectiveness in improving text-to-image generation while maintaining comprehension capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型（UMMs）在从多模态输入生成高质量输出方面的能力，解决被称为传导失语症的问题。作者提出了一种名为UniCorn的自我改进框架，将UMM组织为三个角色——提议者、解决者和评判者，以促进自我对弈和认知模式重建，而无需依赖外部数据。实验结果表明，UniCorn在六个图像生成基准测试中显著提升了性能，在多个任务上达到了最先进的结果，并证明了其在改善文本到图像生成的同时保持理解能力的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
