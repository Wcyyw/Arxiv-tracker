<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 03:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0339</div>
    <div class="row"><div class="card">
<div class="title">Protein Autoregressive Modeling via Multiscale Structure Generation</div>
<div class="meta-line">Authors: Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu</div>
<div class="meta-line">First: 2026-02-04T18:59:49+00:00 · Latest: 2026-02-04T18:59:49+00:00</div>
<div class="meta-line">Comments: ByteDance Seed Tech Report; Page: https://par-protein.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04883v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04883v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://par-protein.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多尺度结构生成的蛋白质自回归建模</div>
<div class="mono" style="margin-top:8px">我们提出了蛋白质自回归建模（PAR），这是第一个用于蛋白质主链生成的多尺度自回归框架，通过粗到细的下一尺度预测。利用蛋白质的层次特性，PAR生成的结构类似于雕刻雕像，形成粗略的拓扑结构，并在不同尺度上细化结构细节。为此，PAR由三个关键组件组成：（i）多尺度下采样操作，在训练过程中表示不同尺度的蛋白质结构；（ii）自回归变换器，编码多尺度信息并生成条件嵌入以指导结构生成；（iii）基于流的主链解码器，根据这些嵌入生成主链原子。此外，自回归模型受到曝光偏差的影响，由于训练和生成过程的不匹配，显著降低了结构生成质量。我们通过采用噪声上下文学习和定期采样有效缓解了这一问题，使主链生成更加稳健。值得注意的是，PAR表现出强大的零样本泛化能力，支持灵活的人类提示条件生成和基元支架，而无需微调。在无条件生成基准上，PAR有效学习蛋白质分布，生成高设计质量的主链，并表现出良好的扩展性。这些特性共同确立了PAR作为蛋白质结构生成的有前景框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a robust framework for protein backbone generation that addresses the limitations of existing autoregressive models. The authors introduce protein autoregressive modeling (PAR), which employs a multi-scale autoregressive approach to generate protein structures by refining coarse topologies into detailed configurations. Key experimental findings indicate that PAR effectively mitigates exposure bias through techniques like noisy context learning and scheduled sampling, resulting in high-quality backbone generation and strong zero-shot generalization capabilities, allowing for flexible conditional generation without the need for fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种新框架，用于蛋白质主链生成，以解决现有自回归模型的局限性。作者提出了蛋白质自回归建模（PAR），采用多尺度方法通过分层预测来生成蛋白质结构，逐步细化粗略拓扑。主要发现表明，PAR通过噪声上下文学习和调度采样等技术有效缓解了曝光偏差，从而实现高质量的主链生成和强大的零样本泛化能力，允许灵活的条件生成而无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Continual Learning for Model Adaptability in Internet of Things</div>
<div class="meta-line">Authors: Ajesh Koyatan Chathoth</div>
<div class="meta-line">First: 2026-02-04T18:59:14+00:00 · Latest: 2026-02-04T18:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04881v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物联网中模型适应性的对比持续学习</div>
<div class="mono" style="margin-top:8px">物联网（IoT）部署在非平稳的动态环境中运行，其中传感器漂移、用户行为演变和异构用户隐私要求等因素可能影响应用效用。持续学习（CL）通过在不发生灾难性遗忘的情况下随时间调整模型来解决这个问题。同时，对比学习作为一种强大的表示学习范式，已在自监督方式中提高了鲁棒性和样本效率。本文回顾了对比持续学习（CCL）在物联网中的应用，将算法设计（重放、正则化、蒸馏、提示）与物联网系统现实（TinyML约束、间歇性连接、隐私）相连接。我们提出了一个统一的问题表述，推导出结合对比损失和蒸馏损失的共同目标，提出了一个面向物联网的参考架构，用于设备端、边缘和基于云的CCL，并提供了评估协议和指标的指导。最后，我们强调了与物联网领域相关的独特开放挑战，例如跨表格和流式物联网数据、概念漂移、联邦设置和能量感知训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance model adaptability in Internet of Things (IoT) environments, which are characterized by dynamic conditions such as sensor drift and evolving user behaviors. The authors employ a method called contrastive continual learning (CCL), which integrates contrastive learning with continual learning techniques to improve model robustness and efficiency while addressing IoT-specific challenges like privacy and connectivity. Key experimental findings indicate that the proposed CCL framework effectively combines contrastive and distillation losses, providing a structured approach to model adaptation that can handle various IoT data types and conditions, while also identifying unique challenges in the IoT domain such as concept drift and energy-aware training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高物联网（IoT）环境中模型的适应性，这些环境具有非平稳条件和多变的用户需求。作者采用了一种称为对比持续学习（CCL）的方法，该方法将对比学习技术与持续学习原则相结合，以应对传感器漂移和用户行为变化等挑战。主要发现表明，CCL能够有效提高模型的鲁棒性和样本效率，同时适应物联网的限制，论文还提出了一个参考架构以及评估协议，以指导该领域未来的研究。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过人机协作的贝叶斯优化进行个性化图像生成</div>
<div class="mono" style="margin-top:8px">想象爱丽丝心中有一幅特定的图像 $x^\ast$，比如她童年时成长的街道景象。为了生成那幅确切的图像，她通过多轮提示引导生成模型，最终得到了图像 $x^{p*}$。尽管 $x^{p*}$ 与 $x^\ast$ 相对接近，但爱丽丝发现使用语言提示很难缩小这个差距。本文旨在通过观察即使在语言达到极限后，人类仍能判断新图像 $x^+$ 是否比 $x^{p*}$ 更接近 $x^\ast$ 来缩小这一差距。基于这一观察，我们开发了 MultiBO（多选优先贝叶斯优化），该方法根据 $x^{p*}$ 精心生成 $K$ 幅新图像，获取用户的优先反馈，利用反馈指导扩散模型，最终生成一组新的 $K$ 幅图像。我们展示了在 $B$ 轮用户反馈内，即使生成模型对 $x^\ast$ 没有信息，也能更接近 $x^\ast$。来自 $30$ 位用户的定性评分，以及与 $5$ 个基线的定量指标比较，显示出良好的结果，表明人类的多选反馈可以有效地用于个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve personalized image generation by addressing the limitations of language prompts in conveying specific visual concepts. The authors propose a method called MultiBO (Multi-Choice Preferential Bayesian Optimization), which generates multiple images based on user feedback to refine the output closer to the user&#x27;s desired image. Experimental results demonstrate that after several rounds of user feedback, the method significantly narrows the gap between the generated images and the user&#x27;s intended image, as evidenced by qualitative assessments from 30 users and quantitative comparisons with five baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决语言提示在引导生成模型中的局限性来改善个性化图像生成。作者提出了一种名为MultiBO（多选偏好贝叶斯优化）的方法，该方法基于用户反馈生成多个图像，以迭代地优化输出。实验结果表明，在经过几轮用户反馈后，生成的图像可以显著接近用户期望的图像，这一点通过30名用户的定性评估和与五个基线方法的定量比较得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</div>
<div class="meta-line">Authors: Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-02T18:34:35+00:00 · Latest: 2026-02-04T17:38:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02437v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02437v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through two complementary reasoning paradigms. We incorporate world knowledge-enhanced textual reasoning into generation to infer implicit knowledge, and leverage editing capabilities for fine-grained editing-like visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared architecture, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for textual reasoning, alongside an agent-generated corpus for visual refinement. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniReason 1.0：一个统一的推理框架，用于与世界知识对齐的图像生成和编辑</div>
<div class="mono" style="margin-top:8px">统一的多模态模型在需要深度推理的复杂合成任务中常常表现不佳，通常将文本到图像生成和图像编辑视为孤立的能力，而不是相互关联的推理步骤。为了解决这个问题，我们提出了UniReason，一个通过两种互补的推理范式来协调这两项任务的统一框架。我们将增强世界知识的文本推理纳入生成中，以推断隐含知识，并利用编辑能力进行细粒度的编辑式视觉细化，通过自我反思进一步纠正视觉错误。这种方法在共享架构中统一了生成和编辑，反映了人类认知过程中的规划与细化。我们通过系统构建一个大型推理中心数据集（约30万样本），涵盖五个主要知识领域（例如，文化常识、物理等）来支持这一框架，同时提供一个代理生成的视觉细化语料库。大量实验表明，UniReason在推理密集型基准测试（如WISE、KrisBench和UniREditBench）上实现了先进的性能，同时保持了优越的综合合成能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of unified multimodal models in complex synthesis tasks that require deep reasoning, as these models often treat text-to-image generation and image editing as separate processes. The authors propose UniReason, a unified framework that integrates these tasks through complementary reasoning paradigms, incorporating world knowledge-enhanced textual reasoning for implicit knowledge inference and editing capabilities for visual refinement. Experimental results show that UniReason outperforms existing models on reasoning-intensive benchmarks such as WISE, KrisBench, and UniREditBench while also demonstrating strong general synthesis capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型在需要深度推理的复杂合成任务中的表现，因为这些模型通常将文本到图像生成和图像编辑视为独立的过程。作者提出了UniReason，一个统一框架，将增强世界知识的文本推理与图像生成和编辑能力相结合，从而实现更连贯的推理过程。实验结果表明，UniReason在WISE、KrisBench和UniREditBench等推理密集型基准测试中显著优于现有模型，同时也展现出强大的通用合成能力。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Modeling via Drifting</div>
<div class="meta-line">Authors: Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</div>
<div class="meta-line">First: 2026-02-04T17:06:49+00:00 · Latest: 2026-02-04T17:06:49+00:00</div>
<div class="meta-line">Comments: Project page: https://lambertae.github.io/projects/drifting/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04770v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04770v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lambertae.github.io/projects/drifting/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>漂移生成建模</div>
<div class="mono" style="margin-top:8px">生成建模可以被表述为学习一个映射 f，使其推前分布与数据分布匹配。推前行为可以在推理时迭代进行，例如在扩散和流动模型中。在本文中，我们提出了一种新的范式，称为漂移模型，它在训练过程中演变推前分布，并自然地允许一步推理。我们引入了一个漂移场，控制样本运动，并在分布匹配时达到平衡。这导致了一个训练目标，使神经网络优化器能够演变分布。在实验中，我们的一步生成器在 256 x 256 分辨率下在 ImageNet 上取得了最先进的结果，潜在空间的 FID 为 1.54，像素空间的 FID 为 1.61。我们希望我们的工作为高质量的一步生成开辟新的机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve generative modeling by developing a method that allows for efficient one-step inference while maintaining high-quality output. The authors introduce a new paradigm called Drifting Models, which involves evolving the pushforward distribution during training through a drifting field that governs sample movement until equilibrium is reached. Experimental results demonstrate that their one-step generator achieves state-of-the-art performance on ImageNet at 256 x 256 resolution, with an FID score of 1.54 in latent space and 1.61 in pixel space, indicating significant advancements in generative modeling efficiency and quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过开发一种方法来提高生成建模的效率，使其能够在保持高质量输出的同时实现高效的一步推理。作者提出了一种新的范式，称为漂移模型，该模型通过漂移场在训练过程中演变推前分布，直到样本运动达到平衡。实验结果表明，他们的一步生成器在256 x 256分辨率的ImageNet上达到了最先进的性能，潜在空间的FID得分为1.54，像素空间的FID得分为1.61。</div>
</details>
</div>
<div class="card">
<div class="title">Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation</div>
<div class="meta-line">Authors: Lei Tong, Zhihua Liu, Chaochao Lu, Dino Oglic, Tom Diethe, Philip Teare, Sotirios A. Tsaftaris, Chen Jin</div>
<div class="meta-line">First: 2025-09-29T13:49:28+00:00 · Latest: 2026-02-04T17:04:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://leitong02.github.io/causaladapter/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24798v5">Abs</a> · <a href="https://arxiv.org/pdf/2509.24798v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://leitong02.github.io/causaladapter/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method supports causal interventions on target attributes and consistently propagates their effects to causal dependents while preserving the core identity of the image. Unlike prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling with two attribute-regularization strategies: (i) prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and (ii) a conditioned token contrastive loss that disentangles attribute factors and reduces spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, including up to a 91% reduction in MAE on Pendulum for accurate attribute control and up to an 87% reduction in FID on ADNI for high-fidelity MRI generation. These results demonstrate robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation. Code and models will be released at: https://leitong02.github.io/causaladapter/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果适配器：驯服文本到图像扩散以实现忠实的反事实生成</div>
<div class="mono" style="margin-top:8px">我们提出了因果适配器，这是一个模块化框架，旨在适应冻结的文本到图像扩散骨干网络以进行反事实图像生成。我们的方法支持对目标属性的因果干预，并始终将其影响传播到因果依赖项，同时保持图像的核心身份。与依赖于提示工程而没有明确因果结构的先前方法不同，因果适配器利用结构因果建模和两种属性正则化策略：（i）提示对齐注入，将因果属性与文本嵌入对齐以实现精确的语义控制，以及（ii）条件令牌对比损失，解耦属性因素并减少虚假相关性。因果适配器在合成和真实世界数据集上均实现了最先进的性能，包括在Pendulum上实现高达91%的MAE减少以进行准确的属性控制，以及在ADNI上实现高达87%的FID减少以进行高保真MRI生成。这些结果展示了强大的、可推广的反事实编辑，具有忠实的属性修改和强大的身份保留。代码和模型将发布在：https://leitong02.github.io/causaladapter/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective counterfactual image generation that maintains the core identity of images while allowing for attribute modifications. The authors introduce Causal-Adapter, a modular framework that utilizes structural causal modeling and two attribute-regularization strategies: prompt-aligned injection for semantic control and a conditioned token contrastive loss to disentangle attribute factors. Experimental results show that Causal-Adapter achieves state-of-the-art performance, with a 91% reduction in mean absolute error on the Pendulum dataset and an 87% reduction in Fréchet Inception Distance on the ADNI dataset, indicating its effectiveness in robust counterfactual editing and faithful attribute modification.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过适应文本到图像的扩散模型来改善反事实图像生成，同时保持图像的核心身份。作者提出了Causal-Adapter，这是一个模块化框架，采用结构因果建模和两种属性正则化策略：用于语义控制的提示对齐注入和用于解耦属性因素的条件令牌对比损失。实验结果表明，Causal-Adapter在性能上达到了最先进的水平，在Pendulum数据集上实现了91%的平均绝对误差减少，以实现属性控制，并在ADNI数据集上实现了87%的Fréchet Inception距离减少，以生成高保真MRI，表明其有效且稳健的反事实编辑能力。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Long-Tail Bias via Prompt-Controlled Diffusion Augmentation</div>
<div class="meta-line">Authors: Buddhi Wijenayake, Nichula Wasalathilake, Roshan Godaliyadda, Vijitha Herath, Parakrama Ekanayake, Vishal M. Patel</div>
<div class="meta-line">First: 2026-02-04T16:49:16+00:00 · Latest: 2026-02-04T16:49:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04749v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04749v1">PDF</a> · <a href="https://github.com/Buddhi19/SyntheticGen.git}{Github">Code1</a> · <a href="https://github.com/Buddhi19/SyntheticGen.git">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic segmentation of high-resolution remote-sensing imagery is critical for urban mapping and land-cover monitoring, yet training data typically exhibits severe long-tailed pixel imbalance. In the dataset LoveDA, this challenge is compounded by an explicit Urban/Rural split with distinct appearance and inconsistent class-frequency statistics across domains. We present a prompt-controlled diffusion augmentation framework that synthesizes paired label--image samples with explicit control of both domain and semantic composition. Stage~A uses a domain-aware, masked ratio-conditioned discrete diffusion model to generate layouts that satisfy user-specified class-ratio targets while respecting learned co-occurrence structure. Stage~B translates layouts into photorealistic, domain-consistent images using Stable Diffusion with ControlNet guidance. Mixing the resulting ratio and domain-controlled synthetic pairs with real data yields consistent improvements across multiple segmentation backbones, with gains concentrated on minority classes and improved Urban and Rural generalization, demonstrating controllable augmentation as a practical mechanism to mitigate long-tail bias in remote-sensing segmentation. Source codes, pretrained models, and synthetic datasets are available at \href{https://github.com/Buddhi19/SyntheticGen.git}{Github}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过提示控制的扩散增强减轻长尾偏差</div>
<div class="mono" style="margin-top:8px">高分辨率遥感图像的语义分割对于城市制图和土地覆盖监测至关重要，但训练数据通常表现出严重的长尾像素不平衡。在数据集LoveDA中，这一挑战因城市/乡村的明确划分而加剧，导致不同领域之间外观和类频统计不一致。我们提出了一种提示控制的扩散增强框架，合成具有明确领域和语义组成控制的配对标签-图像样本。阶段A使用领域感知的掩码比率条件离散扩散模型生成满足用户指定类比率目标的布局，同时尊重学习到的共现结构。阶段B使用带有ControlNet指导的稳定扩散将布局转换为照片真实感、领域一致的图像。将生成的比率和领域控制的合成对与真实数据混合，带来了多个分割骨干网络的一致性提升，增益集中在少数类上，并改善了城市和乡村的泛化，展示了可控增强作为减轻遥感分割中长尾偏差的实用机制。源代码、预训练模型和合成数据集可在\href{https://github.com/Buddhi19/SyntheticGen.git}{Github}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the severe long-tailed pixel imbalance in semantic segmentation of high-resolution remote-sensing imagery, particularly in the LoveDA dataset which features an Urban/Rural split with varying class frequencies. The authors propose a prompt-controlled diffusion augmentation framework that consists of two stages: the first stage generates layouts using a domain-aware, masked ratio-conditioned discrete diffusion model to meet user-defined class-ratio targets, while the second stage translates these layouts into photorealistic images with Stable Diffusion and ControlNet guidance. The experimental results show that combining the synthetic pairs generated through this method with real data leads to significant improvements in segmentation performance, especially for minority classes, and enhances generalization across Urban and Rural domains, effectively mitigating long-tail bias in remote-sensing segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决高分辨率遥感图像语义分割训练数据中严重的长尾像素不平衡问题，特别是在具有城市/乡村划分和不同类别频率的LoveDA数据集中。作者提出了一种提示控制的扩散增强框架，分为两个阶段：第一阶段使用领域感知的掩蔽比例条件离散扩散模型生成满足用户定义类别比例目标的布局，第二阶段使用带有ControlNet指导的稳定扩散将这些布局转换为逼真的图像。实验结果表明，将合成样本与真实数据结合使用显著提高了分割性能，尤其是对少数类的表现，并增强了城市和乡村领域的泛化能力，表明可控增强有效缓解了遥感分割任务中的长尾偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Prompt Elicitation for Text-to-Image Generation</div>
<div class="meta-line">Authors: Xinyi Wen, Lena Hegemann, Xiaofu Jin, Shuai Ma, Antti Oulasvirta</div>
<div class="meta-line">First: 2026-02-04T16:24:46+00:00 · Latest: 2026-02-04T16:24:46+00:00</div>
<div class="meta-line">Comments: ACM International Conference on Intelligent User Interfaces (IUI) 2026, March 23-26, Paphos, Cyprus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04713v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning text-to-image generation with user intent remains challenging, for users who provide ambiguous inputs and struggle with model idiosyncrasies. We propose Adaptive Prompt Elicitation (APE), a technique that adaptively asks visual queries to help users refine prompts without extensive writing. Our technical contribution is a formulation of interactive intent inference under an information-theoretic framework. APE represents latent intent as interpretable feature requirements using language model priors, adaptively generates visual queries, and compiles elicited requirements into effective prompts. Evaluation on IDEA-Bench and DesignBench shows that APE achieves stronger alignment with improved efficiency. A user study with challenging user-defined tasks demonstrates 19.8% higher alignment without workload overhead. Our work contributes a principled approach to prompting that, for general users, offers an effective and efficient complement to the prevailing prompt-based interaction paradigm with text-to-image models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像生成的自适应提示引导</div>
<div class="mono" style="margin-top:8px">将文本到图像生成与用户意图对齐仍然具有挑战性，尤其是对于提供模糊输入并且难以适应模型特性的用户。我们提出了自适应提示引导（APE），这是一种自适应地提出视觉查询的技术，帮助用户在不需要大量写作的情况下细化提示。我们的技术贡献是在信息论框架下对交互意图推断的公式化。APE将潜在意图表示为可解释的特征需求，使用语言模型先验，自适应生成视觉查询，并将引导的需求汇编成有效的提示。在IDEA-Bench和DesignBench上的评估表明，APE在提高效率的同时实现了更强的对齐。针对具有挑战性的用户定义任务的用户研究表明，APE在没有工作负担增加的情况下实现了19.8%的更高对齐。我们的工作为提示提供了一种有原则的方法，为普通用户提供了对当前基于提示的文本到图像模型交互范式的有效且高效的补充。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of text-to-image generation with user intent, particularly for users who provide ambiguous inputs. The authors introduce Adaptive Prompt Elicitation (APE), a method that interactively asks visual queries to help users refine their prompts without requiring extensive writing. Experimental results on IDEA-Bench and DesignBench indicate that APE achieves better alignment with user intent and increased efficiency, with a user study revealing a 19.8% improvement in alignment without adding to user workload.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善文本到图像生成与用户意图之间的对齐，特别是对于提供模糊输入的用户。作者提出了自适应提示引导（APE）方法，该方法通过交互生成视觉查询，帮助用户在无需大量书写的情况下细化提示。IDEA-Bench和DesignBench上的实验结果表明，APE在对齐和效率方面表现更佳，用户研究显示在不增加用户工作负担的情况下，对齐度提高了19.8%，因此为文本到图像模型中的提示提供了一种更有效的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating Disability Representations in Text-to-Image Models</div>
<div class="meta-line">Authors: Yang Yian, Yu Fan, Liudmila Zavolokina, Sarah Ebling</div>
<div class="meta-line">First: 2026-02-04T15:54:25+00:00 · Latest: 2026-02-04T15:54:25+00:00</div>
<div class="meta-line">Comments: 21 pages, 9 figures. References included</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04687v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>调查文本到图像模型中的残疾表现</div>
<div class="mono" style="margin-top:8px">文本到图像生成模型在从文本描述中生成高质量视觉内容方面取得了显著进展，但关于它们如何表现社会群体的担忧仍然存在。尽管性别和种族等特征受到越来越多的关注，残疾表现仍然未被充分探索。本研究通过分析Stable Diffusion XL和DALL-E 3的输出，使用结构化提示设计调查残疾人在AI生成图像中的表现。我们通过比较通用残疾提示与特定残疾类别提示之间的图像相似性来分析残疾表现。此外，我们评估减轻策略如何影响残疾描绘，重点评估通过情感极性分析进行的情感框架，结合自动和人工评估。我们的研究结果揭示了持续的表现不平衡，并强调了持续评估和完善生成模型以促进更具多样性和包容性的残疾表现的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the underexplored issue of disability representations in text-to-image generative models, motivated by the growing concerns regarding the portrayal of social groups in AI-generated content. The researchers employed a structured prompt design to analyze outputs from Stable Diffusion XL and DALL-E 3, comparing image similarities between generic disability prompts and specific disability categories. The findings indicate persistent representational imbalances in how disabilities are depicted, emphasizing the necessity for ongoing evaluation and refinement of generative models to achieve more diverse and inclusive representations of disability.</div>
<div class="mono" style="margin-top:8px">本研究关注文本到图像生成模型中残疾表现的不足，尽管这些模型在从文本生成视觉内容方面取得了显著进展。研究人员采用结构化提示设计，分析了Stable Diffusion XL和DALL-E 3的输出，比较了通用残疾提示与特定残疾类别提示之间的图像相似性。研究结果表明，AI生成图像中仍存在表现不平衡，强调了对这些模型进行持续评估和改进以实现更具多样性和包容性的残疾表现的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution</div>
<div class="meta-line">Authors: Bowen Chai, Zheng Chen, Libo Zhu, Wenbo Li, Yong Guo, Yulun Zhang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-06T14:35:59+00:00 · Latest: 2026-02-04T14:34:40+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026. Code is available at: https://github.com/bowenchai/QuantVSR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04485v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.04485v2">PDF</a> · <a href="https://github.com/bowenchai/QuantVSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown superior performance in real-world video super-resolution (VSR). However, the slow processing speeds and heavy resource consumption of diffusion models hinder their practical application and deployment. Quantization offers a potential solution for compressing the VSR model. Nevertheless, quantizing VSR models is challenging due to their temporal characteristics and high fidelity requirements. To address these issues, we propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a spatio-temporal complexity aware (STCA) mechanism, where we first utilize the calibration dataset to measure both spatial and temporal complexities for each layer. Based on these statistics, we allocate layer-specific ranks to the low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine the FP and low-bit branches to achieve simultaneous optimization. In addition, we propose a learnable bias alignment (LBA) module to reduce the biased quantization errors. Extensive experiments on synthetic and real-world datasets demonstrate that our method obtains comparable performance with the FP model and significantly outperforms recent leading low-bit quantization methods. Code is available at: https://github.com/bowenchai/QuantVSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantVSR：用于真实世界视频超分辨率的低比特后训练量化</div>
<div class="mono" style="margin-top:8px">扩散模型在真实世界视频超分辨率（VSR）中表现出色。然而，扩散模型的处理速度慢和资源消耗大阻碍了其实际应用和部署。量化为压缩VSR模型提供了潜在解决方案。然而，由于VSR模型的时间特性和高保真度要求，量化VSR模型具有挑战性。为了解决这些问题，我们提出了QuantVSR，一种用于真实世界VSR的低比特量化模型。我们提出了一种时空复杂度感知（STCA）机制，首先利用校准数据集测量每一层的空间和时间复杂度。基于这些统计数据，我们为低秩全精度（FP）辅助分支分配层特定的秩。随后，我们联合优化FP和低比特分支以实现同步优化。此外，我们提出了一种可学习的偏差对齐（LBA）模块，以减少偏差量化误差。在合成和真实世界数据集上的大量实验表明，我们的方法在性能上与FP模型相当，并显著优于最近的领先低比特量化方法。代码可在以下链接获取：https://github.com/bowenchai/QuantVSR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the practical application of diffusion models in video super-resolution (VSR), which are hindered by slow processing speeds and high resource consumption. The authors propose QuantVSR, a low-bit quantization model that incorporates a spatio-temporal complexity aware (STCA) mechanism to measure spatial and temporal complexities for each layer, allowing for layer-specific rank allocation. Experimental results on synthetic and real-world datasets show that QuantVSR achieves performance comparable to full-precision models while significantly outperforming recent low-bit quantization methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决扩散模型在视频超分辨率（VSR）中的处理速度慢和资源消耗高的问题，来增强其实际应用。作者提出了QuantVSR，一种低比特量化模型，采用时空复杂度感知（STCA）机制，根据空间和时间复杂度测量和分配层特定的排名。对合成和真实世界数据集的实验结果表明，QuantVSR的性能与全精度模型相当，同时显著优于现有的低比特量化方法。</div>
</details>
</div>
<div class="card">
<div class="title">VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration</div>
<div class="meta-line">Authors: Jaeyoon Jung, Yejun Yoon, Seunghyun Yoon, Kunwoo Park</div>
<div class="meta-line">First: 2026-02-04T14:12:55+00:00 · Latest: 2026-02-04T14:12:55+00:00</div>
<div class="meta-line">Comments: A system description paper for the AVerImaTeC shared task at the Ninth FEVER Workshop (co-located with EACL 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04587v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04587v1">PDF</a> · <a href="https://github.com/ssu-humane/VILLAIN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VILLAIN在AVerImaTeC：通过多智能体协作验证图像-文本声明</div>
<div class="mono" style="margin-top:8px">本文描述了VILLAIN，一个通过基于提示的多智能体协作验证图像-文本声明的多模态事实检查系统。在AVerImaTeC共享任务中，VILLAIN在多个事实检查阶段采用视觉-语言模型智能体。从通过额外网络收集丰富的知识库中检索文本和视觉证据。为了识别关键信息并解决证据项之间的不一致性，特定模态和跨模态智能体生成分析报告。在后续阶段，基于这些报告生成问答对。最后，裁决预测智能体根据图像-文本声明和生成的问答对产生验证结果。我们的系统在所有评估指标上排名第一。源代码可在https://github.com/ssu-humane/VILLAIN公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the verification of image-text claims through a collaborative multimodal approach. The authors developed VILLAIN, a fact-checking system that utilizes vision-language model agents in a multi-stage process, retrieving textual and visual evidence from an enriched knowledge store. The key findings indicate that VILLAIN effectively identifies inconsistencies and generates accurate analysis reports, leading to the successful prediction of verification outcomes, ultimately achieving the top rank in the AVerImaTeC shared task across all evaluation metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过系统的方法增强图像-文本声明的验证。作者开发了VILLAIN，一个多模态事实检查系统，利用基于提示的多代理协作，采用视觉-语言模型代理从丰富的知识库中检索文本和视觉证据。主要实验结果表明，VILLAIN成功生成分析报告和问答对，最终得出的验证结果在AVerImaTeC共享任务的所有评估指标中排名第一。</div>
</details>
</div>
<div class="card">
<div class="title">Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration</div>
<div class="meta-line">Authors: Jiaheng Liu, Yuanxing Zhang, Shihao Li, Xinping Lei</div>
<div class="meta-line">First: 2026-02-04T14:01:44+00:00 · Latest: 2026-02-04T14:01:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04575v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04575v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling&#x27;&#x27; manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator&#x27;s high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user&#x27;s role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe&#x27;&#x27; into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vibe AIGC：通过代理编排进行内容生成的新范式</div>
<div class="mono" style="margin-top:8px">在过去十年中，生成性人工智能（AI）的轨迹被以规模法则驱动的模型中心范式主导。尽管在视觉保真度上取得了显著飞跃，但这种方法遇到了“可用性天花板”，表现为意图-执行差距（即创作者的高层意图与当前单次模型的随机性、黑箱特性之间的根本差异）。在本文中，受Vibe编码的启发，我们引入了\textbf{Vibe AIGC}，一种通过代理编排进行内容生成的新范式，代表了分层多代理工作流的自主合成。在这一范式下，用户的角色超越了传统的提示工程，演变为指挥官，提供一个Vibe，一个包含审美偏好、功能逻辑等的高层次表示。一个集中式的元规划者则作为系统架构师，将这个“Vibe”分解为可执行、可验证和自适应的代理管道。通过从随机推理转向逻辑编排，Vibe AIGC弥合了人类想象与机器执行之间的差距。我们认为，这一转变将重新定义人类与AI的协作经济，将AI从脆弱的推理引擎转变为强大的系统级工程合作伙伴，从而使复杂的长期数字资产的创建实现民主化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of the current model-centric paradigm in generative artificial intelligence, particularly the Intent-Execution Gap that hinders usability. The authors propose the Vibe AIGC paradigm, which utilizes agentic orchestration to create hierarchical multi-agent workflows, allowing users to act as Commanders who provide a high-level Vibe that encapsulates their creative intent. The key findings indicate that by shifting from stochastic inference to logical orchestration, Vibe AIGC effectively bridges the gap between human imagination and machine execution, thereby enhancing the collaborative potential between humans and AI in generating complex digital assets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前生成性人工智能模型中心范式的局限性，特别是阻碍可用性的意图执行差距。作者提出了Vibe AIGC范式，该范式利用代理编排自主合成分层多代理工作流，使用户能够作为指挥者提供称为Vibe的高层次表示。主要实验结果表明，这种方法成功地从随机推理转变为逻辑编排，从而弥合人类意图与机器执行之间的差距，并有可能将人工智能转变为在复杂数字资产创建中更有效的合作伙伴。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Degradation with Vision Language Model</div>
<div class="meta-line">Authors: Guanzhou Lan, Chenyi Liao, Yuqi Yang, Qianli Ma, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</div>
<div class="meta-line">First: 2026-02-04T13:51:15+00:00 · Latest: 2026-02-04T13:51:15+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04565v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04565v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解视觉语言模型下的退化</div>
<div class="mono" style="margin-top:8px">理解视觉退化是计算机视觉中的一个关键但具有挑战性的问题。尽管最近的视觉-语言模型（VLMs）在定性描述方面表现出色，但它们在理解图像退化背后的参数物理方面往往不足。在这项工作中，我们将退化理解重新定义为一个层次结构预测任务，要求同时估计退化类型、参数键及其连续物理值。尽管这些子任务在不同的空间中操作，但我们证明它们可以统一在一个自回归下一个标记预测范式下，其误差受值空间量化网格的限制。基于这一见解，我们引入了DU-VLM，一个通过监督微调和使用结构化奖励的强化学习训练的多模态思维链模型。此外，我们展示了DU-VLM可以作为预训练扩散模型的零-shot 控制器，实现高保真图像恢复，而无需微调生成主干。我们还引入了\textbf{DU-110k}，一个包含110,000个干净-退化对及其物理注释的大规模数据集。大量实验表明，我们的方法在准确性和鲁棒性方面显著优于通用基线，并展现出对未见分布的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in understanding visual degradations in computer vision, particularly how Vision-Language Models (VLMs) struggle with the underlying parametric physics of these degradations. The authors propose a novel method that redefines degradation understanding as a hierarchical structured prediction task, integrating the estimation of degradation types, parameter keys, and their continuous physical values within a unified autoregressive next-token prediction framework. Experimental results show that their model, DU-VLM, significantly outperforms generalist baselines in accuracy and robustness, and it can effectively function as a zero-shot controller for pre-trained diffusion models, facilitating high-fidelity image restoration without requiring fine-tuning of the generative backbone, supported by the introduction of the DU-110k dataset containing 110,000 clean-degraded pairs with physical annotations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决计算机视觉中理解视觉退化的挑战，特别是现有视觉语言模型在掌握基础参数物理方面的局限性。作者提出了一种新方法，将退化理解重新定义为一个层次化结构预测任务，整合了退化类型、参数键及其连续物理值的估计，并在统一的自回归下一个标记预测框架中进行。实验结果表明，他们的模型DU-VLM经过监督微调和强化学习训练后，在准确性和鲁棒性方面显著优于通用基线，并展示了对预训练扩散模型的有效零-shot控制，能够在不进行额外微调的情况下实现高保真图像恢复，同时引入了包含11万个干净-退化对及物理注释的DU-110k数据集。</div>
</details>
</div>
<div class="card">
<div class="title">Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models</div>
<div class="meta-line">Authors: Cem Eteke, Enzo Tartaglione</div>
<div class="meta-line">First: 2026-02-04T13:39:00+00:00 · Latest: 2026-02-04T13:39:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04549v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04549v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Nix和Fix：利用扩散模型实现3D高斯点云1000倍压缩</div>
<div class="mono" style="margin-top:8px">3D高斯点云（3DGS）彻底改变了新视角渲染。与隐式表示从密集空间点推断不同，3DGS使用稀疏高斯。这使得实时性能成为可能，但增加了空间需求，阻碍了沉浸式通信等应用的发展。3DGS压缩作为一个领域应运而生，旨在缓解这一问题。尽管在低比特率下取得了显著进展，但压缩会引入显著降低视觉质量的伪影。我们提出了NiFi，一种通过伪影感知的基于扩散的一步蒸馏恢复极端3DGS压缩的方法。我们展示了该方法在极低比特率下（低至0.1 MB）实现了最先进的感知质量，并在可比的感知性能下实现了对3DGS的1000倍速率提升。代码将在接受后开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high space requirements of 3D Gaussian Splatting (3DGS), which, while enabling real-time novel view rendering, limits its applications in areas like immersive communication due to compression artifacts at low rates. The authors propose a novel method called NiFi, which utilizes artifact-aware, diffusion-based one-step distillation for extreme compression of 3DGS. Experimental results demonstrate that NiFi achieves state-of-the-art perceptual quality at extremely low data rates, reaching as low as 0.1 MB and providing up to 1000x compression improvement over traditional 3DGS while maintaining comparable visual performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决3D高斯点云（3DGS）所需的高空间要求，尽管3DGS能够实现实时的新视角渲染，但由于压缩伪影的存在，限制了其在沉浸式通信等领域的应用。作者提出了一种名为NiFi的新方法，利用基于伪影感知的扩散模型进行一步蒸馏，以实现3DGS的极端压缩。实验结果表明，NiFi在极低的数据率下实现了最先进的感知质量，数据量低至0.1 MB，并在保持可比视觉性能的同时，达到了近1000倍的压缩率。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian PINNs for uncertainty-aware inverse problems (BPINN-IP)</div>
<div class="meta-line">Authors: Ali Mohammad-Djafari</div>
<div class="meta-line">First: 2026-02-04T11:42:57+00:00 · Latest: 2026-02-04T11:42:57+00:00</div>
<div class="meta-line">Comments: submitted to ICIP 2006 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04459v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The main contribution of this paper is to develop a hierarchical Bayesian formulation of PINNs for linear inverse problems, which is called BPINN-IP. The proposed methodology extends PINN to account for prior knowledge on the nature of the expected NN output, as well as its weights. Also, as we can have access to the posterior probability distributions, naturally uncertainties can be quantified. Also, variational inference and Monte Carlo dropout are employed to provide predictive means and variances for reconstructed images. Un example of applications to deconvolution and super-resolution is considered, details of the different steps of implementations are given, and some preliminary results are presented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于不确定性感知逆问题的贝叶斯PINNs (BPINN-IP)</div>
<div class="mono" style="margin-top:8px">本文的主要贡献是开发一种用于线性逆问题的PINNs的层次贝叶斯公式，称为BPINN-IP。所提出的方法扩展了PINN，以考虑对期望神经网络输出及其权重的先验知识。此外，由于我们可以访问后验概率分布，自然可以量化不确定性。同时，采用变分推断和蒙特卡洛丢弃法提供重建图像的预测均值和方差。考虑了去卷积和超分辨率的应用示例，给出了不同实现步骤的详细信息，并呈现了一些初步结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for uncertainty quantification in linear inverse problems by developing a hierarchical Bayesian formulation of Physics-Informed Neural Networks (PINNs), termed BPINN-IP. The methodology incorporates prior knowledge about the expected neural network output and its weights, allowing for the estimation of posterior probability distributions. Key experimental findings demonstrate the application of BPINN-IP in deconvolution and super-resolution tasks, showcasing the ability to provide predictive means and variances for reconstructed images through variational inference and Monte Carlo dropout techniques.</div>
<div class="mono" style="margin-top:8px">本文通过引入一种分层贝叶斯公式的物理信息神经网络（PINNs），称为BPINN-IP，解决了线性逆问题中的不确定性挑战。该方法通过结合对预期神经网络输出及其权重的先验知识，增强了传统PINNs的能力，从而能够通过后验概率分布量化不确定性。实验结果表明，该方法在去卷积和超分辨率等应用中的有效性，初步结果显示使用变分推断和蒙特卡洛丢弃技术重建图像的预测均值和方差得到了改善。</div>
</details>
</div>
<div class="card">
<div class="title">DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training &amp; Inference</div>
<div class="meta-line">Authors: Zihan Wang, Hao Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yiqun Zhang, Jinghao Lin, Haihua Yang, Xiaozhong Ji</div>
<div class="meta-line">First: 2026-01-26T13:57:48+00:00 · Latest: 2026-02-04T11:31:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18496v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18496v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus &quot;find it but fail to use it,&quot; leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DEEPMED：通过多跳医学搜索数据和可控的代理训练与推理构建医学深度研究代理</div>
<div class="mono" style="margin-top:8px">医学推理模型受到参数知识的限制，因此容易遗忘和产生幻觉。深度研究（DR）模型将输出基于可验证的工具证据，并在一般领域表现强劲，但直接转移到医学领域的收益相对有限。我们将其归因于两个差距：任务特征和工具使用扩展。医学问题需要在知识密集的临床背景中进行证据解释；而一般的DR模型可以检索信息，但往往缺乏临床背景推理，因此“找到但未能使用”，使得医学能力限制了性能。此外，在医学场景中，盲目扩展工具调用可能会引入噪声背景，干扰敏感的医学推理，并促使沿错误路径重复寻求证据。因此，我们提出了DeepMed。对于数据，我们部署了一种多跳医学搜索问答合成方法，支持模型在医学背景中应用DR范式。对于训练，我们引入了一种难度感知的回合惩罚，以抑制过度的工具调用增长。对于推理，我们引入了一个监控器，以帮助在受控步骤内验证假设，避免背景腐烂。总体而言，在七个医学基准上，DeepMed平均提高了其基础模型9.79\%，并超越了更大的医学推理和DR模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of medical reasoning models, which struggle with knowledge retention and are prone to errors. The authors propose DeepMed, a model that utilizes a multi-hop med-search QA synthesis method to enhance the application of DeepResearch paradigms in medical contexts. Key experimental findings indicate that DeepMed improves its base model&#x27;s performance by an average of 9.79% across seven medical benchmarks, surpassing larger medical reasoning and DeepResearch models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决医学推理模型的局限性，这些模型通常因依赖参数知识而面临遗忘和幻觉等问题。作者提出了DeepMed，采用多跳医学搜索问答合成方法，以增强DeepResearch模型在医学背景下的应用。主要实验结果表明，DeepMed在七个医学基准测试中平均提高了9.79%，并超越了更大规模的医学推理和DeepResearch模型，有效应对了临床环境中证据解释和工具使用扩展的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data</div>
<div class="meta-line">Authors: Dmitry Karpov</div>
<div class="meta-line">First: 2026-02-04T11:14:29+00:00 · Latest: 2026-02-04T11:14:29+00:00</div>
<div class="meta-line">Comments: Accepted to EACL 2026 (LoResMT workshop)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04442v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>没有通用解决方案：使用合成和原始数据构建翻译系统，支持巴什基尔语、哈萨克语、吉尔吉斯语、鞑靼语和楚瓦什语</div>
<div class="mono" style="margin-top:8px">我们探讨了五种突厥语言对的机器翻译：俄语-巴什基尔语、俄语-哈萨克语、俄语-吉尔吉斯语、英语-鞑靼语、英语-楚瓦什语。使用LoRA对合成数据进行微调nllb-200-distilled-600M，哈萨克语的chrF++达到了49.71，巴什基尔语为46.94。使用DeepSeek-V3.2和检索到的相似示例进行提示，楚瓦什语的chrF++达到了39.47。对于鞑靼语，零-shot或基于检索的方法达到了chrF++ 41.6，而吉尔吉斯语的零-shot方法达到了45.6。我们发布了数据集和获得的权重。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve machine translation for five Turkic languages: Bashkir, Kazakh, Kyrgyz, Tatar, and Chuvash. The authors employed fine-tuning of the nllb-200-distilled-600M model with LoRA on synthetic data, achieving chrF++ scores of 49.71 for Kazakh and 46.94 for Bashkir. Additionally, they utilized DeepSeek-V3.2 for Chuvash, resulting in a chrF++ score of 39.47, while zero-shot and retrieval-based methods for Tatar yielded a score of 41.6, and a zero-shot approach for Kyrgyz reached 45.6. The dataset and model weights have been made publicly available.</div>
<div class="mono" style="margin-top:8px">本研究针对五种突厥语言（巴什基尔语、哈萨克语、吉尔吉斯语、鞑靼语和楚瓦什语）的机器翻译挑战进行探讨，旨在为这些语言开发有效的翻译系统。作者采用了对合成数据进行LoRA微调的nllb-200-distilled-600M模型，哈萨克语和巴什基尔语的chrF++得分分别达到了49.71和46.94。此外，他们使用DeepSeek-V3.2处理楚瓦什语，得分为39.47，而零-shot和检索基础方法在鞑靼语和吉尔吉斯语中分别得分为41.6和45.6，数据集和模型权重已公开发布。</div>
</details>
</div>
<div class="card">
<div class="title">Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare</div>
<div class="meta-line">Authors: Aavash Chhetri, Bibek Niroula, Pratik Shrestha, Yash Raj Shrestha, Lesley A Anderson, Prashnna K Gyawali, Loris Bazzani, Binod Bhattarai</div>
<div class="meta-line">First: 2026-02-04T10:50:15+00:00 · Latest: 2026-02-04T10:50:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04416v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04416v1">PDF</a> · <a href="https://github.com/bhattarailab/Med-MMFL-Benchmark">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Med-MMFL：医疗领域的多模态联邦学习基准</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）使得去中心化医疗机构之间能够协作训练模型，同时保护数据隐私。然而，医疗领域的FL基准仍然稀缺，现有的努力主要集中在单模态或双模态上，并且涵盖的医疗任务范围有限。这一差距凸显了标准化评估的必要性，以推动对医疗多模态联邦学习（MMFL）的系统理解。为此，我们推出了Med-MMFL，这是医疗领域首个全面的MMFL基准，涵盖多种模态、任务和联邦场景。我们的基准评估六种具有代表性的最先进FL算法，涵盖不同的聚合策略、损失公式和正则化技术。它涉及2到4种模态的数据集，共包括10种独特的医疗模态，包括文本、病理图像、心电图、X光、放射学报告和多种MRI序列。实验在自然联邦、合成IID和合成非IID设置中进行，以模拟现实世界的异质性。我们评估分割、分类、模态对齐（检索）和视觉问答（VQA）任务。为了支持未来在现实医疗环境下的多模态联邦学习（MMFL）方法的可重复性和公平比较，我们发布了完整的基准实现，包括数据处理和分区管道，网址为https://github.com/bhattarailab/Med-MMFL-Benchmark。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the lack of comprehensive benchmarks for multimodal federated learning (MMFL) in the medical field, where existing benchmarks have primarily focused on unimodal or bimodal approaches. The authors introduce Med-MMFL, a benchmark that evaluates six state-of-the-art federated learning algorithms across various medical modalities and tasks, including segmentation, classification, and retrieval. Key experimental findings demonstrate the benchmark&#x27;s capability to assess performance in diverse settings, including naturally federated, synthetic IID, and synthetic non-IID scenarios, thereby facilitating reproducibility and fair comparison of future MMFL methods in realistic medical contexts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决医疗领域多模态联邦学习（MMFL）基准缺乏的问题，现有基准的范围有限，主要集中在单模态或双模态数据上。作者提出了Med-MMFL，一个评估六种最先进的联邦学习算法的基准，涵盖了多种医疗任务和模态，包括文本、病理图像和心电图。关键实验结果表明，该基准在不同聚合策略和损失公式的评估中有效，适用于包括自然联邦和合成环境在内的多样化设置，从而促进未来MMFL方法在现实医疗环境中的可重复性和比较。</div>
</details>
</div>
<div class="card">
<div class="title">LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration</div>
<div class="meta-line">Authors: Jue Gong, Zihan Zhou, Jingkai Wang, Shu Li, Libo Liu, Jianliang Lan, Yulun Zhang</div>
<div class="meta-line">First: 2026-02-04T10:37:46+00:00 · Latest: 2026-02-04T10:37:46+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures. The code and model will be at https://github.com/gobunu/LCUDiff</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04406v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04406v1">PDF</a> · <a href="https://github.com/gobunu/LCUDiff">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LCUDiff：用于忠实人体恢复的潜在容量升级扩散</div>
<div class="mono" style="margin-top:8px">现有的人体中心图像恢复方法常常在恢复忠实度上存在不足，特别是在人体恢复（HBR）方面。最近的基于扩散的恢复方法通常适应预训练的文本到图像扩散模型，其中变分自编码器（VAE）可能显著限制恢复的忠实度。我们提出了LCUDiff，这是一种稳定的一步框架，将预训练的潜在扩散模型从4通道潜在空间升级到16通道潜在空间。对于VAE微调，使用通道分裂蒸馏（CSD）保持前四个通道与预训练先验对齐，同时将额外通道分配给有效编码高频细节。我们进一步设计了保持先验适应（PPA），以平滑地弥合4通道扩散骨干与更高维度16通道潜在之间的不匹配。此外，我们提出了一种解码器路由器（DeR），用于基于恢复质量评分注释的每样本解码器路由，改善了不同条件下的视觉质量。在合成和真实世界数据集上的实验显示，在轻微降级下具有更高的忠实度和更少的伪影，同时保持一步效率。代码和模型将发布在 https://github.com/gobunu/LCUDiff。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing methods in restoring degraded human-centric images, particularly in achieving higher fidelity in human body restoration. The authors propose LCUDiff, a one-step framework that enhances a pre-trained latent diffusion model by upgrading its latent space from 4 channels to 16 channels, utilizing channel splitting distillation for VAE fine-tuning and prior-preserving adaptation to manage dimensional mismatches. Experimental results demonstrate that LCUDiff achieves competitive fidelity and fewer artifacts in both synthetic and real-world datasets, while maintaining efficiency in the restoration process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有方法在恢复退化的人体图像时，特别是在实现高保真度的人体恢复方面的局限性。作者提出了LCUDiff，这是一种一步到位的框架，通过将预训练的潜在扩散模型的潜在空间从4通道升级到16通道，利用通道分割蒸馏进行VAE微调，并采用保留先验适应来使模型与预训练先验对齐。实验结果表明，LCUDiff在轻微退化下实现了更高的保真度和更少的伪影，同时保持了恢复过程的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</div>
<div class="meta-line">Authors: Jue Gong, Tingyu Yang, Jingkai Wang, Zheng Chen, Xing Liu, Hong Gu, Yulun Zhang, Xiaokang Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T09:24:11+00:00 · Latest: 2026-02-04T09:28:03+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures. Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19742v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.19742v2">PDF</a> · <a href="https://github.com/gobunu/HAODiff">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centered images often suffer from severe generic degradation during transmission and are prone to human motion blur (HMB), making restoration challenging. Existing research lacks sufficient focus on these issues, as both problems often coexist in practice. To address this, we design a degradation pipeline that simulates the coexistence of HMB and generic noise, generating synthetic degraded data to train our proposed HAODiff, a human-aware one-step diffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG), which leverages high-quality images, residual noise (LQ minus HQ), and HMB segmentation masks as training targets. It produces a positive-negative prompt pair for classifier-free guidance (CFG) in a single diffusion step. The resulting adaptive dual prompts let HAODiff exploit CFG more effectively, boosting robustness against diverse degradations. For fair evaluation, we introduce MPII-Test, a benchmark rich in combined noise and HMB cases. Extensive experiments show that our HAODiff surpasses existing state-of-the-art (SOTA) methods in terms of both quantitative metrics and visual quality on synthetic and real-world datasets, including our introduced MPII-Test. Code is available at: https://github.com/gobunu/HAODiff.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAODiff：通过双提示引导的人类感知一步扩散</div>
<div class="mono" style="margin-top:8px">以人为中心的图像在传输过程中常常遭受严重的通用退化，并容易出现人类运动模糊（HMB），使得恢复变得具有挑战性。现有研究对这些问题的关注不足，因为这两种问题在实践中往往共存。为了解决这个问题，我们设计了一个退化管道，模拟HMB和通用噪声的共存，生成合成退化数据以训练我们提出的HAODiff，这是一种人类感知的一步扩散。具体而言，我们提出了三分支双提示引导（DPG），利用高质量图像、残余噪声（LQ减去HQ）和HMB分割掩码作为训练目标。它在单个扩散步骤中生成正负提示对，用于无分类器引导（CFG）。生成的自适应双提示使HAODiff能够更有效地利用CFG，提高对各种退化的鲁棒性。为了公平评估，我们引入了MPII-Test，这是一个丰富的结合噪声和HMB案例的基准。大量实验表明，我们的HAODiff在合成和真实世界数据集（包括我们引入的MPII-Test）上，在定量指标和视觉质量方面均超越了现有的最先进（SOTA）方法。代码可在：https://github.com/gobunu/HAODiff获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of restoring human-centered images that often experience severe degradation and human motion blur during transmission. The authors developed a degradation pipeline to simulate the coexistence of human motion blur and generic noise, which was used to generate synthetic degraded data for training their proposed method, HAODiff, a human-aware one-step diffusion model. The key experimental findings indicate that HAODiff, utilizing a triple-branch dual-prompt guidance approach, significantly outperforms existing state-of-the-art methods in both quantitative metrics and visual quality on synthetic and real-world datasets, including the newly introduced MPII-Test benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决人类中心图像恢复中面临的挑战，这些图像常常同时遭受人类运动模糊（HMB）和一般性降解。作者开发了一种降解管道来模拟这些问题，并创建了合成降解数据以训练他们提出的方法HAODiff，该方法采用人类感知的一步扩散方法。他们引入了一种三分支双提示引导系统，利用高质量图像、残余噪声和HMB分割掩码，从而在扩散过程中改善无分类器引导。实验结果表明，HAODiff在各种数据集（包括新引入的MPII-Test基准）上，在定量指标和视觉质量方面均优于现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Norm$\times$Direction: Restoring the Missing Query Norm in Vision Linear Attention</div>
<div class="meta-line">Authors: Weikang Meng, Yadan Luo, Liangyu Huo, Yingjian Li, Yaowei Wang, Xin Li, Zheng Zhang</div>
<div class="meta-line">First: 2025-06-26T10:47:39+00:00 · Latest: 2026-02-04T09:07:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21137v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.21137v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention mitigates the quadratic complexity of softmax attention but suffers from a critical loss of expressiveness. We identify two primary causes: (1) The normalization operation cancels the query norm, which breaks the correlation between a query&#x27;s norm and the spikiness (entropy) of the attention distribution as in softmax attention. (2) Standard techniques for enforcing non-negativity cause destructive information loss by nullifying valid inner-product interactions. To address these challenges, we introduce NaLaFormer, a novel linear attention mechanism built upon a norm$\times$direction (ND) decomposition of the query and key vectors. We leverage each component to solve a distinct problem: The query norm is injected into our kernel to create a query-norm-aware map that restores the attention distribution&#x27;s spikiness. The direction vectors are processed by a geometric, cosine-based similarity metric that guarantees non-negativity while preserving the rich, fine-grained information of the inner product. We validate NaLaFormer through a comprehensive multi-modal evaluation, where it sets new state-of-the-art benchmarks for linear attention. Our model achieves up to a 7.5% accuracy gain on ImageNet-1K and a 4.7% mIoU improvement on ADE20K over comparable baselines. It demonstrates profound efficiency, reducing peak memory by a transformative 92.3% in token-intensive super-resolution tasks (70K+ tokens). NaLaFormer&#x27;s versatility is further confirmed as it surpasses strong baselines like Mamba on common-sense reasoning and sets a new state-of-the-art on the Long Range Arena (LRA) benchmark. Source code can be found in the supplementary materials.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>范数$\times$方向：恢复视觉线性注意中的缺失查询范数</div>
<div class="mono" style="margin-top:8px">线性注意减轻了softmax注意的平方复杂度，但在表达能力上遭受了严重损失。我们确定了两个主要原因：（1）归一化操作取消了查询范数，这打破了查询范数与注意分布的尖锐性（熵）之间的相关性，如同softmax注意所示。（2）强制非负性的标准技术通过使有效的内积交互无效而导致破坏性的信息损失。为了解决这些挑战，我们引入了NaLaFormer，这是一种基于查询和键向量的范数$\times$方向（ND）分解的新型线性注意机制。我们利用每个组件解决不同的问题：查询范数被注入到我们的核中，以创建一个查询范数感知的映射，恢复注意分布的尖锐性。方向向量通过几何的基于余弦的相似性度量进行处理，确保非负性，同时保留内积的丰富细粒度信息。我们通过全面的多模态评估验证了NaLaFormer，在此过程中，它为线性注意设定了新的最先进基准。我们的模型在ImageNet-1K上实现了高达7.5%的准确率提升，在ADE20K上实现了4.7%的mIoU改善，超越了可比基线。它展示了深远的效率，在令牌密集的超分辨率任务中（70K+令牌）将峰值内存减少了92.3%。NaLaFormer的多功能性进一步得到确认，因为它在常识推理上超越了强基线Mamba，并在长距离竞技场（LRA）基准上设定了新的最先进水平。源代码可以在补充材料中找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of linear attention mechanisms, which, while reducing computational complexity, lose expressiveness due to the cancellation of query norms and destructive information loss from standard non-negativity techniques. The authors propose NaLaFormer, a novel linear attention mechanism that utilizes a norm times direction decomposition of query and key vectors to restore the correlation between query norms and attention distribution spikiness. Experimental results demonstrate that NaLaFormer achieves state-of-the-art performance, with up to a 7.5% accuracy increase on ImageNet-1K and a 4.7% mIoU improvement on ADE20K, while also significantly reducing peak memory usage by 92.3% in token-intensive tasks and outperforming strong baselines in common-sense reasoning and the Long Range Arena benchmark.</div>
<div class="mono" style="margin-top:8px">本研究解决了线性注意力机制的局限性，特别是由于查询范数的消失和标准非负性技术导致的信息损失而造成的表达能力下降。为了解决这些问题，作者提出了NaLaFormer，这是一种新颖的线性注意力机制，利用查询和键向量的范数乘方向分解。实验结果表明，NaLaFormer在性能上达到了新的最先进水平，在ImageNet-1K上提高了7.5%的准确率，在ADE20K上提高了4.7%的mIoU，同时在令牌密集型任务中将峰值内存使用减少了92.3%，并在常识推理和长距离竞技场基准测试中超越了强基线。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:01:55+00:00 · Latest: 2026-02-04T09:01:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双提示调优的主动CLIP适应的显式不确定性建模</div>
<div class="mono" style="margin-top:8px">预训练的视觉-语言模型如CLIP展现出强大的迁移能力，但在有限标注预算下将其适应于下游图像分类任务仍然具有挑战性。在主动学习环境中，模型必须从大量未标记数据中选择最具信息量的样本进行标注。现有方法通常通过基于熵的标准或表示聚类来估计不确定性，而没有从模型的角度显式建模不确定性。在本研究中，我们提出了一种基于双提示调优的主动CLIP适应的稳健不确定性建模框架。我们在CLIP的文本分支中引入了两个可学习的提示。正提示增强了与轻量调优的视觉嵌入相对应的任务特定文本嵌入的可区分性，提高了分类的可靠性。同时，负提示以相反的方式训练，以显式建模预测标签正确的概率，为指导主动样本选择提供了原则性的不确定性信号。不同微调范式下的广泛实验表明，我们的方法在相同标注预算下始终优于现有的主动学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the adaptation of pre-trained vision-language models like CLIP for image classification tasks, particularly in scenarios with limited annotated data. The authors propose a novel uncertainty modeling framework that utilizes dual-prompt tuning, where two learnable prompts are introduced in the textual branch of CLIP to enhance task-specific embeddings and model uncertainty. Experimental results show that this approach significantly outperforms existing active learning methods, providing better classification reliability and more effective sample selection under the same annotation budget.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善预训练的视觉-语言模型（如CLIP）在有限标注预算下的图像分类任务适应性。作者提出了一种双提示调优方法，在CLIP的文本分支中引入两个可学习的提示，以增强不确定性建模。正提示提高了任务特定文本嵌入的可区分性，而负提示则明确建模正确标签预测的概率，为主动样本选择提供了清晰的不确定性信号。实验结果表明，该方法在相同标注预算下始终优于现有的主动学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Foundation Models for Mitotic Figure Classification</div>
<div class="meta-line">Authors: Jonas Ammeling, Jonathan Ganz, Emely Rosbach, Ludwig Lausser, Christof A. Bertram, Katharina Breininger, Marc Aubreville</div>
<div class="meta-line">Venue: Machine.Learning.for.Biomedical.Imaging. 2026 (2026)</div>
<div class="meta-line">First: 2025-08-06T13:30:40+00:00 · Latest: 2026-02-04T08:41:55+00:00</div>
<div class="meta-line">Comments: Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2026:003</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.04441v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.04441v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of deep learning models is known to scale with data quantity and diversity. In pathology, as in many other medical imaging domains, the availability of labeled images for a specific task is often limited. Self-supervised learning techniques have enabled the use of vast amounts of unlabeled data to train large-scale neural networks, i.e., foundation models, that can address the limited data problem by providing semantically rich feature vectors that can generalize well to new tasks with minimal training effort increasing model performance and robustness. In this work, we investigate the use of foundation models for mitotic figure classification. The mitotic count, which can be derived from this classification task, is an independent prognostic marker for specific tumors and part of certain tumor grading systems. In particular, we investigate the data scaling laws on multiple current foundation models and evaluate their robustness to unseen tumor domains. Next to the commonly used linear probing paradigm, we also adapt the models using low-rank adaptation (LoRA) of their attention mechanisms. We compare all models against end-to-end-trained baselines, both CNNs and Vision Transformers. Our results demonstrate that LoRA-adapted foundation models provide superior performance to those adapted with standard linear probing, reaching performance levels close to 100% data availability with only 10% of training data. Furthermore, LoRA-adaptation of the most recent foundation models almost closes the out-of-domain performance gap when evaluated on unseen tumor domains. However, full fine-tuning of traditional architectures still yields competitive performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基准测试基础模型用于有丝分裂图像分类</div>
<div class="mono" style="margin-top:8px">深度学习模型的性能与数据的数量和多样性相关。在病理学中，特定任务的标记图像的可用性通常有限。自监督学习技术使得可以利用大量未标记数据来训练大规模神经网络，即基础模型，这些模型通过提供语义丰富的特征向量来解决有限数据问题，能够以最小的训练努力很好地推广到新任务，从而提高模型的性能和鲁棒性。在本研究中，我们探讨了基础模型在有丝分裂图像分类中的应用。通过这一分类任务可以得出的有丝分裂计数是特定肿瘤的独立预后标志，并且是某些肿瘤分级系统的一部分。特别地，我们研究了多个当前基础模型的数据扩展规律，并评估它们对未见肿瘤领域的鲁棒性。除了常用的线性探测范式外，我们还通过低秩适应（LoRA）调整模型的注意力机制。我们将所有模型与端到端训练的基线进行比较，包括CNN和视觉变换器。我们的结果表明，LoRA适应的基础模型在性能上优于使用标准线性探测的模型，仅用10%的训练数据就达到了接近100%数据可用性的性能水平。此外，最近基础模型的LoRA适应几乎消除了在未见肿瘤领域评估时的领域外性能差距。然而，传统架构的完全微调仍然能产生具有竞争力的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of limited labeled data in pathology by exploring the application of foundation models for mitotic figure classification, which is crucial for tumor grading. The researchers employed self-supervised learning techniques and evaluated various foundation models, including adaptations using low-rank adaptation (LoRA) of attention mechanisms, comparing their performance against traditional end-to-end-trained models. The findings indicate that LoRA-adapted foundation models significantly outperform those using standard linear probing, achieving near-optimal performance with only 10% of the training data and effectively reducing the performance gap on unseen tumor domains.</div>
<div class="mono" style="margin-top:8px">本研究通过探索基础模型在有丝分裂图像分类中的应用，解决了病理学中标记数据有限的挑战，这对肿瘤分级至关重要。作者采用自监督学习技术，研究了多种基础模型的数据扩展规律，同时使用线性探测和低秩适应（LoRA）方法。研究结果表明，LoRA适应的模型显著优于使用标准线性探测的模型，仅用10%的训练数据便能接近最佳性能，并有效缩小了在未见肿瘤领域的性能差距，相较于传统架构表现竞争力依然强劲。</div>
</details>
</div>
<div class="card">
<div class="title">AccidentSim: Generating Vehicle Collision Videos with Physically Realistic Collision Trajectories from Real-World Accident Reports</div>
<div class="meta-line">Authors: Xiangwen Zhang, Qian Zhang, Longfei Han, Qiang Qu, Xiaoming Chen, Weidong Cai</div>
<div class="meta-line">First: 2025-03-26T15:50:42+00:00 · Latest: 2026-02-04T08:34:05+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.20654v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.20654v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collecting real-world vehicle accident videos for autonomous driving research is challenging due to their rarity and complexity. While existing driving video generation methods may produce visually realistic videos, they often fail to deliver physically realistic simulations because they lack the capability to generate accurate post-collision trajectories. In this paper, we introduce AccidentSim, a novel framework that generates physically realistic vehicle collision videos by extracting and utilizing the physical clues and contextual information available in real-world vehicle accident reports. Specifically, AccidentSim leverages a reliable physical simulator to replicate post-collision vehicle trajectories from the physical and contextual information in the accident reports and to build a vehicle collision trajectory dataset. This dataset is then used to fine-tune a language model, enabling it to respond to user prompts and predict physically consistent post-collision trajectories across various driving scenarios based on user descriptions. Finally, we employ Neural Radiance Fields (NeRF) to render high-quality backgrounds, merging them with the foreground vehicles that exhibit physically realistic trajectories to generate vehicle collision videos. Experimental results demonstrate that the videos produced by AccidentSim excel in both visual and physical authenticity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AccidentSim：从真实事故报告生成具有物理真实碰撞轨迹的车辆碰撞视频</div>
<div class="mono" style="margin-top:8px">由于真实车辆事故视频的稀缺性和复杂性，收集用于自动驾驶研究的真实世界车辆事故视频具有挑战性。虽然现有的驾驶视频生成方法可能产生视觉上真实的视频，但它们往往无法提供物理上真实的模拟，因为缺乏生成准确碰撞后轨迹的能力。本文介绍了AccidentSim，一个新颖的框架，通过提取和利用真实车辆事故报告中的物理线索和上下文信息，生成物理真实的车辆碰撞视频。具体而言，AccidentSim利用可靠的物理模拟器，根据事故报告中的物理和上下文信息复制碰撞后车辆轨迹，并构建车辆碰撞轨迹数据集。该数据集随后用于微调语言模型，使其能够响应用户提示，并根据用户描述预测在各种驾驶场景中物理一致的碰撞后轨迹。最后，我们采用神经辐射场（NeRF）渲染高质量背景，将其与展示物理真实轨迹的前景车辆合并，以生成车辆碰撞视频。实验结果表明，AccidentSim生成的视频在视觉和物理真实性方面均表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the difficulty of collecting real-world vehicle accident videos for autonomous driving studies due to their rarity and complexity, as existing methods often lack physical realism in simulations. The authors introduce AccidentSim, a framework that generates realistic vehicle collision videos by utilizing physical clues and contextual information from real-world accident reports, employing a reliable physical simulator to replicate post-collision trajectories and create a dataset for fine-tuning a language model. The experimental results show that the videos produced by AccidentSim achieve high levels of both visual and physical authenticity, effectively merging high-quality backgrounds with vehicles that exhibit realistic trajectories.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于由于真实世界交通事故视频的稀缺性和复杂性，收集这些视频以用于自动驾驶研究非常困难。作者提出了AccidentSim框架，该框架通过利用真实交通事故报告中的物理线索和上下文信息生成物理真实的车辆碰撞视频。主要实验结果表明，AccidentSim生成的视频在视觉和物理真实性上都表现出色，能够有效模拟碰撞后的轨迹，结合了可靠的物理模拟器和神经辐射场用于背景渲染。</div>
</details>
</div>
<div class="card">
<div class="title">Light Up Your Face: A Physically Consistent Dataset and Diffusion Model for Face Fill-Light Enhancement</div>
<div class="meta-line">Authors: Jue Gong, Zihan Zhou, Jingkai Wang, Xiaohong Liu, Yulun Zhang, Xiaokang Yang</div>
<div class="meta-line">First: 2026-02-04T08:03:41+00:00 · Latest: 2026-02-04T08:03:41+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures. The code and model will be available at https://github.com/gobunu/Light-Up-Your-Face</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04300v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04300v1">PDF</a> · <a href="https://github.com/gobunu/Light-Up-Your-Face">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Face fill-light enhancement (FFE) brightens underexposed faces by adding virtual fill light while keeping the original scene illumination and background unchanged. Most face relighting methods aim to reshape overall lighting, which can suppress the input illumination or modify the entire scene, leading to foreground-background inconsistency and mismatching practical FFE needs. To support scalable learning, we introduce LightYourFace-160K (LYF-160K), a large-scale paired dataset built with a physically consistent renderer that injects a disk-shaped area fill light controlled by six disentangled factors, producing 160K before-and-after pairs. We first pretrain a physics-aware lighting prompt (PALP) that embeds the 6D parameters into conditioning tokens, using an auxiliary planar-light reconstruction objective. Building on a pretrained diffusion backbone, we then train a fill-light diffusion (FiLitDiff), an efficient one-step model conditioned on physically grounded lighting codes, enabling controllable and high-fidelity fill lighting at low computational cost. Experiments on held-out paired sets demonstrate strong perceptual quality and competitive full-reference metrics, while better preserving background illumination. The dataset and model will be at https://github.com/gobunu/Light-Up-Your-Face.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>点亮你的脸：一个物理一致的数据集和用于面部补光增强的扩散模型</div>
<div class="mono" style="margin-top:8px">面部补光增强（FFE）通过添加虚拟补光来照亮曝光不足的面部，同时保持原始场景照明和背景不变。大多数面部重光方法旨在重塑整体照明，这可能会抑制输入照明或修改整个场景，导致前景与背景不一致，并与实际的FFE需求不匹配。为了支持可扩展学习，我们引入了LightYourFace-160K（LYF-160K），这是一个大型配对数据集，采用物理一致的渲染器构建，注入由六个解耦因子控制的盘形区域补光，生成16万对前后样本。我们首先预训练一个物理感知照明提示（PALP），将6D参数嵌入条件令牌中，使用辅助平面光重建目标。在预训练的扩散骨干网络基础上，我们训练了一个补光扩散模型（FiLitDiff），这是一个高效的一步模型，基于物理基础的照明代码进行条件化，使得在低计算成本下实现可控和高保真的补光。对保留的配对集的实验表明，感知质量强，且全参考指标具有竞争力，同时更好地保留背景照明。数据集和模型将发布在https://github.com/gobunu/Light-Up-Your-Face。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve face fill-light enhancement (FFE) by addressing the limitations of existing methods that often alter overall scene illumination, leading to inconsistencies. The authors introduce LightYourFace-160K (LYF-160K), a large-scale dataset created using a physically consistent renderer that generates 160,000 paired images with controlled fill light based on six disentangled factors. They pretrain a physics-aware lighting prompt (PALP) to embed these parameters into conditioning tokens and develop a fill-light diffusion model (FiLitDiff) that allows for efficient, controllable, and high-fidelity lighting enhancement. Experimental results show that the proposed method achieves strong perceptual quality and competitive metrics while effectively preserving background illumination.</div>
<div class="mono" style="margin-top:8px">本研究解决了面部补光增强（FFE）面临的挑战，旨在在不改变原始场景照明的情况下，照亮曝光不足的面部。为了应对现有方法常常导致前景与背景照明不一致的局限性，作者引入了LightYourFace-160K（LYF-160K），这是一个由物理一致渲染器生成的包含16万对图像的大规模数据集。他们开发了一种补光扩散模型（FiLitDiff），利用预训练的物理感知照明提示，根据六个解耦因子有条件地应用补光，达到高感知质量并保持背景照明，同时高效运行。实验结果表明，该方法在感知质量和全参考指标方面优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning</div>
<div class="meta-line">Authors: Zeming Wei, Qiaosheng Zhang, Xia Hu, Xingcheng Xu</div>
<div class="meta-line">First: 2026-02-04T05:18:38+00:00 · Latest: 2026-02-04T05:18:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04224v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04224v1">PDF</a> · <a href="https://github.com/weizeming/RAPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs&#x27; safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAPO：风险意识偏好优化用于可推广的安全推理</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在其思维链（CoT）推理方面取得了巨大的成功，但也面临与基本语言模型类似的安全问题。特别是，尽管算法旨在引导它们故意拒绝有害提示并进行安全推理，但这一过程往往无法针对多样化和复杂的越狱攻击进行推广。在这项工作中，我们将这些失败归因于安全推理过程的推广，特别是它们在复杂攻击提示下的不足。我们提供了理论和实证证据，表明需要更充分的安全推理过程来防御高级攻击提示。在此基础上，我们提出了一种风险意识偏好优化（RAPO）框架，使LRM能够自适应地识别和解决其思维内容中的安全风险。大量实验表明，RAPO成功地在多样化攻击提示下自适应地推广多个LRMs的安全推理，同时保持一般效用，为LRM安全提供了一种强大的对齐技术。我们的代码可在https://github.com/weizeming/RAPO获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the safety issues faced by Large Reasoning Models (LRMs) in their chain-of-thought reasoning, particularly their inability to generalize against complex jailbreak attacks. The authors propose a Risk-Aware Preference Optimization (RAPO) framework that enhances the safe reasoning process by enabling LRMs to adaptively identify and mitigate safety risks. Experimental results show that RAPO effectively generalizes safe reasoning across various attack prompts while maintaining overall utility, thus providing a robust alignment technique for improving LRM safety.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决大型推理模型（LRMs）在应对有害提示时面临的安全问题，因为现有算法往往无法对复杂的越狱攻击进行有效泛化。作者提出了一种风险意识偏好优化（RAPO）框架，通过使LRMs能够自适应地识别和减轻安全风险，从而增强安全推理过程。实验结果表明，RAPO能够有效地在各种攻击提示中泛化安全推理，同时保持整体效用，从而为提高LRM安全性提供了一种稳健的对齐技术。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive 1D Video Diffusion Autoencoder</div>
<div class="meta-line">Authors: Yao Teng, Minxuan Lin, Xian Liu, Shuai Wang, Xiao Yang, Xihui Liu</div>
<div class="meta-line">First: 2026-02-04T05:11:12+00:00 · Latest: 2026-02-04T05:11:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04220v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应一维视频扩散自编码器</div>
<div class="mono" style="margin-top:8px">最近的视频生成模型主要依赖于将像素空间视频压缩为潜在表示的视频自编码器。然而，现有的视频自编码器存在三个主要限制：（1）固定速率压缩在简单视频上浪费令牌，（2）不灵活的CNN架构阻碍了可变长度潜在建模，以及（3）确定性解码器难以从压缩潜在中恢复适当的细节。为了解决这些问题，我们提出了一维扩散视频自编码器（One-DVA），这是一个基于变换器的框架，用于自适应一维编码和基于扩散的解码。编码器采用基于查询的视觉变换器提取时空特征并生成潜在表示，而可变长度的丢弃机制动态调整潜在长度。解码器是一个像素空间扩散变换器，以潜在作为输入条件重建视频。通过两阶段训练策略，One-DVA在相同压缩比下在重建指标上实现了与3D-CNN VAE相当的性能。更重要的是，它支持自适应压缩，因此可以实现更高的压缩比。为了更好地支持下游潜在生成，我们进一步规范化One-DVA潜在分布以进行生成建模，并微调其解码器以减轻生成过程造成的伪影。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to overcome the limitations of existing video autoencoders, which include fixed-rate compression, inflexible architectures, and deterministic decoding that hinder effective video generation. The authors propose the One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework that utilizes query-based vision transformers for spatiotemporal feature extraction and a variable-length dropout mechanism for adaptive latent representation. Experimental results demonstrate that One-DVA achieves reconstruction performance comparable to 3D-CNN VAEs at the same compression ratios while enabling higher adaptive compression ratios and improving the quality of generated videos by regularizing the latent distribution and fine-tuning the decoder to reduce artifacts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服现有视频自编码器的局限性，包括固定速率压缩、灵活性不足的架构以及确定性解码，这些都妨碍了有效的视频重建。作者提出了一维扩散视频自编码器（One-DVA），这是一种基于变压器的框架，利用基于查询的视觉变压器进行时空特征提取，并采用可变长度的丢弃机制来实现自适应潜在表示。实验结果表明，One-DVA在相同压缩比下的重建性能与3D-CNN VAE相当，同时支持更高的自适应压缩和改进的潜在分布正则化，以便于生成建模。</div>
</details>
</div>
<div class="card">
<div class="title">From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction</div>
<div class="meta-line">Authors: Yanjie Tong, Peng Chen</div>
<div class="meta-line">First: 2026-02-04T04:39:23+00:00 · Latest: 2026-02-04T04:39:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04201v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从稀疏传感器到连续场：用于时空重建的STRIDE</div>
<div class="mono" style="margin-top:8px">从稀疏点传感器测量重建高维时空场是学习参数 PDE 动力学的一个核心挑战。现有方法往往难以在轨迹和参数设置之间进行泛化，或依赖于与离散化相关的解码器，这些解码器在网格和分辨率之间无法自然转移。我们提出了 STRIDE（时空递归隐式解码器），这是一个两阶段框架，将短时间窗口的传感器测量映射到潜在状态，并使用调制的隐式神经表示（INR）解码器在任意查询位置重建场。使用傅里叶多分量和多层神经网络（FMMNN）作为 INR 主干改善了复杂空间场的表示，并比基于正弦的 INR 产生更稳定的优化。我们提供了条件理论证明：在低维参数不变集上点测量的稳定延迟可观测性下，重建算子通过有限维嵌入进行分解，使得 STRIDE 类型架构成为自然的近似器。在四个涵盖混沌动力学和波传播的挑战性基准上的实验表明，STRIDE 在极其稀疏的感知下优于强基线，支持超分辨率，并对噪声保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements, which existing methods struggle to generalize across different trajectories and parameter settings. The authors propose STRIDE, a two-stage framework that utilizes a temporal encoder to map sensor measurements to a latent state and employs a modulated implicit neural representation decoder for reconstructing fields at arbitrary locations. Experimental results demonstrate that STRIDE outperforms strong baseline methods in scenarios with extremely sparse sensing, supports super-resolution capabilities, and maintains robustness against noise across four challenging benchmarks involving chaotic dynamics and wave propagation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从稀疏点传感器测量中重建高维时空场的挑战，而现有方法在不同轨迹和参数设置下的泛化能力较弱。作者提出了STRIDE，这是一种两阶段框架，利用时间编码器将传感器测量映射到潜在状态，并采用调制隐式神经表示解码器在任意位置重建场。实验结果表明，STRIDE在涉及混沌动力学和波传播的四个具有挑战性的基准测试中优于强基线方法，能够有效支持超分辨率，并在极其稀疏的传感情况下保持对噪声的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution</div>
<div class="meta-line">Authors: Hyeonjae Kim, Dongjin Kim, Eugene Jin, Tae Hyun Kim</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-02-04T04:16:38+00:00 · Latest: 2026-02-04T04:16:38+00:00</div>
<div class="meta-line">Comments: AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在流匹配进行连续退化建模以实现真实世界超分辨率</div>
<div class="mono" style="margin-top:8px">尽管基于深度学习的超分辨率（SR）方法在合成退化场景（如双三次下采样）中表现出色，但它们在处理具有复杂非线性退化（如噪声、模糊和压缩伪影）的真实图像时常常表现不佳。最近为解决这一问题的努力涉及到真实低分辨率（LR）和高分辨率（HR）图像对的艰苦编制，通常仅限于几个特定的下采样因子。为应对这些挑战，我们的工作引入了一种新颖的框架，能够通过利用潜在退化空间与流匹配，从单个HR图像合成真实的LR图像。我们的方法在未见的退化水平下生成具有真实伪影的LR图像，从而促进大规模真实世界SR训练数据集的创建。全面的定量和定性评估验证了我们的合成LR图像准确复制了真实世界的退化。此外，使用我们的数据集训练的传统和任意尺度SR模型始终能产生更好的HR结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of deep learning-based super-resolution methods on real-world images that exhibit complex degradations, which traditional approaches struggle to handle. The authors propose a novel framework that synthesizes realistic low-resolution images from a single high-resolution image by utilizing latent flow matching to capture the degradation space. Experimental results demonstrate that the synthetic low-resolution images generated by this method closely mimic real-world degradations, and super-resolution models trained on these datasets significantly outperform those trained on conventional datasets, achieving better high-resolution outcomes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高基于深度学习的超分辨率方法在处理具有复杂降质的真实图像时的表现，而传统方法对此往往难以应对。作者提出了一种新颖的框架，通过利用潜在流匹配，从单一高分辨率图像合成真实的低分辨率图像，以生成在不同未见降质水平下的真实伪影。实验结果表明，合成的低分辨率图像与真实世界的降质情况高度相似，基于这些数据集训练的超分辨率模型在高分辨率结果上显著优于传统数据集训练的模型。</div>
</details>
</div>
<div class="card">
<div class="title">MAMBO-G: Magnitude-Aware Mitigation for Boosted Guidance</div>
<div class="meta-line">Authors: Shangwen Zhu, Qianyu Peng, Zhilei Shu, Yuting Hu, Zhantao Yang, Han Zhang, Zhao Pu, Andy Zheng, Xinyu Cui, Jian Zhao, Ruili Feng, Fan Cheng</div>
<div class="meta-line">First: 2025-08-05T13:41:05+00:00 · Latest: 2026-02-04T03:53:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03442v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.03442v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-fidelity text-to-image and text-to-video generation typically relies on Classifier-Free Guidance (CFG), but achieving optimal results often demands computationally expensive sampling schedules. In this work, we propose MAMBO-G, a training-free acceleration framework that significantly reduces computational cost by dynamically optimizing guidance magnitudes. We observe that standard CFG schedules are inefficient, applying disproportionately large updates in early steps that hinder convergence speed. MAMBO-G mitigates this by modulating the guidance scale based on the update-to-prediction magnitude ratio, effectively stabilizing the trajectory and enabling rapid convergence. This efficiency is particularly vital for resource-intensive tasks like video generation. Our method serves as a universal plug-and-play accelerator, achieving up to 3x speedup on Stable Diffusion v3.5 (SD3.5) and 4x on Lumina. Most notably, MAMBO-G accelerates the 14B-parameter Wan2.1 video model by 2x while preserving visual fidelity, offering a practical solution for efficient large-scale video synthesis. Our implementation follows a mainstream open-source diffusion framework and is plug-and-play with existing pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAMBO-G: 关注幅度的增强引导减缓</div>
<div class="mono" style="margin-top:8px">高保真文本到图像和文本到视频生成通常依赖于无分类器引导（CFG），但实现最佳结果往往需要计算成本高昂的采样计划。在这项工作中，我们提出了MAMBO-G，这是一种无训练加速框架，通过动态优化引导幅度显著降低计算成本。我们观察到标准CFG计划效率低下，在早期步骤中施加不成比例的大更新，阻碍了收敛速度。MAMBO-G通过根据更新与预测幅度比调节引导规模来缓解这一问题，有效稳定轨迹并实现快速收敛。这种效率对于视频生成等资源密集型任务尤为重要。我们的方法作为一种通用的即插即用加速器，在Stable Diffusion v3.5（SD3.5）上实现了高达3倍的加速，在Lumina上实现了4倍的加速。最值得注意的是，MAMBO-G将14B参数的Wan2.1视频模型加速了2倍，同时保持视觉保真度，为高效的大规模视频合成提供了实用解决方案。我们的实现遵循主流开源扩散框架，并与现有管道即插即用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of high-fidelity text-to-image and text-to-video generation, which often relies on computationally expensive Classifier-Free Guidance (CFG) sampling schedules. The authors introduce MAMBO-G, a training-free acceleration framework that optimizes guidance magnitudes dynamically to reduce computational costs. Experimental results demonstrate that MAMBO-G achieves up to 3x speedup on Stable Diffusion v3.5 and 4x on Lumina, while also accelerating the 14B-parameter Wan2.1 video model by 2x without compromising visual fidelity, making it a practical solution for large-scale video synthesis tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本到图像和文本到视频生成的效率，这通常依赖于计算成本高昂的无分类器引导（CFG）。作者提出了MAMBO-G，这是一种无训练的加速框架，通过动态优化引导幅度来降低计算成本。实验结果表明，MAMBO-G在Stable Diffusion v3.5上实现了最高3倍的加速，在Lumina上实现了4倍的加速，同时在不影响视觉保真度的情况下，将14B参数的Wan2.1视频模型加速了2倍，从而为大规模视频合成任务提供了实用的解决方案。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
