<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-06 03:22</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260106_0322</div>
    <div class="row"><div class="card">
<div class="title">Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty</div>
<div class="meta-line">Authors: Uğurcan Özalp</div>
<div class="meta-line">First: 2026-01-02T16:33:17+00:00 · Latest: 2026-01-02T16:33:17+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic&#x27;s epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机演员-评论家：通过时间随机不确定性减轻高估</div>
<div class="mono" style="margin-top:8px">强化学习中的离策略演员-评论家方法通过时间差分更新训练评论家，并将其作为策略（演员）的学习信号。这种设计通常比纯粹的在策略方法具有更高的样本效率。然而，评论网络往往系统性地高估价值估计。通常通过引入基于不确定性估计的悲观偏差来解决这个问题。目前的方法采用集成来量化评论家的认知不确定性——由于数据有限和模型模糊而导致的不确定性——以缩放悲观更新。在这项工作中，我们提出了一种新的算法，称为随机演员-评论家（STAC），它结合了时间（一步）随机不确定性——源于随机转移、奖励和策略引起的贝尔曼目标变异性——以缩放时间差分更新中的悲观偏差，而不是依赖于认知不确定性。STAC使用单一的分布式评论网络来建模时间回报不确定性，并对评论和演员网络应用dropout进行正则化。我们的结果表明，仅基于分布式评论的悲观主义足以减轻高估，并自然导致在随机环境中的风险厌恶行为。引入dropout通过正则化进一步提高了训练的稳定性和性能。通过这种设计，STAC使用单一的分布式评论网络实现了更高的计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of systematic overestimation in off-policy actor-critic methods in reinforcement learning, which can hinder sample efficiency. The authors propose a novel algorithm called Stochastic Actor-Critic (STAC) that utilizes temporal aleatoric uncertainty to adjust pessimistic biases in temporal-difference updates, rather than relying on epistemic uncertainty from ensembling. Experimental results demonstrate that STAC effectively mitigates overestimation and promotes risk-averse behavior in stochastic environments, while also enhancing training stability and performance through the use of dropout regularization in both the critic and actor networks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决强化学习中离线策略演员-评论家方法中价值估计的系统性高估，这可能会影响样本效率。作者提出了一种新算法，称为随机演员-评论家（STAC），该算法利用时间上的随机不确定性来调整时间差分更新中的悲观偏差，而不是依赖于认知不确定性。实验结果表明，STAC通过单一的分布式评论家网络有效地减轻了高估，在随机环境中促进了规避风险的行为，并通过引入dropout进行正则化，提高了训练的稳定性和性能。</div>
</details>
</div>
<div class="card">
<div class="title">ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation</div>
<div class="meta-line">Authors: Shin Seong Kim, Minjung Shin, Hyunin Cho, Youngjung Uh</div>
<div class="meta-line">First: 2025-12-29T07:06:57+00:00 · Latest: 2026-01-02T08:13:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23245v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23245v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://minjung-s.github.io/asemconsist">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ASemConsist：用于无训练身份一致生成的自适应语义特征控制</div>
<div class="mono" style="margin-top:8px">最近的文本到图像扩散模型显著提高了视觉质量和文本对齐。然而，在不同场景描述中生成一系列图像，同时保持一致的角色身份仍然是一项具有挑战性的任务。现有方法往往在保持身份一致性和确保每幅图像提示对齐之间存在权衡。在本文中，我们介绍了一种新颖的框架ASemconsist，通过选择性文本嵌入修改来解决这一挑战，实现对角色身份的明确语义控制，而不牺牲提示对齐。此外，基于我们对FLUX中填充嵌入的分析，我们提出了一种语义控制策略，将填充嵌入重新用作语义容器。此外，我们引入了一种自适应特征共享策略，自动评估文本模糊性，并仅对模糊身份提示施加约束。最后，我们提出了一种统一的评估协议，即一致性质量评分（CQS），将身份保留和每幅图像文本对齐整合为一个综合指标，明确捕捉两者之间的性能不平衡。我们的框架实现了最先进的性能，有效克服了之前的权衡。项目页面：https://minjung-s.github.io/asemconsist</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of generating a sequence of images with consistent character identity while maintaining alignment with diverse scene descriptions in text-to-image diffusion models. The authors introduce ASemConsist, a framework that employs selective text embedding modification for explicit semantic control over character identity, alongside a strategy that utilizes padding embeddings as semantic containers. The experimental results demonstrate that ASemConsist achieves state-of-the-art performance, effectively balancing identity preservation and prompt alignment, as measured by the newly proposed Consistency Quality Score (CQS).</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在文本到图像扩散模型中生成一系列图像时，如何在与多样场景描述对齐的同时保持角色身份一致性的问题。作者提出了一种名为ASemConsist的新框架，该框架利用选择性文本嵌入修改来实现对角色身份的显式语义控制，并提出了一种将填充嵌入重新利用为语义容器的语义控制策略。关键实验结果表明，ASemConsist通过有效平衡身份保留和每幅图像文本对齐，达到了最先进的性能，并通过新引入的一致性质量评分（CQS）进行测量。</div>
</details>
</div>
<div class="card">
<div class="title">SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation</div>
<div class="meta-line">Authors: Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang</div>
<div class="meta-line">First: 2026-01-02T06:31:52+00:00 · Latest: 2026-01-02T06:31:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00590v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00590v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/SafeMo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aigeeksgroup.github.io/SafeMo">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model&#x27;s benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeMo：基于语言的可信文本到运动生成的去学习</div>
<div class="mono" style="margin-top:8px">基于扩散骨干的文本到运动（T2M）生成实现了强大的现实感和对齐。近年来，T2M方法的安全性问题引起了关注；现有方法通过替换离散的VQ-VAE代码本条目来引导模型远离不安全行为。然而，基于离散代码本替换的方法存在两个关键缺陷：首先，替换被良性提示重用的代码本条目会导致日常任务的漂移，降低模型的良性表现；其次，基于离散令牌的方法引入量化和平滑损失，导致伪影和不流畅的过渡。此外，现有的文本到运动数据集自然包含不安全的意图和相应的运动，使其不适合安全驱动的机器学习。为了解决这些挑战，我们提出了SafeMo，一个可信的运动生成框架，集成了最小运动去学习（MMU），这是一种两阶段的机器去学习策略，能够在连续空间中安全生成人类运动，保持连续运动学而不丢失代码本，并与当前基线相比提供强大的安全-效用权衡。此外，我们提出了第一个安全文本到运动数据集SafeMoVAE-29K，集成了重写的安全文本提示和连续精炼的运动，以实现可信的人类运动去学习。基于DiP，SafeMo高效生成具有自然过渡的安全人类运动。实验表明，SafeMo在不安全提示上表现出有效的去学习性能，在HumanML3D和Motion-X上分别达到2.5倍和14.4倍更高的遗忘集FID，相较于之前的SOTA人类运动去学习方法LCR，在安全提示上的良性表现更好或相当。代码：https://github.com/AIGeeksGroup/SafeMo。网站：https://aigeeksgroup.github.io/SafeMo。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address safety concerns in text-to-motion (T2M) generation, particularly the limitations of existing methods that replace discrete VQ-VAE codebook entries, which can degrade model performance on benign tasks. The authors propose SafeMo, a framework that employs Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy that allows for safe human motion generation in continuous space while maintaining kinematic integrity. Experimental results show that SafeMo significantly improves unlearning performance, achieving 2.5x and 14.4x higher forget-set FID scores on HumanML3D and Motion-X datasets, respectively, compared to the previous state-of-the-art method, while preserving or enhancing benign performance on safe prompts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本到运动（T2M）生成方法的安全性，这些方法因可能产生不安全行为而引发了关注。作者提出了SafeMo框架，采用最小运动遗忘（MMU）这一两阶段机器遗忘策略，以生成安全的人类运动，同时保持连续的运动学。实验结果表明，SafeMo显著提高了遗忘性能，在HumanML3D和Motion-X数据集上分别实现了比之前的最先进方法高出2.5倍和14.4倍的遗忘集FID分数，同时在安全提示上的良性表现保持不变或有所改善。</div>
</details>
</div>
<div class="card">
<div class="title">AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization</div>
<div class="meta-line">Authors: Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">First: 2025-12-29T15:26:25+00:00 · Latest: 2026-01-02T06:21:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23537v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23537v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject&#x27;s attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyMS：用于布局引导和无训练多主体定制的自下而上注意力解耦</div>
<div class="mono" style="margin-top:8px">多主体定制旨在将多个用户指定的主体合成到一个连贯的图像中。为了解决主体缺失或冲突等问题，最近的研究结合了布局指导，以提供明确的空间约束。然而，现有方法仍然难以平衡三个关键目标：文本对齐、主体身份保留和布局控制，同时对额外训练的依赖进一步限制了它们的可扩展性和效率。本文提出了AnyMS，一种新颖的无训练框架，用于布局引导的多主体定制。AnyMS利用三种输入条件：文本提示、主体图像和布局约束，并引入了一种自下而上的双层注意力解耦机制，以协调它们在生成过程中的整合。具体而言，全球解耦将文本和视觉条件之间的交叉注意力分开，以确保文本对齐。局部解耦将每个主体的注意力限制在其指定区域，从而防止主体冲突，确保身份保留和布局控制。此外，AnyMS采用预训练的图像适配器提取与扩散模型对齐的主体特征，消除了主体学习或适配器调优的需求。大量实验表明，AnyMS实现了最先进的性能，支持复杂的组合并扩展到更多主体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve multi-subject customization in image synthesis by addressing challenges such as subject conflicts and alignment issues while avoiding the limitations of additional training. The authors propose AnyMS, a training-free framework that utilizes a bottom-up dual-level attention decoupling mechanism, integrating text prompts, subject images, and layout constraints effectively. Experimental results show that AnyMS achieves state-of-the-art performance in generating coherent images with complex compositions while maintaining subject identity and layout control without the need for subject-specific learning or tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决与主题冲突和缺失元素相关的挑战，同时保持文本对齐和身份保留，来改善图像合成中的多主题定制。作者提出了AnyMS，这是一种无训练框架，利用自下而上的双层注意力解耦机制，有效整合文本提示、主题图像和布局约束。实验结果表明，AnyMS在生成具有复杂构图的连贯图像方面达到了最先进的性能，并且能够扩展以容纳更多主题，而无需额外的训练。</div>
</details>
</div>
<div class="card">
<div class="title">OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot</div>
<div class="meta-line">Authors: Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang</div>
<div class="meta-line">First: 2025-10-08T08:19:15+00:00 · Latest: 2026-01-02T06:03:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06751v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.06751v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OBS-Diff：一次性扩散模型的精确剪枝</div>
<div class="mono" style="margin-top:8px">大规模文本到图像的扩散模型虽然强大，但计算成本高昂。现有的一次性网络剪枝方法由于扩散模型的迭代去噪特性，难以直接应用。为了解决这一问题，本文提出了OBS-Diff，一种新颖的一次性剪枝框架，能够实现大规模文本到图像扩散模型的准确且无训练的压缩。具体而言，(i) OBS-Diff 复兴了经典的最优脑外科医生（OBS），将其适应于现代扩散模型的复杂架构，并支持多样的剪枝粒度，包括无结构、N:M 半结构和结构（MHA 头和 FFN 神经元）稀疏性；(ii) 为了使剪枝标准与扩散过程的迭代动态对齐，我们从误差累积的角度审视问题，提出了一种新颖的时间步感知 Hessian 构造，结合对数递减加权方案，赋予早期时间步更大的重要性，以减轻潜在的误差累积；(iii) 此外，提出了一种计算高效的分组顺序剪枝策略，以摊销昂贵的校准过程。大量实验表明，OBS-Diff 实现了扩散模型的最先进的一次性剪枝，在视觉质量下降最小的情况下加速推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs associated with large-scale text-to-image diffusion models, which traditional one-shot network pruning methods struggle to optimize due to the models&#x27; iterative denoising nature. The authors introduce OBS-Diff, a novel one-shot pruning framework that adapts the Optimal Brain Surgeon method for the complex architectures of diffusion models, allowing for various pruning granularities. Key experimental results demonstrate that OBS-Diff achieves state-of-the-art performance in one-shot pruning, significantly accelerating inference while maintaining minimal degradation in visual quality.</div>
<div class="mono" style="margin-top:8px">该研究解决了大规模文本到图像扩散模型的高计算成本问题，这限制了其实际应用。作者提出了OBS-Diff，这是一种一键修剪框架，旨在将最佳脑外科手术方法适应于扩散模型的独特架构，允许多种修剪粒度。实验结果表明，OBS-Diff在一键修剪方面达到了最先进的性能，显著加快了推理速度，同时保持了视觉质量，降级最小。</div>
</details>
</div>
<div class="card">
<div class="title">AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</div>
<div class="meta-line">Authors: Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu</div>
<div class="meta-line">First: 2026-01-02T04:22:18+00:00 · Latest: 2026-01-02T04:22:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00561v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00561v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N&#x27;&#x27; judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AEGIS：探索统一多模态模型的世界知识能力极限</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在多样任务中应用世界知识的能力仍然是一个关键的未解决挑战。现有基准测试不足，仅提供孤立的单任务评估，诊断能力有限。为弥补这一差距，我们提出AEGIS（即评估编辑、生成、解释-理解以实现超智能），这是一个涵盖视觉理解、生成、编辑和交错生成的综合多任务基准。AEGIS包含1050个具有挑战性的手动标注问题，涵盖21个主题（包括STEM、人文学科、日常生活等）和6种推理类型。为了具体评估UMMs在世界知识范围内的表现而不使用模糊指标，我们进一步提出了基于确定性清单的评估（DCE），该协议用原子“是/否”判断替代模糊的提示评分，以增强评估的可靠性。我们的广泛实验表明，大多数UMMs在世界知识方面存在严重缺陷，且在复杂推理时表现显著下降。此外，简单的插件推理模块可以部分缓解这些脆弱性，突显了未来研究的有希望方向。这些结果强调了基于世界知识的推理作为UMMs的一个关键前沿的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of Unified Multimodal Models (UMMs) in applying world knowledge across various tasks, as existing benchmarks are limited to single-task evaluations. The authors introduce AEGIS, a multi-task benchmark consisting of 1,050 manually-annotated questions across 21 topics and 6 reasoning types, aimed at assessing visual understanding, generation, editing, and interleaved generation. Experimental results indicate that most UMMs show significant deficits in world knowledge, with performance notably declining in complex reasoning scenarios, although the integration of simple plug-in reasoning modules can somewhat alleviate these issues, suggesting a potential avenue for future exploration.</div>
<div class="mono" style="margin-top:8px">本研究解决了统一多模态模型（UMMs）在有效应用世界知识于各种任务中的挑战，因为现有基准不足以进行全面评估。为了解决这个问题，作者提出了AEGIS，一个包含1050个手动标注问题的多任务基准，涵盖21个主题和6种推理类型，旨在评估视觉理解、生成和编辑。实验结果表明，大多数UMMs在世界知识方面存在显著缺陷，尤其是在面对复杂推理任务时，尽管集成简单的插件推理模块显示出改善性能的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">RAG-BioQA: A Retrieval-Augmented Generation Framework for Long-Form Biomedical Question Answering</div>
<div class="meta-line">Authors: Lovely Yeswanth Panchumarthi, Sumalatha Saleti, Sai Prasad Gudari, Atharva Negi, Praveen Raj Budime, Harsit Upadhya</div>
<div class="meta-line">First: 2025-10-02T02:49:09+00:00 · Latest: 2026-01-02T03:53:15+00:00</div>
<div class="meta-line">Comments: Submitted to ICAEI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01612v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01612v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapidly growth of biomedical literature creates challenges acquiring specific medical information. Current biomedical question-answering systems primarily focus on short-form answers, failing to provide comprehensive explanations necessary for clinical decision-making. We present RAG-BioQA, a retrieval-augmented generation framework for long-form biomedical question answering. Our system integrates BioBERT embeddings with FAISS indexing for retrieval and a LoRA fine-tuned FLAN-T5 model for answer generation. We train on 181k QA pairs from PubMedQA, MedDialog, and MedQuAD, and evaluate on a held-out PubMedQA test set. We compare four retrieval strategies: dense retrieval (FAISS), BM25, ColBERT, and MonoT5. Our results show that domain-adapted dense retrieval outperforms zero-shot neural re-rankers, with the best configuration achieving 0.24 BLEU-1 and 0.29 ROUGE-1. Fine-tuning improves BERTScore by 81\% over the base model. We release our framework to support reproducible biomedical QA research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG-BioQA：一种用于长篇生物医学问答的检索增强生成框架</div>
<div class="mono" style="margin-top:8px">生物医学文献的快速增长带来了获取特定医疗信息的挑战。目前的生物医学问答系统主要集中于短答案，未能提供临床决策所需的全面解释。我们提出了RAG-BioQA，一种用于长篇生物医学问答的检索增强生成框架。我们的系统将BioBERT嵌入与FAISS索引结合用于检索，并使用经过LoRA微调的FLAN-T5模型进行答案生成。我们在来自PubMedQA、MedDialog和MedQuAD的181k QA对上进行训练，并在保留的PubMedQA测试集上进行评估。我们比较了四种检索策略：密集检索（FAISS）、BM25、ColBERT和MonoT5。我们的结果表明，领域适应的密集检索优于零-shot神经重排序器，最佳配置达到0.24 BLEU-1和0.29 ROUGE-1。微调使BERTScore比基础模型提高了81%。我们发布了我们的框架以支持可重复的生物医学QA研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing volume of biomedical literature presents challenges in obtaining specific medical information, particularly as existing systems focus on short-form answers that lack the depth needed for clinical decision-making. To address this, the authors developed RAG-BioQA, a retrieval-augmented generation framework designed for long-form biomedical question answering, which combines BioBERT embeddings with FAISS indexing for retrieval and a LoRA fine-tuned FLAN-T5 model for generating answers. Training on 181,000 question-answer pairs and evaluating on a PubMedQA test set, the study found that domain-adapted dense retrieval methods significantly outperformed zero-shot neural re-rankers, achieving a BLEU-1 score of 0.24 and a ROUGE-1 score of 0.29, with fine-tuning resulting in an 81% improvement in BERTScore over the base model.</div>
<div class="mono" style="margin-top:8px">随着生物医学文献的快速增长，获取特定医学信息面临挑战，尤其是在需要详细解释的临床决策中。为了解决这个问题，作者开发了RAG-BioQA，一个用于长篇生物医学问答的检索增强生成框架，该框架结合了BioBERT嵌入和FAISS索引进行检索，并使用LoRA微调的FLAN-T5模型生成答案。该研究在181,000个来自不同生物医学数据集的问题-答案对上进行训练，并在PubMedQA测试集上进行评估，结果表明，领域适应的密集检索方法显著优于零样本神经重排序器，BLEU-1得分为0.24，ROUGE-1得分为0.29，而微调使BERTScore相比基础模型提高了81%。</div>
</details>
</div>
<div class="card">
<div class="title">FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection</div>
<div class="meta-line">Authors: Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang</div>
<div class="meta-line">First: 2026-01-02T02:36:48+00:00 · Latest: 2026-01-02T02:36:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeText：通过注意力定位和频谱字形注入实现扩散变换器中的无训练文本渲染</div>
<div class="mono" style="margin-top:8px">大规模文本到图像（T2I）扩散模型在开放域合成方面表现出色，但在精确文本渲染方面仍然存在困难，尤其是在多行布局、密集排版和长尾脚本（如中文）中。以往的解决方案通常需要昂贵的再训练或严格的外部布局约束，这可能会降低美观性并限制灵活性。我们提出了\textbf{FreeText}，一个无训练、即插即用的框架，通过利用\emph{扩散变换器（DiT）}模型的内在机制来改善文本渲染。\textbf{FreeText}将问题分解为\emph{写在哪里}和\emph{写什么}。对于\emph{写在哪里}，我们通过读取内生图像到文本注意力的标记级空间归因来定位写作区域，使用类似水槽的标记作为稳定的空间锚点，并进行拓扑感知的细化以生成高置信度的掩码。对于\emph{写什么}，我们引入了频谱调制字形注入（SGMI），它通过频域带通调制注入与噪声对齐的字形先验，以增强字形结构并抑制语义泄漏（渲染概念而非单词）。在长文本基准、CVTG和我们的CLT-Bench上对Qwen-Image、FLUX.1-dev和SD3变体的广泛实验表明，文本可读性持续提高，同时在很大程度上保持语义对齐和美学质量，推理开销适中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of large-scale text-to-image diffusion models in rendering precise text, particularly for complex layouts and diverse scripts without the need for retraining. The authors propose FreeText, a training-free framework that enhances text rendering by utilizing the intrinsic mechanisms of Diffusion Transformer models. Key experimental findings demonstrate that FreeText significantly improves text readability while maintaining semantic alignment and aesthetic quality across various benchmarks, with only a modest increase in inference overhead.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本生成模型中文本渲染的效果，这些模型在处理复杂布局和多样化脚本时常常面临困难，而无需昂贵的再训练。作者提出了FreeText，这是一个无训练的框架，将文本渲染任务分为确定写在哪里和写什么，利用注意力定位进行空间锚定，并通过频谱调制字形注入来改善字形结构。实验结果表明，FreeText在多个基准测试中显著提高了文本可读性，同时保持了语义一致性和美学质量，仅略微增加了推理时间。</div>
</details>
</div>
<div class="card">
<div class="title">All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations</div>
<div class="meta-line">Authors: Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan</div>
<div class="meta-line">First: 2026-01-02T02:20:57+00:00 · Latest: 2026-01-02T02:20:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00533v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00533v1">PDF</a> · <a href="https://github.com/Friskknight/ORCANet-SEUD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在平滑演变的未知天气退化下的全能视频修复</div>
<div class="mono" style="margin-top:8px">全能图像修复旨在通过单一模型从多种未知退化中恢复干净图像。但将此任务扩展到视频面临独特挑战。现有方法主要关注逐帧退化变化，忽视了现实世界退化过程中的时间连续性。在实践中，退化类型和强度随时间平滑演变，且多种退化可能共存或逐渐过渡。本文介绍了平滑演变的未知退化（SEUD）场景，其中主动退化集和退化强度随时间连续变化。为支持该场景，我们设计了一个灵活的合成管道，生成具有单一、复合和演变退化的时间一致性视频。为应对SEUD场景中的挑战，我们提出了一种全能递归条件自适应提示网络（ORCANet）。首先，粗略强度估计去雾（CIED）模块使用物理先验估计雾霾强度，并提供粗略去雾特征作为初始化。其次，流提示生成（FPG）模块提取退化特征。FPG生成捕捉段级退化类型的静态提示和适应帧级强度变化的动态提示。此外，标签感知监督机制提高了不同退化下静态提示表示的可区分性。大量实验表明，ORCANet在修复质量、时间一致性和鲁棒性方面优于基于图像和视频的基线。代码可在https://github.com/Friskknight/ORCANet-SEUD获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video restoration by addressing the challenges posed by smoothly evolving unknown weather degradations, which are often overlooked in existing methods that focus on frame-wise variations. The authors introduce a novel scenario called Smoothly Evolving Unknown Degradations (SEUD) and propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet) to tackle these challenges. Experimental results demonstrate that ORCANet significantly outperforms traditional image and video restoration methods in terms of restoration quality, temporal consistency, and robustness, effectively handling both static and dynamic degradation features.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决平滑演变的未知天气退化所带来的挑战来增强视频恢复，而现有方法往往忽视这一点。作者引入了一种新场景，称为平滑演变的未知退化（SEUD），并提出了一种全新的递归条件自适应提示网络（ORCANet）来应对这一问题。关键实验结果表明，ORCANet在恢复质量、时间一致性和鲁棒性方面显著优于图像和视频基线。</div>
</details>
</div>
<div class="card">
<div class="title">Federated Customization of Large Models: Approaches, Experiments, and Insights</div>
<div class="meta-line">Authors: Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato</div>
<div class="meta-line">First: 2026-01-02T01:45:52+00:00 · Latest: 2026-01-02T01:45:52+00:00</div>
<div class="meta-line">Comments: 8 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大模型的联邦定制：方法、实验与见解</div>
<div class="mono" style="margin-top:8px">本文探讨了大模型的联邦定制，并强调了它在联邦学习框架中所面临的主要挑战。我们回顾了几种流行的大模型定制技术，包括完全微调、高效微调、提示工程、前缀微调、知识蒸馏和检索增强生成。然后，我们讨论了如何在联邦学习框架中实现这些技术。此外，我们对联邦前缀微调进行了实验，据我们所知，这是首次在联邦学习环境中应用前缀微调。实验验证了其可行性，性能接近集中式方法。与其他三种联邦定制方法的进一步比较显示了其竞争力、令人满意的效率和一致的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the challenges of federated customization of large models within the federated learning framework, motivated by the need for effective model adaptation in decentralized environments. The authors review various customization techniques such as full fine-tuning, efficient fine-tuning, and prefix-tuning, and they specifically focus on implementing federated prefix-tuning, which is presented as a novel approach. Experimental results indicate that federated prefix-tuning achieves performance levels comparable to centralized methods and outperforms three other federated customization techniques in terms of efficiency and robustness.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在联邦学习框架下大模型的联邦定制面临的挑战，动机是为了在去中心化环境中有效适应模型。作者回顾了多种定制技术，如全量微调和提示工程，并特别对联邦前缀微调进行了实验，标志着该方法在联邦环境中的首次应用。结果表明，联邦前缀微调的性能与集中式方法相当，并在效率和鲁棒性方面优于其他三种联邦定制技术。</div>
</details>
</div>
<div class="card">
<div class="title">FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation</div>
<div class="meta-line">Authors: Zebin Yao, Lei Ren, Huixing Jiang, Wei Chen, Xiaojie Wang, Ruifan Li, Fangxiang Feng</div>
<div class="meta-line">First: 2025-04-22T14:55:23+00:00 · Latest: 2026-01-02T01:08:59+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Nihukat/FreeGraftor</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15958v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.15958v4">PDF</a> · <a href="https://github.com/Nihukat/FreeGraftor">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Subject-driven image generation aims to synthesize novel scenes that faithfully preserve subject identity from reference images while adhering to textual guidance. However, existing methods struggle with a critical trade-off between fidelity and efficiency. Tuning-based approaches rely on time-consuming and resource-intensive, subject-specific optimization, while zero-shot methods often fail to maintain adequate subject consistency. In this work, we propose FreeGraftor, a training-free framework that addresses these limitations through cross-image feature grafting. Specifically, FreeGraftor leverages semantic matching and position-constrained attention fusion to transfer visual details from reference subjects to the generated images. Additionally, our framework introduces a novel noise initialization strategy to preserve the geometry priors of reference subjects, facilitating robust feature matching. Extensive qualitative and quantitative experiments demonstrate that our method enables precise subject identity transfer while maintaining text-aligned scene synthesis. Without requiring model fine-tuning or additional training, FreeGraftor significantly outperforms existing zero-shot and training-free approaches in both subject fidelity and text alignment. Furthermore, our framework can seamlessly extend to multi-subject generation, making it practical for real-world deployment. Our code is available at https://github.com/Nihukat/FreeGraftor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeGraftor：无训练的跨图像特征嫁接用于主题驱动的文本到图像生成</div>
<div class="mono" style="margin-top:8px">主题驱动的图像生成旨在合成新颖场景，忠实保留参考图像中的主题身份，同时遵循文本指导。然而，现有方法在保真度和效率之间存在关键权衡。基于调优的方法依赖于耗时且资源密集的主题特定优化，而零-shot方法往往无法保持足够的主题一致性。在本研究中，我们提出了FreeGraftor，一个无训练的框架，通过跨图像特征嫁接来解决这些局限性。具体而言，FreeGraftor利用语义匹配和位置约束的注意力融合，将视觉细节从参考主题转移到生成的图像。此外，我们的框架引入了一种新颖的噪声初始化策略，以保留参考主题的几何先验，促进稳健的特征匹配。大量定性和定量实验表明，我们的方法能够实现精确的主题身份转移，同时保持文本对齐的场景合成。在不需要模型微调或额外训练的情况下，FreeGraftor在主题保真度和文本对齐方面显著优于现有的零-shot和无训练方法。此外，我们的框架可以无缝扩展到多主题生成，使其在实际应用中具有可行性。我们的代码可在 https://github.com/Nihukat/FreeGraftor 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve subject-driven image generation by addressing the trade-off between fidelity and efficiency in existing methods. The authors propose FreeGraftor, a training-free framework that utilizes cross-image feature grafting, semantic matching, and position-constrained attention fusion to effectively transfer visual details from reference images to generated scenes. Experimental results indicate that FreeGraftor achieves precise subject identity transfer and maintains text alignment without the need for model fine-tuning, outperforming existing zero-shot and training-free methods, and demonstrating its capability for multi-subject generation suitable for practical applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法在保真度和效率之间的权衡，来改善基于主题的图像生成。作者提出了FreeGraftor，这是一种无训练的框架，利用跨图像特征嫁接、语义匹配和位置约束注意力融合，有效地将参考图像中的视觉细节转移到生成的场景中。实验结果表明，FreeGraftor在不需要模型微调的情况下，实现了精确的主题身份转移，并保持了文本对齐，超越了现有的零-shot和无训练方法，并展示了其适用于实际应用的多主题生成能力。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments</div>
<div class="meta-line">Authors: Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu</div>
<div class="meta-line">First: 2025-12-30T23:04:29+00:00 · Latest: 2026-01-01T21:35:11+00:00</div>
<div class="meta-line">Comments: 43 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24504v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24504v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地图思维：基础模型代理如何探索、记忆和推理地图环境</div>
<div class="mono" style="margin-top:8px">地图环境为表示空间结构提供了基本媒介。因此，理解基础模型（FM）代理如何理解和在此类环境中行动对于实现可靠的基于地图的推理和应用至关重要。然而，现有的大多数FM空间能力评估依赖于静态地图输入或基于文本的查询，忽视了空间理解的互动和经验驱动特性。本文提出了一种互动评估框架，以分析FM代理如何在符号地图环境中探索、记忆和推理。代理逐步探索由道路、交叉口和兴趣点（POI）组成的部分可观察网格地图，在每一步仅接收局部观察。然后使用六种空间任务评估空间理解。通过系统地变化探索策略、记忆表示和推理方案，我们揭示了这些组件的不同功能角色。探索主要影响经验获取，但对最终推理准确性的影响有限。相比之下，记忆表示在巩固空间经验中发挥核心作用，结构化记忆，特别是顺序和基于图的表示，显著提高了在结构密集型任务（如路径规划）上的表现。推理方案进一步影响存储的空间知识的使用，先进的提示支持更有效的多步骤推理。我们进一步观察到，空间推理性能在模型版本和规模达到某一能力阈值后趋于饱和，这表明改善基于地图的空间理解需要针对空间表示和推理量身定制的机制，而不仅仅是规模的扩大。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how foundation model (FM) agents understand and interact with map environments, motivated by the need for reliable map-based reasoning and applications. The authors propose an interactive evaluation framework that allows FM agents to explore grid-based maps while receiving local observations, assessing their spatial understanding through six types of spatial tasks. The findings indicate that exploration strategies mainly influence experience acquisition, while memory representation, particularly structured formats like sequential and graph-based representations, significantly enhances performance in tasks requiring spatial structure, such as path planning. Additionally, reasoning schemes affect the application of spatial knowledge, with advanced prompts facilitating better multi-step inference, and the study reveals that improvements in spatial reasoning performance plateau beyond a certain model capability threshold, suggesting the necessity for specialized mechanisms in spatial representation and reasoning rather than mere scaling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强对基础模型（FM）代理如何与地图环境互动及推理的理解，这对空间推理应用至关重要。作者提出了一种互动评估框架，使FM代理能够在接收局部观察的同时探索基于网格的地图，并通过六种类型的空间任务评估空间理解。研究结果表明，探索策略主要影响经验获取，但对推理准确性影响有限，而记忆表示，特别是结构化记忆，显著提高了在需要空间结构的任务（如路径规划）中的表现。此外，研究还发现推理方案影响多步骤推理的有效性，并且在不同模型版本中，空间推理性能达到饱和点，这表明空间理解的进步更多依赖于量身定制的机制，而非单纯的模型扩展。</div>
</details>
</div>
<div class="card">
<div class="title">Imitation from Observations with Trajectory-Level Generative Embeddings</div>
<div class="meta-line">Authors: Yongtao Qu, Shangzhe Li, Weitong Zhang</div>
<div class="meta-line">First: 2026-01-01T19:38:37+00:00 · Latest: 2026-01-01T19:38:37+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00452v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于轨迹级生成嵌入的观察模仿</div>
<div class="mono" style="margin-top:8px">我们考虑从观察中进行离线模仿学习（LfO），在这种情况下，专家演示稀缺且可用的离线次优数据与专家行为相距甚远。许多现有的分布匹配方法在这种情况下表现不佳，因为它们施加了严格的支持约束，并依赖脆弱的一步模型，使得从不完美数据中提取有用信号变得困难。为了解决这个挑战，我们提出了TGE，一种用于离线LfO的轨迹级生成嵌入，通过估计在离线轨迹数据上训练的时间扩散模型的潜在空间中的专家状态密度，构建一个密集、平滑的替代奖励。通过利用学习到的扩散嵌入的平滑几何，TGE捕捉长时间范围的动态，并有效地弥合不相交支持之间的差距，确保即使离线数据在分布上与专家不同，也能提供稳健的学习信号。实证结果表明，所提出的方法在一系列D4RL运动和操控基准测试中始终与之前的离线LfO方法相匹配或超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of offline imitation learning from observations (LfO) in scenarios where expert demonstrations are limited and available data is suboptimal. The authors introduce a method called TGE, which utilizes trajectory-level generative embeddings to create a smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. Experimental results demonstrate that TGE consistently matches or surpasses existing offline LfO methods across various D4RL locomotion and manipulation benchmarks, effectively capturing long-horizon dynamics and providing a robust learning signal despite distributional discrepancies in the data.</div>
<div class="mono" style="margin-top:8px">本研究解决了观察下的离线模仿学习（LfO）中的挑战，专家演示有限且可用数据次优。作者提出了一种名为TGE的方法，该方法利用轨迹级生成嵌入，通过在离线轨迹数据上训练的时间扩散模型中估计专家状态密度来创建平滑的替代奖励。实验结果表明，TGE在各种D4RL运动和操作基准测试中始终与现有的离线LfO方法相匹配或超越，有效捕捉长期动态，并在数据分布差异的情况下提供稳健的学习信号。</div>
</details>
</div>
<div class="card">
<div class="title">A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection</div>
<div class="meta-line">Authors: Miseon Park, Kijung Yoon</div>
<div class="meta-line">First: 2026-01-01T19:11:33+00:00 · Latest: 2026-01-01T19:11:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00446v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00446v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间序列基础模型在异常检测中的适应策略比较研究</div>
<div class="mono" style="margin-top:8px">时间序列异常检测对复杂系统的可靠运行至关重要，但大多数现有方法需要广泛的任务特定训练。我们探讨了在大规模异构数据上预训练的时间序列基础模型（TSFMs）是否可以作为异常检测的通用骨干。通过在多个基准上的系统实验，我们比较了零-shot推理、完整模型适应和参数高效微调（PEFT）策略。我们的结果表明，TSFMs在AUC-PR和VUS-PR上超越了任务特定基线，特别是在严重类别不平衡的情况下。此外，LoRA、OFT和HRA等PEFT方法不仅降低了计算成本，而且在大多数情况下与完整微调相匹配或超越，表明即使在为预测预训练的情况下，TSFMs也可以高效地适应异常检测。这些发现使TSFMs成为可扩展和高效的时间序列异常检测的有前景的通用模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve time series anomaly detection, which is crucial for the reliable operation of complex systems, by utilizing time series foundation models (TSFMs) that require less task-specific training. The authors conducted systematic experiments comparing zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies across multiple benchmarks. The key findings reveal that TSFMs significantly outperform task-specific baselines in terms of AUC-PR and VUS-PR, especially in scenarios with severe class imbalance, and that PEFT methods like LoRA, OFT, and HRA not only lower computational costs but also achieve comparable or superior performance to full fine-tuning, suggesting the effectiveness of TSFMs for scalable anomaly detection applications.</div>
<div class="mono" style="margin-top:8px">本研究探讨了时间序列基础模型（TSFM）在异常检测中的潜力，动机是现有方法需要大量特定任务的训练。研究通过系统实验比较了零样本推理、全模型适应和参数高效微调（PEFT）策略在多个基准上的表现。结果表明，TSFM在AUC-PR和VUS-PR方面显著优于特定任务的基线，特别是在严重类别不平衡的情况下，并且PEFT方法如LoRA、OFT和HRA能够降低计算成本，同时在大多数情况下达到或超过全微调的性能，表明TSFM在可扩展的异常检测应用中是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</div>
<div class="meta-line">Authors: Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang</div>
<div class="meta-line">First: 2026-01-01T09:11:09+00:00 · Latest: 2026-01-01T09:11:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model&#x27;s activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model&#x27;s overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActErase：一种无训练的精确概念擦除范式，通过激活修补</div>
<div class="mono" style="margin-top:8px">最近文本到图像扩散模型的进展展示了显著的生成能力，但也引发了关于安全性、版权和伦理影响的重大担忧。现有的概念擦除方法通过从预训练模型中移除敏感概念来应对这些风险，但大多数方法依赖于数据密集和计算成本高昂的微调，这构成了一个关键限制。为了克服这些挑战，我们提出了一种新颖的无训练方法（ActErase）用于高效的概念擦除，灵感来自于观察到模型的激活主要由通用概念组成，只有极小的部分可以表示目标概念。具体而言，所提方法通过提示对分析识别激活差异区域，提取目标激活并在前向传递过程中动态替换输入激活。对三个关键擦除任务（裸体、艺术风格和物体移除）的全面评估表明，我们的无训练方法实现了最先进的擦除性能，同时有效保留了模型的整体生成能力。我们的方法在对抗攻击下也表现出强大的鲁棒性，为扩散模型中的轻量级而有效的概念操作建立了新的即插即用范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the safety, copyright, and ethical concerns associated with text-to-image diffusion models, particularly through the lens of concept erasure. The authors propose a novel training-free method called ActErase, which identifies activation difference regions via prompt-pair analysis and dynamically replaces input activations during forward passes to achieve efficient concept erasure. Experimental results show that ActErase outperforms existing methods in three critical erasure tasks—nudity, artistic style, and object removal—while maintaining the overall generative capabilities of the model and demonstrating robustness against adversarial attacks.</div>
<div class="mono" style="margin-top:8px">本研究解决了文本到图像扩散模型中现有概念消除方法的局限性，这些方法通常需要大量的微调并且计算成本高。作者提出了一种新的无训练方法ActErase，通过提示对分析识别激活差异区域，并在前向传播过程中替换输入激活，以实现高效的概念消除。实验结果表明，ActErase在消除敏感概念（如裸体、艺术风格和物体移除）方面优于现有的最先进方法，同时保持了模型的整体生成能力，并表现出对对抗攻击的强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Graph Fine-Tuning with Adversarial Graph Prompting</div>
<div class="meta-line">Authors: Ziyan Zhang, Bo Jiang, Jin Tang</div>
<div class="meta-line">First: 2026-01-01T06:20:10+00:00 · Latest: 2026-01-01T06:20:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00229v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00229v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过对抗图提示进行鲁棒图微调</div>
<div class="mono" style="margin-top:8px">参数高效微调（PEFT）方法已成为将预训练GNN模型适应于下游任务的主导范式。然而，现有的PEFT方法通常对图拓扑和节点属性/特征的各种噪声和攻击表现出显著的脆弱性。为了解决这个问题，我们首次提出将对抗学习集成到图提示中，并开发了一种新颖的对抗图提示（AGP）框架，以实现鲁棒的图微调。我们的AGP有两个关键方面。首先，我们将AGP的一般问题表述为一个最小-最大优化问题，并开发了一种交替优化方案来解决它。对于内部最大化，我们提出了联合投影梯度下降（JointPGD）算法，以生成强对抗噪声。对于外部最小化，我们采用一个简单而有效的模块来学习最佳节点提示，以抵消对抗噪声。其次，我们证明了所提出的AGP在理论上可以解决图拓扑和节点噪声。这证实了我们的AGP微调方法在各种图噪声下的多功能性和鲁棒性。需要注意的是，所提出的AGP是一种通用方法，可以与各种预训练的GNN模型集成，以增强它们在下游任务中的鲁棒性。在多个基准任务上的广泛实验验证了AGP方法相较于最先进方法的鲁棒性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the robustness of Parameter-Efficient Fine-Tuning (PEFT) methods for adapting pre-trained Graph Neural Network (GNN) models, which are often vulnerable to noise and attacks. The authors propose a novel Adversarial Graph Prompting (AGP) framework that integrates adversarial learning into graph prompting, formulated as a min-max optimization problem. Key experimental results demonstrate that AGP effectively addresses both graph topology and node noise, showing significant improvements in robustness and effectiveness compared to existing state-of-the-art methods across various benchmark tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强参数高效微调（PEFT）方法在将预训练图神经网络（GNN）适应于下游任务时的鲁棒性，而这些方法通常对噪声和攻击表现出显著的脆弱性。作者提出了一种新颖的对抗图提示（AGP）框架，将对抗学习整合到图提示中，形成一个最小-最大优化问题，并通过交替优化方案进行求解。实验结果表明，AGP有效地解决了图拓扑和节点噪声问题，在多个基准任务中相比现有的最先进方法显示出显著的鲁棒性和有效性提升。</div>
</details>
</div>
<div class="card">
<div class="title">AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</div>
<div class="meta-line">Authors: Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen</div>
<div class="meta-line">Venue: ICLR 2024</div>
<div class="meta-line">First: 2023-10-29T10:03:49+00:00 · Latest: 2026-01-01T03:03:39+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.18961v12">Abs</a> · <a href="https://arxiv.org/pdf/2310.18961v12">PDF</a> · <a href="https://github.com/zqhang/AnomalyCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnomalyCLIP：面向零样本异常检测的对象无关提示学习</div>
<div class="mono" style="margin-top:8px">零样本异常检测（ZSAD）需要使用辅助数据训练的检测模型，在目标数据集中检测异常而无需任何训练样本。这是一项关键任务，因为由于各种原因（例如数据隐私），训练数据无法访问，但这也很具挑战性，因为模型需要对不同领域中的异常进行泛化，其中前景对象、异常区域和背景特征（例如不同产品/器官上的缺陷/肿瘤）的外观可能会显著变化。最近，大型预训练视觉-语言模型（VLMs），如CLIP，在包括异常检测在内的各种视觉任务中展示了强大的零样本识别能力。然而，由于VLMs更关注建模前景对象的类别语义，而不是图像中的异常/正常性，因此它们的ZSAD性能较弱。在本文中，我们介绍了一种新方法，即AnomalyCLIP，以适应CLIP在不同领域的准确ZSAD。AnomalyCLIP的关键见解是学习对象无关的文本提示，捕捉图像中通用的正常性和异常性，而不考虑其前景对象。这使我们的模型能够专注于异常图像区域，而不是对象语义，从而实现对多种类型对象的普遍正常性和异常性识别。在17个真实世界异常检测数据集上的大规模实验表明，AnomalyCLIP在来自各种缺陷检测和医学成像领域的高度多样化类别语义的数据集中，达到了优越的零样本检测和分割异常的性能。代码将发布在https://github.com/zqhang/AnomalyCLIP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve zero-shot anomaly detection (ZSAD) in scenarios where training data is unavailable due to privacy concerns, which poses challenges in generalizing across diverse domains. The authors propose AnomalyCLIP, a method that adapts the pre-trained vision-language model CLIP by learning object-agnostic text prompts that focus on capturing normality and abnormality in images, independent of foreground objects. Experimental results demonstrate that AnomalyCLIP significantly enhances zero-shot performance in detecting and segmenting anomalies across 17 real-world datasets, showcasing its effectiveness in various defect inspection and medical imaging contexts.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善在缺乏训练数据（由于隐私问题）情况下的零样本异常检测（ZSAD），这在不同领域的各种异常泛化中带来了挑战。作者提出了一种新方法AnomalyCLIP，该方法通过学习与对象无关的文本提示来适应预训练的视觉语言模型CLIP，重点捕捉图像中的一般正常性和异常性，而不是前景对象的语义。实验结果表明，AnomalyCLIP在17个不同的真实世界数据集（包括缺陷检测和医学成像领域）中显著提高了零样本检测和分割异常的性能。</div>
</details>
</div>
<div class="card">
<div class="title">FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers</div>
<div class="meta-line">Authors: Ruichen Chen, Keith G. Mills, Di Niu</div>
<div class="meta-line">First: 2025-03-19T17:44:21+00:00 · Latest: 2025-12-31T22:29:52+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/cccrrrccc/FP4DiT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.15465v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.15465v3">PDF</a> · <a href="https://github.com/cccrrrccc/FP4DiT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Models (DM) have revolutionized the text-to-image visual generation process. However, the large computational cost and model footprint of DMs hinders practical deployment, especially on edge devices. Post-training quantization (PTQ) is a lightweight method to alleviate these burdens without the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8 \blue{(i.e., 4-bit weights and 8-bit activations)} on integer-based PTQ, two key limitations remain: First, while most existing DM PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier, which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like the PixArt series, Hunyuan and others adopt fundamentally different transformer backbones to achieve superior image synthesis. Second, integer (INT) quantization is prevailing in DM PTQ but does not align well with the network weight and activation distribution, while Floating-Point Quantization (FPQ) is still under-investigated, yet it holds the potential to better align the weight and activation distributions in low-bit settings for DiT. In this paper, we introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization. Specifically, we extend and generalize the Adaptive Rounding PTQ technique to adequately calibrate weight quantization for FPQ and demonstrate that DiT activations depend on input patch data, necessitating robust online activation quantization techniques. Experimental results demonstrate that FP4DiT achieves higher CLIP, ImageReward and HPSv2 performance compared to integer-based PTQ at the W4A6 and W4A8 precision levels while generating convincing visual content on PixArt-$α$, PixArt-$Σ$ and Hunyuan.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FP4DiT：有效的浮点量化方法用于扩散变换器</div>
<div class="mono" style="margin-top:8px">扩散模型（DM）彻底改变了文本到图像的视觉生成过程。然而，DM的高计算成本和模型占用空间阻碍了其在边缘设备上的实际部署。后训练量化（PTQ）是一种轻量级方法，可以在不需要训练或微调的情况下减轻这些负担。尽管最近的DM PTQ方法在基于整数的PTQ上实现了W4A8（即4位权重和8位激活），但仍然存在两个关键限制：首先，虽然大多数现有的DM PTQ方法在使用卷积U-Net的经典DM（如Stable Diffusion XL、1.5或更早版本）上进行评估，但新的扩散变换器（DiT）模型（如PixArt系列、Hunyuan等）采用根本不同的变换器骨干，以实现更优的图像合成。其次，整数（INT）量化在DM PTQ中占主导地位，但与网络权重和激活分布不太匹配，而浮点量化（FPQ）仍然未得到充分研究，但它有潜力在低位设置中更好地对齐DiT的权重和激活分布。在本文中，我们介绍了FP4DiT，这是一种利用FPQ实现W4A6量化的PTQ方法。具体而言，我们扩展和概括了自适应舍入PTQ技术，以充分校准FPQ的权重量化，并证明DiT激活依赖于输入补丁数据， necessitating robust online activation quantization techniques。实验结果表明，FP4DiT在W4A6和W4A8精度水平上相比基于整数的PTQ实现了更高的CLIP、ImageReward和HPSv2性能，同时在PixArt-$α$、PixArt-$Σ$和Hunyuan上生成了令人信服的视觉内容。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs and model sizes of Diffusion Models (DM), which limit their deployment on edge devices. The authors propose FP4DiT, a post-training quantization (PTQ) method that utilizes floating-point quantization (FPQ) to achieve W4A6 quantization, extending the Adaptive Rounding PTQ technique for better weight calibration. Experimental results indicate that FP4DiT outperforms integer-based PTQ methods in terms of CLIP, ImageReward, and HPSv2 metrics at both W4A6 and W4A8 precision levels while effectively generating high-quality visual content.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决扩散模型（DM）高计算成本和模型体积的问题，这限制了它们在边缘设备上的部署。作者提出了一种名为FP4DiT的后训练量化（PTQ）方法，该方法利用浮点量化（FPQ）实现W4A6量化，增强了扩散变换器（DiT）中权重和激活分布的对齐。实验结果表明，FP4DiT在W4A6和W4A8精度水平上优于基于整数的PTQ方法，获得了更高的CLIP、ImageReward和HPSv2性能，同时有效生成高质量的视觉内容。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Intelligence Meets BD-RIS-Enabled AmBC: Challenges, Opportunities, and Practical Insights</div>
<div class="meta-line">Authors: Abd Ullah Khan, Uman Khalid, Trung Q. Duong, Hyundong Shin</div>
<div class="meta-line">First: 2025-12-29T11:43:41+00:00 · Latest: 2025-12-31T21:19:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23400v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23400v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A beyond-diagonal reconfigurable intelligent surface (BD-RIS) is an innovative type of reconfigurable intelligent surface (RIS) that has recently been proposed and is considered a revolutionary advancement in wave manipulation. Unlike the mutually disconnected arrangement of elements in traditional RISs, BD-RIS creates cost-effective and simple inter-element connections, allowing for greater freedom in configuring the amplitude and phase of impinging waves. However, there are numerous underlying challenges in realizing the advantages associated with BD-RIS, prompting the research community to actively investigate cutting-edge schemes and algorithms in this direction. Particularly, the passive beamforming design for BD-RIS under specific environmental conditions has become a major focus in this research area. In this article, we provide a systematic introduction to BD-RIS, elaborating on its functional principles concerning architectural design, promising advantages, and classification. Subsequently, we present recent advances and identify a series of challenges and opportunities. Additionally, we consider a specific case study where beamforming is designed using four different algorithms, and we analyze their performance with respect to sum rate and computation cost. To augment the beamforming capabilities in 6G BD-RIS with quantum enhancement, we analyze various hybrid quantum-classical machine learning (ML) models to improve beam prediction performance, employing real-world communication Scenario 8 from the DeepSense 6G dataset. Consequently, we derive useful insights about the practical implications of BD-RIS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量子智能与BD-RIS驱动的自适应波束形成：挑战、机遇与实践见解</div>
<div class="mono" style="margin-top:8px">超对角可重构智能表面（BD-RIS）是一种创新型可重构智能表面（RIS），最近被提出，并被认为是波动操控的革命性进展。与传统RIS中元素的相互独立排列不同，BD-RIS创建了成本效益高且简单的元素间连接，允许在配置入射波的幅度和相位上有更大的自由度。然而，实现BD-RIS相关优势的过程中存在许多潜在挑战，这促使研究界积极探索这一方向的前沿方案和算法。特别是在特定环境条件下，BD-RIS的被动波束形成设计已成为该研究领域的主要关注点。本文系统介绍了BD-RIS，详细阐述其在建筑设计、潜在优势和分类方面的功能原理。随后，我们展示了最新进展，并识别出一系列挑战和机遇。此外，我们考虑了一个具体案例研究，其中使用四种不同算法设计波束形成，并分析它们在总速率和计算成本方面的性能。为了增强6G BD-RIS中的波束形成能力，我们分析了各种混合量子-经典机器学习（ML）模型，以提高波束预测性能，采用来自DeepSense 6G数据集的真实通信场景8。因此，我们得出了关于BD-RIS实际应用的有用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to explore the potential of beyond-diagonal reconfigurable intelligent surfaces (BD-RIS) in wave manipulation, addressing the challenges that arise from their implementation. The study employs a systematic introduction to BD-RIS, detailing its architectural design and advantages, followed by a case study that evaluates the performance of passive beamforming using four different algorithms in terms of sum rate and computation cost. The findings indicate that hybrid quantum-classical machine learning models can enhance beam prediction performance in 6G BD-RIS, providing valuable insights into its practical applications in real-world communication scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探索超对角可重构智能表面（BD-RIS）在波动操控中的潜力，同时解决其实施中出现的挑战。研究系统性地介绍了BD-RIS，讨论了其建筑设计、优势和分类，并重点关注特定环境条件下的被动波束成形设计。主要实验结果表明，分析了四种不同的波束成形算法，显示出在总速率和计算成本方面的不同性能，同时采用混合量子-经典机器学习模型来增强真实通信场景中的波束预测性能，为BD-RIS在6G应用中的能力提供了实用见解。</div>
</details>
</div>
<div class="card">
<div class="title">It&#x27;s Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</div>
<div class="meta-line">Authors: Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros</div>
<div class="meta-line">First: 2025-12-31T19:47:49+00:00 · Latest: 2025-12-31T19:47:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.00090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>永远不嫌晚：训练扩散模型中的噪声优化以恢复崩溃</div>
<div class="mono" style="margin-top:8px">当给定相同的文本提示时，现代文本到图像模型表现出惊人的模式崩溃程度，这在采样多个图像时可见。虽然之前的工作试图通过引导机制来引导模型，或通过生成大量候选项并对其进行精炼来解决此问题，但在本研究中，我们采取了不同的方向，旨在通过噪声优化实现生成的多样性。具体而言，我们展示了一个简单的噪声优化目标可以在保持基础模型保真度的同时减轻模式崩溃。我们还分析了噪声的频率特性，并表明具有不同频率特征的替代噪声初始化可以改善优化和搜索。我们的实验表明，噪声优化在生成质量和多样性方面产生了更优的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of mode collapse in contemporary text-to-image models, which limits the diversity of generated images from the same text prompt. The authors propose a novel approach that utilizes noise optimization to enhance diversity in image generation, contrasting with previous methods that relied on guidance mechanisms or candidate refinement. Their experiments reveal that implementing a simple noise optimization objective not only reduces mode collapse but also maintains the fidelity of the original model, with findings indicating that different noise initializations can further improve both optimization and generation quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当代文本到图像模型中出现的模式崩溃问题，这导致从相同文本提示生成的图像缺乏多样性。作者提出了一种新方法，通过噪声优化来增强图像生成的多样性，这与之前依赖于引导机制或候选图像精炼的方法形成对比。实验结果表明，这种噪声优化不仅减少了模式崩溃，还保持了原始模型的保真度，研究发现不同的噪声初始化可以进一步改善优化和生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</div>
<div class="meta-line">Authors: Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu</div>
<div class="meta-line">First: 2025-12-31T18:58:30+00:00 · Latest: 2025-12-31T18:58:30+00:00</div>
<div class="meta-line">Comments: Project Page https://hjrphoebus.github.io/X-Dub</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25066v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25066v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hjrphoebus.github.io/X-Dub">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-driven visual dubbing aims to synchronize a video&#x27;s lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject&#x27;s lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从修复到编辑：一个自我引导的上下文丰富视觉配音框架</div>
<div class="mono" style="margin-top:8px">音频驱动的视觉配音旨在将视频的嘴唇动作与新语音同步，但根本上受到理想训练数据缺乏的挑战：配对视频中只有主体的嘴唇动作不同，而所有其他视觉条件相同。现有方法通过基于掩膜的修复范式来规避这一问题，其中不完整的视觉条件迫使模型同时幻觉缺失内容并同步嘴唇，导致视觉伪影、身份漂移和同步不良。在本研究中，我们提出了一种新颖的自我引导框架，将视觉配音从一个不适定的修复任务重新构建为一个良好条件的视频到视频编辑问题。我们的方法采用扩散变换器，首先作为数据生成器，合成理想的训练数据：每个真实样本的嘴唇改变伴随视频，形成视觉对齐的视频对。然后，基于DiT的音频驱动编辑器在这些对上进行端到端训练，利用完整且对齐的输入视频帧，专注于精确的音频驱动嘴唇修改。这种完整的、帧对齐的输入条件为编辑器提供了丰富的视觉上下文，提供完整的身份线索、场景交互和连续的时空动态。利用这种丰富的上下文，根本上使我们的方法能够实现高度准确的嘴唇同步、忠实的身份保留以及对具有挑战性的实际场景的卓越鲁棒性。我们进一步引入了一种时间步自适应的多阶段学习策略，作为必要组件，以解开扩散时间步之间相互冲突的编辑目标，从而促进稳定训练，并提高嘴唇同步和视觉保真度。此外，我们提出了ContextDubBench，一个全面的基准数据集，用于在多样且具有挑战性的实际应用场景中进行稳健评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of audio-driven visual dubbing, particularly the lack of ideal training data that leads to visual artifacts and poor synchronization. The authors propose a self-bootstrapping framework that transforms the dubbing task from an inpainting problem into a video-to-video editing problem using a Diffusion Transformer to generate ideal training data, creating visually aligned video pairs. Experimental results demonstrate that this approach achieves highly accurate lip synchronization, preserves identity effectively, and shows robustness in challenging scenarios, aided by a timestep-adaptive multi-phase learning strategy and the introduction of a new benchmark dataset, ContextDubBench, for evaluation purposes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善音频驱动的视觉配音，该技术面临缺乏理想训练数据的问题，即仅在嘴唇动作不同的配对视频。作者提出了一种自引导框架，将视觉配音任务转变为视频到视频的编辑问题，利用扩散变换器生成理想训练数据，通过创建嘴唇变化的伴随视频。实验结果表明，该方法实现了高度准确的嘴唇同步，有效保持了身份，并在具有挑战性的场景中表现出鲁棒性，得益于时间步自适应的多阶段学习策略和新基准数据集ContextDubBench的引入，以便进行评估。</div>
</details>
</div>
<div class="card">
<div class="title">HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films</div>
<div class="meta-line">Authors: Rongji Xun, Junjie Yuan, Zhongjie Wang</div>
<div class="meta-line">First: 2025-12-31T16:18:07+00:00 · Latest: 2025-12-31T16:18:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24946v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24946v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing open-source film restoration methods show limited performance compared to commercial methods due to training with low-quality synthetic data and employing noisy optical flows. In addition, high-resolution films have not been explored by the open-source methods.We propose HaineiFRDM(Film Restoration Diffusion Model), a film restoration framework, to explore diffusion model&#x27;s powerful content-understanding ability to help human expert better restore indistinguishable film defects.Specifically, we employ a patch-wise training and testing strategy to make restoring high-resolution films on one 24GB-VRAMR GPU possible and design a position-aware Global Prompt and Frame Fusion Modules.Also, we introduce a global-local frequency module to reconstruct consistent textures among different patches. Besides, we firstly restore a low-resolution result and use it as global residual to mitigate blocky artifacts caused by patching process.Furthermore, we construct a film restoration dataset that contains restored real-degraded films and realistic synthetic data.Comprehensive experimental results conclusively demonstrate the superiority of our model in defect restoration ability over existing open-source methods. Code and the dataset will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HaineiFRDM：探索扩散以修复快速运动影片中的缺陷</div>
<div class="mono" style="margin-top:8px">现有的开源影片修复方法由于使用低质量合成数据进行训练和采用噪声光流，表现相较于商业方法有限。此外，高分辨率影片尚未被开源方法探索。我们提出HaineiFRDM（影片修复扩散模型），这是一个影片修复框架，旨在探索扩散模型强大的内容理解能力，以帮助人类专家更好地修复难以区分的影片缺陷。具体而言，我们采用了基于补丁的训练和测试策略，使得在一台24GB-VRAM的GPU上修复高分辨率影片成为可能，并设计了位置感知的全局提示和帧融合模块。此外，我们引入了全局-局部频率模块，以重建不同补丁之间一致的纹理。此外，我们首次修复低分辨率结果，并将其作为全局残差，以减轻补丁过程造成的块状伪影。此外，我们构建了一个包含修复后的真实降质影片和真实合成数据的影片修复数据集。全面的实验结果明确证明了我们模型在缺陷修复能力上优于现有的开源方法。代码和数据集将会发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of open-source film restoration methods, which currently lag behind commercial alternatives due to reliance on low-quality synthetic data and noisy optical flows. The authors propose HaineiFRDM, a film restoration framework that utilizes a diffusion model to enhance the restoration of indistinguishable film defects. Key experimental findings indicate that their approach, which includes a patch-wise training strategy and innovative modules for texture reconstruction and artifact mitigation, significantly outperforms existing open-source methods in restoring high-resolution films, demonstrating its effectiveness in defect restoration. The authors also created a new dataset of restored films to support their research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高电影修复方法的性能，现有的开源方法受到低质量合成数据和噪声光流的限制。作者提出了HaineiFRDM，一个利用扩散模型的电影修复框架，以增强对难以区分的电影缺陷的修复，采用适合在单个GPU上处理高分辨率电影的块状训练和测试策略。实验结果表明，HaineiFRDM在缺陷修复方面显著优于现有的开源方法，通过新构建的真实降级电影和真实合成数据的数据集展示了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach</div>
<div class="meta-line">Authors: Yuchen Jiao, Na Li, Changxiao Cai, Gen Li</div>
<div class="meta-line">First: 2025-12-31T15:35:53+00:00 · Latest: 2025-12-31T15:35:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24927v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24927v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.
  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一阶扩散采样器真的更慢吗？一种快速前值方法</div>
<div class="mono" style="margin-top:8px">高阶常微分方程求解器已成为加速扩散概率模型（DPM）采样的标准工具，这促使人们普遍认为一阶方法本质上更慢，而提高离散化阶数是加速生成的主要途径。本文挑战了这一观点，从一个互补的角度重新审视加速：除了求解器阶数外，DPM评估在反向时间动态中的位置可以显著影响低神经函数评估（NFE）范围内的采样准确性。我们提出了一种新颖的无训练一阶采样器，其主要离散化误差与DDIM的误差符号相反。在算法上，该方法通过廉价的一步前瞻预测器近似前值评估。我们提供了理论保证，表明所得到的采样器在保持一阶收敛的同时，能够证明性地近似理想的前值轨迹。在标准图像生成基准（CIFAR-10、ImageNet、FFHQ和LSUN）上，所提采样器在相同的NFE预算下持续提高样本质量，并且可以与最先进的高阶采样器竞争，有时甚至超越它们。总体而言，结果表明DPM评估的位置为加速扩散采样提供了一个额外且在很大程度上独立的设计角度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the common perception that first-order diffusion probabilistic model (DPM) samplers are inherently slower than higher-order methods, by exploring the impact of DPM evaluation placement on sampling accuracy. The authors introduce a novel training-free, first-order sampler that utilizes a one-step lookahead predictor to approximate forward-value evaluations, resulting in a leading discretization error that contrasts with that of DDIM. Experimental results demonstrate that this new sampler consistently enhances sample quality across various image generation benchmarks while operating within the same neural function evaluation budget, showing competitive performance against higher-order samplers and suggesting that evaluation placement is a crucial factor in accelerating diffusion sampling.</div>
<div class="mono" style="margin-top:8px">本文探讨了第一阶扩散概率模型（DPM）采样器固有速度较慢的普遍看法，通过研究DPM评估位置对采样准确性的影响。作者提出了一种新颖的无训练第一阶采样器，该采样器利用一步前瞻预测器来近似前值评估，从而导致与DDIM相反的主导离散化误差。实验结果表明，该新采样器在CIFAR-10和ImageNet等多个图像生成基准上，在相同的神经函数评估预算内持续提高样本质量，并且能够与最先进的高阶采样器竞争甚至超越，表明评估位置是加速扩散采样的重要因素。</div>
</details>
</div>
<div class="card">
<div class="title">OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation</div>
<div class="meta-line">Authors: Meng Lan, Lefei Zhang, Xiaomeng Li</div>
<div class="meta-line">First: 2025-12-31T13:41:16+00:00 · Latest: 2025-12-31T13:41:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24861v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&#x27;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OFL-SAM2：使用在线少样本学习器的无提示SAM2以实现高效医学图像分割</div>
<div class="mono" style="margin-top:8px">Segment Anything Model 2 (SAM2) 在视频数据中展示了显著的可提示视觉分割能力，显示出扩展到涉及3D体积和时间相关2D图像序列的医学图像分割（MIS）任务的潜力。然而，将SAM2适应于MIS面临多个挑战，包括需要大量标注医学数据进行微调和高质量的手动提示，这两者都劳动密集且需要医学专家的干预。为了解决这些挑战，我们引入了OFL-SAM2，一个无提示的SAM2框架，用于标签高效的MIS。我们的核心思想是利用有限的标注样本训练一个轻量级映射网络，捕捉医学知识并将通用图像特征转化为目标特征，从而为每帧提供额外的区分性目标表示，消除手动提示的需求。关键是，映射网络支持在推理过程中在线参数更新，增强模型在测试序列中的泛化能力。从技术上讲，我们引入了两个关键组件：（1）一个在线少样本学习器，训练映射网络使用有限数据生成目标特征；（2）一个自适应融合模块，动态整合目标特征与由冻结的SAM2生成的记忆注意特征，从而实现准确且稳健的目标表示。在三个不同的MIS数据集上的广泛实验表明，OFL-SAM2在有限训练数据下实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of the Segment Anything Model 2 (SAM2) for medical image segmentation (MIS), addressing the challenges posed by the need for extensive annotated data and manual prompts. The authors propose OFL-SAM2, a prompt-free framework that utilizes a lightweight mapping network trained on limited annotated samples to convert generic image features into target features, thus eliminating the reliance on manual prompts. Experimental results across three diverse MIS datasets show that OFL-SAM2 achieves state-of-the-art performance while requiring significantly less training data.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升Segment Anything Model 2 (SAM2)在医学图像分割（MIS）任务中的应用，解决了对大量标注数据和手动提示的需求带来的挑战。作者提出了OFL-SAM2，这是一种无提示的框架，利用在有限标注样本上训练的轻量级映射网络，将通用图像特征转换为目标特征，从而消除了对手动提示的需求。在三个不同的MIS数据集上的实验结果表明，OFL-SAM2在需要显著较少训练数据的情况下达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI</div>
<div class="meta-line">Authors: Srija Mukhopadhyay, Sathwik Reddy, Shruthi Muthukumar, Jisun An, Ponnurangam Kumaraguru</div>
<div class="meta-line">First: 2025-12-31T13:16:45+00:00 · Latest: 2025-12-31T13:16:45+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24848v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24848v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Personalized AI agents rely on access to a user&#x27;s digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PrivacyBench：评估个性化人工智能隐私的对话基准</div>
<div class="mono" style="margin-top:8px">个性化人工智能代理依赖于对用户数字足迹的访问，这通常包括来自私人电子邮件、聊天和购买历史的敏感数据。然而，这种访问带来了根本的社会和隐私风险：缺乏社会背景意识的系统可能无意中暴露用户秘密，威胁数字福祉。我们介绍了PrivacyBench，这是一个具有社会基础的数据集的基准，包含嵌入的秘密和多轮对话评估，以测量秘密的保留。对检索增强生成（RAG）助手的测试表明，它们在多达26.56%的交互中泄露秘密。一个关注隐私的提示将泄露降低到5.12%，但这一措施仅提供部分缓解。检索机制继续不加区分地访问敏感数据，这将隐私保护的全部负担转移到生成器上。这造成了单点故障，使当前架构在大规模部署时不安全。我们的发现强调了迫切需要结构性、隐私设计的保障，以确保每个人都能享有一个伦理和包容的网络。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the privacy risks associated with personalized AI agents that utilize sensitive user data from various digital footprints. The authors introduce PrivacyBench, a benchmark designed to evaluate privacy through socially grounded datasets and a multi-turn conversational framework aimed at measuring the preservation of secrets. Experimental results show that Retrieval-Augmented Generation (RAG) assistants leak sensitive information in up to 26.56% of interactions, although using a privacy-aware prompt reduces this leakage to 5.12; however, the reliance on the retrieval mechanism still poses significant privacy risks, highlighting the need for improved privacy safeguards in AI systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于个性化人工智能代理使用来自用户数字足迹的敏感数据所带来的隐私风险。作者提出了PrivacyBench，这是一个旨在通过社会基础数据集和专注于秘密保护的多轮对话评估来评估隐私的基准。实验结果表明，增强检索生成（RAG）助手在高达26.56%的交互中泄露用户秘密，尽管使用隐私意识提示可以将泄露降低到5.12，但这种方法仅部分缓解了问题，因为检索机制仍然不加区分地访问敏感数据，突显了在人工智能架构中改进隐私保护措施的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Reconstructing Hand-Held Objects in 3D from Images and Videos</div>
<div class="meta-line">Authors: Jane Wu, Georgios Pavlakos, Georgia Gkioxari, Jitendra Malik</div>
<div class="meta-line">First: 2024-04-09T17:55:41+00:00 · Latest: 2025-12-31T08:23:22+00:00</div>
<div class="meta-line">Comments: 3DV 2026, Project page: https://janehwu.github.io/mcc-ho</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.06507v4">Abs</a> · <a href="https://arxiv.org/pdf/2404.06507v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://janehwu.github.io/mcc-ho">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from Internet videos. Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels. At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects. With these insights in mind, we present a scalable paradigm for hand-held object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets. Given a monocular RGB video, we aim to reconstruct hand-held object geometry in 3D, over time. In order to obtain the best performing single frame model, we first present MCC-Hand-Object (MCC-HO), which jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative model using GPT-4(V) to retrieve a 3D object model that matches the object in the image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR). RAR provides unified object geometry across all frames, and the result is rigidly aligned with both the input images and 3D MCC-HO observations in a temporally consistent manner. Experiments demonstrate that our approach achieves state-of-the-art performance on lab and Internet image/video datasets. We make our code and models available on the project website: https://janehwu.github.io/mcc-ho</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从图像和视频中重建手持物体的3D模型</div>
<div class="mono" style="margin-top:8px">手部操作的物体（即操控物）从互联网视频中重建特别具有挑战性。手不仅遮挡了物体的大部分，而且物体通常只在少量图像像素中可见。同时，在这种情况下出现了两个强有力的锚点：（1）估计的3D手有助于消除物体的位置和尺度的歧义；（2）相对于所有可能的物体，操控物的集合相对较小。基于这些见解，我们提出了一种可扩展的手持物体重建范式，建立在大型语言/视觉模型和3D物体数据集的最新突破之上。给定单目RGB视频，我们旨在随时间重建手持物体的3D几何形状。为了获得最佳性能的单帧模型，我们首先提出了MCC-Hand-Object（MCC-HO），它在给定单个RGB图像和推断的3D手作为输入的情况下，联合重建手和物体的几何形状。随后，我们使用GPT-4(V)提示文本到3D生成模型，以检索与图像中的物体匹配的3D物体模型；我们称这种对齐为检索增强重建（RAR）。RAR在所有帧中提供统一的物体几何形状，结果与输入图像和3D MCC-HO观察结果在时间上保持一致。实验表明，我们的方法在实验室和互联网图像/视频数据集上达到了最先进的性能。我们在项目网站上提供我们的代码和模型：https://janehwu.github.io/mcc-ho</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of reconstructing hand-held objects from Internet videos, where occlusion by the hand and limited visibility of the object complicate the process. The authors propose a scalable paradigm that leverages recent advancements in large language/vision models and 3D object datasets, focusing on reconstructing the geometry of hand-held objects in 3D from monocular RGB videos. Their method includes the development of MCC-Hand-Object (MCC-HO) for single frame reconstruction and a Retrieval-Augmented Reconstruction (RAR) technique that aligns 3D object models with the input images, achieving state-of-the-art performance on both lab and Internet datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决从互联网视频中重建手持物体的挑战，因为手的遮挡和物体的可见性有限使得这一过程复杂化。作者提出了一种可扩展的重建范式，利用大型语言/视觉模型和3D物体数据集的最新进展。他们的方法首先使用名为MCC-Hand-Object的模型从单个RGB图像中联合重建手和物体的几何形状，然后采用文本到3D生成模型对齐和检索3D物体模型，称为增强检索重建。实验结果表明，该方法在实验室和互联网数据集上达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</div>
<div class="meta-line">Authors: Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao</div>
<div class="meta-line">First: 2025-12-31T08:10:03+00:00 · Latest: 2025-12-31T08:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24702v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24702v1">PDF</a> · <a href="https://github.com/AHideoKuzeA/Evol-SAM3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &quot;generate-then-segment&quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &quot;Generate-Evaluate-Evolve&quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化而非训练：通过进化提示实现零-shot推理分割</div>
<div class="mono" style="margin-top:8px">推理分割要求模型解释复杂的、依赖上下文的语言查询，以实现像素级定位。目前主流方法严重依赖监督微调（SFT）或强化学习（RL）。然而，SFT存在灾难性遗忘和领域依赖的问题，而RL常常受到训练不稳定和对预定义奖励函数的严格依赖的限制。尽管最近的无训练方法规避了这些训练负担，但它们在根本上受到静态推理范式的限制。这些方法通常依赖于单次“生成-然后-分割”链，导致推理深度不足，缺乏自我纠正语言幻觉或空间误解的能力。在本文中，我们挑战这些限制，提出EVOL-SAM3，一个新颖的零-shot框架，将推理分割重新构建为推理时的进化搜索过程。EVOL-SAM3不依赖于固定提示，而是维护一组提示假设，并通过“生成-评估-进化”循环迭代优化它们。我们引入一个视觉竞技场，通过无参考的成对比赛评估提示适应性，并使用语义突变算子注入多样性和纠正语义错误。此外，一个异构竞技场模块将几何先验与语义推理结合，以确保稳健的最终选择。大量实验表明，EVOL-SAM3不仅显著超越静态基线，而且在零-shot设置下在具有挑战性的ReasonSeg基准上显著超过完全监督的最先进方法。代码可在https://github.com/AHideoKuzeA/Evol-SAM3获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current reasoning segmentation methods, which rely on supervised fine-tuning and reinforcement learning, both of which face issues like catastrophic forgetting and training instability. The authors propose a novel zero-shot framework called EVOL-SAM3 that reformulates reasoning segmentation as an inference-time evolutionary search process, utilizing a population of prompt hypotheses refined through a &#x27;Generate-Evaluate-Evolve&#x27; loop. Experimental results show that EVOL-SAM3 significantly outperforms static baselines and surpasses fully supervised state-of-the-art methods on the ReasonSeg benchmark in a zero-shot setting.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前推理分割方法的局限性，这些方法通常依赖于监督微调或强化学习，而这两者都有显著缺陷。作者提出了一种名为EVOL-SAM3的新型零-shot框架，将推理分割重新构建为推理过程中的进化搜索，利用一组提示假设，通过“生成-评估-进化”循环进行迭代优化。实验结果表明，EVOL-SAM3在零-shot设置下显著优于静态基线，并超越了完全监督的最先进方法，在ReasonSeg基准测试中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models</div>
<div class="meta-line">Authors: Chang-Han Yeh, Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Ting-Hsuan Chen, Yu-Lun Liu</div>
<div class="meta-line">First: 2024-07-01T17:59:12+00:00 · Latest: 2025-12-31T08:08:03+00:00</div>
<div class="meta-line">Comments: Project page: https://jimmycv07.github.io/DiffIR2VR_web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.01519v5">Abs</a> · <a href="https://arxiv.org/pdf/2407.01519v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jimmycv07.github.io/DiffIR2VR_web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DiffIR2VR-Zero, a zero-shot framework that enables any pre-trained image restoration diffusion model to perform high-quality video restoration without additional training. While image diffusion models have shown remarkable restoration capabilities, their direct application to video leads to temporal inconsistencies, and existing video restoration methods require extensive retraining for different degradation types. Our approach addresses these challenges through two key innovations: a hierarchical latent warping strategy that maintains consistency across both keyframes and local frames, and a hybrid token merging mechanism that adaptively combines optical flow and feature matching. Through extensive experiments, we demonstrate that our method not only maintains the high-quality restoration of base diffusion models but also achieves superior temporal consistency across diverse datasets and degradation conditions, including challenging scenarios like 8$\times$ super-resolution and severe noise. Importantly, our framework works with any image restoration diffusion model, providing a versatile solution for video enhancement without task-specific training or modifications. Project page: https://jimmycv07.github.io/DiffIR2VR_web/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffIR2VR-Zero：基于扩散图像恢复模型的零样本视频恢复</div>
<div class="mono" style="margin-top:8px">我们提出了DiffIR2VR-Zero，这是一个零样本框架，使任何预训练的图像恢复扩散模型能够在无需额外训练的情况下执行高质量的视频恢复。尽管图像扩散模型显示出显著的恢复能力，但其直接应用于视频会导致时间不一致，而现有的视频恢复方法需要针对不同的退化类型进行广泛的再训练。我们的方法通过两个关键创新来解决这些挑战：一种层次化的潜在变形策略，保持关键帧和局部帧之间的一致性，以及一种混合令牌合并机制，自适应地结合光流和特征匹配。通过广泛的实验，我们证明了我们的方法不仅保持了基础扩散模型的高质量恢复，还在不同数据集和退化条件下实现了优越的时间一致性，包括8$\times$超分辨率和严重噪声等挑战场景。重要的是，我们的框架适用于任何图像恢复扩散模型，提供了一种无需特定任务训练或修改的视频增强通用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enable high-quality video restoration using pre-trained image restoration diffusion models without the need for additional training, addressing the limitations of existing methods that require extensive retraining for different degradation types. The authors propose DiffIR2VR-Zero, a zero-shot framework that incorporates a hierarchical latent warping strategy to ensure temporal consistency across keyframes and local frames, along with a hybrid token merging mechanism that combines optical flow and feature matching. Experimental results indicate that this approach not only preserves the high-quality restoration capabilities of the original diffusion models but also significantly improves temporal consistency across various datasets and degradation scenarios, including challenging conditions such as 8× super-resolution and severe noise.</div>
<div class="mono" style="margin-top:8px">本研究的动机是利用预训练的图像恢复扩散模型实现高质量的视频恢复，而无需额外训练，从而解决视频中的时间一致性问题。作者提出了DiffIR2VR-Zero，这是一个零-shot框架，结合了分层潜在变形策略和混合标记合并机制，以确保关键帧和局部帧之间的一致性，同时自适应地结合光流和特征匹配。实验结果表明，该方法不仅保持了原始扩散模型的高质量恢复能力，还在各种数据集和降级条件下实现了更好的时间一致性，包括8倍超分辨率和严重噪声等复杂场景。</div>
</details>
</div>
<div class="card">
<div class="title">ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</div>
<div class="meta-line">Authors: Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</div>
<div class="meta-line">Venue: AAAI 2026 poster</div>
<div class="meta-line">First: 2025-12-19T07:27:19+00:00 · Latest: 2025-12-31T06:37:00+00:00</div>
<div class="meta-line">Comments: Accepted for poster presentation at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17298v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model&#x27;s temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProCache：具有选择性计算的约束感知特征缓存用于扩散变换器加速</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）在生成建模中已达到最先进的性能，但其高计算成本阻碍了实时部署。虽然特征缓存通过利用时间冗余提供了一个有前景的无训练加速解决方案，但现有方法存在两个主要限制：（1）均匀缓存间隔未能与DiT的非均匀时间动态对齐；（2）使用过大缓存间隔的简单特征重用可能导致严重的误差累积。在本研究中，我们分析了DiT特征在去噪过程中的演变，并揭示特征变化和误差传播在时间和深度上高度变化。基于此，我们提出了ProCache，一个无训练的动态特征缓存框架，通过两个核心组件解决这些问题：（i）一个约束感知缓存模式搜索模块，通过离线约束采样生成非均匀激活调度，针对模型的时间特性；（ii）一个选择性计算模块，在深层块和高重要性标记中选择性计算缓存段，以最小的开销减轻误差累积。在PixArt-alpha和DiT上的大量实验表明，ProCache实现了高达1.96倍和2.90倍的加速，且质量下降微乎其微，显著优于先前的基于缓存的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computational cost of Diffusion Transformers (DiTs), which limits their real-time application despite their state-of-the-art performance in generative modeling. The authors propose ProCache, a training-free dynamic feature caching framework that incorporates a constraint-aware caching pattern search module and a selective computation module to optimize feature caching. Experimental results on PixArt-alpha and DiT show that ProCache can achieve acceleration rates of up to 1.96x and 2.90x with minimal quality degradation, outperforming existing caching methods.</div>
<div class="mono" style="margin-top:8px">本研究针对扩散变换器（DiTs）在生成建模中表现优异但计算成本高的问题，限制了其实时应用。作者提出了ProCache，这是一种无训练的动态特征缓存框架，结合了约束感知缓存模式搜索模块和选择性计算模块，以优化特征缓存和计算。对PixArt-alpha和DiT的实验结果表明，ProCache在质量损失极小的情况下实现了最高1.96倍和2.90倍的加速，显著优于现有的缓存方法。</div>
</details>
</div>
<div class="card">
<div class="title">Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</div>
<div class="meta-line">Authors: Yoonho Lee, Joseph Boen, Chelsea Finn</div>
<div class="meta-line">First: 2025-11-11T07:14:13+00:00 · Latest: 2025-12-31T05:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07919v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.07919v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反馈下降：通过成对比较进行开放式文本优化</div>
<div class="mono" style="margin-top:8px">我们介绍了\textit{反馈下降}，这是一个通过结构化文本反馈优化文本工件——提示、代码和分子——的框架，而不是仅依赖标量奖励。通过保留详细的批评而不是将其压缩为二元偏好，反馈下降扩大了偏好学习中的信息瓶颈，使得在文本空间而非权重空间中进行定向优化成为可能。我们展示了上下文学习可以将结构化反馈转化为类似梯度的方向信息，从而实现有针对性的编辑。与之前将判断压缩为单一比特的方法不同，我们的评估者将每个比较与文本反馈配对，这作为高带宽监督。迭代循环完全在推理时进行，而不修改任何模型权重，并且与任务无关。我们在三个不同领域评估了反馈下降，发现其优于最先进的提示优化（GEPA）、强化学习方法（GRPO、REINVENT），甚至专门的基于图的分子优化器。在DOCKSTRING分子发现基准中，反馈下降识别出新型药物样分子，超越了一个包含超过260,000个化合物的数据库的99.9百分位数，涵盖六个蛋白质靶点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the study is to enhance text optimization by utilizing structured textual feedback instead of relying solely on scalar rewards. The authors introduce a framework called Feedback Descent, which captures detailed critiques through pairwise comparisons and transforms this feedback into gradient-like information for targeted edits, all while maintaining model weights unchanged during inference. Experimental results demonstrate that Feedback Descent significantly outperforms existing methods, including state-of-the-art prompt optimization and reinforcement learning techniques, particularly in the DOCKSTRING molecule discovery benchmark, where it identifies novel drug-like molecules that exceed the 99.9th percentile of a large compound database.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过利用结构化的文本反馈来改善文本优化，而不是依赖传统的标量奖励。作者提出了一种名为反馈下降的框架，该框架处理详细的批评意见，以实现文本空间的定向优化。实验结果表明，反馈下降在多个领域的表现优于现有方法，包括最先进的提示优化和强化学习技术，特别是在DOCKSTRING分子发现基准中，它识别出超过99.9百分位的大型化合物数据库中的新型药物分子，涵盖多个蛋白质靶点。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
