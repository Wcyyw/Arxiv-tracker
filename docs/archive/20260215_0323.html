<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-15 03:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260215_0323</div>
    <div class="row"><div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&#x27;&#x27; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &quot;boot-time compute&quot; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缩放验证比缩放策略学习更有效于视觉-语言-动作对齐</div>
<div class="mono" style="margin-top:8px">通用机器人的长期愿景依赖于它们理解和执行自然语言指令的能力。视觉-语言-动作（VLA）模型在实现这一目标方面取得了显著进展，但它们生成的动作仍可能与给定指令不一致。本文探讨了测试时验证作为缩小“意图-动作差距”的手段。我们首先描述了具身指令跟随的测试时缩放法则，并证明联合缩放重述指令和生成动作的数量大大增加了测试时样本多样性，通常比独立缩放每个维度更有效地恢复正确动作。为了利用这些缩放法则，我们提出了CoVer，一个用于视觉-语言-动作对齐的对比验证器，并展示了我们的架构在额外计算资源和数据下的良好扩展性。然后，我们引入了“启动时计算”和一个层次化的VLA验证推理管道。在部署时，我们的框架从视觉-语言模型（VLM）预计算一组多样化的重述指令，为每个指令重复生成动作候选，然后使用验证器选择最佳的高层提示和低层动作片段。与在相同数据上进行的策略预训练相比，我们的验证方法在SIMPLER基准上在分布内获得22%的增益，在分布外获得13%的增益，并在真实世界实验中进一步提高了45%。在PolaRiS基准上，CoVer在任务进展上获得14%的增益，在成功率上获得9%的增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the alignment of actions generated by Vision-Language-Action (VLA) models with natural language instructions, addressing the persistent issue of misalignment. The authors propose a method called CoVer, a contrastive verifier that utilizes test-time verification and scaling laws to enhance sample diversity and action recovery efficiency. Experimental results demonstrate that this verification approach outperforms traditional scaling policy pre-training, achieving 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, along with a 45% improvement in real-world scenarios, and a 14% increase in task progress and 9% in success rate on the PolaRiS benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强视觉-语言-动作（VLA）模型生成的动作与自然语言指令之间的对齐，解决持续存在的错位问题。作者提出了一种名为CoVer的对比验证器，利用测试时验证来缩小意图与动作之间的差距，通过同时扩展重述指令和生成动作的数量。实验结果表明，该方法优于传统的缩放策略预训练，在SIMPLER基准上实现了22%的分布内增益和13%的分布外增益，在现实场景中提高了45%，并在PolaRiS基准上实现了任务进展14%和成功率9%的提升。</div>
</details>
</div>
<div class="card">
<div class="title">MonarchRT: Efficient Attention for Real-Time Video Generation</div>
<div class="meta-line">Authors: Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen</div>
<div class="meta-line">First: 2026-02-12T18:56:53+00:00 · Latest: 2026-02-12T18:56:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12271v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12271v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MonarchRT：实时视频生成的高效注意力机制</div>
<div class="mono" style="margin-top:8px">使用扩散变换器进行实时视频生成受到3D自注意力的二次成本限制，尤其是在步数少且自回归的实时环境中，错误会随着时间累积，每个去噪步骤必须携带更多信息。在这种情况下，我们发现先前的稀疏注意力近似失效，尽管在双向和多步扩散中表现良好。具体而言，我们观察到视频注意力并不可靠地稀疏，而是结合了由时空位置驱动的显著周期性结构、动态稀疏语义对应和密集混合，超出了即使是oracle top-k注意力的表示能力。基于这一见解，我们提出了Monarch-RT，一种用于视频扩散模型的结构化注意力参数化，通过Monarch矩阵对注意力进行因式分解。通过适当对齐的块结构和我们扩展的平铺Monarch参数化，我们在保持计算效率的同时实现了高表达能力。我们进一步通过微调和自定义Triton内核克服了参数化的开销。我们首先验证了Monarch-RT在仅为双向模型设计的现有稀疏基线上的高效性。我们进一步观察到，Monarch-RT在应用于最先进的模型Self-Forcing时，达到高达95%的注意力稀疏性且没有质量损失，使Monarch-RT成为实时视频生成中高能力稀疏注意力参数化的开创性工作。我们的优化实现超越了Nvidia RTX 5090、H100和B200 GPU上的FlashAttention-2、FlashAttention-3和FlashAttention-4内核，提供了1.4-11.8倍的内核加速。这使我们首次能够在单个RTX 5090上以16 FPS实现真正的实时视频生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of real-time video generation using Diffusion Transformers, particularly the inefficiencies caused by the quadratic cost of 3D self-attention. The authors propose Monarch-RT, a structured attention parameterization that utilizes Monarch matrices to enhance computational efficiency while maintaining high expressivity in video diffusion models. Experimental results demonstrate that Monarch-RT achieves up to 95% attention sparsity without compromising quality, outperforming existing sparse attention baselines and achieving significant speedups over previous kernels, enabling true real-time video generation at 16 FPS on a single RTX 5090 GPU.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用扩散变换器进行实时视频生成的局限性，特别是在自回归设置中，由于3D自注意力的平方成本导致的低效。作者提出了Monarch-RT，一种利用Monarch矩阵的结构化注意力参数化方法，以提高计算效率，同时在视频扩散模型中保持高表达能力。实验结果表明，Monarch-RT在不影响质量的情况下实现了高达95%的注意力稀疏性，超越了现有的稀疏注意力方法，并在多种Nvidia GPU上实现了显著的速度提升，使得在单个RTX 5090上以16 FPS实现实时视频生成成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Autonomous Mathematics Research</div>
<div class="meta-line">Authors: Tony Feng, Trieu H. Trinh, Garrett Bingham, Dawsen Hwang, Yuri Chervonyi, Junehyuk Jung, Joonkyung Lee, Carlo Pagano, Sang-hyun Kim, Federico Pasqualotto, Sergei Gukov, Jonathan N. Lee, Junsu Kim, Kaiying Hou, Golnaz Ghiasi, Yi Tay, YaGuang Li, Chenkai Kuang, Yuan Liu, Hanzhao Lin, Evan Zheran Liu, Nigamaa Nayakanti, Xiaomeng Yang, Heng-Tze Cheng, Demis Hassabis, Koray Kavukcuoglu, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-02-10T18:50:15+00:00 · Latest: 2026-02-12T18:27:29+00:00</div>
<div class="meta-line">Comments: 35 pages. Accompanied blog post https://deepmind.google/blog/accelerating-mathematical-and-scientific-discovery-with-gemini-deep-think/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10177v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10177v2">PDF</a> · <a href="https://github.com/google-deepmind/superhuman/tree/main/aletheia">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom&#x27;s Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest quantifying standard levels of autonomy and novelty of AI-assisted results, as well as propose a novel concept of human-AI interaction cards for transparency. We conclude with reflections on human-AI collaboration in mathematics and share all prompts as well as model outputs at https://github.com/google-deepmind/superhuman/tree/main/aletheia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自主数学研究</div>
<div class="mono" style="margin-top:8px">最近基础模型的进展产生了能够在国际数学奥林匹克中达到金牌标准的推理系统。然而，从竞赛级问题解决转向专业研究需要浏览大量文献并构建长期证明。在这项工作中，我们介绍了Aletheia，一个数学研究代理，它以自然语言迭代生成、验证和修订解决方案。具体而言，Aletheia由一个高级版本的Gemini Deep Think驱动，适用于具有挑战性的推理问题，采用了一种超越奥林匹克级问题的新颖推理时间扩展法则，并通过密集的工具使用来应对数学研究的复杂性。我们展示了Aletheia从奥林匹克问题到博士级练习的能力，特别是通过几个在AI辅助数学研究中的独特里程碑：(a) 一篇由AI生成的研究论文（Feng26），在计算算术几何中称为特征权重的某些结构常数时没有任何人类干预；(b) 一篇研究论文（LeeSeo26），展示了人类与AI在证明称为独立集的相互作用粒子系统的界限方面的合作；(c) 对Bloom的Erdos猜想数据库中700个开放问题的广泛半自主评估（Feng等，2026a），包括对四个开放问题的自主解决方案。为了帮助公众更好地理解与AI和数学相关的发展，我们建议量化AI辅助结果的标准自主性和新颖性水平，并提出一种新的透明度人类-AI互动卡片概念。我们最后反思了人类与AI在数学中的合作，并在https://github.com/google-deepmind/superhuman/tree/main/aletheia分享所有提示和模型输出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to bridge the gap between competition-level mathematical problem-solving and professional research, which involves extensive literature navigation and long-horizon proofs. The authors introduce Aletheia, a math research agent that utilizes an advanced version of Gemini Deep Think, a novel inference-time scaling law, and intensive tool use to autonomously generate, verify, and revise mathematical solutions in natural language. Key findings include the generation of a research paper on eigenweights without human intervention, a collaborative research paper on independent sets, and an evaluation of 700 open problems, with autonomous solutions to four questions, highlighting the potential of AI in advancing mathematical research and the importance of transparency in human-AI interactions.</div>
<div class="mono" style="margin-top:8px">本研究的动机是将竞赛级数学问题解决转变为专业研究，这涉及到广泛文献的导航和复杂证明的构建。作者介绍了Aletheia，这是一种数学研究代理，利用先进的Gemini Deep Think版本以自然语言迭代生成、验证和修订解决方案。主要发现包括生成一篇关于特征权重的研究论文而无需人工干预、关于独立集的合作论文，以及对700个开放问题的评估，其中四个问题得到了自主解决，展示了AI辅助数学研究的重大进展，并提出了理解人机协作的新框架。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting</div>
<div class="meta-line">Authors: Chutian Ma, Grigorii Pomazkin, Giacinto Paolo Saggese, Paul Smith</div>
<div class="meta-line">First: 2026-01-15T21:26:57+00:00 · Latest: 2026-02-12T17:45:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10863v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally allows user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal auto-regressive integrated models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Regarding stability, the AC-optimized model generated out-of-sample forecasts with 91.1\% reduced vertical variance relative to the MLE-fitted model. In terms of accuracy, the AC-optimized model achieved considerable improvements for medium-to-long-horizon forecasts. While one-step-ahead forecasts exhibited a 7.5\% increase in MAPE, all subsequent horizons experienced an improved accuracy as measured by MAPE of up to 26\%. These results indicate that our metric successfully trains models to produce more stable and accurate multi-step forecasts in exchange for some degradation in one-step-ahead performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越准确性：一种稳定性意识的多时间跨度预测指标</div>
<div class="mono" style="margin-top:8px">传统的时间序列预测方法仅优化准确性。这个目标忽视了时间一致性，即随着预测起点的变化，模型预测同一未来事件的一致性。我们引入了预测准确性和一致性评分（简称预测AC评分），用于以考虑多时间跨度准确性和稳定性的方式衡量概率多时间跨度预测的质量。我们的评分还允许用户指定权重，以平衡准确性和一致性要求。作为一个应用示例，我们将该评分实现为一个可微分的目标函数，用于训练季节性自回归积分模型，并在M4小时基准数据集上进行评估。结果显示，相较于传统的最大似然估计，取得了显著的改进。在稳定性方面，AC优化模型生成的样本外预测相较于MLE拟合模型的垂直方差减少了91.1%。在准确性方面，AC优化模型在中长期预测中取得了显著改进。尽管一步预测的MAPE增加了7.5%，但所有后续时间跨度的准确性均有所提高，MAPE改善幅度高达26%。这些结果表明，我们的指标成功地训练模型以产生更稳定和准确的多步预测，尽管在一步预测性能上有所下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional time series forecasting methods that prioritize accuracy without considering temporal consistency. The authors propose a new metric, the forecast accuracy and coherence score (forecast AC score), which evaluates the quality of probabilistic multi-horizon forecasts by incorporating both accuracy and stability, allowing for user-defined weights. Experimental results using the M4 Hourly benchmark dataset show that the AC-optimized model significantly reduces vertical variance by 91.1% compared to the maximum likelihood estimation model and improves medium-to-long-horizon forecast accuracy, with MAPE reductions of up to 26%, despite a slight increase in one-step-ahead forecast error by 7.5%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统时间序列预测方法仅关注准确性而忽视时间一致性的重要性。作者提出了预测准确性和一致性评分（预测AC评分），这是一种新的度量标准，通过考虑准确性和稳定性来评估概率多步预测的质量，并允许用户定义权重。使用M4小时基准数据集的实验结果表明，AC优化模型相比最大似然估计模型显著减少了91.1%的垂直方差，并改善了中到长期预测的准确性，MAPE减少幅度高达26%，尽管一步预测的MAPE增加了7.5%。</div>
</details>
</div>
<div class="card">
<div class="title">SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation</div>
<div class="meta-line">Authors: Chengxi Zeng, Yuxuan Jiang, Ge Gao, Shuai Wang, Duolikun Danier, Bin Zhu, Stevan Rudinac, David Bull, Fan Zhang</div>
<div class="meta-line">First: 2026-02-12T17:01:49+00:00 · Latest: 2026-02-12T17:01:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12173v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12173v1">PDF</a> · <a href="https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM3-LiteText：SAM3文本编码器的解剖学研究以实现高效的视觉-语言分割</div>
<div class="mono" style="margin-top:8px">视觉-语言分割模型如SAM3实现了灵活的、基于提示的视觉定位，但继承了最初为开放式语言理解设计的大型通用文本编码器。在实践中，分割提示短小、结构化且语义受限，导致文本编码器容量的过度配置以及持续的计算和内存开销。本文对视觉-语言分割中的文本提示进行了大规模的解剖分析，涵盖了404,796个真实提示，跨多个基准进行分析。我们的分析揭示了严重的冗余：大多数上下文窗口未被充分利用，词汇使用高度稀疏，尽管存在高维表示，文本嵌入仍位于低维流形上。基于这些发现，我们提出了SAM3-LiteText，一个轻量级文本编码框架，用紧凑的MobileCLIP学生替代原始SAM3文本编码器，并通过知识蒸馏进行优化。在图像和视频分割基准上的广泛实验表明，SAM3-LiteText将文本编码器参数减少了多达88%，显著降低了静态内存占用，同时保持了与原始模型相当的分割性能。代码：https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the inefficiencies of existing vision-language segmentation models, specifically the SAM3 text encoder, which is over-engineered for the short and structured prompts typically used in segmentation tasks. The authors conducted a comprehensive anatomical analysis of 404,796 prompts across various benchmarks, revealing significant redundancy in text encoder usage, including underutilized context windows and sparse vocabulary. In response, they introduced SAM3-LiteText, a streamlined text encoding framework utilizing a compact MobileCLIP student optimized through knowledge distillation, achieving up to an 88% reduction in text encoder parameters while maintaining comparable segmentation performance to the original model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决像SAM3这样的视觉语言分割模型中使用的大型通用文本编码器的低效问题，这些编码器并未针对分割提示的短小和结构化特性进行优化。作者对404,796个真实提示进行了大规模的解剖分析，涵盖多个基准，揭示了文本编码器使用中的显著冗余，包括未充分利用的上下文窗口和稀疏的词汇使用。作为回应，他们开发了SAM3-LiteText，这是一种轻量级文本编码框架，采用经过知识蒸馏优化的紧凑型MobileCLIP学生，从而在保持与原始模型相当的分割性能的同时，实现了文本编码器参数减少高达88%的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像进行内省视觉思维的对话</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的文本推理，这往往导致细粒度视觉信息的丢失。最近提出的“通过图像思考”试图通过外部工具或代码操控图像来缓解这一限制；然而，生成的视觉状态往往与语言语义的基础不够牢固，影响了有效的跨模态对齐——特别是在需要跨越远距离区域或多幅图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了“通过图像对话”，这是一个将视觉操控重新框定为语言引导特征调制的新框架。在富有表现力的语言提示的指导下，模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的紧密耦合。我们在ViLaVT中实例化这一范式，这是一种新型的LVLM，配备了专门为这种交互式视觉推理设计的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT在复杂的多图像和基于视频的空间推理任务上取得了强大且一致的改进，尤其在这些任务上表现出显著的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the reasoning capabilities of large vision-language models (LVLMs), which often lose fine-grained visual information due to their reliance on text-only reasoning. The authors propose a new framework called &#x27;chatting with images&#x27;, which reframes visual manipulation as language-guided feature modulation, allowing for dynamic re-encoding of multiple image regions based on expressive language prompts. Experimental results show that ViLaVT, the LVLM developed under this framework, achieves significant improvements across eight benchmarks, particularly excelling in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强大型视觉语言模型（LVLM）的推理能力，这些模型通常由于依赖文本推理而失去细粒度的视觉信息。作者提出了一种新框架，称为“与图像对话”，将视觉操作重新定义为语言引导的特征调制，允许在表现力丰富的语言提示的指导下动态重新编码多个图像区域。实验结果表明，在这一框架下开发的LVLM ViLaVT在八个基准测试中取得了显著的改进，特别是在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</div>
<div class="meta-line">Authors: Xu Guo, Fulong Ye, Qichao Sun, Liyang Chen, Bingchuan Li, Pengze Zhang, Jiawei Liu, Songtao Zhao, Qian He, Xiangwang Hou</div>
<div class="meta-line">First: 2026-02-12T16:41:52+00:00 · Latest: 2026-02-12T16:41:52+00:00</div>
<div class="meta-line">Comments: Project: https://guoxu1233.github.io/DreamID-Omni/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12160v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://guoxu1233.github.io/DreamID-Omni/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamID-Omni：可控人本音视频生成的统一框架</div>
<div class="mono" style="margin-top:8px">最近基础模型的进展彻底改变了音视频联合生成。然而，现有方法通常将以人为中心的任务（包括基于参考的音视频生成（R2AV）、视频编辑（RV2AV）和音频驱动的视频动画（RA2V））视为孤立的目标。此外，在单一框架内实现对多个角色身份和声音音色的精确、解耦控制仍然是一个未解决的挑战。本文提出了DreamID-Omni，一个可控人本音视频生成的统一框架。具体而言，我们设计了一个对称条件扩散变换器，通过对称条件注入方案集成异构条件信号。为了解决多人物场景中普遍存在的身份-音色绑定失败和说话者混淆问题，我们引入了一种双层解耦策略：在信号层面使用同步RoPE以确保严格的注意力空间绑定，在语义层面使用结构化字幕以建立明确的属性-主体映射。此外，我们设计了一种多任务渐进训练方案，利用弱约束生成先验来规范强约束任务，防止过拟合并协调不同目标。大量实验表明，DreamID-Omni在视频、音频和音视频一致性方面实现了全面的最先进性能，甚至超越了领先的商业模型。我们将发布我们的代码，以弥合学术研究与商业级应用之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing methods in human-centric audio-video generation, which often treat tasks like reference-based audio-video generation and video editing as isolated objectives, leading to challenges in controlling character identities and voice timbres. The authors propose DreamID-Omni, a unified framework that employs a Symmetric Conditional Diffusion Transformer to integrate various conditioning signals and introduces a Dual-Level Disentanglement strategy to mitigate identity-timbre binding failures. Experimental results indicate that DreamID-Omni achieves state-of-the-art performance in video, audio, and audio-visual consistency, surpassing leading commercial models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有方法在以人为中心的音频视频生成中的局限性，这些方法通常将任务视为孤立的，并且在控制角色身份和声音音色方面存在困难。作者提出了DreamID-Omni，这是一个统一框架，采用对称条件扩散变换器来整合各种条件信号，并引入双层解耦策略以解决身份-音色绑定问题。实验结果表明，DreamID-Omni在视频、音频和音频-视觉一致性方面达到了最先进的性能，超越了领先的商业模型。</div>
</details>
</div>
<div class="card">
<div class="title">3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</div>
<div class="meta-line">First: 2026-02-12T16:41:26+00:00 · Latest: 2026-02-12T16:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aczheng-cai.github.io/3dgsnav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DGSNav：通过主动3D高斯点云增强视觉-语言模型在物体导航中的推理</div>
<div class="mono" style="margin-top:8px">物体导航是具身智能的核心能力，使代理能够在未知环境中定位目标物体。最近视觉-语言模型（VLM）的进展促进了零-shot物体导航（ZSON）。然而，现有方法通常依赖于将环境转换为语义地图或文本表示的场景抽象，导致高层决策受到低层感知准确性的限制。在本研究中，我们提出了3DGSNav，一种新颖的ZSON框架，将3D高斯点云（3DGS）嵌入作为VLM的持久记忆，以增强空间推理。通过主动感知，3DGSNav逐步构建环境的3DGS表示，实现前沿感知的第一人称视角的轨迹引导自由视点渲染。此外，我们设计了结构化视觉提示，并将其与思维链（CoT）提示相结合，以进一步改善VLM推理。在导航过程中，实时物体检测器过滤潜在目标，而VLM驱动的主动视点切换执行目标重新验证，确保高效可靠的识别。在多个基准和四足机器人上的真实世界实验中，广泛评估表明我们的方法在与最先进的方法相比时表现出强大且具有竞争力的性能。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve object navigation capabilities in embodied intelligence, particularly in unknown environments where existing vision-language models (VLMs) face limitations due to reliance on low-level perception. The authors introduce 3DGSNav, a novel zero-shot object navigation framework that utilizes 3D Gaussian Splatting (3DGS) as a persistent memory to enhance spatial reasoning in VLMs. Key experimental findings indicate that 3DGSNav, through active perception and structured visual prompts, significantly improves navigation performance, achieving robust results in various benchmarks and real-world tests on a quadruped robot, outperforming state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高具身智能中的物体导航能力，特别是在零样本物体导航（ZSON）场景中，传统方法因依赖低级感知而面临困难。作者提出了一种名为3DGSNav的新框架，利用3D高斯点云（3DGS）作为持久记忆，以增强视觉-语言模型（VLM）的空间推理能力。关键实验结果表明，3DGSNav通过主动感知和结构化视觉提示显著提高了导航性能，在多个基准测试和四足机器人实际测试中表现出色，相较于最先进的方法具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation</div>
<div class="meta-line">Authors: Ziteng Lu, Yushuang Wu, Chongjie Ye, Yuda Qiu, Jing Shao, Xiaoyang Guo, Jiaqing Zhou, Tianlei Hu, Kun Zhou, Xiaoguang Han</div>
<div class="meta-line">First: 2026-02-12T16:37:31+00:00 · Latest: 2026-02-12T16:37:31+00:00</div>
<div class="meta-line">Comments: Project page: https://anonymous.4open.science/w/TexSpot-page-2D91</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12157v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality 3D texture generation remains a fundamental challenge due to the view-inconsistency inherent in current mainstream multi-view diffusion pipelines. Existing representations either rely on UV maps, which suffer from distortion during unwrapping, or point-based methods, which tightly couple texture fidelity to geometric density that limits high-resolution texture generation. To address these limitations, we introduce TexSpot, a diffusion-based texture enhancement framework. At its core is Texlet, a novel 3D texture representation that merges the geometric expressiveness of point-based 3D textures with the compactness of UV-based representation. Each Texlet latent vector encodes a local texture patch via a 2D encoder and is further aggregated using a 3D encoder to incorporate global shape context. A cascaded 3D-to-2D decoder reconstructs high-quality texture patches, enabling the Texlet space learning. Leveraging this representation, we train a diffusion transformer conditioned on Texlets to refine and enhance textures produced by multi-view diffusion methods. Extensive experiments demonstrate that TexSpot significantly improves visual fidelity, geometric consistency, and robustness over existing state-of-the-art 3D texture generation and enhancement approaches. Project page: https://anonymous.4open.science/w/TexSpot-page-2D91.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TexSpot：基于空间均匀点潜在表示的3D纹理增强</div>
<div class="mono" style="margin-top:8px">高质量的3D纹理生成仍然是一个基本挑战，因为当前主流的多视图扩散管道固有的视图不一致性。现有表示要么依赖于UV贴图，这在展开过程中会出现失真，要么依赖于点基方法，这将纹理保真度与几何密度紧密耦合，限制了高分辨率纹理生成。为了解决这些限制，我们引入了TexSpot，一个基于扩散的纹理增强框架。其核心是Texlet，一种新颖的3D纹理表示，结合了点基3D纹理的几何表现力和基于UV表示的紧凑性。每个Texlet潜在向量通过2D编码器编码局部纹理块，并通过3D编码器进一步聚合以纳入全局形状上下文。级联的3D到2D解码器重建高质量纹理块，实现Texlet空间学习。利用这种表示，我们训练了一个基于Texlet的扩散变换器，以精炼和增强多视图扩散方法生成的纹理。大量实验表明，TexSpot在视觉保真度、几何一致性和鲁棒性方面显著优于现有的最先进的3D纹理生成和增强方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenges of high-quality 3D texture generation, particularly the view-inconsistency issues present in existing multi-view diffusion pipelines. The authors propose TexSpot, a diffusion-based texture enhancement framework that introduces Texlet, a new 3D texture representation combining the advantages of point-based and UV-based methods. The experimental results show that TexSpot significantly enhances visual fidelity, geometric consistency, and robustness compared to current state-of-the-art techniques in 3D texture generation and enhancement.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决高质量3D纹理生成中的挑战，特别是现有多视角扩散管道中的视图不一致性。作者提出了TexSpot，这是一种基于扩散的纹理增强框架，引入了Texlet，这是一种结合了基于点和基于UV方法优点的新型3D纹理表示。实验结果表明，TexSpot在视觉保真度、几何一致性和鲁棒性方面显著优于当前最先进的3D纹理生成和增强技术。</div>
</details>
</div>
<div class="card">
<div class="title">FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</div>
<div class="meta-line">Authors: Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie</div>
<div class="meta-line">First: 2026-02-12T16:36:33+00:00 · Latest: 2026-02-12T16:36:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12155v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12155v1">PDF</a> · <a href="https://github.com/HansPolo113/FAIL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAIL：用于图像生成的流匹配对抗模仿学习</div>
<div class="mono" style="margin-top:8px">流匹配模型的后训练——将输出分布与高质量目标对齐——在数学上等同于模仿学习。虽然监督微调有效地模仿专家演示，但无法纠正未见状态下的策略漂移。偏好优化方法解决了这个问题，但需要昂贵的偏好对或奖励建模。我们提出了流匹配对抗模仿学习（FAIL），通过对抗训练最小化策略与专家之间的差异，而无需显式奖励或成对比较。我们推导出两种算法：FAIL-PD利用可微ODE求解器获得低方差路径梯度，而FAIL-PG为离散或计算受限的环境提供了一种黑箱替代方案。仅使用来自Nano Banana pro的13,000个演示微调FLUX，FAIL在提示跟随和美学基准上实现了竞争性能。此外，该框架有效地推广到离散图像和视频生成，并作为一种强健的正则化器，减轻基于奖励的优化中的奖励黑客。代码和数据可在https://github.com/HansPolo113/FAIL获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing imitation learning methods in correcting policy drift in unseen states, particularly in the context of image generation. The authors propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes the divergence between the policy and expert outputs through adversarial training, eliminating the need for explicit rewards or preference pairs. Experimental results demonstrate that FAIL, when fine-tuning the FLUX model with only 13,000 demonstrations, achieves competitive performance on prompt following and aesthetic benchmarks, while also effectively generalizing to discrete image and video generation tasks and serving as a robust regularizer against reward hacking.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决传统监督微调和偏好优化方法的局限性，来提高图像生成中流匹配模型的性能。作者提出了流匹配对抗模仿学习（FAIL），该方法通过对抗训练最小化策略与专家输出之间的差异，而无需显式奖励或成对比较。实验结果表明，FAIL可以仅使用来自Nano Banana pro的13,000个示例对FLUX模型进行微调，在提示跟随和美学基准上取得了竞争性表现，同时在离散图像和视频生成中也表现出良好的泛化能力，并作为一种强健的正则化器来防止基于奖励的优化中的奖励黑客行为。</div>
</details>
</div>
<div class="card">
<div class="title">Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5</div>
<div class="meta-line">Authors: Roberto Balestri</div>
<div class="meta-line">First: 2026-02-12T16:21:03+00:00 · Latest: 2026-02-12T16:21:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong &quot;default white&quot; bias (&gt;96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>中性提示与非中性人群：量化Gemini Flash 2.5图像和GPT图像1.5中的性别和肤色偏见</div>
<div class="mono" style="margin-top:8px">本研究量化了两种广泛使用的商业图像生成器——Gemini Flash 2.5图像（NanoBanana）和GPT图像1.5中的性别和肤色偏见，以测试中性提示是否产生人口统计上中性的输出。我们使用四个语义中性提示生成了3200张逼真的图像。分析采用了结合混合色彩标准化、面部特征遮罩和使用Monk（MST）、PERLA和Fitzpatrick量表的感知均匀肤色量化的严格流程。中性提示产生了高度极化的默认设置。两个模型都表现出强烈的“默认白人”偏见（&gt;96%的输出）。然而，它们在性别上有明显分歧：Gemini偏向女性呈现的对象，而GPT偏向肤色较浅的男性呈现对象。本研究提供了一个大规模的、比较的最先进模型审计，使用了考虑光照的色度方法，区分了合成图像中的美学渲染与基础色素。研究表明，中性提示作为诊断探针而非中性指令。它为审计算法视觉文化提供了一个稳健的框架，并挑战了未标记语言导致包容性表现的社会语言学假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the presence of gender and skin-tone bias in two commercial image generators, Gemini Flash 2.5 Image and GPT Image 1.5, motivated by the assumption that neutral prompts would yield demographically neutral outputs. The researchers generated 3,200 photorealistic images using four semantically neutral prompts and employed a comprehensive analysis method that included hybrid color normalization and skin tone quantification. The findings revealed a significant &#x27;default white&#x27; bias in over 96% of the outputs, with Gemini favoring female-presenting subjects and GPT favoring male-presenting subjects with lighter skin tones, indicating that neutral prompts serve more as diagnostic tools than as neutral instructions in algorithmic visual culture.</div>
<div class="mono" style="margin-top:8px">本研究调查了两个商业图像生成器（Gemini Flash 2.5 Image 和 GPT Image 1.5）中的性别和肤色偏见，动机是基于中性提示会导致人口统计上中性输出的假设。研究人员使用四个语义中性提示生成了3200幅照片级真实感图像，并通过混合颜色标准化、面部特征遮罩和肤色量化方法分析结果。研究发现，超过96%的输出存在显著的“默认白人”偏见，Gemini偏向女性呈现的主体，而GPT则偏向肤色较浅的男性呈现主体，表明中性提示更多地作为诊断工具，而非真正中性的指令。</div>
</details>
</div>
<div class="card">
<div class="title">HLA: Hadamard Linear Attention</div>
<div class="meta-line">Authors: Hanno Ackermann, Hong Cai, Mohsen Ghafoorian, Amirhossein Habibian</div>
<div class="meta-line">First: 2026-02-12T16:16:47+00:00 · Latest: 2026-02-12T16:16:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.
  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HLA: 哈达玛线性注意力</div>
<div class="mono" style="margin-top:8px">注意力机制是变换器成功的重要原因。它依赖于计算标记之间的成对关系。为了减少标准二次注意力的高计算成本，提出了线性注意力作为一种高效的近似方法。它使用在计算成对相似性之前独立应用于输入的核函数。这允许一种高效的计算过程，但实际上相当于一个低阶有理函数来近似softmax。
我们提出了哈达玛线性注意力（HLA）。与之前的线性注意力工作不同，HLA中的非线性不是单独应用于查询和键，而是类似于标准softmax注意力，在计算成对相似性之后应用。将证明所提出的非线性相当于一个高阶有理函数来近似softmax。为所提出的方法推导出一种高效的计算方案，类似于标准线性注意力。与其他方法相比，应用所提出的算法不需要耗时的张量重塑。通过将其应用于一个大型扩散变换器模型进行视频生成，展示了该方法的有效性，这一应用涉及大量的标记。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to reduce the computational cost associated with standard quadratic attention in transformers, which relies on pairwise relations between tokens. The authors propose Hadamard Linear Attention (HLA), which applies nonlinearity after computing pairwise similarities, contrasting with previous methods that apply it separately to queries and keys. Experimental results demonstrate that HLA, which approximates softmax using a higher-degree rational function, is effective when applied to a large diffusion transformer model for video generation, achieving efficiency without the need for time-consuming tensor reshaping.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于降低与标准二次注意力机制相关的计算成本，该机制依赖于成对的令牌关系。作者提出了一种新方法，称为Hadamard线性注意力（HLA），该方法在计算成对相似性后应用非线性，这与之前的方法在查询和键上分别应用非线性形成对比。实验结果表明，HLA在大型扩散变换器模型中有效地近似softmax，使用更高阶的有理函数，同时保持计算效率，无需耗时的张量重塑。</div>
</details>
</div>
<div class="card">
<div class="title">PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback</div>
<div class="meta-line">Authors: Sixiang Chen, Jianyu Lai, Jialin Gao, Hengyu Shi, Zhongying Liu, Tian Ye, Junfeng Luo, Xiaoming Wei, Lei Zhu</div>
<div class="meta-line">First: 2026-02-12T16:16:38+00:00 · Latest: 2026-02-12T16:16:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12127v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PosterOmni：通过任务蒸馏和统一奖励反馈实现通用艺术海报创作</div>
<div class="mono" style="margin-top:8px">图像到海报生成是一项高需求任务，不仅需要局部调整，还需要高水平的设计理解。模型必须生成文本、布局、风格和视觉元素，同时保持语义的忠实性和美学的一致性。该过程跨越两个领域：局部编辑，其中基于ID的生成、重缩放、填充和扩展必须保持具体的视觉实体；以及全球创作，其中布局和风格驱动的任务依赖于对抽象设计概念的理解。这些交织的需求使得图像到海报成为一个多维过程，将保持实体的编辑与基于概念的创作结合在图像提示控制下。为了解决这些挑战，我们提出了PosterOmni，一个通用的艺术海报创作框架，释放基础编辑模型在多任务图像到海报生成中的潜力。PosterOmni通过高效的数据蒸馏奖励管道将局部编辑和全球创作这两个领域整合到一个系统中：(i) 构建覆盖六种任务类型的多场景图像到海报数据集，涵盖基于实体和基于概念的创作；(ii) 在局部和全球专家之间蒸馏知识以进行监督微调；(iii) 应用统一的PosterOmni奖励反馈，以共同对齐所有任务中的视觉实体保持和美学偏好。此外，我们建立了PosterOmni-Bench，一个用于评估局部编辑和全球创作的统一基准。大量实验表明，PosterOmni显著提高了参考遵循、全球构图质量和美学和谐，超越了所有开源基线，甚至超过了几个专有系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the complex task of image-to-poster generation, which requires both local adjustments and a high-level understanding of design principles. The authors propose a framework called PosterOmni that integrates local editing and global creation through a data-distillation-reward pipeline, which includes constructing diverse datasets, knowledge distillation for fine-tuning, and a unified reward feedback system. Experimental results demonstrate that PosterOmni significantly improves reference adherence, global composition quality, and aesthetic harmony, outperforming existing open-source and some proprietary systems.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图像到海报生成的复杂性，这一过程不仅需要局部调整，还需要对设计的高层次理解。作者提出了PosterOmni框架，通过数据蒸馏-奖励管道整合局部编辑和全局创作，该管道包括构建多样化的数据集、专家之间的知识蒸馏以及统一的奖励系统。实验结果表明，PosterOmni在遵循参考、全局构图质量和美学和谐性方面显著提升，超越了现有的开源系统和一些专有系统。</div>
</details>
</div>
<div class="card">
<div class="title">CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation</div>
<div class="meta-line">Authors: Robert Cronshaw, Konstantinos Vilouras, Junyu Yan, Yuning Du, Feng Chen, Steven McDonagh, Sotirios A. Tsaftaris</div>
<div class="meta-line">First: 2026-02-12T14:35:31+00:00 · Latest: 2026-02-12T14:35:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12004v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12004v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CSEval：评估文本到图像生成中的临床语义的框架</div>
<div class="mono" style="margin-top:8px">文本到图像生成在医疗领域的应用日益增多，目的包括数据增强和教育。评估这些生成图像的质量和临床可靠性至关重要。然而，现有方法主要评估图像的真实感或多样性，未能捕捉生成图像是否反映预期的临床语义，如解剖位置和病理。在本研究中，我们提出了临床语义评估器（CSEval），这是一个利用语言模型评估生成图像与其条件提示之间临床语义一致性的框架。我们的实验表明，CSEval能够识别其他指标忽视的语义不一致，并与专家判断相关。CSEval为现有评估方法提供了可扩展且具有临床意义的补充，支持生成模型在医疗保健中的安全采用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for effective evaluation of text-to-image generation in medical applications, particularly to ensure that generated images accurately reflect clinical semantics. The authors introduce the Clinical Semantics Evaluator (CSEval), a framework that utilizes language models to assess the alignment between generated images and their corresponding clinical prompts. Experimental results demonstrate that CSEval successfully identifies semantic inconsistencies that other evaluation methods miss and shows a strong correlation with expert assessments, thereby offering a scalable and clinically relevant tool for enhancing the reliability of generative models in healthcare.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效评估医学应用中的文本到图像生成，特别是确保生成的图像准确反映临床语义，而不仅仅是现实性或多样性。作者提出了临床语义评估器（CSEval），该框架利用语言模型评估生成图像与其相应临床提示之间的对齐情况。实验结果表明，CSEval成功识别了其他评估方法遗漏的语义不一致，并与专家评估有很强的相关性，从而为提高医疗保健中生成模型的可靠性提供了一种可扩展且具有临床意义的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration</div>
<div class="meta-line">Authors: Kfir Goldberg, Elad Richardson, Yael Vinker</div>
<div class="meta-line">First: 2026-02-09T13:00:16+00:00 · Latest: 2026-02-12T14:10:05+00:00</div>
<div class="meta-line">Comments: Project page available at https://inspirationseedspaper.github.io/InspirationSeeds/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08615v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08615v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://inspirationseedspaper.github.io/InspirationSeeds/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵感种子：学习非字面视觉组合以进行生成探索</div>
<div class="mono" style="margin-top:8px">尽管生成模型已成为图像合成的强大工具，但它们通常针对精心设计的文本提示进行优化，有限地支持通常在创意形成之前进行的开放式视觉探索。相比之下，设计师经常从松散连接的视觉参考中汲取灵感，寻求激发新想法的潜在联系。我们提出了灵感种子，这是一种生成框架，将图像生成从最终执行转向探索性构思。给定两幅输入图像，我们的模型生成多样且视觉连贯的组合，揭示输入之间的潜在关系，而无需依赖用户指定的文本提示。我们的方法是前馈的，训练于完全通过视觉手段获得的分解视觉方面的合成三元组：我们使用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向并隔离概念对。通过消除对语言的依赖并实现快速、直观的重组，我们的方法支持在创意工作的早期和模糊阶段进行视觉构思。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of generative models in supporting open-ended visual exploration, which is essential for idea formation among designers. The authors introduce Inspiration Seeds, a generative framework that enables exploratory ideation by producing diverse and visually coherent compositions from two input images, without the need for textual prompts. Key experimental findings demonstrate that the model effectively reveals latent relationships between inputs and facilitates intuitive recombination of visual elements, thereby enhancing the early stages of creative work.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过促进开放式视觉探索来增强图像合成的生成模型，这在设计中的创意形成中通常至关重要。作者提出了Inspiration Seeds，这是一种生成框架，可以从两个输入图像中生成多样且视觉一致的组合，而无需文本提示。关键实验结果表明，该模型有效地揭示了输入之间的潜在关系，并支持直观的视觉构思，因为它是通过使用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向，训练在合成的视觉方面三元组上。</div>
</details>
</div>
<div class="card">
<div class="title">ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation</div>
<div class="meta-line">Authors: Ruihang Xu, Dewei Zhou, Fan Ma, Yi Yang</div>
<div class="meta-line">First: 2025-10-13T04:21:19+00:00 · Latest: 2026-02-12T13:40:52+00:00</div>
<div class="meta-line">Comments: Project Page: https://nenhang.github.io/ContextGen/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11000v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.11000v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nenhang.github.io/ContextGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. To address the absence of a large-scale, high-quality dataset for this task, we introduce IMIG-100K, the first dataset to provide detailed layout and identity annotations specifically designed for Multi-Instance Generation. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods especially in layout control and identity fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextGen：用于身份一致的多实例生成的上下文布局锚定</div>
<div class="mono" style="margin-top:8px">多实例图像生成（MIG）仍然是现代扩散模型面临的重大挑战，因为在实现对物体布局的精确控制和保持多个不同主体的身份方面存在关键限制。为了解决这些限制，我们引入了ContextGen，这是一种新颖的扩散变换器框架，旨在通过布局和参考图像指导多实例生成。我们的方法整合了两个关键技术贡献：上下文布局锚定（CLA）机制，将复合布局图像纳入生成上下文，以稳健地锚定物体在其期望位置，以及身份一致性注意力（ICA），一种创新的注意力机制，利用上下文参考图像确保多个实例的身份一致性。为了解决这一任务缺乏大规模高质量数据集的问题，我们引入了IMIG-100K，这是第一个提供详细布局和身份注释的专门为多实例生成设计的数据集。大量实验表明，ContextGen设定了新的最先进水平，尤其在布局控制和身份保真度方面超越了现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to overcome the challenges in multi-instance image generation (MIG) related to precise control over object layout and maintaining the identity of distinct subjects. The authors propose ContextGen, a Diffusion Transformer framework that utilizes a Contextual Layout Anchoring (CLA) mechanism and Identity Consistency Attention (ICA) to enhance the generation process by anchoring objects in their desired positions and ensuring identity consistency through reference images. Experimental results indicate that ContextGen achieves state-of-the-art performance, significantly improving layout control and identity fidelity compared to existing methods, while also introducing the IMIG-100K dataset for better training and evaluation of MIG tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服现代扩散模型在多实例图像生成（MIG）中面临的挑战，特别是在控制对象布局和保持不同主体身份方面。作者提出了ContextGen，这是一种扩散变换器框架，利用上下文布局锚定（CLA）机制和身份一致性注意力（ICA）来通过锚定对象在其期望位置和通过参考图像确保身份一致性来提高生成准确性。实验结果表明，ContextGen在布局控制和身份保真度方面显著优于现有方法，达到了最新的性能水平，并通过引入IMIG-100K数据集为该任务提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval</div>
<div class="meta-line">Authors: Benjamin Clavié, Atoof Shakir, Jonah Turner, Sean Lee, Aamir Shakir, Makoto P. Kato</div>
<div class="meta-line">First: 2026-02-12T13:37:58+00:00 · Latest: 2026-02-12T13:37:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11941v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11941v1">PDF</a> · <a href="https://huggingface.co/datasets/mixedbread-ai/incompebench-strict">Code1</a> · <a href="https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient">Code2</a> · <a href="https://github.com/mixedbread-ai/incompebench-programs">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IncompeBench：一个宽松许可的细粒度音乐信息检索基准</div>
<div class="mono" style="margin-top:8px">多模态信息检索近年来取得了显著进展，利用深度预训练模型日益强大的多模态能力来表示跨模态的信息。特别是音乐信息检索（MIR）的质量显著提高，神经音乐表示甚至进入了日常生活产品。然而，缺乏高质量的基准来评估音乐检索性能。为了解决这个问题，我们引入了\textbf{IncompeBench}，这是一个经过精心注释的基准，包含$1,574$个宽松许可的高质量音乐片段、$500$个多样化查询和超过$125,000$个个体相关性判断。这些注释是通过多阶段流程创建的，导致人类注释者与生成数据之间的高度一致。生成的数据集可在https://huggingface.co/datasets/mixedbread-ai/incompebench-strict和https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient上公开获取，提示可在https://github.com/mixedbread-ai/incompebench-programs上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for high-quality benchmarks in Music Information Retrieval (MIR) to evaluate retrieval performance effectively. The authors developed IncompeBench, a benchmark consisting of 1,574 permissively licensed music snippets, 500 diverse queries, and over 125,000 individual relevance judgments, created through a multi-stage annotation pipeline that ensured high agreement among human annotators. The experimental results demonstrate the benchmark&#x27;s potential to facilitate advancements in MIR by providing a reliable evaluation framework for various retrieval models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要高质量的基准来评估音乐检索性能，因为现有资源不足。作者介绍了IncompeBench，这是一个经过精心注释的基准，包含1574个许可使用的音乐片段、500个多样化的查询和超过125,000个个体相关性判断，这些都是通过多阶段注释流程创建的。主要发现表明，这些注释在人工注释者之间达成了高度一致，为评估音乐信息检索系统提供了可靠的资源。</div>
</details>
</div>
<div class="card">
<div class="title">Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation</div>
<div class="meta-line">Authors: Pingjun Pan, Tingting Zhou, Peiyao Lu, Tingting Fei, Hongxiang Chen, Chuanjiang Luo</div>
<div class="meta-line">First: 2026-02-12T10:26:15+00:00 · Latest: 2026-02-12T10:26:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11799v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Hi-SAM：一种层次结构感知的多模态大规模推荐框架</div>
<div class="mono" style="margin-top:8px">多模态推荐因物品具有丰富的属性（如文本和图像）而受到关注。基于语义ID的方法有效地将这些信息离散化为紧凑的标记。然而，仍然存在两个挑战：（1）次优标记化：现有方法（如RQ-VAE）缺乏共享跨模态语义与特定模态细节之间的解耦，导致冗余或崩溃；（2）架构-数据不匹配：普通Transformer将语义ID视为平坦流，忽略用户交互、物品和标记的层次结构。将物品扩展为多个标记会增加长度和噪声，使注意力偏向局部细节而非整体语义。我们提出了Hi-SAM，一种具有两个设计的层次结构感知多模态框架：（1）解耦语义标记器（DST）：通过几何感知对齐统一模态，并通过粗到细的策略进行量化。共享代码本提炼共识，而特定模态的代码本从残差中恢复细微差别，通过互信息最小化来强制执行；（2）层次记忆锚点Transformer（HMAT）：通过层次RoPE将位置编码分割为物品间和物品内子空间，以恢复层次结构。它插入锚点标记，将物品浓缩为紧凑的记忆，保留当前物品的细节，同时仅通过压缩摘要访问历史。对真实世界数据集的实验显示在SOTA基准上持续改进，尤其是在冷启动场景中。在为数百万用户服务的大规模社交平台上部署的Hi-SAM在核心在线指标上实现了6.55%的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in multi-modal recommendation systems, specifically the issues of suboptimal tokenization and architecture-data mismatch. The authors propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework that incorporates a Disentangled Semantic Tokenizer (DST) for effective alignment and quantization of modalities, and a Hierarchical Memory-Anchor Transformer (HMAT) to maintain the hierarchical structure of user interactions and items. Experimental results demonstrate that Hi-SAM consistently outperforms state-of-the-art baselines, particularly in cold-start scenarios, achieving a 6.55% improvement in core online metrics when deployed on a large-scale social platform serving millions of users.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态推荐系统中的挑战，特别是次优的标记化和架构-数据不匹配问题。作者提出了Hi-SAM，一个层次结构感知的多模态框架，包括一个用于有效对齐和量化模态的解耦语义标记器（DST），以及一个保持用户交互层次结构的层次记忆锚定变换器（HMAT）。实验结果表明，Hi-SAM在核心在线指标上持续超越最先进的基线，特别是在为数百万用户服务的大型社交平台的冷启动场景中实现了6.55%的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Detecting RLVR Training Data via Structural Convergence of Reasoning</div>
<div class="meta-line">Authors: Hongbo Zhang, Yue Yang, Jianhao Yan, Guangsheng Bao, Yue Zhang, Yue Zhang</div>
<div class="meta-line">First: 2026-02-12T10:17:32+00:00 · Latest: 2026-02-12T10:17:32+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11792v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过推理的结构收敛检测RLVR训练数据</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）是训练现代推理模型的核心，但未公开的训练数据引发了基准污染的担忧。与通过令牌级概率优化模型的预训练方法不同，RLVR基于自生成推理轨迹的奖励反馈微调模型，使得传统的基于似然的检测方法效果不佳。我们展示了RLVR引发的独特行为特征：在RLVR训练中遇到的提示导致生成结果更加僵化和相似，而未见过的提示则保持更大的多样性。我们引入了Min-$k$NN距离，这是一种简单的黑箱检测器，通过对给定提示进行多次补全采样并计算$k$个最小邻近编辑距离的平均值来量化这种崩溃。Min-$k$NN距离不需要访问参考模型或令牌概率。针对多个RLVR训练的推理模型的实验表明，Min-$k$NN距离可靠地区分RL已见示例和未见示例，并且优于现有的成员推断和RL污染检测基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address concerns regarding benchmark contamination in reinforcement learning with verifiable rewards (RLVR), particularly due to undisclosed training data. The authors propose a method called Min-$k$NN Distance, which detects RLVR training data by analyzing the structural convergence of reasoning outputs, focusing on the behavioral signature that emerges during RLVR training. Experimental results demonstrate that Min-$k$NN Distance effectively differentiates between examples seen during RLVR training and those that are unseen, outperforming existing methods for membership inference and RL contamination detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于未公开的训练数据而导致的基准污染问题，这在使用可验证奖励的强化学习（RLVR）中尤为重要，该方法对推理模型的训练至关重要。作者提出了一种新方法，称为Min-$k$NN距离，这是一种黑箱检测器，通过分析模型输出的相似性来测量推理的结构收敛性。实验结果表明，Min-$k$NN距离能够有效区分在RLVR训练中见过的示例和未见过的示例，并且在会员推断和RL污染检测的现有方法中表现优越。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation</div>
<div class="meta-line">Authors: Xiangyu Wu, Dongming Jiang, Feng Yu, Yueying Tian, Jiaqi Tang, Qing-Guo Chen, Yang Yang, Jianfeng Lu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-12T09:12:22+00:00 · Latest: 2026-02-12T09:12:22+00:00</div>
<div class="meta-line">Comments: Accepted for publication at ICLR 2026; 24 pages; 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11743v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11743v1">PDF</a> · <a href="https://github.com/Jinx630/ADTE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mainstream Test-Time Adaptation (TTA) methods for adapting vision-language models, e.g., CLIP, typically rely on Shannon Entropy (SE) at test time to measure prediction uncertainty and inconsistency. However, since CLIP has a built-in bias from pretraining on highly imbalanced web-crawled data, SE inevitably results in producing biased estimates of uncertainty entropy. To address this issue, we notably find and demonstrate that Tsallis Entropy (TE), a generalized form of SE, is naturally suited for characterizing biased distributions by introducing a non-extensive parameter q, with the performance of SE serving as a lower bound for TE. Building upon this, we generalize TE into Adaptive Debiasing Tsallis Entropy (ADTE) for TTA, customizing a class-specific parameter q^l derived by normalizing the estimated label bias from continuously incoming test instances, for each category. This adaptive approach allows ADTE to accurately select high-confidence views and seamlessly integrate with a label adjustment strategy to enhance adaptation, without introducing distribution-specific hyperparameter tuning. Besides, our investigation reveals that both TE and ADTE can serve as direct, advanced alternatives to SE in TTA, without any other modifications. Experimental results show that ADTE outperforms state-of-the-art methods on ImageNet and its five variants, and achieves the highest average performance on 10 cross-domain benchmarks, regardless of the model architecture or text prompts used. Our code is available at https://github.com/Jinx630/ADTE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>测试时适应的自适应去偏置Tsallis熵</div>
<div class="mono" style="margin-top:8px">主流的测试时适应（TTA）方法用于适应视觉-语言模型，例如CLIP，通常依赖于测试时的香农熵（SE）来测量预测的不确定性和不一致性。然而，由于CLIP在高度不平衡的网络爬取数据上进行预训练而产生的内在偏差，SE不可避免地导致产生偏倚的不确定性熵估计。为了解决这个问题，我们显著发现并证明Tsallis熵（TE），作为SE的广义形式，天然适合通过引入非广延参数q来表征偏倚分布，SE的性能作为TE的下界。在此基础上，我们将TE推广为自适应去偏置Tsallis熵（ADTE）用于TTA，为每个类别定制一个通过对持续到来的测试实例的估计标签偏差进行归一化得到的类特定参数q^l。这种自适应方法使ADTE能够准确选择高置信度视图，并与标签调整策略无缝集成，以增强适应性，而无需引入特定于分布的超参数调优。此外，我们的研究表明TE和ADTE都可以作为TTA中SE的直接高级替代品，而无需其他修改。实验结果表明，ADTE在ImageNet及其五个变体上优于最先进的方法，并在10个跨域基准上实现最高的平均性能，无论使用的模型架构或文本提示如何。我们的代码可在https://github.com/Jinx630/ADTE获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of Test-Time Adaptation (TTA) methods for vision-language models like CLIP, which suffer from biased uncertainty estimates due to reliance on Shannon Entropy (SE). The authors propose Adaptive Debiasing Tsallis Entropy (ADTE), a novel method that generalizes Tsallis Entropy (TE) by introducing a class-specific parameter to address label bias in test instances. Experimental results demonstrate that ADTE significantly outperforms existing state-of-the-art methods on ImageNet and achieves the highest average performance across 10 cross-domain benchmarks, indicating its effectiveness in adapting to biased distributions without requiring hyperparameter tuning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视觉语言模型（如CLIP）在测试时适应（TTA）方法中预测不确定性估计的准确性，这些方法受到不平衡训练数据固有偏差的影响。作者提出了一种新方法——自适应去偏Tsallis熵（ADTE），该方法利用Tsallis熵（TE）和特定类别的参数来实时考虑标签偏差。实验结果表明，ADTE在ImageNet和十个跨领域基准测试中显著优于现有的最先进方法，提供了更可靠的不确定性度量，而无需额外的超参数调整。</div>
</details>
</div>
<div class="card">
<div class="title">STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</div>
<div class="meta-line">Authors: Xiaowen Zhang, Zhi Gao, Licheng Jiao, Lingling Li, Qing Li</div>
<div class="meta-line">First: 2026-02-12T08:53:32+00:00 · Latest: 2026-02-12T08:53:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11730v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11730v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&amp;F on MeViS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STVG-R1：通过强化学习激励视频中的实例级推理和定位</div>
<div class="mono" style="margin-top:8px">在视觉语言模型（VLMs）中，文本描述与视觉坐标之间的不对齐常常导致幻觉。这个问题在空间-时间视频定位（STVG）等密集预测任务中尤为严重。以往的方法通常侧重于增强视觉-文本对齐或附加辅助解码器。然而，这些策略不可避免地引入额外的可训练模块，导致显著的标注成本和计算开销。在本研究中，我们提出了一种新颖的视觉提示范式，避免了跨模态对齐坐标的困难问题。具体而言，我们将每帧坐标预测重新表述为一个紧凑的实例级识别问题，通过为每个对象分配一个唯一的、时间一致的ID。这些ID被嵌入到视频中作为视觉提示，为VLMs提供明确且可解释的输入。此外，我们引入了STVG-R1，这是第一个用于STVG的强化学习框架，采用任务驱动的奖励共同优化时间准确性、空间一致性和结构格式正则化。在六个基准上的大量实验证明了我们方法的有效性。STVG-R1在HCSTVG-v2基准上在m_IoU上超越了基线Qwen2.5-VL-7B，取得了20.9%的显著提升，建立了新的最先进水平（SOTA）。令人惊讶的是，STVG-R1在多对象引用视频对象分割任务中也表现出强大的零样本泛化能力，在MeViS上达到了47.3%的SOTA J&amp;F。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of hallucinations in vision-language models (VLMs) caused by misalignment between textual descriptions and visual coordinates, particularly in dense prediction tasks like spatial-temporal video grounding (STVG). The authors propose a novel visual prompting paradigm that reformulates per-frame coordinate prediction as an instance-level identification problem, assigning unique, temporally consistent IDs to each object and embedding these IDs into the video as visual prompts. Experimental results show that their proposed STVG-R1 framework, the first reinforcement learning approach for STVG, significantly outperforms the baseline Qwen2.5-VL-7B by 20.9% on m_IoU in the HCSTVG-v2 benchmark and demonstrates strong zero-shot generalization in multi-object referring video object segmentation tasks, achieving a state-of-the-art 47.3% J&amp;F on MeViS.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉语言模型中文本描述与视觉坐标之间的错位问题，这在密集预测任务（如时空视频定位）中导致幻觉。作者提出了一种新颖的视觉提示范式，将每帧坐标预测重新表述为实例级识别问题，通过为每个对象分配唯一且时间一致的ID，并将这些ID嵌入视频中作为视觉提示。其方法STVG-R1是首个用于时空视频定位的强化学习框架，通过任务驱动的奖励优化时间准确性、空间一致性和结构格式正则化，实验结果显示其在HCSTVG-v2基准上比基线提高了20.9%的m_IoU，并在多对象引用视频对象分割任务中展现出强大的零样本泛化能力，在MeViS上达到47.3%的SOTA J&amp;F。</div>
</details>
</div>
<div class="card">
<div class="title">Dopamine: Brain Modes, Not Brains</div>
<div class="meta-line">Authors: Shervin Ghasemlou</div>
<div class="meta-line">First: 2026-02-12T08:52:09+00:00 · Latest: 2026-02-12T08:52:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11726v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) methods such as \lora{} adapt large pretrained models by adding small weight-space updates. While effective, weight deltas are hard to interpret mechanistically, and they do not directly expose \emph{which} internal computations are reused versus bypassed for a new task. We explore an alternative view inspired by neuromodulation: adaptation as a change in \emph{mode} -- selecting and rescaling existing computations -- rather than rewriting the underlying weights. We propose \methodname{}, a simple activation-space PEFT technique that freezes base weights and learns per-neuron \emph{thresholds} and \emph{gains}. During training, a smooth gate decides whether a neuron&#x27;s activation participates; at inference the gate can be hardened to yield explicit conditional computation and neuron-level attributions.
  As a proof of concept, we study ``mode specialization&#x27;&#x27; on MNIST (0$^\circ$) versus rotated MNIST (45$^\circ$). We pretrain a small MLP on a 50/50 mixture (foundation), freeze its weights, and then specialize to the rotated mode using \methodname{}. Across seeds, \methodname{} improves rotated accuracy over the frozen baseline while using only a few hundred trainable parameters per layer, and exhibits partial activation sparsity (a minority of units strongly active). Compared to \lora{}, \methodname{} trades some accuracy for substantially fewer trainable parameters and a more interpretable ``which-neurons-fire&#x27;&#x27; mechanism. We discuss limitations, including reduced expressivity when the frozen base lacks features needed for the target mode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多巴胺：脑模式，而非大脑</div>
<div class="mono" style="margin-top:8px">参数高效微调（PEFT）方法如\lora{}通过添加小的权重空间更新来适应大型预训练模型。尽管有效，权重增量在机制上难以解释，并且它们并未直接揭示\emph{哪些}内部计算被重用或绕过以适应新任务。我们探索了一种受神经调制启发的替代观点：适应作为\emph{模式}的变化——选择和重新缩放现有计算——而不是重写基础权重。我们提出了\methodname{}，一种简单的激活空间PEFT技术，它冻结基础权重并学习每个神经元的\emph{阈值}和\emph{增益}。在训练过程中，一个平滑的门决定神经元的激活是否参与；在推理时，门可以被硬化以产生明确的条件计算和神经元级别的归因。作为概念验证，我们研究了MNIST（0$^\circ$）与旋转MNIST（45$^\circ$）上的“模式专业化”。我们在50/50混合（基础）上预训练一个小型MLP，冻结其权重，然后使用\methodname{}专门化到旋转模式。在不同种子下，\methodname{}在冻结基线的基础上提高了旋转准确性，同时每层仅使用几百个可训练参数，并表现出部分激活稀疏性（少数单元强烈激活）。与\lora{}相比，\methodname{}在可训练参数显著减少的同时牺牲了一些准确性，并提供了更可解释的“哪些神经元激活”机制。我们讨论了局限性，包括当冻结基础缺乏目标模式所需特征时，表达能力降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of interpreting weight deltas in parameter-efficient fine-tuning (PEFT) methods for large pretrained models. The authors propose a novel approach called \methodname{}, which focuses on adapting models by changing the activation mode rather than altering the underlying weights. In their experiments on MNIST and rotated MNIST, \methodname{} demonstrated improved accuracy for the rotated task compared to a frozen baseline while utilizing significantly fewer trainable parameters and allowing for clearer neuron-level attributions, although it faced limitations in expressivity when the base model lacked necessary features for the target mode.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解释大型预训练模型的参数高效微调（PEFT）方法中的权重更新。作者提出了一种新的方法，称为\methodname{}，该方法通过改变神经元的激活模式而不是修改底层权重来适应模型。实验结果表明，\methodname{}在旋转MNIST任务上的准确性优于冻结基线，同时使用的可训练参数显著减少，并提供了更清晰的神经元激活洞察，尽管当基础模型缺乏目标任务所需特征时，可能会限制表达能力。</div>
</details>
</div>
<div class="card">
<div class="title">U-DAVI: Uncertainty-Aware Diffusion-Prior-Based Amortized Variational Inference for Image Reconstruction</div>
<div class="meta-line">Authors: Ayush Varshney, Katherine L. Bouman, Berthy T. Feng</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-02-12T08:32:11+00:00 · Latest: 2026-02-12T08:32:11+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11704v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11704v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ill-posed imaging inverse problems remain challenging due to the ambiguity in mapping degraded observations to clean images. Diffusion-based generative priors have recently shown promise, but typically rely on computationally intensive iterative sampling or per-instance optimization. Amortized variational inference frameworks address this inefficiency by learning a direct mapping from measurements to posteriors, enabling fast posterior sampling without requiring the optimization of a new posterior for every new set of measurements. However, they still struggle to reconstruct fine details and complex textures. To address this, we extend the amortized framework by injecting spatially adaptive perturbations to measurements during training, guided by uncertainty estimates, to emphasize learning in the most uncertain regions. Experiments on deblurring and super-resolution demonstrate that our method achieves superior or competitive performance to previous diffusion-based approaches, delivering more realistic reconstructions without the computational cost of iterative refinement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U-DAVI：基于不确定性感知扩散先验的摊销变分推断用于图像重建</div>
<div class="mono" style="margin-top:8px">由于将退化观测映射到清晰图像的模糊性，病态成像逆问题仍然具有挑战性。基于扩散的生成先验最近显示出前景，但通常依赖于计算密集型的迭代采样或每实例优化。摊销变分推断框架通过学习从测量到后验的直接映射来解决这种低效性，使得快速后验采样成为可能，而无需为每组新测量优化新的后验。然而，它们在重建细节和复杂纹理方面仍然存在困难。为了解决这个问题，我们通过在训练期间向测量中注入空间自适应扰动（由不确定性估计引导）来扩展摊销框架，以强调在最不确定区域的学习。在去模糊和超分辨率的实验中，我们的方法在性能上优于或与之前的基于扩散的方法具有竞争力，提供了更真实的重建，而无需迭代精炼的计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of ill-posed imaging inverse problems, which often struggle with ambiguity in reconstructing clean images from degraded observations. The authors propose an extension to amortized variational inference by incorporating spatially adaptive perturbations during training, guided by uncertainty estimates, to enhance learning in uncertain regions. Experimental results on deblurring and super-resolution indicate that this method achieves superior or competitive performance compared to existing diffusion-based approaches, providing more realistic image reconstructions without the need for computationally intensive iterative refinement.</div>
<div class="mono" style="margin-top:8px">本研究解决了不适定成像逆问题的挑战，特别是在从降级观测中准确重建干净图像的困难。作者提出了U-DAVI，一种基于不确定性感知扩散先验的摊销变分推断方法，该方法在训练过程中结合了空间自适应扰动，并通过不确定性估计指导，以增强对不确定区域的学习。去模糊和超分辨率的实验结果表明，U-DAVI在性能上优于或与现有的基于扩散的方法相当，提供了更真实的图像重建，同时避免了迭代优化的计算负担。</div>
</details>
</div>
<div class="card">
<div class="title">How Does a Deep Neural Network Look at Lexical Stress in English Words?</div>
<div class="meta-line">Authors: Itai Allouche, Itay Asael, Rotem Rousso, Vered Dassa, Ann Bradlow, Seung-Eun Kim, Matthew Goldrick, Joseph Keshet</div>
<div class="meta-line">Venue: The Journal of the Acoustical Society of America. 159(2), 1348-1358 (2026)</div>
<div class="meta-line">First: 2025-08-10T08:13:40+00:00 · Latest: 2026-02-12T08:16:52+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures, accepted to the Journal of the Acoustical Society of America (JASA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07229v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.07229v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for neural network interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel&#x27;s first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning&#x27;s ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度神经网络如何看待英语单词的词汇重音？</div>
<div class="mono" style="margin-top:8px">尽管神经网络在语音处理方面取得了成功，但它们通常作为黑箱操作，这引发了一个问题：是什么影响了它们的决策，我们如何解读它们？本研究在词汇重音的背景下考察了这个问题。通过读取和自发语音自动构建了一个英语双音节单词的数据集。训练了几种卷积神经网络（CNN）架构，以从缺乏最小重音对的双音节单词的声谱表示中预测重音位置（例如，初重音 WAllet，末重音 exTEND），在保留的测试数据上达到了92%的准确率。层次相关传播（LRP）作为神经网络可解释性分析的技术，揭示了对保留的最小对（PROtest vs. proTEST）的预测最受重音与非重音音节中信息的影响，特别是重音元音的声谱特性。然而，分类器也关注整个单词中的信息。提出了一种特征特定的相关性分析，其结果表明我们表现最佳的分类器受到重音元音的第一和第二共振峰的强烈影响，并且有一些证据表明其音高和第三共振峰也有所贡献。这些结果揭示了深度学习从自然发生的数据中获取重音的分布线索的能力，扩展了基于高度控制刺激的传统语音学研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how deep neural networks interpret lexical stress in English words, addressing the challenge of understanding their decision-making processes. The study utilized a dataset of English disyllabic words derived from both read and spontaneous speech, training various Convolutional Neural Network (CNN) architectures to predict stress positions from spectrographic representations, achieving up to 92% accuracy on test data. The analysis using Layerwise Relevance Propagation (LRP) indicated that predictions were primarily influenced by stressed versus unstressed syllables, particularly the spectral characteristics of stressed vowels, while also showing attention to information throughout the word, suggesting that the classifiers rely heavily on the first and second formants of stressed vowels, with some contribution from pitch and the third formant.</div>
<div class="mono" style="margin-top:8px">本研究探讨了深度神经网络如何解释英语单词中的词汇重音，旨在理解其决策过程。研究利用从朗读和自发语音中提取的英语双音节单词数据集，训练了多种卷积神经网络（CNN）架构，以从声谱图表示中预测重音位置，在测试数据上达到了92%的准确率。层次相关传播（LRP）的应用显示，最小对的预测主要受到重音和非重音音节的影响，特别是重音元音的声谱特征，同时也表明分类器考虑了整个单词的信息，重音元音的第一和第二共振峰具有显著相关性，音高和第三共振峰也有一定贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Remote Sensing Retrieval-Augmented Generation: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model</div>
<div class="meta-line">Authors: Congcong Wen, Yiting Lin, Xiaokang Qu, Nan Li, Yong Liao, Xiang Li, Hui Lin</div>
<div class="meta-line">First: 2025-04-07T12:13:43+00:00 · Latest: 2026-02-12T07:08:47+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Geoscience and Remote Sensing Magazine (GRSM)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.04988v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.04988v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in VLMs has demonstrated impressive capabilities across a variety of tasks in the natural image domain. Motivated by these advancements, the remote sensing community has begun to adopt VLMs for remote sensing vision-language tasks, including scene understanding, image captioning, and visual question answering. However, existing remote sensing VLMs typically rely on closed-set scene understanding and focus on generic scene descriptions, yet lack the ability to incorporate external knowledge. This limitation hinders their capacity for semantic reasoning over complex or context-dependent queries that involve domain-specific or world knowledge. To address these challenges, we first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset, which comprises high-resolution satellite imagery and detailed textual descriptions for 14,141 well-known landmarks from 175 countries, integrating both remote sensing domain knowledge and broader world knowledge. Building upon this dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation (RS-RAG) framework, which consists of two key components. The Multi-Modal Knowledge Vector Database Construction module encodes remote sensing imagery and associated textual knowledge into a unified vector space. The Knowledge Retrieval and Response Generation module retrieves and re-ranks relevant knowledge based on image and/or text queries, and incorporates the retrieved content into a knowledge-augmented prompt to guide the VLM in producing contextually grounded responses. We validated the effectiveness of our approach on three representative vision-language tasks, including image captioning, image classification, and visual question answering, where RS-RAG significantly outperformed state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>遥感检索增强生成：通过多模态数据集和检索增强生成模型连接遥感图像与综合知识</div>
<div class="mono" style="margin-top:8px">最近在视觉语言模型（VLMs）方面的进展展示了其在自然图像领域多种任务中的出色能力。受到这些进展的启发，遥感社区开始采用VLMs进行遥感视觉语言任务，包括场景理解、图像描述和视觉问答。然而，现有的遥感VLMs通常依赖于封闭集场景理解，专注于通用场景描述，缺乏整合外部知识的能力。这一局限性妨碍了它们在涉及领域特定或世界知识的复杂或依赖上下文的查询中进行语义推理的能力。为了解决这些挑战，我们首先引入了一个多模态遥感世界知识（RSWK）数据集，该数据集包含来自175个国家的14,141个著名地标的高分辨率卫星图像和详细文本描述，整合了遥感领域知识和更广泛的世界知识。在此数据集的基础上，我们提出了一种新颖的遥感检索增强生成（RS-RAG）框架，该框架由两个关键组件组成。多模态知识向量数据库构建模块将遥感图像和相关文本知识编码到统一的向量空间中。知识检索和响应生成模块根据图像和/或文本查询检索和重新排序相关知识，并将检索到的内容整合到知识增强提示中，以指导VLM生成上下文相关的响应。我们在三个代表性的视觉语言任务上验证了我们方法的有效性，包括图像描述、图像分类和视觉问答，其中RS-RAG显著超越了最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the capabilities of remote sensing vision-language models (VLMs) by integrating external knowledge, addressing their limitations in semantic reasoning for complex queries. The authors introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset, which includes high-resolution satellite images and detailed descriptions of 14,141 landmarks across 175 countries. They developed the Remote Sensing Retrieval-Augmented Generation (RS-RAG) framework, which combines a Multi-Modal Knowledge Vector Database Construction module and a Knowledge Retrieval and Response Generation module to improve the contextual grounding of responses. Experimental results demonstrated that RS-RAG significantly outperformed existing state-of-the-art methods in image captioning, image classification, and visual question answering tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决遥感视觉语言模型（VLM）在处理复杂查询时无法整合外部知识的局限性，来增强其能力。作者引入了一个多模态的遥感世界知识（RSWK）数据集，该数据集包含来自175个国家的14,141个地标的高分辨率卫星图像和详细描述，并开发了一个遥感检索增强生成（RS-RAG）框架。该框架包括一个多模态知识向量数据库构建模块和一个知识检索与响应生成模块，能够检索相关知识并将其整合到响应中。实验结果表明，RS-RAG在图像描述、图像分类和视觉问答任务中显著优于现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks</div>
<div class="meta-line">Authors: Dongxiao He, Wenxuan Sun, Yongqi Huang, Jitao Zhao, Di Jin</div>
<div class="meta-line">First: 2026-02-12T06:25:21+00:00 · Latest: 2026-02-12T06:25:21+00:00</div>
<div class="meta-line">Comments: 16 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11629v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11629v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GP2F：通过预训练图神经网络的自适应融合进行跨域图提示</div>
<div class="mono" style="margin-top:8px">图提示学习（GPL）最近作为一种有前景的范式出现，用于预训练图模型的下游适应，缓解了预训练目标与下游任务之间的不一致。最近，GPL的重点已从域内转向跨域场景，这更接近于现实世界应用，其中预训练源和下游目标在数据分布上往往存在显著差异。然而，为什么GPL在这种领域转变下仍然有效尚未被探索。通过实证观察，我们发现代表性的GPL方法在跨域设置中与两个简单基线（完全微调（FT）和线性探测（LP））具有竞争力，这激励我们深入理解提示机制。我们提供了理论分析，证明联合利用这两个互补分支的估计误差小于单独使用任一分支，正式证明跨域GPL受益于预训练知识与任务特定适应之间的整合。基于这一见解，我们提出了GP2F，一种双分支GPL方法，明确实例化两个极端：（1）一个保留预训练知识的冻结分支，以及（2）一个具有轻量适配器的适应分支，用于任务特定适应。然后，我们通过对比损失和拓扑一致损失在拓扑约束下执行自适应融合。在跨域少样本节点和图分类的广泛实验中，我们的方法优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve the adaptation of pre-trained graph models to cross-domain scenarios, where data distributions differ significantly between pre-training and downstream tasks. The authors propose GP2F, a dual-branch Graph Prompt Learning method that combines a frozen branch retaining pre-trained knowledge with an adapted branch utilizing lightweight adapters for task-specific adjustments. Experimental results show that GP2F outperforms existing methods in cross-domain few-shot node and graph classification tasks, demonstrating the effectiveness of integrating pre-trained knowledge with task-specific adaptation through adaptive fusion under topology constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善预训练图模型在跨域场景中的适应性，这些场景中数据分布差异显著。作者提出了GP2F，一种双分支图提示学习方法，结合了保留预训练知识的冻结分支和使用轻量级适配器进行任务特定适应的适应分支。实验结果表明，GP2F在跨域少样本节点和图分类任务中优于现有方法，确认了在拓扑约束下整合预训练知识与任务特定适应的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement</div>
<div class="meta-line">Authors: Chen Zhao, Chenyu Dong, Weiling Cai, Yueyue Wang</div>
<div class="meta-line">First: 2024-03-03T12:17:49+00:00 · Latest: 2026-02-12T05:19:23+00:00</div>
<div class="meta-line">Comments: IEEE Transactions on Geoscience and Remote Sensing (TGRS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.01497v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.01497v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Underwater visuals undergo various complex degradations, inevitably influencing the efficiency of underwater vision tasks. Recently, diffusion models were employed to underwater image enhancement (UIE) tasks, and gained SOTA performance. However, these methods fail to consider the physical properties and underwater imaging mechanisms in the diffusion process, limiting information completion capacity of diffusion models. In this paper, we introduce a novel UIE framework, named PA-Diff, designed to exploiting the knowledge of physics to guide the diffusion process.
  PA-Diff consists of Physics Prior Generation (PPG) Branch, Implicit Neural Reconstruction (INR) Branch, and Physics-aware Diffusion Transformer (PDT) Branch. Our designed PPG branch aims to produce the prior knowledge of physics. With utilizing the physics prior knowledge to guide the diffusion process, PDT branch can obtain underwater-aware ability and model the complex distribution in real-world underwater scenes. INR Branch can learn robust feature representations from diverse underwater image via implicit neural representation, which reduces the difficulty of restoration for PDT branch. Extensive experiments prove that our method achieves best performance on UIE tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于变换器的物理感知扩散模型学习用于水下图像增强</div>
<div class="mono" style="margin-top:8px">水下视觉经历各种复杂的退化，必然影响水下视觉任务的效率。最近，扩散模型被应用于水下图像增强（UIE）任务，并取得了SOTA性能。然而，这些方法未能考虑扩散过程中的物理特性和水下成像机制，限制了扩散模型的信息补全能力。本文介绍了一种新颖的UIE框架，称为PA-Diff，旨在利用物理知识指导扩散过程。PA-Diff由物理先验生成（PPG）分支、隐式神经重建（INR）分支和物理感知扩散变换器（PDT）分支组成。我们设计的PPG分支旨在生成物理的先验知识。通过利用物理先验知识指导扩散过程，PDT分支可以获得水下感知能力，并建模现实水下场景中的复杂分布。INR分支可以通过隐式神经表示从多样的水下图像中学习鲁棒的特征表示，从而降低PDT分支的恢复难度。大量实验证明我们的方法在UIE任务上实现了最佳性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing diffusion models in underwater image enhancement (UIE) that do not account for physical properties and imaging mechanisms, which affects their performance. The authors propose a new framework called PA-Diff, which integrates a Physics Prior Generation Branch, an Implicit Neural Reconstruction Branch, and a Physics-aware Diffusion Transformer Branch to enhance the diffusion process using physics knowledge. Experimental results demonstrate that PA-Diff achieves state-of-the-art performance in UIE tasks, effectively modeling complex underwater scenes and improving restoration capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有扩散模型在水下图像增强（UIE）中存在的局限性，这些模型未考虑水下环境的物理特性和成像机制。作者提出了一种名为PA-Diff的新框架，该框架集成了物理先验生成（PPG）分支、隐式神经重建（INR）分支和物理感知扩散变换器（PDT）分支，以利用基于物理的知识增强扩散过程。实验结果表明，PA-Diff在UIE任务中实现了最先进的性能，有效地建模复杂的水下场景分布并改善了恢复能力。</div>
</details>
</div>
<div class="card">
<div class="title">MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</div>
<div class="meta-line">Authors: Chieh-Yun Chen, Zhonghao Wang, Qi Chen, Zhifan Ye, Min Shi, Yue Zhao, Yinan Zhao, Hui Qu, Wei-An Lin, Yiru Shen, Ajinkya Kale, Irfan Essa, Humphrey Shi</div>
<div class="meta-line">First: 2025-11-25T18:49:21+00:00 · Latest: 2026-02-12T04:40:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20629v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.20629v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapReduce LoRA：推进生成模型多偏好优化的帕累托前沿</div>
<div class="mono" style="margin-top:8px">基于人类反馈的强化学习（RLHF）与奖励模型的结合，提升了生成模型与人类审美和感知偏好的对齐。然而，联合优化多个奖励往往会产生对齐成本，改善一个维度的同时降低其他维度的表现。为了解决这个问题，我们提出了两种互补的方法：MapReduce LoRA和奖励感知的令牌嵌入（RaTE）。MapReduce LoRA并行训练特定偏好的LoRA专家，并迭代合并它们以优化共享基础模型；RaTE学习奖励特定的令牌嵌入，在推理时组合以实现灵活的偏好控制。在文本到图像生成（Stable Diffusion 3.5 Medium和FLUX.1-dev）上的实验显示，GenEval、PickScore和OCR分别提高了36.1%、4.6%和55.7%，以及32.7%、4.3%和67.1%。在文本到视频生成（HunyuanVideo）中，视觉和运动质量分别提高了48.1%和90.0%。在语言任务Helpful Assistant中，使用Llama-2 7B，帮助性和无害性分别提高了43.4%和136.7%。我们的框架在各个模态中设定了新的最先进的多偏好对齐方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of aligning generative models with multiple human aesthetic and perceptual preferences, which often leads to a trade-off in performance across different dimensions. The authors propose two methods, MapReduce LoRA and Reward-aware Token Embedding (RaTE), where MapReduce LoRA trains preference-specific experts in parallel and merges them, while RaTE develops reward-specific token embeddings for better preference control during inference. Experimental results demonstrate significant improvements in various tasks, including a 36.1% increase in GenEval and a 48.1% enhancement in visual quality for Text-to-Video generation, establishing a new state-of-the-art in multi-preference alignment across different modalities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强生成模型与人类审美和感知偏好的对齐，解决联合优化多个奖励所带来的对齐成本问题。作者提出了两种方法：MapReduce LoRA，该方法并行训练特定偏好的专家并合并它们以优化共享模型；以及奖励感知的令牌嵌入（RaTE），该方法学习特定奖励的令牌嵌入以实现灵活的偏好控制。实验结果显示，在各项任务中显著提高了性能，包括GenEval提高了36.1%，以及文本到视频生成的视觉质量提高了48.1%，在不同模态中建立了多偏好对齐的新状态。</div>
</details>
</div>
<div class="card">
<div class="title">The Determinism of Randomness: Latent Space Degeneracy in Diffusion Model</div>
<div class="meta-line">Authors: Song Yan, Chenfeng Wang, Wei Zhai, Xinliang Bi, Jian Yang, Yusen Zhang, Yunwei Lan, Tao Zhang, GuanYe Xiong, Min Li, Zheng-Jun Zha</div>
<div class="meta-line">First: 2025-11-11T02:12:38+00:00 · Latest: 2026-02-12T03:33:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07756v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.07756v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models draw the initial latent from an isotropic Gaussian distribution (all directions equally likely). But in practice, changing only the random seed can sharply alter image quality and prompt faithfulness. We explain this by distinguishing the isotropic prior from the semantics induced by the sampling map: while the prior is direction-agnostic, the mapping from latent noise to semantics has semantic-invariant directions and semantic-sensitive directions, so different seeds can lead to very different semantic outcomes. Motivated by this view, we propose a training-free inference procedure that (i) suppresses seed-specific, semantic-irrelevant variation via distribution-preserving semantic erasure, (ii) reinforces prompt-relevant semantic directions through timestep-aggregated horizontal injection, and (iii) applies a simple spherical retraction to stay near the prior&#x27;s typical set. Across multiple backbones and benchmarks, our method consistently improves alignment and generation quality over standard sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机性的决定论：扩散模型中的潜在空间退化</div>
<div class="mono" style="margin-top:8px">扩散模型从各向同性高斯分布中提取初始潜在变量（所有方向同样可能）。但在实践中，仅改变随机种子就可以显著改变图像质量和提示的忠实度。我们通过区分各向同性先验与由采样映射引起的语义来解释这一点：虽然先验与方向无关，但从潜在噪声到语义的映射具有语义不变方向和语义敏感方向，因此不同的种子可能导致非常不同的语义结果。基于这一观点，我们提出了一种无训练推理程序，(i) 通过保持分布的语义擦除来抑制种子特定的、与语义无关的变化，(ii) 通过时间步聚合的水平注入来增强与提示相关的语义方向，以及 (iii) 应用简单的球面收缩以保持接近先验的典型集合。在多个基础模型和基准测试中，我们的方法始终提高了对齐和生成质量，超越了标准采样。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the impact of random seed variations on image quality and prompt fidelity in diffusion models, motivated by the observation that different seeds can yield significantly different semantic outcomes despite an isotropic Gaussian prior. The authors propose a training-free inference method that includes distribution-preserving semantic erasure to reduce irrelevant variations, timestep-aggregated horizontal injection to enhance relevant semantic directions, and a spherical retraction to maintain proximity to the prior&#x27;s typical set. Experimental results demonstrate that this approach consistently enhances alignment and generation quality across various model architectures and benchmarks compared to standard sampling methods.</div>
<div class="mono" style="margin-top:8px">本研究探讨了扩散模型中由于随机种子变化导致的图像质量和提示保真度的不一致性，动机在于区分各向同性高斯先验和采样映射的语义。作者提出了一种无训练推理方法，通过保持分布的语义消除来减少与种子相关的无关变化，通过时间步聚合的水平注入增强相关语义方向，并采用球面收缩保持接近先验的典型集合。实验结果表明，该方法在多个模型架构和基准测试中，相较于标准采样技术，始终提高了对齐和生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars</div>
<div class="meta-line">Authors: Zhongju Wang, Zhenhong Sun, Beier Wang, Yifu Wang, Daoyi Dong, Huadong Mo, Hongdong Li</div>
<div class="meta-line">First: 2026-02-11T04:31:13+00:00 · Latest: 2026-02-12T02:26:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10516v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10516v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DXTalker：在表现力丰富的3D对话头像中统一身份、口型同步、情感和空间动态</div>
<div class="mono" style="margin-top:8px">基于音频的3D对话头像生成在虚拟沟通、数字人类和互动媒体中变得越来越重要，头像必须保持身份、与语音同步口型、表达情感并展现逼真的空间动态，共同定义了表现力的更广泛目标。然而，由于训练数据不足、受限的主体身份、狭窄的音频表示和有限的显式可控性，实现这一目标仍然具有挑战性。本文提出了3DXTalker，通过数据策划的身份建模、丰富的音频表示和空间动态可控性，实现表现力丰富的3D对话头像。3DXTalker通过2D到3D的数据策划管道和解耦表示实现可扩展的身份建模，缓解数据稀缺并改善身份泛化。然后，我们引入超越标准语音嵌入的逐帧幅度和情感线索，确保优越的口型同步和细腻的表达调节。这些线索通过基于流匹配的变换器统一，以实现连贯的面部动态。此外，3DXTalker还支持自然的头部姿态运动生成，同时通过基于提示的条件支持风格化控制。大量实验表明，3DXTalker在统一框架内集成了口型同步、情感表达和头部姿态动态，在3D对话头像生成中实现了卓越的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the expressivity of audio-driven 3D talking avatars for applications in virtual communication and interactive media, addressing challenges related to identity preservation, lip synchronization, emotional expression, and spatial dynamics. The authors propose a method called 3DXTalker, which utilizes a data-curated identity modeling approach, audio-rich representations, and controllable spatial dynamics to improve avatar generation. Experimental results demonstrate that 3DXTalker effectively integrates lip synchronization, emotional expression, and head-pose dynamics, achieving superior performance in generating expressive 3D talking avatars compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高虚拟通信和互动媒体中3D说话头像的表现力，解决与身份保留、唇部同步、情感表达和空间动态相关的挑战。作者提出了3DXTalker，利用数据策划的身份建模方法、丰富的音频表示和可控的空间动态来改善头像生成。实验结果表明，3DXTalker有效地整合了唇部同步、情感表达和头部动态，在生成表现力丰富的3D说话头像方面优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260213_0352.html">20260213_0352</a>
<a href="archive/20260212_0359.html">20260212_0359</a>
<a href="archive/20260211_0404.html">20260211_0404</a>
<a href="archive/20260210_0406.html">20260210_0406</a>
<a href="archive/20260209_0325.html">20260209_0325</a>
<a href="archive/20260208_0323.html">20260208_0323</a>
<a href="archive/20260207_0339.html">20260207_0339</a>
<a href="archive/20260206_0339.html">20260206_0339</a>
<a href="archive/20260205_0341.html">20260205_0341</a>
<a href="archive/20260204_0347.html">20260204_0347</a>
<a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
