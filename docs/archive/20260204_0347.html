<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 03:47</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0347</div>
    <div class="row"><div class="card">
<div class="title">PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</div>
<div class="meta-line">Authors: Zehong Ma, Ruihan Xu, Shiliang Zhang</div>
<div class="meta-line">First: 2026-02-02T18:59:42+00:00 · Latest: 2026-02-02T18:59:42+00:00</div>
<div class="meta-line">Comments: Project Pages: https://zehong-ma.github.io/PixelGen/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02493v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02493v1">PDF</a> · <a href="https://github.com/Zehong-Ma/PixelGen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zehong-ma.github.io/PixelGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PixelGen：像素扩散超越潜在扩散与感知损失</div>
<div class="mono" style="margin-top:8px">像素扩散以端到端的方式直接在像素空间生成图像，避免了两阶段潜在扩散中由变分自编码器引入的伪影和瓶颈。然而，优化包含许多感知无关信号的高维像素流形是具有挑战性的，这使得现有的像素扩散方法落后于潜在扩散模型。我们提出了PixelGen，一个简单的像素扩散框架，具有感知监督。PixelGen引入了两个互补的感知损失，以引导扩散模型学习更有意义的感知流形，而不是建模完整的图像流形。LPIPS损失有助于学习更好的局部模式，而基于DINO的感知损失则增强了全局语义。在感知监督下，PixelGen超越了强大的潜在扩散基线。在ImageNet-256上，它在仅使用80个训练周期的情况下实现了5.11的FID，无需无分类器引导，并在大规模文本到图像生成中展示了良好的扩展性能，GenEval得分为0.79。PixelGen不需要变分自编码器、潜在表示和辅助阶段，提供了一个更简单但更强大的生成范式。代码可在https://github.com/Zehong-Ma/PixelGen公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve image generation by addressing the limitations of existing pixel diffusion methods, which struggle with optimizing high-dimensional pixel manifolds. The authors introduce PixelGen, a pixel diffusion framework that employs perceptual supervision through two complementary losses: an LPIPS loss for local patterns and a DINO-based loss for global semantics. Experimental results show that PixelGen achieves a Fréchet Inception Distance (FID) of 5.11 on ImageNet-256 in just 80 training epochs, outperforming strong latent diffusion baselines and demonstrating effective scaling for large-scale text-to-image generation with a GenEval score of 0.79.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有像素扩散方法的局限性来改进图像生成，这些方法在高维像素流形和感知无关信号方面存在困难。作者提出了PixelGen，这是一种通过感知监督引入两种互补损失的像素扩散框架：LPIPS损失用于更好的局部模式学习，DINO基础的损失用于增强全局语义。实验结果表明，PixelGen超越了强大的潜在扩散基线，在仅80个训练周期内在ImageNet-256上达到了5.11的FID，并在大规模文本到图像生成中展示了有效的扩展，GenEval得分为0.79。</div>
</details>
</div>
<div class="card">
<div class="title">UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</div>
<div class="meta-line">Authors: Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-02T18:34:35+00:00 · Latest: 2026-02-02T18:34:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02437v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02437v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniReason 1.0：一个统一的推理框架，用于与世界知识对齐的图像生成和编辑</div>
<div class="mono" style="margin-top:8px">统一的多模态模型在需要深度推理的复杂合成任务中常常表现不佳，通常将文本到图像生成和图像编辑视为孤立的能力，而不是相互关联的推理步骤。为了解决这个问题，我们提出了UniReason，一个通过双重推理范式协调这两项任务的统一框架。我们将生成形式化为增强世界知识的规划，以注入隐含约束，并利用编辑能力进行细粒度的视觉细化，通过自我反思进一步纠正视觉错误。这种方法在共享表示中统一了生成和编辑，反映了人类认知过程中的规划与细化。我们通过系统构建一个覆盖五个主要知识领域（例如，文化常识、物理等）的以推理为中心的大规模数据集（约30万样本）来支持该框架，同时提供一个用于视觉自我纠正的代理生成语料库。大量实验表明，UniReason在推理密集型基准测试（如WISE、KrisBench和UniREditBench）上实现了先进的性能，同时保持了卓越的综合合成能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the performance of unified multimodal models in complex synthesis tasks that require deep reasoning, as these models often treat text-to-image generation and image editing as separate processes. The authors propose UniReason, a unified framework that integrates these tasks through a dual reasoning paradigm, enhancing generation with world knowledge and utilizing editing for visual refinement. Experimental results show that UniReason significantly outperforms existing models on reasoning-intensive benchmarks like WISE, KrisBench, and UniREditBench, while also demonstrating strong general synthesis capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高统一多模态模型在需要深度推理的复杂合成任务中的表现，因为这些模型通常将文本到图像生成和图像编辑视为独立的过程。作者提出了UniReason，一个通过双重推理范式整合这两项任务的统一框架，通过世界知识增强生成，并利用编辑进行视觉细化。实验结果表明，UniReason在WISE、KrisBench和UniREditBench等推理密集型基准测试中显著优于现有模型，同时也展示了强大的通用合成能力。</div>
</details>
</div>
<div class="card">
<div class="title">Maximizing Reliability with Bayesian Optimization</div>
<div class="meta-line">Authors: Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</div>
<div class="meta-line">First: 2026-02-02T18:31:58+00:00 · Latest: 2026-02-02T18:31:58+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过贝叶斯优化最大化可靠性</div>
<div class="mono" style="margin-top:8px">贝叶斯优化（BO）是一种流行的、样本高效的昂贵黑箱优化技术。在制造中出现的一个问题是最大化设计的可靠性，或等价地最小化故障概率，该设计受到随机扰动的影响——这是一个可能涉及极其罕见故障的问题（$P_\mathrm{fail} = 10^{-6}-10^{-8}$）。在这项工作中，我们提出了基于汤普森采样和知识梯度的两种BO方法，后者近似于最小化故障概率对数的一步贝叶斯最优策略。这两种方法都结合了重要性采样，以针对极小的故障概率。实证结果表明，所提出的方法在极端和非极端情况下均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of maximizing reliability in manufacturing designs that are subject to random perturbations, particularly when dealing with extremely rare failure probabilities. The authors propose two Bayesian optimization methods based on Thompson sampling and knowledge gradient, which aim to minimize the logarithm of the failure probability while incorporating importance sampling to effectively target very small failure rates. Experimental results demonstrate that these proposed methods outperform existing optimization techniques in both extreme and non-extreme scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过最小化极少失败的概率来提高制造设计的可靠性，这一概率可以低至10^{-6}到10^{-8}。作者提出了两种基于汤普森采样和知识梯度的贝叶斯优化方法，其中后者近似于最小化失败概率对数的最优策略。实验结果表明，这些方法在极端和非极端情况下均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-02T17:51:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过人机协作贝叶斯优化进行个性化图像生成</div>
<div class="mono" style="margin-top:8px">想象爱丽丝心中有一幅特定的图像 $x^\ast$，比如她童年时成长的街道景象。为了生成那幅确切的图像，她通过多轮提示引导生成模型，最终得到了图像 $x^{p*}$。尽管 $x^{p*}$ 与 $x^\ast$ 相当接近，爱丽丝发现使用语言提示很难缩小这个差距。本文旨在通过观察即使在语言达到极限后，人类仍能判断新图像 $x^+$ 是否比 $x^{p*}$ 更接近 $x^\ast$ 来缩小这一差距。基于这一观察，我们开发了 MultiBO（多选优先贝叶斯优化），该方法根据 $x^{p*}$ 精心生成 $K$ 幅新图像，获取用户的优先反馈，利用反馈引导扩散模型，最终生成一组新的 $K$ 幅图像。我们展示了在 $B$ 轮用户反馈内，即使生成模型对 $x^\ast$ 没有信息，也能更接近 $x^\ast$。来自 $30$ 位用户的定性评分，以及与 $5$ 个基线的定量指标比较，显示出良好的结果，表明人类的多选反馈可以有效用于个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve personalized image generation by addressing the limitations of language prompts in guiding generative models. The authors propose a method called MultiBO (Multi-Choice Preferential Bayesian Optimization), which generates multiple images based on user feedback to refine the output iteratively. Experimental results indicate that after several rounds of user feedback, the generated images can significantly closer match the user&#x27;s desired image, demonstrating the effectiveness of human feedback in enhancing the personalization of image generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于生成与个体心理想象紧密匹配的个性化图像的挑战，而仅通过语言提示往往难以实现。作者提出了一种名为MultiBO（多选偏好贝叶斯优化）的方法，该方法基于用户反馈生成多个候选图像，以迭代地优化输出。实验结果表明，在经过几轮用户反馈后，生成的图像可以显著接近用户期望的图像，30名用户的定性评估和与五个基线方法的定量比较证明了人类反馈在个性化图像生成中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Personalized Reward Model for Vision Generation</div>
<div class="meta-line">Authors: Yibin Wang, Yuhang Zang, Feng Han, Jiazi Bu, Yujie Zhou, Cheng Jin, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-02T17:44:21+00:00 · Latest: 2026-02-02T17:44:21+00:00</div>
<div class="meta-line">Comments: Website: https://codegoat24.github.io/UnifiedReward/flex</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02380v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02380v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codegoat24.github.io/UnifiedReward/flex">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一个性化奖励模型用于视觉生成</div>
<div class="mono" style="margin-top:8px">最近多模态奖励模型（RMs）的进展显著推动了视觉生成的发展。现有框架通常采用Bradley-Terry风格的偏好建模或利用生成性VLM作为评判者，随后通过强化学习优化视觉生成模型。然而，当前的RMs存在固有的局限性：它们通常遵循一刀切的范式，假设单一的偏好分布或依赖固定的评估标准。因此，它们对特定内容的视觉线索不敏感，导致与主观和上下文相关的人类偏好系统性不一致。为此，受到人类评估的启发，我们提出了UnifiedReward-Flex，一个统一的个性化奖励模型，用于视觉生成，将奖励建模与灵活和上下文自适应推理相结合。具体而言，给定一个提示和生成的视觉内容，它首先解释语义意图并基于视觉证据进行定位，然后通过在预定义和自生成的高层维度下实例化细粒度标准，动态构建分层评估。我们的训练流程遵循两个阶段：（1）我们首先从先进的闭源VLM中提取结构化、高质量的推理轨迹，以引导SFT，使模型具备灵活和上下文自适应的推理行为；（2）然后我们在精心策划的偏好对上执行直接偏好优化（DPO），进一步增强推理的真实性和区分性对齐。为了验证有效性，我们将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛的结果证明了其优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing multimodal reward models in visual generation, which often adopt a one-size-fits-all approach and fail to account for content-specific visual cues. The authors propose UnifiedReward-Flex, a personalized reward model that combines reward modeling with flexible, context-adaptive reasoning. Their method involves a two-stage training process that distills high-quality reasoning from advanced VLMs and optimizes preferences using curated pairs. Experimental results show that integrating UnifiedReward-Flex into the GRPO framework significantly enhances image and video synthesis performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态奖励模型在视觉生成中的局限性，这些模型通常采用一刀切的方法，未能考虑特定上下文的人类偏好。作者提出了UnifiedReward-Flex，这是一种个性化奖励模型，将奖励建模与灵活的上下文自适应推理相结合。他们的方法包括一个两阶段的训练过程，首先从先进的VLM中提取高质量推理，然后通过直接偏好优化来增强偏好。实验结果表明，将UnifiedReward-Flex集成到GRPO框架中显著提高了图像和视频合成的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Future frame prediction in chest and liver cine MRI using the PCA respiratory motion model: comparing transformers and dynamically trained recurrent neural networks</div>
<div class="meta-line">Authors: Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli</div>
<div class="meta-line">First: 2024-10-08T10:21:43+00:00 · Latest: 2026-02-02T17:21:22+00:00</div>
<div class="meta-line">Comments: 43 pages, 19 figures, revised version (including transformer experiments, evaluation on liver MRI data, statistical analysis...)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05882v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.05882v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Respiratory motion complicates accurate irradiation of thoraco-abdominal tumors in radiotherapy, as treatment-system latency entails target-location uncertainties. This work addresses frame forecasting in chest and liver cine MRI to compensate for such delays. We investigate RNNs trained with online learning algorithms, enabling adaptation to changing respiratory patterns via on-the-fly parameter updates, and transformers, increasingly common in time series forecasting for their ability to capture long-term dependencies. Experiments were conducted using 12 sagittal thoracic and upper-abdominal cine-MRI sequences from ETH Zürich and OvGU. PCA decomposes the Lucas-Kanade optical-flow field into static deformations and low-dimensional time-dependent weights. We compare various methods forecasting the latter: linear filters, population and sequence-specific encoder-only transformers, and RNNs trained with real-time recurrent learning (RTRL), unbiased online recurrent optimization, decoupled neural interfaces, and sparse one-step approximation (SnAp-1). Predicted displacements were used to warp the reference frame and generate future images. Prediction accuracy decreased with the horizon h. Linear regression performed best at short horizons (1.3mm geometrical error at h=0.32s, ETH Zürich data), while RTRL and SnAp-1 outperformed the other algorithms at medium-to-long horizons, with geometrical errors below 1.4mm and 2.8mm on the sequences from ETH Zürich and OvGU (the latter featuring higher motion variability, noise, and lower contrast), respectively. The sequence-specific transformer was competitive for low-to-medium horizons, but transformers remained overall limited by data scarcity and domain shift between datasets. Predicted frames visually resembled the ground truth, with notable errors occurring near the diaphragm at end-inspiration and regions affected by out-of-plane motion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于PCA呼吸运动模型的胸部和肝脏动态MRI未来帧预测：比较变换器和动态训练的递归神经网络</div>
<div class="mono" style="margin-top:8px">呼吸运动使放射治疗中胸腹部肿瘤的准确照射变得复杂，因为治疗系统的延迟导致目标位置的不确定性。本研究针对胸部和肝脏动态MRI中的帧预测，以补偿这些延迟。我们研究了使用在线学习算法训练的递归神经网络（RNN），使其能够通过实时参数更新适应变化的呼吸模式，以及在时间序列预测中越来越常见的变换器，因其能够捕捉长期依赖关系。实验使用了来自苏黎世联邦理工学院和奥斯纳布吕克大学的12个矢状面胸部和上腹部动态MRI序列。PCA将Lucas-Kanade光流场分解为静态变形和低维时间依赖权重。我们比较了预测后者的各种方法：线性滤波器、特定人群和序列的仅编码器变换器，以及使用实时递归学习（RTRL）、无偏在线递归优化、解耦神经接口和稀疏一步近似（SnAp-1）训练的RNN。预测位移用于扭曲参考帧并生成未来图像。预测精度随着预测范围h的增加而降低。在短预测范围内（h=0.32s，苏黎世联邦理工学院数据），线性回归表现最佳（几何误差为1.3mm），而RTRL和SnAp-1在中到长预测范围内优于其他算法，几何误差在苏黎世联邦理工学院和奥斯纳布吕克大学的序列中分别低于1.4mm和2.8mm（后者具有更高的运动变异性、噪声和较低的对比度）。特定序列的变换器在低到中预测范围内具有竞争力，但变换器总体上受到数据稀缺和数据集之间领域转移的限制。预测帧在视觉上与真实值相似，但在吸气末期的膈肌附近和受平面外运动影响的区域出现显著误差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of respiratory motion in radiotherapy, which leads to uncertainties in target location during treatment. The authors compare the effectiveness of recurrent neural networks (RNNs) trained with online learning algorithms and transformers for future frame prediction in chest and liver cine MRI. Experiments using 12 cine-MRI sequences revealed that while linear regression excelled at short prediction horizons, RNNs trained with real-time recurrent learning and sparse one-step approximation outperformed other methods at medium-to-long horizons, achieving geometrical errors below 1.4mm and 2.8mm, respectively, despite challenges such as data scarcity and domain shifts between datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决呼吸运动和治疗系统延迟带来的挑战，提高放射治疗中胸腹部肿瘤照射的准确性。研究采用了对比在线学习算法训练的递归神经网络（RNN）和变换器在胸部和肝脏动态MRI中进行未来帧预测的方法。主要实验结果表明，尽管线性回归在短期预测中表现最佳，但使用实时递归学习和稀疏一步近似方法的RNN在中长期预测中优于其他算法，对于不同数据集的几何误差分别低于1.4mm和2.8mm，尽管变换器由于数据稀缺和领域转移显示出局限性。</div>
</details>
</div>
<div class="card">
<div class="title">No time to train! Training-Free Reference-Based Instance Segmentation</div>
<div class="meta-line">Authors: Miguel Espinosa, Chenhongyi Yang, Linus Ericsson, Steven McDonagh, Elliot J. Crowley</div>
<div class="meta-line">First: 2025-07-03T16:59:01+00:00 · Latest: 2026-02-02T16:47:36+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02798v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02798v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无时间训练！无训练参考基础的实例分割</div>
<div class="mono" style="margin-top:8px">图像分割模型的性能历来受到收集大规模标注数据的高成本的限制。Segment Anything Model (SAM) 通过可提示的、与语义无关的分割范式缓解了这一原始问题，但仍然需要手动视觉提示或复杂的领域依赖提示生成规则来处理新图像。为了减少这一新负担，我们的工作研究了在仅提供一小组参考图像的情况下进行物体分割的任务。我们的关键见解是利用基础模型学习的强语义先验，识别参考图像与目标图像之间的对应区域。我们发现，建立对应关系可以自动生成实例级分割掩码，用于下游任务，并通过一种多阶段、无训练的方法实现我们的想法，该方法包括 (1) 内存库构建；(2) 表示聚合；(3) 语义感知特征匹配。我们的实验显示分割指标显著改善，在 COCO FSOD 上达到最先进的性能 (36.8% nAP)，在 PASCAL VOC Few-Shot 上达到 (71.2% nAP50)，并在跨领域 FSOD 基准上超越现有的无训练方法 (22.4% nAP)。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of image segmentation models that rely on extensive annotated datasets, which are costly to collect. The authors propose a training-free method for object segmentation using only a small set of reference images, leveraging strong semantic priors from foundation models to establish correspondences between reference and target images. Their experimental results demonstrate significant improvements in segmentation metrics, achieving state-of-the-art performance on COCO FSOD with 36.8% nAP, PASCAL VOC Few-Shot with 71.2% nAP50, and surpassing existing training-free methods on the Cross-Domain FSOD benchmark with 22.4% nAP.</div>
<div class="mono" style="margin-top:8px">本研究解决了图像分割模型在收集大规模标注数据时面临的高成本问题。作者提出了一种无训练的方法，通过利用一小组参考图像进行物体分割，利用基础模型的语义先验在参考图像和目标图像之间建立对应关系。实验结果表明，在分割指标上取得了显著改善，在COCO FSOD、PASCAL VOC少样本任务上达到了最先进的性能，并在跨域FSOD基准上超越了现有的无训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image</div>
<div class="meta-line">Authors: Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</div>
<div class="meta-line">First: 2025-09-02T13:17:11+00:00 · Latest: 2026-02-02T16:22:10+00:00</div>
<div class="meta-line">Comments: Project page is at https://learn-to-steer-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02295v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://learn-to-steer-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model&#x27;s internal representations.
  We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model&#x27;s cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces in the cross-attention maps, rather than learning true spatial patterns. We solve this by augmenting our training data with samples generated using prompts with incorrect relation words, which encourages the classifier to avoid linguistic shortcuts and learn spatial patterns from the attention maps. Our method dramatically improves spatial accuracy: from 20% to 61% on FLUX.1-dev and from 7% to 54% on SD2.1 across standard benchmarks. It also generalizes to multiple relations with significantly improved accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于数据驱动的损失函数用于文本到图像的推理时优化</div>
<div class="mono" style="margin-top:8px">文本到图像的扩散模型可以生成惊人的视觉效果，但在儿童认为微不足道的任务中往往失败——例如将狗放在泰迪熊的右侧而不是左侧。当组合变得更加不寻常时——如长颈鹿在飞机上方——这些失败变得更加明显。现有方法试图通过模型微调或使用手工设计的次优损失进行测试时优化来修复这些空间推理失败。我们提出直接从模型的内部表示中学习这些目标，而不是强加我们对空间编码的假设。
我们引入了Learn-to-Steer，一个新颖的框架，学习用于测试时优化的数据驱动目标，而不是手工制作。我们的关键见解是训练一个轻量级分类器，从扩散模型的交叉注意力图中解码空间关系，然后在推理过程中将该分类器作为学习的损失函数。训练这样的分类器面临一个意外的挑战：它们可以通过检测交叉注意力图中的语言痕迹来走捷径，而不是学习真实的空间模式。我们通过用使用错误关系词的提示生成的样本增强训练数据，解决了这个问题，这鼓励分类器避免语言捷径，并从注意力图中学习空间模式。我们的方法显著提高了空间准确性：在FLUX.1-dev上从20%提高到61%，在SD2.1的标准基准上从7%提高到54%。它还在多个关系上具有显著提高的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the spatial reasoning failures in text-to-image diffusion models, which struggle with tasks that require accurate placement of objects. The authors propose a novel framework called Learn-to-Steer, which learns data-driven objectives for test-time optimization instead of relying on handcrafted loss functions. The experimental results demonstrate a significant improvement in spatial accuracy, with performance increasing from 20% to 61% on FLUX.1-dev and from 7% to 54% on SD2.1 across standard benchmarks, showcasing the effectiveness of the learned loss functions in enhancing model performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决文本到图像扩散模型在空间推理方面的失败，这些模型在需要准确放置物体的任务中表现不佳。作者提出了一种名为Learn-to-Steer的新框架，该框架为测试时优化学习数据驱动的损失函数，而不是依赖手工制作的损失函数。通过训练一个轻量级分类器从模型的交叉注意力图中解码空间关系，并增强训练数据以防止语言捷径，该方法显著提高了空间准确性，在FLUX.1-dev上从20%提高到61%，在SD2.1上从7%提高到54%。</div>
</details>
</div>
<div class="card">
<div class="title">Alignment-Aware Model Adaptation via Feedback-Guided Optimization</div>
<div class="meta-line">Authors: Gaurav Bhatt, Aditya Chinchure, Jiawei Zhou, Leonid Sigal</div>
<div class="meta-line">First: 2026-02-02T16:03:16+00:00 · Latest: 2026-02-02T16:03:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02258v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02258v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过反馈引导优化的对齐感知模型适应</div>
<div class="mono" style="margin-top:8px">微调是将基础模型适应于下游任务的主要机制；然而，标准方法在很大程度上孤立地优化任务目标，而未考虑次要但关键的对齐目标（例如，安全性和幻觉避免）。因此，下游微调可能会降低对齐性，并未能纠正先前存在的错位行为。我们提出了一种对齐感知的微调框架，通过基于策略梯度的正则化集成外部对齐信号的反馈。我们的方法引入了一种自适应门控机制，动态平衡每个样本的监督梯度和对齐驱动的梯度，优先考虑不确定或错位的案例，同时允许良好对齐的示例遵循标准的监督更新。该框架进一步学习对完全错位输入的弃权行为，将保守响应直接纳入微调模型中。在一般和特定领域的指令微调基准上的实验表明，在不牺牲下游任务性能的情况下，持续减少有害和幻觉输出。额外分析显示对对抗性微调、基于提示的攻击和不安全初始化的鲁棒性，确立了自适应门控对齐优化作为一种有效的对齐保持和对齐恢复模型适应方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of standard fine-tuning methods for adapting foundation models to downstream tasks, which often neglect critical alignment objectives such as safety and hallucination avoidance. The authors propose an alignment-aware fine-tuning framework that utilizes feedback from an external alignment signal through policy-gradient-based regularization, incorporating an adaptive gating mechanism to balance supervised and alignment-driven gradients on a per-sample basis. Experimental results on various instruction-tuning benchmarks indicate that this approach consistently reduces harmful and hallucinated outputs while maintaining performance on downstream tasks, demonstrating its robustness against adversarial fine-tuning and unsafe initializations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决通常被忽视的关键对齐目标（如安全性和幻觉避免）来改善基础模型在下游任务中的微调过程。作者提出了一种对齐感知的微调框架，该框架利用来自外部对齐信号的反馈，通过基于策略梯度的正则化，结合自适应门控机制，在每个样本的基础上平衡监督和对齐驱动的梯度。实验结果表明，该方法在保持下游任务性能的同时，持续减少有害和幻觉输出，展示了其对抗对抗性微调和不安全初始化的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</div>
<div class="meta-line">Authors: Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, Yong-Jin Liu, Jie Tang</div>
<div class="meta-line">First: 2025-12-05T17:38:55+00:00 · Latest: 2026-02-02T16:00:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05905v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05905v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (a framework toward \textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCAIL：通过上下文学习3D一致性姿态表示实现工作室级角色动画</div>
<div class="mono" style="margin-top:8px">尽管最近取得了一些进展，实现符合工作室级制作标准的角色动画仍然具有挑战性。现有方法可以将运动从驱动视频转移到参考图像，但在涉及复杂运动和跨身份动画的野外场景中，往往无法保持结构保真度和时间一致性。在这项工作中，我们提出了\textbf{SCAIL}（一个旨在通过\textbf{I}n-context \textbf{L}earning实现\textbf{S}tudio级\textbf{C}haracter \textbf{A}nimation的框架），该框架旨在通过两个关键创新来解决这些挑战。首先，我们提出了一种新颖的3D姿态表示，提供了更强大和灵活的运动信号。其次，我们在扩散变换器架构中引入了全上下文姿态注入机制，使得对完整运动序列进行有效的时空推理成为可能。为了符合工作室级要求，我们开发了一个策划的数据管道，确保多样性和质量，并建立了一个全面的基准进行系统评估。实验表明，\textbf{SCAIL}实现了最先进的性能，并推动角色动画朝着工作室级的可靠性和真实感发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve character animation to meet studio-grade production standards, addressing issues of structural fidelity and temporal consistency in complex motion scenarios. The authors present SCAIL, a framework that incorporates a novel 3D pose representation and a full-context pose injection mechanism within a diffusion-transformer architecture to enhance spatio-temporal reasoning. Experimental results demonstrate that SCAIL achieves state-of-the-art performance, significantly advancing character animation towards greater reliability and realism suitable for studio-level requirements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高角色动画以满足专业制作标准，解决现有方法在复杂场景中结构保真度和时间一致性方面的局限性。作者提出了SCAIL框架，利用新颖的3D姿势表示和扩散变换器架构中的全上下文姿势注入机制，以增强运动序列的时空推理。实验结果表明，SCAIL实现了最先进的性能，显著提升了角色动画的可靠性和真实感。</div>
</details>
</div>
<div class="card">
<div class="title">DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation</div>
<div class="meta-line">Authors: Wei Pan, Huiguo He, Hiuyi Cheng, Yilin Shi, Lianwen Jin</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-28T03:58:15+00:00 · Latest: 2026-02-02T15:53:38+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23624v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.23624v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep generative models have advanced text-to-online handwriting generation (TOHG), which aims to synthesize realistic pen trajectories conditioned on textual input and style references. However, most existing methods still primarily focus on character- or word-level generation, resulting in inefficiency and a lack of holistic structural modeling when applied to full text lines. To address these issues, we propose DiffInk, the first latent diffusion Transformer framework for full-line handwriting generation. We first introduce InkVAE, a novel sequential variational autoencoder enhanced with two complementary latent-space regularization losses: (1) an OCR-based loss enforcing glyph-level accuracy, and (2) a style-classification loss preserving writing style. This dual regularization yields a semantically structured latent space where character content and writer styles are effectively disentangled. We then introduce InkDiT, a novel latent diffusion Transformer that integrates target text and reference styles to generate coherent pen trajectories. Experimental results demonstrate that DiffInk outperforms existing state-of-the-art (SOTA) methods in both glyph accuracy and style fidelity, while significantly improving generation efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffInk：一种基于字形和风格的潜在扩散变换器用于文本到在线手写生成</div>
<div class="mono" style="margin-top:8px">深度生成模型推动了文本到在线手写生成（TOHG）的发展，旨在合成基于文本输入和风格参考的真实笔迹轨迹。然而，大多数现有方法仍主要集中在字符或单词级别的生成，导致在应用于完整文本行时效率低下且缺乏整体结构建模。为了解决这些问题，我们提出了DiffInk，这是第一个用于完整行手写生成的潜在扩散变换器框架。我们首先介绍了InkVAE，这是一种新颖的序列变分自编码器，增强了两种互补的潜在空间正则化损失：（1）基于OCR的损失，强制字形级别的准确性，以及（2）保持书写风格的风格分类损失。这种双重正则化产生了一个语义结构化的潜在空间，在该空间中，字符内容和书写者风格有效地解耦。然后，我们介绍了InkDiT，这是一种新颖的潜在扩散变换器，集成了目标文本和参考风格，以生成连贯的笔迹轨迹。实验结果表明，DiffInk在字形准确性和风格保真度方面均优于现有的最先进（SOTA）方法，同时显著提高了生成效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency and structural modeling of text-to-online handwriting generation, which has been limited by existing methods that focus on character- or word-level generation. The authors propose DiffInk, a latent diffusion Transformer framework designed for full-line handwriting generation, utilizing a novel sequential variational autoencoder called InkVAE that incorporates two complementary latent-space regularization losses: an OCR-based loss for glyph-level accuracy and a style-classification loss for preserving writing style. Experimental results indicate that DiffInk surpasses existing state-of-the-art methods in both glyph accuracy and style fidelity, while also enhancing generation efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高文本到在线手写生成的效率和结构建模能力，而现有方法主要集中在字符或单词级别的生成，存在局限性。作者提出了DiffInk，这是一种用于全线手写生成的潜在扩散Transformer框架，采用了一种名为InkVAE的新型序列变分自编码器，结合了两种互补的潜在空间正则化损失，以确保字形准确性和风格保留。实验结果表明，DiffInk在字形准确性和风格保真度方面均超过了当前的最先进方法，同时显著提高了生成效率。</div>
</details>
</div>
<div class="card">
<div class="title">Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification</div>
<div class="meta-line">Authors: Miriam Doh, Aditya Gulati, Corinna Canali, Nuria Oliver</div>
<div class="meta-line">First: 2026-01-15T15:23:38+00:00 · Latest: 2026-02-02T15:50:47+00:00</div>
<div class="meta-line">Comments: 22 pages, 15 figures; v2 - fix typo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11651v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11651v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women&#x27;s faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men&#x27;s; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors&#x27; views.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美学作为结构性伤害：文本到图像生成与分类中的算法外貌主义</div>
<div class="mono" style="margin-top:8px">本文探讨了算法外貌主义——基于外貌的系统性优待——在文本到图像（T2I）生成AI和下游性别分类任务中的表现。通过分析使用Stable Diffusion 2.1和3.5 Medium生成的26,400个合成面孔，我们展示了生成AI模型如何系统性地将面部吸引力与积极属性关联，反之亦然，反映了社会构建的偏见而非基于证据的相关性。此外，我们发现三种性别分类算法在输入面孔的属性上存在显著的性别偏见。我们的研究结果揭示了三种关键伤害：（1）T2I模型中吸引力与积极属性关联的系统性编码；（2）分类系统中的性别差异，女性面孔，特别是那些生成于负面属性的面孔，其误分类率显著高于男性；（3）通过年龄同质化、性别曝光模式和地理简化加剧新模型中的美学约束。这些趋同模式揭示了算法外貌主义作为一种系统基础设施在AI视觉系统中运作，通过表现和识别加剧现有的不平等。
免责声明：本研究包含反映外貌与社会构建属性（包括性别、种族和与社会期望相关的特征）之间刻板印象关联的视觉和文本内容。本研究中发现的任何此类关联均源于嵌入生成AI系统中的偏见，而非经验真理或作者观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates algorithmic lookism, defined as the preferential treatment based on physical appearance, in text-to-image generative AI and gender classification tasks. The authors analyze 26,400 synthetic faces generated using Stable Diffusion 2.1 and 3.5 Medium to reveal that generative AI models link facial attractiveness with positive attributes, reflecting societal biases rather than factual correlations. The study uncovers significant gender bias in classification algorithms, showing that women&#x27;s faces, especially those with negative attributes, experience higher misclassification rates compared to men&#x27;s, highlighting the harmful effects of aesthetic biases in AI systems that exacerbate existing inequalities.</div>
<div class="mono" style="margin-top:8px">本文研究了算法外貌主义，即在文本到图像生成AI和性别分类任务中基于外貌的优待。作者分析了使用Stable Diffusion 2.1和3.5 Medium生成的26,400张合成面孔，揭示这些模型系统性地将面部吸引力与积极特征联系起来，反映了社会偏见而非事实相关性。研究还发现分类算法中存在显著的性别偏见，显示女性面孔，尤其是那些带有负面特征的面孔，遭遇的误分类率高于男性，突显了AI系统中美学偏见的有害影响，加剧了现有的不平等。</div>
</details>
</div>
<div class="card">
<div class="title">Geometry- and Relation-Aware Diffusion for EEG Super-Resolution</div>
<div class="meta-line">Authors: Laura Yao, Gengwei Zhang, Moajjem Chowdhury, Yunmei Liu, Tianlong Chen</div>
<div class="meta-line">First: 2026-02-02T15:44:20+00:00 · Latest: 2026-02-02T15:44:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02238v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几何与关系感知的脑电图超分辨率扩散模型</div>
<div class="mono" style="margin-top:8px">最近的脑电图（EEG）空间超分辨率（SR）方法，通过直接预测可见通道的缺失信号或将潜在扩散生成建模适应于时间数据，虽然显示出质量的改善，但往往缺乏对生理空间结构的感知，从而限制了空间生成性能。为了解决这个问题，我们引入了TopoDiff，一种针对EEG空间超分辨率的几何与关系感知扩散模型。TopoDiff受到人类专家如何解释空间EEG模式的启发，结合了从EEG拓扑表示中派生的拓扑感知图像嵌入，为空间生成提供全球几何上下文，并结合一个动态通道关系图，编码电极间关系并随时间动态演变。该设计产生了一个空间基础的EEG空间超分辨率框架，具有一致的性能提升。在多个涵盖不同应用的EEG数据集上，包括用于情感识别的SEED/SEED-IV、PhysioNet运动想象（MI/MM）和用于癫痫检测的TUSZ，我们的方法在生成保真度上取得了显著提升，并在下游EEG任务性能上带来了显著改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the quality of electroencephalography (EEG) spatial super-resolution methods, which often fail to consider the physiological spatial structure, limiting their performance. The authors propose TopoDiff, a geometry- and relation-aware diffusion model that utilizes topology-aware image embeddings and a dynamic channel-relation graph to improve spatial generation. Experimental results demonstrate that TopoDiff significantly enhances generation fidelity and improves performance across various EEG datasets, including those for emotion recognition and seizure detection.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高脑电图（EEG）空间超分辨率方法的质量，这些方法往往未能考虑生理空间结构，从而限制了其性能。作者提出了TopoDiff，这是一种几何和关系感知的扩散模型，结合了拓扑感知的图像嵌入和动态通道关系图，以改善空间生成。实验结果表明，TopoDiff显著提高了生成的保真度，并在多个EEG应用中改善了性能，包括情感识别和癫痫检测，涵盖了SEED/SEED-IV和PhysioNet等多个数据集。</div>
</details>
</div>
<div class="card">
<div class="title">Show, Don&#x27;t Tell: Morphing Latent Reasoning into Image Generation</div>
<div class="meta-line">Authors: Harold Haodong Chen, Xinxiang Yin, Wen-Jie Shu, Hongfei Zhang, Zixin Zhang, Chenfei Liao, Litao Guo, Qifeng Chen, Ying-Cong Chen</div>
<div class="meta-line">First: 2026-02-02T15:29:48+00:00 · Latest: 2026-02-02T15:29:48+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/EnVision-Research/LatentMorph</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02227v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02227v1">PDF</a> · <a href="https://github.com/EnVision-Research/LatentMorph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>展示，而非叙述：将潜在推理转化为图像生成</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）生成取得了显著进展，但现有方法往往缺乏在生成过程中动态推理和精炼的能力，这是人类创造力的标志。目前的推理增强范式大多依赖于显式思维过程，其中中间推理在固定步骤中解码为离散文本，频繁进行图像解码和重新编码，导致效率低下、信息丢失和认知不匹配。为了解决这一问题，我们提出了LatentMorph，一个将隐式潜在推理无缝集成到T2I生成过程中的新框架。LatentMorph的核心引入了四个轻量级组件：（i）用于将中间生成状态总结为紧凑视觉记忆的凝聚器，（ii）用于将潜在思维转化为可操作指导的翻译器，（iii）用于动态引导下一个图像标记预测的塑形器，以及（iv）经过强化学习训练的调用器，用于自适应确定何时调用推理。通过在连续潜在空间中完全进行推理，LatentMorph避免了显式推理的瓶颈，并实现了更自适应的自我精炼。大量实验表明，LatentMorph（I）在GenEval上将基础模型Janus-Pro的性能提升了16%，在T2I-CompBench上提升了25%；（II）在WISE和IPV-Txt等抽象推理任务上比显式范式（如TwiG）分别提高了15%和11%；（III）同时将推理时间减少了44%，标记消耗减少了51%；（IV）在推理调用上与人类直觉的认知一致性达到71%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current text-to-image generation methods, which often struggle with dynamic reasoning and refinement during the generation process. The authors introduce LatentMorph, a framework that integrates implicit latent reasoning into text-to-image generation, utilizing four components: a condenser for summarizing intermediate states, a translator for converting latent thoughts into guidance, a shaper for steering image predictions, and an RL-trained invoker for determining reasoning invocation. Experimental results show that LatentMorph improves the Janus-Pro model by 16% on GenEval and 25% on T2I-CompBench, outperforms explicit reasoning paradigms by 15% and 11% on abstract reasoning tasks, reduces inference time by 44%, and achieves 71% cognitive alignment with human intuition on reasoning invocation.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过引入动态推理和细化来改善文本到图像生成，这对于人类创造力至关重要。作者提出了一种名为LatentMorph的新框架，该框架通过四个组件将隐式潜在推理集成到生成过程中：用于总结中间状态的浓缩器、将潜在思想转化为指导的翻译器、用于引导图像预测的塑形器，以及用于自适应推理调用的强化学习训练的调用器。实验结果表明，LatentMorph使Janus-Pro模型在GenEval上提升了16%，在T2I-CompBench上提升了25%，在抽象推理任务上超越了显式推理范式15%和11%，减少了44%的推理时间，并在推理调用上与人类直觉的认知一致性达到了71%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation</div>
<div class="meta-line">Authors: Yifu Luo, Xinhao Hu, Keyu Fan, Haoyuan Sun, Zeyu Chen, Bo Xia, Tiantian Zhang, Yongzhe Chang, Xueqian Wang</div>
<div class="meta-line">First: 2025-10-15T11:18:12+00:00 · Latest: 2026-02-02T15:20:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13418v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13418v2">PDF</a> · <a href="https://github.com/xingzhejun/Mask-GRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches. The code is available on https://github.com/xingzhejun/Mask-GRPO</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习与掩蔽生成模型的结合：用于文本到图像生成的Mask-GRPO</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在文本到图像（T2I）生成中受到越来越多的关注。然而，大多数现有的RL方法都是针对扩散模型或自回归模型，忽视了一个重要的替代方案：掩蔽生成模型。在本研究中，我们提出了Mask-GRPO，这是第一个将基于组相对策略优化（GRPO）的RL纳入这一被忽视范式的方法。我们的核心见解是重新定义转移概率，这与当前的方法不同，并将解掩过程表述为一个多步骤决策问题。为了进一步增强我们的方法，我们探索了几种有用的策略，包括去除KL约束、应用降维策略和过滤低质量样本。使用Mask-GRPO，我们改进了基础模型Show-o，在标准T2I基准和偏好对齐上取得了显著提升，超越了现有的最先进方法。代码可在https://github.com/xingzhejun/Mask-GRPO获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing reinforcement learning approaches in text-to-image generation, which primarily focus on diffusion and autoregressive models while neglecting masked generative models. The authors introduce Mask-GRPO, a novel method that integrates Group Relative Policy Optimization (GRPO) into masked generative models by redefining transition probabilities and framing the unmasking process as a multi-step decision-making problem. Experimental results demonstrate that Mask-GRPO significantly enhances the performance of the base model, Show-o, achieving notable improvements on standard text-to-image benchmarks and preference alignment, surpassing current state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过将强化学习与被遮蔽生成模型相结合，提升文本到图像生成的效果，而这一领域的现有方法大多忽视了这一重要的替代方案。作者提出了Mask-GRPO，这是一种新颖的方法，利用群体相对策略优化（GRPO）重新定义转移概率，并将去遮蔽过程视为一个多步骤决策问题。实验结果表明，Mask-GRPO显著提升了基础模型Show-o的性能，在标准文本到图像基准测试和偏好对齐方面取得了显著进展，超越了当前的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Generating Physically Sound Designs from Text and a Set of Physical Constraints</div>
<div class="meta-line">Authors: Gregory Barber, Todd C. Henry, Mulugeta A. Haile</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-02T15:19:00+00:00 · Latest: 2026-02-02T15:19:00+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02213v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design&#x27;s visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从文本和一组物理约束生成物理合理的设计</div>
<div class="mono" style="margin-top:8px">我们提出了TIDES，一种基于文本描述和一组物理约束生成物理合理设计的文本信息设计方法。TIDES联合优化结构（拓扑）和视觉属性。使用预训练的文本-图像模型来测量设计与文本提示的视觉对齐，并使用可微分的物理模拟器来测量其物理性能。我们在一系列结构优化问题上评估TIDES，这些问题在不同的载荷和支撑条件下、不同的分辨率下进行，并通过在实验室中对挤出和3D打印的2D梁设计进行三点弯曲测试进行实验。我们发现它可以联合优化这两个目标，并返回满足工程设计要求（合规性和密度）的设计，同时利用文本指定的特征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a method for generating designs that are both visually appealing and physically sound based on textual descriptions and specified physical constraints. The authors introduce TIDES, which employs a pre-trained text-image model to assess visual alignment with text prompts and a differentiable physics simulator to evaluate physical performance, optimizing both structural and visual properties. Experimental results demonstrate that TIDES effectively balances these objectives, producing designs that meet engineering requirements for compliance and density while incorporating features derived from the textual input, validated through structural optimization problems and physical testing such as the 3-point bending test on 2D beam designs.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种基于文本描述和指定物理约束生成视觉吸引且物理合理的设计的方法。所提出的方法TIDES利用预训练的文本-图像模型确保与文本提示的视觉一致性，并使用可微分物理模拟器评估物理性能。实验结果表明，TIDES有效地优化了结构和视觉属性，生成满足合规性和密度等工程要求的设计，同时融入了文本输入中提取的特征。</div>
</details>
</div>
<div class="card">
<div class="title">RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</div>
<div class="meta-line">Authors: Yan Chen, Yi Wen, Wei Li, Junchao Liu, Yong Guo, Jie Hu, Xinghao Chen</div>
<div class="meta-line">First: 2025-08-26T16:06:17+00:00 · Latest: 2026-02-02T15:12:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.19154v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.19154v2">PDF</a> · <a href="https://github.com/YanCHEN-fr/RDDM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and image generation. These models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM obviates this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP)-&gt;Image Restoration (IR) pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts many challenges. To this end, we propose: (1) a RAW-domain VAE (RVAE), encoding sensor RAW and decoding it into an enhanced linear domain image, to solve the out-of-distribution (OOD) issues between the different domain distributions; (2) a configurable multi-bayer (CMB) LoRA module, adapting diverse RAW Bayer patterns such as RGGB, BGGR, etc. To compensate for the deficiency in the dataset, we develop a scalable data synthesis pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Extensive experiments demonstrate RDDM&#x27;s superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts. Codes will be publicly available at https://github.com/YanCHEN-fr/RDDM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RDDM：实践RAW域扩散模型用于真实世界图像恢复</div>
<div class="mono" style="margin-top:8px">我们提出了RAW域扩散模型（RDDM），这是一种端到端的扩散模型，能够直接从传感器RAW数据恢复照片级真实图像。尽管最近的sRGB域扩散方法取得了令人印象深刻的结果，但它们在高保真度和图像生成之间陷入了困境。这些模型处理有损的sRGB输入，并忽视了在许多场景中传感器RAW图像的可获取性，例如在边缘设备中的图像和视频捕捉，导致性能不佳。RDDM通过直接在RAW域恢复图像，消除了这一限制，取代了传统的两阶段图像信号处理（ISP）-&gt;图像恢复（IR）流程。然而，简单地将预训练的扩散模型适应到RAW域面临许多挑战。为此，我们提出：（1）一个RAW域变分自编码器（RVAE），对传感器RAW进行编码并解码为增强的线性域图像，以解决不同域分布之间的分布外（OOD）问题；（2）一个可配置的多拜耳（CMB）LoRA模块，适应多种RAW拜耳模式，如RGGB、BGGR等。为了弥补数据集的不足，我们开发了一个可扩展的数据合成管道，从现有的sRGB数据集中合成RAW LQ-HQ对，以进行大规模训练。大量实验表明，RDDM在高于最先进的sRGB扩散方法方面具有优势，产生更高保真的结果且伪影更少。代码将公开发布在https://github.com/YanCHEN-fr/RDDM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to address the limitations of existing sRGB-domain diffusion methods in restoring images from sensor RAW data, which often leads to a compromise between fidelity and generation quality. The authors introduce the RAW domain diffusion model (RDDM), an end-to-end approach that directly restores images in the RAW domain, thereby eliminating the traditional two-stage image signal processing and restoration pipeline. Key experimental findings indicate that RDDM outperforms state-of-the-art sRGB diffusion methods, achieving higher fidelity results with fewer artifacts, supported by a scalable data synthesis pipeline for training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有sRGB域扩散模型在从传感器RAW数据恢复高保真图像时的局限性，这在实际应用中常常被忽视。作者提出了RAW域扩散模型（RDDM），该模型直接在RAW域中恢复图像，从而消除了传统的两阶段图像信号处理和图像恢复流程。关键实验结果表明，RDDM在图像保真度上优于最先进的sRGB扩散方法，并减少了伪影，得益于有效训练的可扩展数据合成管道。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-02-02T15:09:51+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>文本到图像扩散中的批评引导强化遗忘</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型中的机器遗忘旨在去除目标概念，同时保持整体效用。先前的扩散遗忘方法通常依赖于监督权重编辑或全局惩罚；强化学习（RL）方法虽然灵活，但通常优化稀疏的轨迹末端奖励，导致高方差更新和弱信用分配。我们提出了一种通用的RL框架，用于扩散遗忘，将去噪视为一个顺序决策过程，并引入具有噪声步奖励的时间步感知批评者。具体而言，我们在噪声潜变量上训练基于CLIP的奖励预测器，并使用其每步信号计算反向扩散核的策略梯度更新的优势估计。我们的算法易于实现，支持离策略重用，并可与标准文本到图像骨干网络结合。在多个概念上，该方法实现了比强基线更好或可比的遗忘，同时保持图像质量和良好的提示保真度；消融实验表明（i）每步批评者和（ii）噪声条件奖励是稳定性和有效性的关键。我们发布了代码和评估脚本，以促进可重复性和未来基于RL的扩散遗忘研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance machine unlearning in text-to-image diffusion models by effectively removing targeted concepts while maintaining overall utility. The authors propose a reinforcement learning framework that treats denoising as a sequential decision-making process and incorporates a timestep-aware critic with noisy-step rewards. The experimental results demonstrate that their method achieves comparable or superior forgetting of concepts compared to strong baselines, while preserving image quality and prompt fidelity, with key findings indicating that per-step critics and noisy-conditioned rewards are crucial for the method&#x27;s stability and effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究解决了文本到图像扩散模型中的机器遗忘问题，旨在有效去除特定概念，同时保持整体模型的实用性。作者提出了一种强化学习框架，将去噪过程视为一个顺序决策任务，结合了使用噪声步奖励的时间步感知评论家。实验结果表明，该方法在概念遗忘方面与现有强基线相比具有可比或更优的表现，同时保持图像质量和提示保真度，关键发现表明使用逐步评论家和噪声条件奖励显著增强了稳定性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PIQL: Projective Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Xinchen Han, Hossam Afifi, Michel Marot</div>
<div class="meta-line">First: 2025-01-15T16:17:02+00:00 · Latest: 2026-02-02T14:52:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.08907v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.08907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline Reinforcement Learning (RL) faces a fundamental challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) employs expectile regression to achieve in-sample learning. Nevertheless, IQL relies on a fixed expectile hyperparameter and a density-based policy improvement method, both of which impede its adaptability and performance. In this paper, we propose Projective IQL (PIQL), a projective variant of IQL enhanced with a support constraint. In the policy evaluation stage, PIQL substitutes the fixed expectile hyperparameter with a projection-based parameter and extends the one-step value estimation to a multi-step formulation. In the policy improvement stage, PIQL adopts a support constraint instead of a density constraint, ensuring closer alignment with the policy evaluation. Theoretically, we demonstrate that PIQL maintains the expectile regression and in-sample learning framework, guarantees monotonic policy improvement, and introduces a progressively more rigorous criterion for advantageous actions. Experiments on D4RL and NeoRL2 benchmarks demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIQL：具有支持约束的投影隐式Q学习用于离线强化学习</div>
<div class="mono" style="margin-top:8px">离线强化学习（RL）面临着由分布外（OOD）动作引起的外推误差的基本挑战。隐式Q学习（IQL）采用期望回归实现样本内学习。然而，IQL依赖于固定的期望超参数和基于密度的策略改进方法，这两者都妨碍了其适应性和性能。本文提出了投影IQL（PIQL），这是IQL的一种投影变体，增强了支持约束。在策略评估阶段，PIQL用基于投影的参数替代固定的期望超参数，并将一步值估计扩展为多步公式。在策略改进阶段，PIQL采用支持约束而不是密度约束，确保与策略评估的更紧密对齐。从理论上讲，我们证明了PIQL保持期望回归和样本内学习框架，保证单调策略改进，并引入了一个逐步更严格的有利动作标准。在D4RL和NeoRL2基准上的实验表明，在不同领域中实现了稳健的增益，整体上达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the extrapolation errors in Offline Reinforcement Learning (RL) caused by out-of-distribution actions, which hinder the performance of existing methods like Implicit Q-Learning (IQL). The authors propose Projective IQL (PIQL), which modifies IQL by replacing the fixed expectile hyperparameter with a projection-based parameter and extending the value estimation to a multi-step approach, while also implementing a support constraint during policy improvement. Experimental results on D4RL and NeoRL2 benchmarks show that PIQL achieves significant performance improvements across various domains, establishing state-of-the-art results in Offline RL.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决离线强化学习中由于分布外动作引起的外推误差，这妨碍了现有方法（如隐式Q学习IQL）的性能。作者提出了项目隐式Q学习（PIQL），通过用基于投影的参数替代固定的期望分位数超参数，并将价值估计扩展到多步方法，同时在策略改进中实施支持约束，从而增强了IQL。D4RL和NeoRL2基准上的实验结果表明，PIQL在各个领域实现了显著的性能提升，确立了离线强化学习的最新成果。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics</div>
<div class="meta-line">Authors: Sangwoo Shin, BumJun Kim, Kyelim Lee, Moongyu Jeon, Albert No</div>
<div class="meta-line">First: 2026-02-02T14:17:08+00:00 · Latest: 2026-02-02T14:17:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive language models (ARMs) suffer from the reversal curse: after learning that &quot;$A$ is $B$&quot;, they often fail on the reverse query &quot;$B$ is $A$&quot;. Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing &quot;[MASK] is $B$&quot; during training does not necessarily teach the model to handle the reverse prompt &quot;$B$ is [MASK]&quot;. We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过注意力和训练动态理解掩蔽扩散模型中的逆转诅咒缓解</div>
<div class="mono" style="margin-top:8px">自回归语言模型（ARMs）遭受逆转诅咒：在学习到&quot;$A$是$B$&quot;后，它们通常在反向查询&quot;$B$是$A$&quot;上失败。基于掩蔽扩散的语言模型（MDMs）在更弱的形式上表现出这种失败，但其根本原因仍不清楚。一个常见的解释将这种缓解归因于任意顺序的训练目标。然而，在训练期间观察&quot;[MASK]是$B$&quot;并不一定教会模型处理反向提示&quot;$B$是[MASK]&quot;。我们表明，这种缓解源于架构结构及其与训练的相互作用。在一个单层Transformer编码器中，权重共享通过使前向和反向注意力分数正相关来耦合这两个方向。在相同的设置中，我们进一步表明，相应的梯度是对齐的，因此最小化前向损失也会减少反向损失。在受控玩具任务和大规模扩散语言模型上的实验支持这些机制，解释了为什么MDMs部分克服了在强ARMs中持续存在的失败模式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the reversal curse in autoregressive language models, where models struggle with reverse queries after learning direct relationships. The authors focus on masked diffusion-based language models to understand why they exhibit this issue to a lesser extent. They employ a one-layer Transformer encoder to demonstrate that the architectural structure and training dynamics, specifically weight sharing and aligned gradients, contribute to the mitigation of the reversal curse. Experimental results from controlled tasks and large-scale models confirm that these mechanisms help MDMs partially overcome the limitations seen in traditional ARMs.</div>
<div class="mono" style="margin-top:8px">本研究探讨了自回归语言模型中的反转诅咒，即模型在学习关系后在反向查询中遇到困难。研究重点是掩蔽扩散语言模型，这种模型在较小程度上表现出这一问题，并试图澄清这种缓解的原因。通过分析架构结构和训练动态，作者证明了一层变换器编码器中的权重共享导致正相关的前向和反向查询注意力分数，并且梯度是对齐的，从而允许前向和反向损失的同时最小化。来自受控任务和大规模模型的实验结果证实了这些发现，为掩蔽扩散模型如何部分缓解自回归模型中持续存在的问题提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics</div>
<div class="meta-line">Authors: Nima Shoghi, Yuxuan Liu, Yuning Shen, Rob Brekelmans, Pan Li, Quanquan Gu</div>
<div class="meta-line">First: 2026-02-02T14:13:28+00:00 · Latest: 2026-02-02T14:13:28+00:00</div>
<div class="meta-line">Comments: For associated project page, see https://bytedance-seed.github.io/ConfRover/starmd</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bytedance-seed.github.io/ConfRover/starmd">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD&#x27;s joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展的时空SE(3)扩散模型用于长时间尺度蛋白质动力学</div>
<div class="mono" style="margin-top:8px">分子动力学（MD）模拟仍然是研究蛋白质动力学的金标准，但其计算成本限制了对生物相关时间尺度的访问。最近的生成模型在加速模拟方面显示出希望，但由于架构限制、误差积累和时空动力学建模不足，它们在长时间生成方面面临挑战。我们提出了STAR-MD（分子动力学的时空自回归展开），这是一种可扩展的SE(3)等变扩散模型，能够在微秒时间尺度上生成物理上合理的蛋白质轨迹。我们的关键创新是一个因果扩散变换器，具有联合时空注意力，能够有效捕捉复杂的时空依赖关系，同时避免现有方法的内存瓶颈。在标准ATLAS基准测试中，STAR-MD在所有指标上都达到了最先进的性能——相比于之前的方法，显著提高了构象覆盖率、结构有效性和动态保真度。STAR-MD成功外推生成稳定的微秒级轨迹，而基线方法则在此失败，且在扩展展开过程中保持高结构质量。我们的综合评估揭示了当前模型在长时间生成方面的严重局限，同时证明了STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of molecular dynamics simulations in studying protein dynamics due to their high computational costs and challenges in long-horizon generation. The authors introduce STAR-MD, a scalable SE(3)-equivariant diffusion model that utilizes a causal diffusion transformer with joint spatio-temporal attention to generate physically plausible protein trajectories over microsecond timescales. Experimental results demonstrate that STAR-MD outperforms existing methods on the ATLAS benchmark, achieving superior conformational coverage, structural validity, and dynamic fidelity, while successfully generating stable microsecond-scale trajectories that maintain high structural quality throughout the simulation process.</div>
<div class="mono" style="margin-top:8px">本研究解决了分子动力学模拟在研究蛋白质动态方面的局限性，主要是由于其高计算成本和在长时间生成中的挑战。作者提出了STAR-MD，这是一种可扩展的SE(3)等变扩散模型，利用具有联合时空注意力的因果扩散变换器生成微秒级的物理合理蛋白质轨迹。在ATLAS基准测试中的实验结果表明，STAR-MD在构象覆盖、结构有效性和动态保真度方面优于现有方法，成功生成稳定的微秒级轨迹，同时保持高结构质量，从而突显了增强蛋白质功能探索的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies</div>
<div class="meta-line">Authors: Olga Graf, Dhrupal Patel, Peter Groß, Charlotte Lempp, Matthias Hein, Fabian Heinemann</div>
<div class="meta-line">First: 2026-02-02T14:07:33+00:00 · Latest: 2026-02-02T14:07:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02124v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过类感知马哈拉诺比斯距离评估已知和新型异常的临床前组织病理学毒性</div>
<div class="mono" style="margin-top:8px">药物诱导的毒性仍然是临床前开发和早期临床试验失败的主要原因。早期检测不良反应对于减少流失和加速安全药物的开发至关重要。组织病理学评估仍然是毒性评估的金标准，但它在很大程度上依赖于专家病理学家，造成大规模筛查的瓶颈。为了解决这一挑战，我们引入了一种基于人工智能的异常检测框架，用于毒理学研究中啮齿动物肝脏的组织病理学全切片图像（WSIs）。该系统识别健康组织和已知病理（异常），并且有训练数据可用。此外，它还可以将稀有病理作为分布外（OOD）发现进行检测，而无需训练数据。我们生成了一个新的数据集，包含健康组织和已知病理的逐像素注释，并利用这些数据通过低秩适应（LoRA）微调预训练的视觉变换器（DINOv2）以进行组织分割。最后，我们使用马哈拉诺比斯距离提取OOD检测的特征。为了更好地考虑组织学数据中的类依赖性变异性，我们建议使用类特定阈值。我们通过假阴性和假阳性率的均值优化阈值，结果仅有0.16\%的病理组织被分类为健康，0.35\%的健康组织被分类为病理。应用于具有已知毒理学发现的小鼠肝脏WSIs，该框架准确检测异常，包括稀有的OOD形态。这项工作展示了人工智能驱动的组织病理学在支持临床前工作流程、减少后期失败和提高药物开发效率方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of drug-induced toxicity, a major cause of failure in preclinical development and early clinical trials, by improving the detection of adverse effects through histopathological evaluation. The authors developed an AI-based anomaly detection framework that utilizes a novel dataset of pixelwise annotations to fine-tune a pre-trained Vision Transformer for tissue segmentation, and employs Mahalanobis distance for out-of-distribution detection. The key findings indicate that the system effectively identifies known pathologies and rare anomalies with a very low misclassification rate, achieving only 0.16% of pathological tissue misclassified as healthy and 0.35% of healthy tissue misclassified as pathological, thereby demonstrating the potential of AI in enhancing preclinical workflows and drug development efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决药物诱导毒性问题，这在临床前开发和临床试验中是导致失败的重要原因，通过改善不良反应的早期检测来应对这一挑战。作者开发了一种基于人工智能的异常检测框架，针对啮齿动物肝脏的组织病理全切片图像，利用一种新的像素级注释数据集，并通过低秩适应对预训练的视觉变换器进行微调以进行组织分割。主要发现表明，该框架有效识别已知病理和稀有的分布外异常，误分类率低，仅有0.16%的病理组织被误分类为健康，0.35%的健康组织被误分类为病理，从而展示了其提升临床前工作流程和药物开发效率的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training</div>
<div class="meta-line">Authors: Xin Ding, Yun Chen, Sen Zhang, Kao Zhang, Nenglun Chen, Peibei Cao, Yongwei Wang, Fei Wu</div>
<div class="meta-line">First: 2026-02-02T13:55:49+00:00 · Latest: 2026-02-02T13:55:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02114v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过矩阵形式的EDM和自适应邻域训练增强基于扩散的定量可控图像生成</div>
<div class="mono" style="margin-top:8px">连续条件扩散模型（CCDM）是一种基于扩散的框架，旨在生成基于连续回归标签的高质量图像。尽管CCDM在多个数据集上相较于先前的方法显示出明显优势，但仍存在显著局限性，并且最近被一种基于GAN的方法，即CcGAN-AVAR所超越。这些局限性主要源于其依赖于过时的扩散框架以及由于长采样轨迹导致的低采样效率。为了解决这些问题，我们提出了一种改进的CCDM框架，称为iCCDM，该框架结合了更先进的\textit{阐明扩散模型}（EDM）框架，并进行了 substantial modifications，以提高生成质量和采样效率。具体而言，iCCDM引入了一种新颖的矩阵形式EDM公式以及自适应邻域训练策略。在四个基准数据集上的大量实验，涵盖从$64\times64$到$256\times256$的图像分辨率，证明iCCDM始终优于现有方法，包括最先进的大规模文本到图像扩散模型（例如，Stable Diffusion 3、FLUX.1和Qwen-Image），在显著降低采样成本的同时实现更高的生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the performance of diffusion-based image generation models, particularly the Continuous Conditional Diffusion Model (CCDM), which has been outperformed by GAN-based methods due to its outdated framework and low sampling efficiency. The authors propose an improved framework called iCCDM, which integrates the advanced Elucidated Diffusion Model (EDM) and introduces a novel matrix-form EDM formulation along with an adaptive vicinal training strategy. Experimental results across four benchmark datasets show that iCCDM consistently surpasses existing methods, including state-of-the-art large-scale text-to-image models, by achieving higher image generation quality and significantly reducing sampling costs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善连续条件扩散模型（CCDM）以生成高质量图像，因其在性能上已被基于GAN的方法超越，主要由于其过时的框架和低采样效率。作者提出了一种改进版本，称为iCCDM，结合了先进的阐明扩散模型（EDM），并引入了一种新颖的矩阵形式的公式以及自适应近邻训练策略。跨越四个基准数据集的实验结果表明，iCCDM在图像生成质量上持续超越现有方法，包括领先的文本到图像扩散模型，同时显著降低了采样成本。</div>
</details>
</div>
<div class="card">
<div class="title">FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space</div>
<div class="meta-line">Authors: FSVideo Team, Qingyu Chen, Zhiyuan Fang, Haibin Huang, Xinwei Huang, Tong Jin, Minxuan Lin, Bo Liu, Celong Liu, Chongyang Ma, Xing Mei, Xiaohui Shen, Yaojie Shen, Fuwen Tan, Angtian Wang, Xiao Yang, Yiding Yang, Jiamin Yuan, Lingxi Zhang, Yuxin Zhang</div>
<div class="meta-line">First: 2026-02-02T13:37:38+00:00 · Latest: 2026-02-02T13:37:38+00:00</div>
<div class="meta-line">Comments: Project Page: https://kingofprank.github.io/fsvideo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02092v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02092v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kingofprank.github.io/fsvideo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FSVideo：高压缩潜在空间中的快速视频扩散模型</div>
<div class="mono" style="margin-top:8px">我们介绍了FSVideo，一个基于变换器的快速图像到视频（I2V）扩散框架。我们的框架建立在以下关键组件之上：1.) 一个具有高压缩潜在空间的新视频自编码器（$64\times64\times4$时空下采样比），实现了具有竞争力的重建质量；2.) 一个扩散变换器（DIT）架构，具有新的层内存设计，以增强DIT内层间信息流和上下文重用；3.) 通过少量步骤的DIT上采样器实现的多分辨率生成策略，以提高视频保真度。我们的最终模型包含一个14B DIT基础模型和一个14B DIT上采样器，在性能上与其他流行的开源模型相当，同时速度快一个数量级。我们在本报告中讨论了我们的模型设计和训练策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a fast and efficient image-to-video diffusion framework that can operate in a highly-compressed latent space. The authors introduce FSVideo, which utilizes a new video autoencoder with a significant spatial-temporal downsampling ratio, a diffusion transformer architecture with enhanced inter-layer information flow, and a multi-resolution generation strategy. Experimental results show that FSVideo achieves competitive performance compared to other open-source models while being an order of magnitude faster, demonstrating its effectiveness in generating high-fidelity videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种快速高效的图像到视频扩散框架，同时在高度压缩的潜在空间中保持高视频质量。作者提出了FSVideo，该框架结合了一种具有显著时空下采样比的新视频自编码器、旨在改善信息流的扩散变换器架构，以及一种多分辨率生成策略以增强视频保真度。实验结果表明，FSVideo包含一个140亿参数的DIT基础模型和一个140亿参数的DIT上采样器，其性能与现有模型相当，同时速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance</div>
<div class="meta-line">Authors: Hyunsuk Chung, Caren Han, Yerin Choi, Seungyeon Ji, Jinwoo Kim, Eun-Jung Holden, Kyungreem Han</div>
<div class="meta-line">First: 2026-02-02T13:00:57+00:00 · Latest: 2026-02-02T13:00:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FiLoRA：可控特征依赖的聚焦与忽略LoRA</div>
<div class="mono" style="margin-top:8px">多模态基础模型整合了跨模态的异构信号，但其预测如何依赖特定内部特征组以及这种依赖是否可以有意控制仍然不甚了解。现有的快捷和虚假行为研究主要依赖事后分析或特征移除，提供的见解有限，无法判断在不改变任务语义的情况下是否可以调节依赖。我们引入FiLoRA（聚焦与忽略LoRA），这是一种指令条件的、参数高效的适应框架，能够在保持预测目标不变的情况下显式控制内部特征依赖。FiLoRA将适应分解为特征组对齐的LoRA模块，并应用指令条件门控，使自然语言指令作为计算级控制信号，而不是任务重新定义。在文本-图像和音频-视觉基准测试中，我们展示了指令条件门控在内部计算中引发一致且因果的变化，选择性地放大或抑制核心和虚假特征组，而不修改标签空间或训练目标。进一步分析表明，FiLoRA在虚假特征干预下表现出更好的鲁棒性，揭示了一种超越相关性驱动学习的依赖调节机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to understand and control how multimodal foundation models rely on specific internal feature groups during predictions. The authors introduce FiLoRA, a parameter-efficient adaptation framework that utilizes instruction-conditioned gating to enable explicit control over feature reliance while maintaining the original predictive objective. Experimental results across text-image and audio-visual benchmarks demonstrate that this method allows for consistent and causal shifts in internal computation, effectively amplifying or suppressing certain feature groups, and improving robustness against spurious feature interventions without altering the task semantics.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解和控制多模态基础模型在预测中对特定内部特征组的依赖。作者提出了FiLoRA，这是一种参数高效的适应框架，利用指令条件门控来显式控制特征依赖，同时保持预测目标不变。跨文本-图像和音频-视觉基准的实验结果表明，该方法能够一致且因果地改变内部计算，有效地放大或抑制某些特征组，并在不改变训练目标的情况下提高对虚假特征干预的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SIDiffAgent: Self-Improving Diffusion Agent</div>
<div class="meta-line">Authors: Shivank Garg, Ayush Singh, Gaurav Kumar Nayak</div>
<div class="meta-line">First: 2026-02-02T12:53:21+00:00 · Latest: 2026-02-02T12:53:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02051v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02051v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse&quot; as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIDiffAgent：自我改进的扩散代理</div>
<div class="mono" style="margin-top:8px">文本到图像的扩散模型彻底改变了生成性人工智能，使高质量和照片级真实感的图像合成成为可能。然而，它们的实际部署仍受到多种限制的阻碍：对提示措辞的敏感性、语义解释的模糊性（例如，“鼠标”作为动物与计算机外设的区别）、扭曲解剖等伪影，以及对精心设计的输入提示的需求。现有方法通常需要额外的训练，并提供有限的可控性，限制了它们在现实应用中的适应性。我们介绍了自我改进的扩散代理（SIDiffAgent），这是一个无训练的代理框架，利用Qwen系列模型（Qwen-VL、Qwen-Image、Qwen-Edit、Qwen-Embedding）来解决这些挑战。SIDiffAgent自主管理提示工程，检测并纠正不良生成，并执行细粒度的伪影去除，从而产生更可靠和一致的输出。它进一步通过在数据库中存储先前经验的记忆来实现迭代自我改进。这个过去经验的数据库随后用于在代理管道的每个阶段注入基于提示的指导。我们的模型在GenAIBench上实现了平均VQA得分0.884，显著超越了开源、专有模型和代理方法。我们将在接受后公开发布我们的代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to enhance the practical deployment of text-to-image diffusion models, which face challenges such as sensitivity to prompt phrasing and the generation of artifacts. The authors introduce the Self-Improving Diffusion Agent (SIDiffAgent), a training-free framework that utilizes the Qwen family of models to autonomously manage prompt engineering, detect and correct poor image generations, and remove artifacts. Experimental results show that SIDiffAgent achieved an average VQA score of 0.884 on GenAIBench, significantly surpassing both open-source and proprietary models, demonstrating its effectiveness in producing reliable and consistent outputs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于增强文本到图像扩散模型的实际应用，这些模型面临着对提示措辞敏感和生成伪影等挑战。作者提出了自我改进扩散代理（SIDiffAgent），这是一种无训练的框架，利用Qwen系列模型自主管理提示工程、纠正不良生成并去除伪影。实验结果表明，SIDiffAgent在GenAIBench上获得了0.884的平均VQA分数，显著优于现有的开源和专有模型，表明其在生成可靠和一致的输出方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving</div>
<div class="meta-line">Authors: Guosheng Zhao, Yaozeng Wang, Xiaofeng Wang, Zheng Zhu, Tingdong Yu, Guan Huang, Yongchen Zai, Ji Jiao, Changliang Xue, Xiaole Wang, Zhen Yang, Futang Zhu, Xingang Wang</div>
<div class="meta-line">First: 2026-02-02T12:02:27+00:00 · Latest: 2026-02-02T12:02:27+00:00</div>
<div class="meta-line">Comments: 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02002v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniDriveDreamer：一种用于自动驾驶的单阶段多模态世界模型</div>
<div class="mono" style="margin-top:8px">世界模型在自动驾驶的数据合成中展现了显著的潜力。然而，现有方法主要集中在单一模态生成，通常专注于多摄像头视频或激光雷达序列合成。本文提出了UniDriveDreamer，一种用于自动驾驶的单阶段统一多模态世界模型，能够直接生成多模态未来观测，而无需依赖中间表示或级联模块。我们的框架引入了一种针对激光雷达的变分自编码器（VAE），用于编码输入的激光雷达序列，同时还包括一个用于多摄像头图像的视频VAE。为了确保跨模态兼容性和训练稳定性，我们提出了统一潜在锚定（ULA），明确对齐两种模态的潜在分布。对齐的特征通过扩散变换器融合并处理，联合建模它们的几何对应关系和时间演变。此外，结构化场景布局信息作为条件信号按模态投影，以指导合成。大量实验表明，UniDriveDreamer在视频和激光雷达生成方面均优于之前的最先进方法，同时在下游任务中也取得了可测量的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance data synthesis for autonomous driving by addressing the limitations of existing methods that focus on single-modality generation. The authors propose UniDriveDreamer, a single-stage unified multimodal world model that generates multimodal future observations directly, utilizing a LiDAR-specific variational autoencoder and a video variational autoencoder, while ensuring cross-modal compatibility through Unified Latent Anchoring. Experimental results indicate that UniDriveDreamer surpasses previous state-of-the-art techniques in both video and LiDAR generation, demonstrating significant improvements in downstream tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法集中于单一模态生成的局限性，来增强自动驾驶中的数据合成。作者提出了UniDriveDreamer，这是一种单阶段统一的多模态世界模型，能够直接生成多模态的未来观察，利用了针对LiDAR的变分自编码器和视频变分自编码器。关键实验结果表明，UniDriveDreamer在生成视频和LiDAR数据方面超越了之前的最先进方法，同时在下游任务中也取得了性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models</div>
<div class="meta-line">Authors: Pablo Domingo-Gregorio, Javier Ruiz-Hidalgo</div>
<div class="meta-line">First: 2026-02-02T11:47:48+00:00 · Latest: 2026-02-02T11:47:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01991v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01991v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用潜在向量预测实现图像生成中的局部控制</div>
<div class="mono" style="margin-top:8px">扩散模型作为文本到图像生成的主要方法，通过文本描述生成高质量图像。然而，仅通过文本实现对所需图像的详细控制仍然是一项繁琐的试错工作。最近的方法引入了图像级控制与文本提示，利用先前的图像提取条件信息，如边缘、分割和深度图。尽管有效，这些方法在整个图像上均匀应用条件，限制了局部控制。本文提出了一种新方法，使用户定义的图像区域能够实现精确的局部控制，同时将剩余区域的生成任务留给扩散模型，根据原始提示自主生成。我们的方法引入了一种新的训练框架，结合了掩膜特征和额外的损失项，利用在任何扩散步骤中初始潜在向量的预测来增强当前步骤与潜在空间中最终样本之间的对应关系。大量实验表明，我们的方法有效合成了具有受控局部条件的高质量图像。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve localized control in image generation using diffusion models, which traditionally rely on text prompts and can be cumbersome for achieving specific details. The authors propose a novel methodology that allows for precise control over user-defined regions of an image while enabling the diffusion model to generate the remaining areas autonomously. Their approach includes a new training framework that utilizes masking features and an additional loss term to enhance the correspondence between the current diffusion step and the final sample in the latent space. Experimental results show that this method successfully synthesizes high-quality images with effective local conditions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善使用扩散模型进行图像生成时的局部控制，传统方法主要依赖文本提示，通常需要大量试错才能获得详细结果。作者提出了一种新方法，允许对图像的特定区域进行精确控制，同时使扩散模型能够根据原始提示自主生成其余区域。他们的方法包括一个新的训练框架，利用掩膜特征和额外的损失项来增强当前扩散步骤与潜在空间中最终样本之间的关系。实验结果表明，该方法成功合成了具有有效局部控制的高质量图像。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images</div>
<div class="meta-line">Authors: Shuai Yang, Ziyue Huang, Jiaxin Chen, Qingjie Liu, Yunhong Wang</div>
<div class="meta-line">First: 2026-02-02T11:03:01+00:00 · Latest: 2026-02-02T11:03:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01954v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越开放词汇：遥感图像中物体检测的多模态提示</div>
<div class="mono" style="margin-top:8px">遥感中的开放词汇物体检测通常依赖于仅文本提示来指定目标类别，隐含假设推理时的类别查询可以通过预训练引起的文本-视觉对齐可靠地确定。在实际应用中，由于任务和应用特定的类别语义，这一假设在遥感场景中常常失效，导致开放词汇设置下类别指定不稳定。为了解决这一局限性，我们提出了RS-MPOD，一个多模态开放词汇检测框架，通过结合实例基础的视觉提示、文本提示及其多模态集成，重新定义类别指定。RS-MPOD引入了一个视觉提示编码器，从示例实例中提取基于外观的类别线索，实现无文本类别指定，并且在两种模态均可用时，使用多模态融合模块整合视觉和文本信息。在标准、跨数据集和细粒度遥感基准上的大量实验表明，视觉提示在语义模糊和分布变化下提供了更可靠的类别指定，而多模态提示则提供了一种灵活的替代方案，在文本语义良好对齐时仍具竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve open-vocabulary object detection in remote sensing images, which often struggles with unstable category specification due to reliance on text-only prompting. The authors propose a multimodal open-vocabulary detection framework called RS-MPOD, which integrates instance-grounded visual prompts and textual prompts to enhance category specification. Experimental results demonstrate that visual prompting leads to more reliable category specification in the presence of semantic ambiguity and distribution shifts, while multimodal prompting remains competitive when textual semantics are well aligned.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善遥感图像中的开放词汇目标检测，该方法常因依赖仅文本提示而导致类别规范不稳定。作者提出了一种名为RS-MPOD的新框架，结合实例基础的视觉提示和文本提示，以增强类别规范。实验结果表明，在语义模糊和分布变化的情况下，视觉提示能够实现更可靠的类别识别，而当文本语义良好对齐时，多模态方法仍然有效。</div>
</details>
</div>
<div class="card">
<div class="title">PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting</div>
<div class="meta-line">Authors: Abdul Joseph Fofanah, Lian Wen, David Chen</div>
<div class="meta-line">First: 2026-02-02T10:40:07+00:00 · Latest: 2026-02-02T10:40:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01936v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01936v1">PDF</a> · <a href="https://github.com/afofanah/MCPST">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIMCST：基于物理的多相共识与时空少样本学习用于交通流预测</div>
<div class="mono" style="margin-top:8px">准确的交通流预测仍然是智能交通系统中的一个基本挑战，特别是在跨领域、数据稀缺的场景中，有限的历史数据阻碍了模型训练和泛化。城市移动网络的复杂时空依赖性和非线性动态进一步复杂化了不同城市之间的少样本学习。本文提出了MCPST，一种新颖的多相共识时空框架，用于少样本交通预测，将交通预测重新概念化为多相共识学习问题。我们的框架引入了三个核心创新：（1）一个多相引擎，通过扩散、同步和谱嵌入建模交通动态，以实现全面的动态特征描述；（2）一个自适应共识机制，动态融合相特定的预测，同时强制一致性；（3）一个结构化的元学习策略，以最小的数据快速适应新城市。我们建立了广泛的理论保证，包括具有有界近似误差的表示定理和少样本适应的泛化界限。通过在四个真实世界数据集上的实验，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨领域少样本设置中超越了十四种最先进的方法，提高了预测准确性，同时减少了所需的训练数据并提供了可解释的见解。实现代码可在https://github.com/afofanah/MCPST获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of accurate traffic flow prediction in intelligent transportation systems, especially in scenarios with limited historical data. The authors propose a novel framework called MCPST, which reconceptualizes traffic prediction as a multi-phase consensus learning problem, utilizing a multi-phase engine, an adaptive consensus mechanism, and a structured meta-learning strategy. Experimental results demonstrate that MCPST significantly outperforms fourteen state-of-the-art methods across various datasets, enhancing prediction accuracy while minimizing the amount of training data needed and offering interpretable insights into the predictions.</div>
<div class="mono" style="margin-top:8px">本研究解决了智能交通系统中准确交通流预测的挑战，特别是在历史数据有限的情况下。作者提出了一种名为MCPST的新框架，将交通预测重新概念化为多阶段共识学习问题，并结合了多阶段引擎、自适应共识机制和结构化元学习策略。实验结果表明，MCPST在多个数据集上显著优于十四种最先进的方法，提高了预测准确性，同时减少了所需的训练数据，并提供了可解释的见解。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260202_0324.html">20260202_0324</a>
<a href="archive/20260201_0320.html">20260201_0320</a>
<a href="archive/20260131_0332.html">20260131_0332</a>
<a href="archive/20260130_0332.html">20260130_0332</a>
<a href="archive/20260129_0327.html">20260129_0327</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
