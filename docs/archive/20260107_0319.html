<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-07 03:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260107_0319</div>
    <div class="row"><div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-05T18:56:34+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：一个统一的视觉生成器，具有交错的全模态上下文</div>
<div class="mono" style="margin-top:8px">我们提出了VINO，一个统一的视觉生成器，在一个框架内执行图像和视频的生成与编辑。VINO不依赖于特定任务的模型或每种模态的独立模块，而是使用一个共享的扩散骨干网络，基于文本、图像和视频进行条件化，从而在一个模型下实现广泛的视觉创作和编辑任务。具体而言，VINO将视觉-语言模型（VLM）与多模态扩散变换器（MMDiT）结合，其中多模态输入被编码为交错的条件令牌，然后用于指导扩散过程。该设计支持多参考基础、长格式指令跟随以及在静态和动态内容中保持一致的身份，同时避免特定模态的架构组件。为了训练这样一个统一的系统，我们引入了一个多阶段训练管道，逐步将视频生成基础模型扩展为一个统一的多任务生成器，能够处理图像和视频的输入和输出。在多样的生成和编辑基准测试中，VINO展示了强大的视觉质量、忠实的指令跟随、改进的参考和属性保留，以及更可控的多身份编辑。我们的结果突显了可扩展统一视觉生成的实际路径，以及交错的上下文计算作为通用视觉创作基础的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the development of VINO is to create a unified visual generator capable of handling both image and video generation and editing tasks within a single framework, eliminating the need for task-specific models. The method involves coupling a vision-language model with a Multimodal Diffusion Transformer, where multimodal inputs are encoded as interleaved conditioning tokens to guide the diffusion process. Key experimental findings indicate that VINO achieves strong visual quality, accurate instruction following, and effective preservation of references and attributes across various benchmarks, demonstrating its potential for scalable unified visual generation.</div>
<div class="mono" style="margin-top:8px">VINO的开发动机是创建一个统一的视觉生成器，能够在单一框架内处理图像和视频的生成与编辑任务，从而消除对特定任务模型的需求。该方法结合了视觉语言模型和多模态扩散变换器，将多模态输入编码为交错的条件标记，以指导扩散过程。实验结果表明，VINO在各种生成和编辑基准测试中实现了高视觉质量，有效遵循指令，保持参考和属性，并允许更可控的多身份编辑。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?</div>
<div class="meta-line">Authors: Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le</div>
<div class="meta-line">Venue: NeurIPS 2024</div>
<div class="meta-line">First: 2025-12-26T07:40:11+00:00 · Latest: 2026-01-05T18:55:51+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, 4 tables; NeurIPS 2024 format</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21907v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialBench：代理能否分析真实世界的空间生物数据？</div>
<div class="mono" style="margin-top:8px">空间转录组学检测的规模和复杂性迅速增加，使得计算分析成为生物发现的主要瓶颈。尽管前沿AI代理在软件工程和一般数据分析方面有了显著改善，但尚不清楚它们是否能够从混乱的真实世界空间数据集中提取生物学见解。我们介绍了SpatialBench，这是一个基于来自五种空间技术和七个任务类别的实际空间分析工作流程衍生的146个可验证问题的基准。每个问题提供了分析步骤之前的实验数据快照和一个确定性的评分器，用于评估关键生物结果的恢复。关于前沿模型的基准数据表明，基础模型的准确性仍然较低（在模型家族中为20-38%），并且存在强烈的模型-任务和模型-平台交互。设计的工具对性能有很大的实证影响，这表明工具、提示、控制流和执行环境应作为一流对象进行评估和改进。SpatialBench既是一个测量工具，也是一个诊断视角，用于开发能够真实、透明和可重复地与真实空间数据集交互的代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational analysis challenges posed by the increasing scale and complexity of spatial transcriptomics assays, which hinder biological discovery. The authors introduce SpatialBench, a benchmark consisting of 146 verifiable problems derived from real-world spatial analysis workflows across various technologies and task categories. Experimental results reveal that the accuracy of frontier AI models in solving these problems is low, ranging from 20-38%, with significant interactions between model-task and model-platform, highlighting the need for improved design in tools and methodologies for better performance in analyzing spatial datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决空间转录组测定中日益增加的规模和复杂性所带来的计算挑战，这些挑战阻碍了生物发现。作者介绍了SpatialBench，这是一个由146个可验证问题组成的基准，源自各种技术和任务类别的实际空间分析工作流程。实验结果表明，前沿AI模型在分析这些数据集时的准确率仍然较低，范围为20-38%，并强调了模型任务和模型平台之间的显著相互作用，这表明工具和提示的设计对性能有重要影响，应在未来的开发中优先考虑。</div>
</details>
</div>
<div class="card">
<div class="title">DARC: Drum accompaniment generation with fine-grained rhythm control</div>
<div class="meta-line">Authors: Trey Brosnan</div>
<div class="meta-line">First: 2026-01-05T18:55:43+00:00 · Latest: 2026-01-05T18:55:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARC：具有细粒度节奏控制的鼓伴奏生成</div>
<div class="mono" style="margin-top:8px">在音乐创作中，快速原型制作对于探索和完善想法至关重要，但现有的生成工具在用户需要结构控制和风格灵活性时往往不够理想。先前的干声到干声生成方法可以基于其他音乐干声进行条件生成，但对节奏的控制有限，而音色转移方法允许用户指定特定的节奏，但无法基于音乐上下文进行条件生成。我们介绍了DARC，一种生成鼓伴奏模型，它同时基于其他干声的音乐上下文和明确的节奏提示（如打击乐或敲击轨道）进行条件生成。通过参数高效的微调，我们增强了STAGE，一个最先进的鼓干声生成器，提供细粒度的节奏控制，同时保持音乐上下文的意识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the process of music creation by providing tools that allow for both structural control and stylistic flexibility in drum accompaniment generation. The authors introduce DARC, a generative model that integrates musical context from other stems with explicit rhythm prompts, utilizing parameter-efficient fine-tuning to improve upon the existing STAGE drum stem generator. Key experimental findings demonstrate that DARC successfully achieves fine-grained rhythm control while maintaining awareness of the musical context, addressing the limitations of previous methods in generative music tools.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过提供工具来增强音乐创作，使鼓伴奏生成能够在结构控制和风格灵活性之间取得平衡。作者提出了DARC，这是一种生成鼓伴奏的模型，通过参数高效的微调来增强现有的鼓音轨生成器STAGE，使其能够同时响应音乐上下文和明确的节奏提示。主要实验结果表明，DARC成功实现了细粒度的节奏控制，同时保持了对音乐上下文的敏感性，解决了之前音乐生成工具的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation</div>
<div class="meta-line">Authors: Salim Khazem</div>
<div class="meta-line">First: 2026-01-05T17:03:45+00:00 · Latest: 2026-01-05T17:03:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02273v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02273v1">PDF</a> · <a href="https://github.com/salimkhazem/Seglab.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TopoLoRA-SAM：面向拓扑的基础分割器参数高效适应薄结构和跨域二元语义分割</div>
<div class="mono" style="margin-top:8px">基础分割模型如Segment Anything Model (SAM)通过大规模预训练展现出强大的零样本泛化能力，但将其适应于特定领域的语义分割仍然具有挑战性，特别是对于薄结构（例如视网膜血管）和噪声模态（例如SAR影像）。全面微调计算成本高且存在灾难性遗忘的风险。我们提出了\textbf{TopoLoRA-SAM}，一种面向拓扑的参数高效二元语义分割适应框架。TopoLoRA-SAM将低秩适应（LoRA）注入冻结的ViT编码器，并结合轻量级空间卷积适配器和可选的通过可微分clDice的拓扑感知监督。我们在五个基准上评估了我们的方法，涵盖视网膜血管分割（DRIVE、STARE、CHASE\_DB1）、息肉分割（Kvasir-SEG）和SAR海洋/陆地分割（SL-SSDD），并与U-Net、DeepLabV3+、SegFormer和Mask2Former进行比较。TopoLoRA-SAM在数据集上实现了最佳的视网膜平均Dice和最佳的整体平均Dice，同时仅训练了\textbf{5.2\%}的模型参数（$\sim$4.9M）。在具有挑战性的CHASE\_DB1数据集上，我们的方法显著提高了分割准确性和鲁棒性，证明了面向拓扑的参数高效适应可以匹配或超越完全微调的专业模型。代码可在：https://github.com/salimkhazem/Seglab.git获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting foundation segmentation models like the Segment Anything Model (SAM) for domain-specific tasks, particularly for thin structures and noisy modalities, where full fine-tuning is costly and prone to catastrophic forgetting. The authors propose TopoLoRA-SAM, a topology-aware and parameter-efficient adaptation framework that integrates Low-Rank Adaptation (LoRA) into a frozen ViT encoder, supplemented with a spatial convolutional adapter and optional topology-aware supervision. Experimental results show that TopoLoRA-SAM achieves superior performance in binary semantic segmentation across multiple benchmarks, including retinal vessel and polyp segmentation, achieving the best average Dice scores while training only 5.2% of the model parameters, and significantly enhancing accuracy on the challenging CHASE_DB1 dataset compared to fully fine-tuned models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决将基础分割模型（如Segment Anything Model，SAM）适应于特定领域语义分割的挑战，特别是针对细结构和噪声模态。作者提出了TopoLoRA-SAM，这是一种拓扑感知和参数高效的适应框架，将低秩适应（LoRA）引入冻结的ViT编码器，并辅以轻量级空间卷积适配器和可选的拓扑感知监督。实验结果表明，TopoLoRA-SAM在多个数据集（包括视网膜血管和息肉分割）上实现了最佳平均Dice分数，同时仅训练5.2%的模型参数，表明其在提高分割准确性和鲁棒性方面的有效性，超过了完全微调的模型。</div>
</details>
</div>
<div class="card">
<div class="title">DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</div>
<div class="meta-line">Authors: Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang</div>
<div class="meta-line">First: 2026-01-05T16:51:45+00:00 · Latest: 2026-01-05T16:51:45+00:00</div>
<div class="meta-line">Comments: Page: https://wrk226.github.io/DiffProxy.html, Code: https://github.com/wrk226/DiffProxy</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02267v1">PDF</a> · <a href="https://github.com/wrk226/DiffProxy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://wrk226.github.io/DiffProxy.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models&#x27; training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffProxy：通过扩散生成的密集代理进行多视角人类网格恢复</div>
<div class="mono" style="margin-top:8px">从多视角图像中恢复人类网格面临一个基本挑战：现实世界的数据集包含不完美的真实标注，导致模型训练偏差，而具有精确监督的合成数据则存在领域差距。本文提出了DiffProxy，一个生成多视角一致的人类代理以进行网格恢复的新框架。DiffProxy的核心在于利用基于扩散的生成先验来弥合合成训练与现实世界泛化之间的差距。其关键创新包括：（1）用于生成多视角一致、像素对齐的人类代理的多条件机制；（2）一个手部细化模块，结合灵活的视觉提示以增强局部细节；（3）一种不确定性感知的测试时缩放方法，在优化过程中提高对挑战性案例的鲁棒性。这些设计确保网格恢复过程有效利用精确的合成真实数据和基于扩散的管道的生成优势。DiffProxy完全在合成数据上训练，在五个现实世界基准上实现了最先进的性能，特别是在遮挡和部分视图的挑战场景中表现出强大的零样本泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges in human mesh recovery from multi-view images, particularly the issues arising from imperfect ground-truth annotations in real-world datasets and the domain gap present in synthetic data. The authors propose DiffProxy, a framework that utilizes diffusion-based generative priors to create multi-view consistent human proxies for improved mesh recovery. Key experimental results indicate that DiffProxy, trained solely on synthetic data, achieves state-of-the-art performance on five real-world benchmarks, demonstrating effective zero-shot generalization in complex scenarios involving occlusions and partial views.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多视角图像中人类网格恢复的挑战，特别是来自真实世界数据集中不完美的真实标注和合成数据中的领域差距。作者提出了DiffProxy，一个利用扩散生成先验创建多视角一致的人类代理以进行网格恢复的框架。关键实验结果表明，DiffProxy仅在合成数据上训练，在五个真实世界基准测试中实现了最先进的性能，尤其在涉及遮挡和部分视图的复杂场景中表现出有效的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion</div>
<div class="meta-line">Authors: Binglei Li, Mengping Yang, Zhiyu Tan, Junping Zhang, Hao Li</div>
<div class="meta-line">First: 2026-01-05T15:32:53+00:00 · Latest: 2026-01-05T15:32:53+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block&#x27;s functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解开MMDiT模块：无训练分析与文本条件扩散的增强</div>
<div class="mono" style="margin-top:8px">基于变换器的扩散模型，特别是由多模态扩散变换器（MMDiT）驱动的模型，如FLUX和Qwen Image，最近取得了突破性进展，促进了文本到图像生成和编辑的激动人心的体验。为了理解基于MMDiT模型的内部机制，现有方法试图分析特定组件（如位置编码和注意力层）的影响。然而，如何不同模块及其与文本条件的交互对合成过程的贡献仍然难以全面理解。本文首先开发了一个系统化的流程，通过移除、禁用和增强相应模块的文本隐状态，全面调查每个模块的功能。我们的分析揭示了：1）语义信息出现在早期模块，细节在后期模块中呈现；2）移除特定模块通常比禁用文本条件的干扰小；3）在选择性模块中增强文本条件可以改善语义属性。基于这些观察，我们进一步提出了新颖的无训练策略，以改善文本对齐、精确编辑和加速。大量实验表明，我们的方法在各种基准测试中表现优于其他方法，并在文本到图像生成、图像编辑和推理加速方面保持灵活性。我们的方法将T2I-Combench++从56.92%提高到63.00%，将GenEval从66.42%提高到71.63%，而不牺牲合成质量。这些结果推动了对MMDiT模型的理解，并为进一步改进提供了宝贵的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to gain a comprehensive understanding of the internal mechanisms of Multimodal Diffusion Transformers (MMDiT) used in text-to-image generation and editing. The authors developed a systematic pipeline to analyze the functionality of different blocks by manipulating textual hidden-states, revealing that semantic information is present in earlier blocks while finer details are rendered in later ones. Key findings indicate that removing specific blocks is less disruptive than disabling text conditions, and enhancing textual conditions in selective blocks improves semantic attributes, leading to novel training-free strategies that enhance text alignment and editing precision, achieving significant performance improvements in various benchmarks without compromising synthesis quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于全面理解用于文本到图像生成和编辑的多模态扩散变换器（MMDiT）的内部机制。作者开发了一个系统化的流程，通过操控文本隐藏状态来分析不同模块的功能，揭示了语义信息出现在早期模块，而细节则在后期模块中呈现。主要发现表明，移除特定模块的干扰通常小于禁用文本条件，并且在选择性模块中增强文本条件可以改善语义属性，这导致了新颖的无训练策略，显著提高了文本对齐和编辑精度，实验结果显示在不影响合成质量的情况下，T2I-Combench++和GenEval的性能指标得到了提升。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing the Unseen: Zooming in the Dark with Event Cameras</div>
<div class="meta-line">Authors: Dachun Kai, Zeyu Xiao, Huyue Zhu, Jiaxiao Wang, Yueyi Zhang, Xiaoyan Sun</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-05T15:31:07+00:00 · Latest: 2026-01-05T15:31:07+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02206v1">PDF</a> · <a href="https://github.com/DachunKai/RetinexEVSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>看见未见：使用事件相机在黑暗中放大</div>
<div class="mono" style="margin-top:8px">本文讨论低光视频超分辨率（LVSR），旨在从低光、低分辨率（LR）输入中恢复高分辨率视频。现有的LVSR方法常常因对比度有限和高频信息不足而难以恢复细节。为克服这些挑战，我们提出了RetinexEVSR，这是第一个事件驱动的LVSR框架，利用高对比度事件信号和受Retinex启发的先验知识，在低光场景下增强视频质量。与之前直接融合退化信号的方法不同，RetinexEVSR引入了一种新颖的双向跨模态融合策略，从噪声事件数据和退化RGB帧中提取和整合有意义的线索。具体而言，设计了一个照明引导的事件增强模块，利用从Retinex模型导出的照明图逐步细化事件特征，从而抑制低光伪影，同时保留高对比度细节。此外，我们提出了一个事件引导的反射增强模块，利用增强的事件特征通过多尺度融合机制动态恢复反射细节。实验结果表明，我们的RetinexEVSR在三个数据集上达到了最先进的性能。值得注意的是，在SDSD基准上，我们的方法相比于之前的基于事件的方法可获得高达2.95 dB的增益，同时运行时间减少65%。代码：https://github.com/DachunKai/RetinexEVSR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve low-light video super-resolution (LVSR) by restoring high-resolution videos from low-light, low-resolution inputs, which often lack fine details due to limited contrast. The authors introduce RetinexEVSR, an event-driven LVSR framework that utilizes high-contrast event signals and Retinex-inspired priors, employing a novel bidirectional cross-modal fusion strategy to enhance video quality. Experimental results demonstrate that RetinexEVSR achieves state-of-the-art performance on three datasets, with a notable 2.95 dB gain on the SDSD benchmark while also reducing runtime by 65% compared to previous event-based methods.</div>
<div class="mono" style="margin-top:8px">本文解决了低光视频超分辨率（LVSR）的问题，旨在改善在光线不足的情况下捕获的低分辨率视频。作者提出了RetinexEVSR，这是一种事件驱动的LVSR框架，利用高对比度事件信号和受Retinex启发的先验知识来提高视频质量。该方法采用双向跨模态融合策略，有效整合来自噪声事件数据和降级RGB帧的信息，从而显著改善细节恢复。实验结果表明，RetinexEVSR在现有方法中表现优异，在SDSD基准上实现了最高2.95 dB的增益，同时相比于之前的事件驱动方法减少了65%的运行时间。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates</div>
<div class="meta-line">Authors: Louise Largeau, Tom Beucler, David Leutwyler, Gregoire Mariethoz, Valerie Chavez-Demoulin, Erwan Koch</div>
<div class="meta-line">First: 2025-07-12T07:04:07+00:00 · Latest: 2026-01-05T14:58:49+00:00</div>
<div class="meta-line">Comments: 47+7 pages, 10+4 figures, 1 table, submitted to WCE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09166v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.09166v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The coarse spatial resolution of gridded climate models, such as general circulation models, limits their direct use in projecting socially relevant variables like extreme precipitation. Most downscaling methods estimate the conditional distributions of extremes by generating large ensembles, complicating the assessment of robustness under distributional transformations, such as those induced by climate change. To better understand and potentially improve robustness, we propose super-resolving the parameters of the target variable&#x27;s probability distribution directly using analytically tractable mappings. Within a perfect-model framework over Switzerland, we demonstrate that vector generalized linear and additive models can super-resolve the generalized extreme value distribution of summer hourly precipitation extremes from coarse precipitation fields and topography. We introduce the notion of a &quot;robustness gap&quot;, defined as the difference in predictive error between present-trained and future-trained models, and use it to diagnose how model structure affects the generalization of each quantile to a pseudo-global warming scenario. By evaluating multiple model configurations, we also identify an upper limit on the super-resolution factor based on the spatial auto- and cross-correlation of precipitation and elevation, beyond which coarse precipitation loses predictive value. Our framework is broadly applicable to variables governed by parametric distributions and offers a model-agnostic diagnostic for understanding when and why empirical downscaling generalizes to climate change and extremes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨气候极端降水超分辨率的稳健性研究</div>
<div class="mono" style="margin-top:8px">网格气候模型（如一般环流模型）的粗糙空间分辨率限制了它们在预测社会相关变量（如极端降水）中的直接应用。大多数降尺度方法通过生成大量集合来估计极端值的条件分布，这使得在分布变换（如气候变化引起的变换）下评估稳健性变得复杂。为了更好地理解并可能改善稳健性，我们提出直接使用解析可处理的映射超分辨目标变量的概率分布参数。在瑞士的完美模型框架内，我们展示了向量广义线性和加法模型可以从粗糙降水场和地形中超分辨夏季每小时降水极端的广义极值分布。我们引入了“稳健性差距”的概念，定义为当前训练模型与未来训练模型之间预测误差的差异，并用它来诊断模型结构如何影响每个分位数对伪全球变暖情景的泛化。通过评估多种模型配置，我们还确定了基于降水和海拔的空间自相关和交叉相关的超分辨率因子的上限，超过该限度，粗糙降水失去预测价值。我们的框架广泛适用于由参数分布控制的变量，并提供了一种与模型无关的诊断方法，以理解经验降尺度何时以及为何能泛化到气候变化和极端事件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of coarse spatial resolution in climate models for projecting extreme precipitation, which is crucial for social planning. The authors propose a method to super-resolve the parameters of the probability distribution of extreme precipitation using vector generalized linear and additive models within a perfect-model framework in Switzerland. Key findings indicate the introduction of a &#x27;robustness gap&#x27; that quantifies predictive error differences between models trained on current and future data, revealing how model structure influences generalization under climate change and identifying a threshold for super-resolution effectiveness based on precipitation and elevation correlations.</div>
<div class="mono" style="margin-top:8px">本研究解决了气候模型中粗糙空间分辨率的局限性，特别是在预测极端降水事件方面，这对社会影响至关重要。作者提出了一种方法，通过在瑞士的完美模型框架内使用向量广义线性和加性模型，超分辨率极端降水的概率分布参数。主要发现揭示了当前和未来数据训练模型之间预测误差的“鲁棒性差距”，并确定了基于降水和海拔的空间相关性超分辨率有效性的阈值，表明超过此限制后，粗糙降水的预测价值显著降低。</div>
</details>
</div>
<div class="card">
<div class="title">BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</div>
<div class="meta-line">Authors: Sunny Gupta, Shounak Das, Amit Sethi</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-05T14:22:20+00:00 · Latest: 2026-01-05T14:22:20+00:00</div>
<div class="meta-line">Comments: Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiPrompt：视觉-文本模型中的双边提示优化用于去偏见</div>
<div class="mono" style="margin-top:8px">视觉语言基础模型如CLIP展现了令人印象深刻的零-shot泛化能力，但仍然容易受到视觉和文本模态之间虚假相关性的影响。现有的去偏见方法通常只针对单一模态（视觉或文本），导致部分鲁棒性和在分布变化下的不稳定适应。我们提出了一种双边提示优化框架（BiPrompt），在测试时适应过程中同时减轻两个模态中非因果特征的依赖。在视觉方面，它采用结构化注意力引导的抑制方法来压制背景激活，并强制因果区域与虚假区域之间的正交预测一致性。在文本方面，它引入了平衡提示归一化，这是一种可学习的重新中心机制，将类别嵌入对齐到各向同性语义空间。通过这些模块，联合最小化虚假线索与预测之间的条件互信息，引导模型朝向因果、领域不变的推理，而无需重新训练或领域监督。在真实世界和合成偏见基准上的广泛评估表明，相较于先前的测试时去偏见方法，在平均和最差组准确性上均有持续改善，确立了一条轻量且有效的路径，朝向可信且基于因果的视觉-语言适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the vulnerabilities of vision-language models, such as CLIP, to spurious correlations in visual and textual modalities, which can lead to unreliable performance. The authors propose a bilateral prompt optimization framework (BiPrompt) that simultaneously optimizes both modalities during test-time adaptation, employing structured attention-guided erasure for visual data and balanced prompt normalization for textual data. Experimental results show that BiPrompt significantly improves both average and worst-group accuracies on various bias benchmarks compared to existing test-time debiasing methods, indicating its effectiveness in promoting causal reasoning without the need for retraining or domain supervision.</div>
<div class="mono" style="margin-top:8px">该研究解决了视觉语言模型如CLIP在视觉和文本模态中对虚假相关性的脆弱性，这可能导致性能不可靠。作者提出了一种双边提示优化框架（BiPrompt），在测试时适应过程中同时减少两个模态中非因果特征的依赖。实验结果表明，BiPrompt在各种真实世界和合成偏差基准测试中显著提高了平均和最差组准确率，相较于现有的测试时去偏方法，表明其在促进可靠和因果基础的视觉语言适应方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Retoucher for Text-To-Image Generation</div>
<div class="meta-line">Authors: Shaocheng Shen, Jianfeng Liang. Chunlei Cai, Cong Geng, Huiyu Duan, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</div>
<div class="meta-line">First: 2026-01-05T12:06:43+00:00 · Latest: 2026-01-05T12:06:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像生成的自主修饰器</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）扩散模型如SDXL和FLUX已实现令人印象深刻的照片真实感，但在四肢、面部、文本等方面仍存在小规模失真。现有的精细化方法要么进行昂贵的迭代再生成，要么依赖于空间基础较弱的视觉语言模型（VLM），导致语义漂移和不可靠的局部编辑。为了解决这一问题，我们提出了自主修饰器，这是一个分层决策驱动框架，将生成后的修正重新表述为类似人类的感知-推理-行动循环。具体而言，我们设计了（1）一个感知代理，学习在文本-图像一致性线索下进行细粒度失真定位的上下文显著性，（2）一个推理代理，通过渐进的偏好对齐进行人类对齐的推理诊断，以及（3）一个行动代理，根据用户偏好自适应规划局部修补。该设计将感知证据、语言推理和可控修正整合为一个统一的自我修正决策过程。为了实现细粒度监督和定量评估，我们进一步构建了GenBlemish-27K，这是一个包含6K T2I图像和12个类别中27K注释伪影区域的数据集。大量实验表明，自主修饰器在感知质量、失真定位和人类偏好对齐方面始终优于最先进的方法，为自我修正和感知可靠的T2I生成建立了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the persistent small-scale distortions in text-to-image (T2I) diffusion models, which affect the quality of generated images. The authors propose a novel framework called Agentic Retoucher, which operates through a hierarchical decision-driven process that mimics human perception and reasoning. This framework includes a perception agent for distortion localization, a reasoning agent for inferential diagnosis, and an action agent for localized inpainting based on user preferences. Experimental results show that Agentic Retoucher significantly improves perceptual quality, distortion localization, and alignment with human preferences compared to existing methods, supported by the creation of a new dataset, GenBlemish-27K, for evaluation purposes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决文本到图像（T2I）扩散模型中持续存在的小规模失真问题，这些失真影响了生成图像的逼真度。作者提出了Agentic Retoucher，一个将后生成修正重新构造成感知-推理-行动循环的分层框架，该框架包括用于失真定位的感知代理、用于推理诊断的推理代理和用于局部修补的行动代理。实验结果表明，Agentic Retoucher在感知质量、失真定位和与人类偏好的对齐方面显著优于现有方法，从而为可靠的T2I生成建立了新的标准。</div>
</details>
</div>
<div class="card">
<div class="title">When in Doubt, Consult: Expert Debate for Sexism Detection via Confidence-Based Routin</div>
<div class="meta-line">Authors: Anwar Alajmi, Gabriele Pergola</div>
<div class="meta-line">First: 2025-12-21T05:48:57+00:00 · Latest: 2026-01-05T11:54:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23732v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23732v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with F1 gains of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% improvement in ICM on EXIST 2025 Task 1.1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有疑问时，请咨询：基于信心的常规专家辩论以检测性别歧视</div>
<div class="mono" style="margin-top:8px">在线性别歧视内容越来越以微妙、依赖上下文的形式出现，逃避传统检测方法。其解释往往依赖于重叠的语言、心理、法律和文化维度，产生混合且有时矛盾的信号，即使在标注数据集中也是如此。这些不一致，加上标签稀缺和类别不平衡，导致决策边界不稳定，并使微调模型忽视更微妙、代表性不足的伤害形式。这些局限性共同指向需要一种设计，明确解决（i）代表性不足，（ii）噪声，以及（iii）数据和模型预测中的概念模糊的综合影响。为应对这些挑战，我们提出了一个两阶段框架，统一（i）针对性训练程序，以适应稀缺和嘈杂数据的监督，以及（ii）选择性、基于推理的推断，以处理模糊或边界案例。我们的训练设置应用类别平衡的焦点损失、类别感知批处理和事后阈值校准，以减轻标签不平衡和嘈杂监督。在推断时，动态路由机制直接对高置信度案例进行分类，并将不确定实例升级到一个新颖的\textit{协作专家判断}（CEJ）模块，该模块提示多个角色并通过法官模型整合他们的推理。我们的方法在多个基准测试中实现了最先进的结果，在EDOS任务A和B上分别获得了+4.48%和+1.30%的F1增益，在EXIST 2025任务1.1上获得了+2.79%的ICM改善。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting subtle and context-dependent sexist content online, which traditional methods struggle to identify due to issues like underrepresentation and conceptual ambiguity. To tackle these problems, the authors propose a two-stage framework that combines targeted training procedures with selective reasoning-based inference. The experimental results demonstrate that this approach achieves state-of-the-art performance, with F1 score improvements of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% increase in ICM on EXIST 2025 Task 1.1.</div>
<div class="mono" style="margin-top:8px">在线上性别歧视内容日益普遍且呈现微妙和依赖上下文的形式，因此需要改进检测方法，以应对语言、心理、法律和文化维度的复杂性。为了解决这些挑战，作者提出了一种两阶段框架，将针对性训练程序与选择性推理推断相结合，以管理数据稀缺、噪声和概念模糊。实验结果表明，该方法在多个基准测试中实现了最先进的性能，EDOS任务A和B的F1得分分别提高了+4.48%和+1.30%，而在EXIST 2025任务1.1中提高了+2.79%的ICM。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding</div>
<div class="meta-line">Authors: Toshihiko Nishimura, Hirofumi Abe, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida</div>
<div class="meta-line">Venue: 19th International Conference on Machine Vision Applications (MVA2025), IEICE Transactions on Information and Systems letter</div>
<div class="meta-line">First: 2026-01-05T11:42:49+00:00 · Latest: 2026-01-05T11:42:49+00:00</div>
<div class="meta-line">Comments: 19</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用2D-VLM进行大规模户外场景理解中的无标签3D分割</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的3D语义分割方法，适用于大规模点云数据，无需注释的3D训练数据或配对的RGB图像。所提出的方法通过虚拟相机将3D点云投影到2D图像上，并通过自然语言提示引导的基础2D模型进行语义分割。3D分割通过加权投票聚合来自多个视角的预测实现。我们的方法优于现有的无训练方法，并且实现了与监督方法相当的分割精度。此外，它支持开放词汇识别，使用户能够使用任意文本查询检测对象，从而克服传统监督方法的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to develop a 3D semantic segmentation method for large-scale outdoor scenes that does not rely on annotated 3D training data or paired RGB images. The authors propose a technique that projects 3D point clouds onto 2D images using virtual cameras and utilizes a foundation 2D model guided by natural language prompts for semantic segmentation. Experimental results demonstrate that this method surpasses existing training-free approaches and achieves segmentation accuracy comparable to supervised methods, while also enabling open-vocabulary recognition for object detection using arbitrary text queries.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种用于大规模点云数据的3D语义分割方法，消除对标注的3D训练数据或配对RGB图像的需求。作者提出了一种新方法，通过虚拟相机将3D点云投影到2D图像上，并利用自然语言提示指导的基础2D模型进行语义分割。实验结果表明，该方法优于现有的无训练方法，并且实现了与监督方法相当的分割精度，同时支持使用任意文本查询进行开放词汇识别。</div>
</details>
</div>
<div class="card">
<div class="title">CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation</div>
<div class="meta-line">Authors: Joohyeon Lee, Jin-Seop Lee, Jee-Hyong Lee</div>
<div class="meta-line">First: 2025-08-14T14:53:53+00:00 · Latest: 2026-01-05T11:17:43+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10710v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10710v2">PDF</a> · <a href="https://github.com/JoohyeonL22/CountCluster">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CountCluster：无训练的对象数量引导，通过交叉注意力图聚类实现文本到图像生成</div>
<div class="mono" style="margin-top:8px">基于扩散的文本到图像生成模型在图像质量和多样性方面表现出色。然而，它们在生成准确反映输入提示中指定的对象数量的图像时仍然存在困难。已有几种方法被提出，这些方法依赖于外部计数模块进行迭代优化或从学习的标记或潜在特征中派生的数量表示。然而，它们在准确反映指定对象数量方面仍然存在局限性，并忽视了一个重要的结构特征——生成图像中对象实例的数量在去噪过程的早期时间步中很大程度上被决定。为了正确反映图像生成的对象数量，早期时间步中对象交叉注意力图中高度激活的区域应与输入对象数量相匹配，同时每个区域应清晰分离。为了解决这个问题，我们提出了\textit{CountCluster}，一种方法，它指导对象交叉注意力图根据输入中指定的对象数量进行聚类，而不依赖于任何外部工具或额外训练。该方法在推理时根据注意力分数将对象交叉注意力图划分为$k$个聚类，定义每个聚类在空间上良好分离的理想分布，并优化潜在变量以与该目标分布对齐。与现有方法相比，我们的方法在对象计数准确性上平均提高了18.5\%p，并在各种提示中展示了优越的数量控制性能。代码将发布在：https://github.com/JoohyeonL22/CountCluster</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the accuracy of object quantity representation in diffusion-based text-to-image generation models, which often fail to generate images that reflect the specified number of objects in input prompts. The authors propose a novel method called CountCluster, which clusters the object cross-attention map according to the specified object count during inference, without requiring external tools or additional training. Experimental results show that CountCluster achieves an average improvement of 18.5 percentage points in object count accuracy compared to existing methods and demonstrates enhanced quantity control across various prompts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高扩散式文本到图像生成模型中物体数量表示的准确性，这些模型通常无法生成反映指定物体数量的图像。作者提出了一种新方法CountCluster，该方法在推理过程中根据指定的物体数量对物体交叉注意力图进行聚类，无需外部工具或额外训练。实验结果表明，CountCluster在物体数量准确性方面比现有方法平均提高了18.5个百分点，并在各种提示中表现出更好的数量控制。</div>
</details>
</div>
<div class="card">
<div class="title">Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors</div>
<div class="meta-line">Authors: Chen Zhu, Huiwen Zhang, Mu He, Yujie Li, Xiaotian Qiao</div>
<div class="meta-line">First: 2026-01-05T10:58:02+00:00 · Latest: 2026-01-05T10:58:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过逐步和相互增强的夜间雾霾先验进行夜间模糊图像增强</div>
<div class="mono" style="margin-top:8px">由于复杂的退化分布，增强夜间模糊图像的可见性具有挑战性。现有方法主要一次处理单一类型的退化（例如，雾霾或低光），忽视了不同退化类型之间的相互作用，导致可见性改善有限。我们观察到低光和雾霾先验之间共享的领域知识可以相互增强，以获得更好的可见性。基于这一关键见解，本文提出了一种新颖的框架，通过相互和逐步增强雾霾和低光先验之间的内在一致性来增强夜间模糊图像的可见性。特别地，我们的模型利用跨视觉和频率域操作的图像、块和像素级专家，逐步恢复全局场景结构、区域模式和细粒度细节。此外，引入了一个频率感知路由器，以自适应地引导每个专家的贡献，确保稳健的图像恢复。大量实验表明，我们的模型在夜间去雾基准测试中在定量和定性上均表现出优越性能。此外，我们展示了模型在白天去雾和低光增强任务中的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the visibility of nighttime hazy images, which is complicated by various degradation types that existing methods fail to address simultaneously. The authors propose a novel framework that enhances visibility by mutually and progressively reinforcing the relationship between haze and low-light priors, utilizing image-, patch-, and pixel-level experts across visual and frequency domains. Experimental results show that this approach significantly outperforms existing methods on nighttime dehazing benchmarks, and it also demonstrates generalizability in daytime dehazing and low-light enhancement tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善夜间雾霾图像的可见性，这一问题因多种退化类型而复杂，现有方法无法同时处理。作者提出了一种新颖的框架，通过相互和逐步强化雾霾和低光先验之间的关系来增强可见性，利用图像、块和像素级专家在视觉和频率域中进行处理。实验结果表明，该方法在夜间去雾基准测试中显著优于现有方法，并且该模型在白天去雾和低光增强任务中也表现出良好的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition</div>
<div class="meta-line">Authors: Julie Keisler, Anastase Alexandre Charantonis, Yannig Goude, Boutheina Oueslati, Claire Monteleoni</div>
<div class="meta-line">First: 2026-01-05T10:33:48+00:00 · Latest: 2026-01-05T10:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01979v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01979v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SerpentFlow：通过共享结构分解进行生成式无配对领域对齐</div>
<div class="mono" style="margin-top:8px">领域对齐广泛指学习来自不同领域的数据分布之间的对应关系。在这项工作中，我们关注一个设置，其中领域尽管在具体实现上存在差异，但共享潜在的结构模式。由于缺乏配对观察，这一任务特别具有挑战性，这消除了跨领域的直接监督。我们引入了一种生成框架，称为SerpentFlow（用于生成领域适应的共享结构分解），用于无配对领域对齐。SerpentFlow将潜在空间中的数据分解为一个对两个领域都通用的共享组件和一个领域特定的组件。通过隔离共享结构并用随机噪声替换领域特定组件，我们在共享表示和目标领域样本之间构建合成训练对，从而使得传统上仅限于配对设置的条件生成模型得以使用。我们将这种方法应用于超分辨率任务，其中共享组件自然对应于低频内容，而高频细节捕捉领域特定的变异性。分隔低频和高频组件的截止频率是通过基于分类器的标准自动确定的，确保了数据驱动和领域自适应的分解。通过生成保留低频结构的伪对，同时注入随机高频实现，我们学习给定共享表示的目标领域的条件分布。我们使用流匹配作为生成管道实现SerpentFlow，尽管该框架与其他条件生成方法兼容。在合成图像、物理过程模拟和气候降尺度任务上的实验表明，该方法有效重建与潜在低频模式一致的高频结构，支持共享结构分解作为无配对领域对齐的有效策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of unpaired domain alignment, where data from different domains share structural patterns but lack direct supervision. The authors propose a generative framework called SerpentFlow, which decomposes data into shared and domain-specific components within a latent space, allowing for the creation of synthetic training pairs. Experimental results show that SerpentFlow effectively reconstructs high-frequency structures while maintaining consistency with low-frequency patterns across various tasks, including super-resolution, synthetic images, and climate downscaling, thus validating the approach&#x27;s effectiveness in unpaired domain alignment.</div>
<div class="mono" style="margin-top:8px">本研究解决了无配对领域对齐的挑战，即来自不同领域的数据共享结构模式但缺乏直接监督。作者提出了一种名为SerpentFlow的生成框架，该框架在潜在空间中将数据分解为共享和领域特定的组件。实验结果表明，SerpentFlow在多个任务中有效重建高频结构，同时保持与低频模式的一致性，包括超分辨率、合成图像、物理过程模拟和气候降尺度，从而验证了共享结构分解在无配对领域对齐中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</div>
<div class="meta-line">Authors: Yuhang Wang, Yanxu Zhu, Dongyuan Lu, Jitao Sang</div>
<div class="meta-line">First: 2025-11-26T09:44:32+00:00 · Latest: 2026-01-05T10:26:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21214v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.21214v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models&#x27; ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我引导防御：通过合成指南实现推理模型的自适应安全对齐</div>
<div class="mono" style="margin-top:8px">推理模型在复杂推理任务中展现了显著的能力。然而，确保它们在对抗性越狱提示下的安全性仍然是一个关键挑战。由于此类提示的隐蔽和欺骗性，它们往往能够逃避内置的安全机制，导致生成有害内容。这凸显了自适应安全对齐方法的必要性，使模型能够自主增强其防御能力以应对对抗性输入。本文介绍了基于合成指南的自适应安全对齐（SGASA）框架，该框架内化模型生成的安全指南，以增强模型抵御有害对抗性提示的能力，同时最小化对良性请求的不必要拒绝。SGASA包括两个关键阶段：数据预合成，生成安全指南和增强提示；以及对齐微调，利用监督微调（SFT）和直接偏好优化（DPO）将这些指南嵌入模型。多个数据集的广泛实验表明，SGASA显著提高了模型的安全性，验证了其自适应和可扩展的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the critical challenge of ensuring the safety of reasoning models against adversarial jailbreak prompts, which can bypass existing safety mechanisms and produce harmful content. The authors propose the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which involves two main stages: Data Pre-synthesis for generating safety guidelines and augmented prompts, and Alignment Fine-tuning that incorporates these guidelines into the model using Supervised Fine-tuning and Direct Preference Optimization. Experimental results across various datasets show that SGASA significantly enhances the safety of reasoning models, demonstrating its effectiveness in adapting to and mitigating adversarial threats while reducing unnecessary refusals of benign requests.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决确保推理模型安全性的问题，特别是针对能够绕过现有安全机制的对抗性越狱提示的挑战。作者提出了基于合成指南的自适应安全对齐框架（SGASA），该框架包括两个主要阶段：数据预合成，用于生成安全指南和增强提示；以及对齐微调，通过监督微调和直接偏好优化将这些指南嵌入模型中。多组数据集的实验结果表明，SGASA显著提高了推理模型的安全性，证明了其在适应和减轻有害对抗输入方面的有效性，同时减少了对良性请求的不必要拒绝。</div>
</details>
</div>
<div class="card">
<div class="title">MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization</div>
<div class="meta-line">Authors: Zhexin Zhang, Yifeng Zhu, Yangyang Xu, Long Chen, Yong Du, Shengfeng He, Jun Yu</div>
<div class="meta-line">First: 2026-01-05T10:01:27+00:00 · Latest: 2026-01-05T10:01:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01955v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \romannumeral1) explicit disentanglement of motion from appearance and \romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MotionAdapter：通过内容感知注意力定制的视频运动转移</div>
<div class="mono" style="margin-top:8px">基于扩散的文本到视频模型，特别是基于扩散变换器架构的模型，最近在生成高质量和时间一致性视频方面取得了显著进展。然而，在视频之间转移复杂运动仍然具有挑战性。在本研究中，我们提出了MotionAdapter，这是一种内容感知的运动转移框架，能够在基于DiT的T2V模型中实现稳健且语义对齐的运动转移。我们的关键见解是，有效的运动转移需要\romannumeral1) 明确将运动与外观解耦和\romannumeral2) 根据目标内容自适应定制运动。MotionAdapter首先通过分析3D全注意力模块中的跨帧注意力来隔离运动，以提取基于注意力的运动场。为了弥合参考视频和目标视频之间的语义差距，我们进一步引入了一个DINO引导的运动定制模块，根据内容对应关系重新排列和精炼运动场。定制的运动场随后用于指导DiT去噪过程，确保合成视频继承参考运动，同时保留目标外观和语义。大量实验表明，MotionAdapter在定性和定量评估中均优于最先进的方法。此外，MotionAdapter自然支持复杂运动转移和运动编辑任务，如缩放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of transferring complex motions between videos using diffusion-based text-to-video models. The authors propose MotionAdapter, a framework that utilizes content-aware motion transfer by disentangling motion from appearance and customizing it to fit the target content. Experimental results show that MotionAdapter significantly outperforms existing methods in both qualitative and quantitative assessments, effectively supporting complex motion transfer and editing tasks such as zooming while ensuring semantic alignment between reference and target videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决在基于扩散的文本到视频模型中转移复杂动作的挑战。作者提出了MotionAdapter，这是一种内容感知的动作转移框架，能够将动作与外观分离并根据目标内容进行定制。该方法通过交叉帧注意力分析来隔离动作，并利用DINO引导的定制模块来精炼动作场，最终指导DiT模型中的去噪过程。实验结果表明，MotionAdapter在定性和定量评估中显著优于现有方法，有效支持复杂动作转移和编辑任务。</div>
</details>
</div>
<div class="card">
<div class="title">Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</div>
<div class="meta-line">Authors: Gabriel Downer, Sean Craven, Damian Ruck, Jake Thomas</div>
<div class="meta-line">First: 2025-07-28T10:57:44+00:00 · Latest: 2026-01-05T09:07:09+00:00</div>
<div class="meta-line">Comments: 9 pages, 9 figures. Jake Thomas served as Editor for this manuscript</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20704v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20704v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models&#x27; alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Text2VLM：将文本数据集适应于评估视觉语言模型中的对齐训练</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在人工智能系统中的日益整合需要强大的模型对齐，特别是在处理结合文本和图像的多模态内容时。现有的评估数据集严重倾向于仅文本提示，导致视觉脆弱性评估不足。为了解决这一问题，我们提出了\textbf{Text2VLM}，一种新颖的多阶段管道，将文本数据集适应为多模态格式，专门设计用于评估VLM对排版提示注入攻击的抵御能力。Text2VLM管道识别原始文本中的有害内容，并将其转换为排版图像，为VLM创建多模态提示。此外，我们对开源VLM的评估突显了在引入视觉输入时，它们对提示注入的易感性增加，揭示了当前模型对齐中的关键弱点。这还与封闭源前沿模型相比存在显著的性能差距。我们通过人工评估验证了Text2VLM，确保提取的显著概念的对齐；文本摘要和输出分类与人类期望一致。Text2VLM提供了一种可扩展的工具，用于全面的安全评估，为VLM开发更强大的安全机制做出贡献。通过增强对多模态脆弱性的评估，Text2VLM在推动VLM在多样化现实应用中的安全部署方面发挥了作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for robust alignment in Visual Language Models (VLMs) due to their increasing integration into AI systems, particularly in handling multimodal content. The authors propose a novel multi-stage pipeline called Text2VLM, which adapts text-only datasets into multimodal formats to evaluate VLMs&#x27; resilience against typographic prompt injection attacks. Experimental results indicate that open-source VLMs are significantly more susceptible to prompt injection when visual inputs are included, highlighting critical weaknesses in their alignment compared to closed-source models, and the Text2VLM pipeline has been validated through human evaluations to ensure alignment with human expectations in text summarization and output classification.</div>
<div class="mono" style="margin-top:8px">本研究的动机是由于视觉语言模型（VLM）在人工智能系统中的日益应用，特别是在处理多模态内容时，需要强大的模型对齐。作者提出了一种名为Text2VLM的新型多阶段管道，该管道将仅包含文本的数据集转换为多模态格式，以评估VLM对排版提示注入攻击的抵御能力。实验结果显示，当引入视觉输入时，开源VLM对提示注入表现出更大的脆弱性，突显了其对齐方面与闭源模型相比的显著弱点，同时Text2VLM管道通过人类评估得到了验证，确认提取概念与人类期望的一致性，从而有助于改善VLM在现实应用中的安全评估。</div>
</details>
</div>
<div class="card">
<div class="title">RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models</div>
<div class="meta-line">Authors: Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu</div>
<div class="meta-line">First: 2025-12-29T06:44:06+00:00 · Latest: 2026-01-05T09:01:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23239v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23239v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RS-Prune：高比例无训练数据剪枝以提高遥感扩散基础模型的效率</div>
<div class="mono" style="margin-top:8px">基于扩散的遥感（RS）生成基础模型对下游任务至关重要。然而，这些模型依赖于大量全球代表性数据，这些数据通常包含冗余、噪声和类别不平衡，降低了训练效率并阻碍了收敛。现有的RS扩散基础模型通常聚合多个分类数据集或应用简单的去重，忽视了生成建模的分布要求和RS图像的异质性。为了解决这些局限性，我们提出了一种无训练的两阶段数据剪枝方法，能够在高剪枝比例下快速选择高质量子集，使初步基础模型快速收敛，并作为生成、下游微调和其他应用的多功能骨干。我们的方法共同考虑了局部信息内容与全球场景级多样性和代表性。首先，基于熵的标准有效地去除低信息样本。接下来，利用RS场景分类数据集作为参考基准，我们进行场景感知聚类，并采用分层抽样以提高聚类效果，同时降低大规模无标签数据的计算成本。最后，通过平衡聚类级别的均匀性和样本代表性，该方法在高剪枝比例下实现了细粒度选择，同时保持整体多样性和代表性。实验表明，即使在剪枝85%的训练数据后，我们的方法显著提高了收敛性和生成质量。此外，使用我们的方法训练的扩散基础模型在下游任务中始终实现了最先进的性能，包括超分辨率和语义图像合成。这种数据剪枝范式为开发RS生成基础模型提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of diffusion-based remote sensing generative foundation models, which are hindered by redundant, noisy, and imbalanced data. The authors propose a training-free, two-stage data pruning method that selects a high-quality subset of data under high pruning ratios, facilitating rapid convergence of a preliminary foundation model. Experimental results demonstrate that pruning 85% of the training data significantly improves both convergence and generation quality, with models trained using this approach achieving state-of-the-art performance in downstream tasks such as super-resolution and semantic image synthesis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高基于扩散的遥感生成基础模型的效率，这些模型受到大数据集中冗余、噪声和类别不平衡的影响。作者提出了一种无训练的两阶段数据修剪方法，在高修剪比下选择高质量的数据子集，从而促进初步基础模型的快速收敛。实验结果表明，修剪85%的训练数据显著提高了收敛性和生成质量，使用该方法训练的模型在超分辨率和语义图像合成等下游任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Wukong&#x27;s 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</div>
<div class="meta-line">Authors: Minghao Yin, Yukang Cao, Kai Han</div>
<div class="meta-line">First: 2025-11-27T13:03:57+00:00 · Latest: 2026-01-05T08:01:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.22425v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.22425v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This empowers WUKONG to support both global texture transitions and identity-preserving texture morphing, catering to diverse generation needs. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悟空的72变：通过流模型实现高保真纹理3D变形</div>
<div class="mono" style="margin-top:8px">我们提出了WUKONG，一个新颖的无训练框架，用于高保真纹理3D变形，输入为一对源和目标提示（图像或文本）。与传统方法不同——这些方法依赖于手动对应匹配和变形轨迹估计（限制了泛化并需要昂贵的预处理）——WUKONG利用基于流的变换器的生成先验，生成具有丰富纹理细节的高保真3D过渡。为了确保平滑的形状过渡，我们利用基于流的生成过程的内在连续性，将变形公式化为最优运输重心问题。我们进一步引入了一种顺序初始化策略，以防止突发的几何扭曲并保持身份一致性。为了忠实地保留纹理，我们提出了一种相似性引导的语义一致性机制，选择性地保留高频细节，并实现对混合动态的精确控制。这使得WUKONG能够支持全球纹理过渡和保持身份的纹理变形，满足多样化的生成需求。大量定量和定性评估表明，WUKONG显著优于最先进的方法，在各种几何和纹理变化中取得了卓越的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to develop a high-fidelity textured 3D morphing framework that overcomes the limitations of conventional methods, which require manual correspondence matching and extensive preprocessing. The authors introduce WUKONG, a training-free framework that utilizes flow-based transformers to achieve smooth and detailed 3D transitions by formulating morphing as an optimal transport barycenter problem. Experimental results show that WUKONG significantly outperforms existing methods in both quantitative and qualitative evaluations, demonstrating superior performance across various geometry and texture variations while maintaining identity coherence and texture fidelity.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种高保真纹理3D变形框架，以克服传统方法的局限性，这些方法需要手动对应匹配和变形轨迹估计。作者提出了WUKONG，这是一种无训练框架，利用基于流的变换器生成平滑的3D过渡，通过将变形公式化为最优运输重心问题，并实施顺序初始化策略以保持几何一致性。实验结果表明，WUKONG在定量和定性评估中显著优于现有方法，在处理各种几何和纹理变化方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance</div>
<div class="meta-line">Authors: Weichen Zhang, Zhui Zhu, Ningbo Li, Shilong Tao, Kebin Liu, Yunhao Liu</div>
<div class="meta-line">First: 2025-08-08T07:27:26+00:00 · Latest: 2026-01-05T04:04:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06084v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06084v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have achieved impressive performance on multimodal reasoning tasks such as visual question answering, image captioning and so on, but their inference cost remains a significant challenge due to the large number of vision tokens processed during the prefill stage. Existing pruning methods often rely on directly using the attention patterns or static text prompt guidance, failing to exploit the dynamic internal signals generated during inference. To address these issues, we propose AdaptInfer, a plug-and-play framework for adaptive vision token pruning in VLMs. First, we introduce a fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise text-to-text attention maps to construct soft priors over text-token importance, allowing more informed scoring of vision tokens at each stage. Second, we perform an offline analysis of cross-modal attention shifts and identify consistent inflection locations in inference, which inspire us to propose a more principled and efficient pruning schedule. Our method is lightweight and plug-and-play, also generalizable across multi-modal tasks. Experimental results have verified the effectiveness of the proposed method. For example, it reduces CUDA latency by 61.3% while maintaining an average accuracy of 93.1% on vanilla LLaVA-1.5-7B. Under the same token budget, AdaptInfer surpasses SOTA in accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaptInfer：具有动态文本指导的视觉语言模型推理的自适应令牌修剪</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在多模态推理任务（如视觉问答、图像描述等）中取得了令人印象深刻的表现，但由于在预填充阶段处理的大量视觉令牌，其推理成本仍然是一个重大挑战。现有的修剪方法通常依赖于直接使用注意力模式或静态文本提示指导，未能利用推理过程中生成的动态内部信号。为了解决这些问题，我们提出了AdaptInfer，一个用于VLMs中自适应视觉令牌修剪的即插即用框架。首先，我们引入了一种细粒度的动态文本指导修剪机制，重用逐层的文本到文本注意力图，以构建文本令牌重要性的软先验，从而在每个阶段更有信息地对视觉令牌进行评分。其次，我们对跨模态注意力变化进行了离线分析，并识别出推理中的一致拐点位置，这激励我们提出一种更有原则和高效的修剪计划。我们的方法轻量且即插即用，且可推广到多模态任务。实验结果验证了所提方法的有效性。例如，在保持93.1%的平均准确率的同时，减少了61.3%的CUDA延迟。在相同的令牌预算下，AdaptInfer在准确性上超越了SOTA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high inference costs associated with vision-language models (VLMs) due to the large number of vision tokens processed during the prefill stage. The authors propose AdaptInfer, a framework that implements a dynamic text-guided pruning mechanism to improve the efficiency of token processing. Key experimental findings demonstrate that AdaptInfer reduces CUDA latency by 61.3% while achieving an average accuracy of 93.1% on the LLaVA-1.5-7B model, outperforming state-of-the-art methods in accuracy under the same token budget.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决视觉语言模型（VLMs）在预填充阶段处理大量视觉标记所带来的显著推理成本。作者提出了AdaptInfer，一个用于自适应视觉标记修剪的框架，利用细粒度的动态文本引导修剪机制，根据层级文本到文本的注意力图改善视觉标记的评分。实验结果表明，AdaptInfer在LLaVA-1.5-7B模型上将CUDA延迟减少了61.3%，同时在相同的标记预算下实现了93.1%的平均准确率，超越了现有的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing</div>
<div class="meta-line">Authors: Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai</div>
<div class="meta-line">First: 2025-12-27T04:12:40+00:00 · Latest: 2026-01-05T04:00:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22455v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22455v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AFA-LoRA：通过激活函数退火实现LoRA中的非线性适应</div>
<div class="mono" style="margin-top:8px">低秩适应（LoRA）是一种广泛采用的参数高效微调（PEFT）方法。然而，其线性适应过程限制了其表达能力。这意味着线性训练与非线性训练之间存在差距。为了解决这个问题，我们提出了AFA-LoRA，这是一种新颖的训练策略，它在保持LoRA无缝合并性的同时，为其带来了非线性表达能力。我们的关键创新是一个退火激活函数，在训练过程中从非线性转变为线性变换，使适配器能够在收敛到可合并的线性形式之前，最初采用更强的表示能力。我们在监督微调、强化学习和推测解码上实现了我们的方法。结果表明，AFA-LoRA缩小了LoRA与全参数训练之间的性能差距。这项工作使得参数高效适应的范式更加强大和实用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of Low-Rank Adaptation (LoRA), which is constrained by its linear adaptation process, thereby reducing its expressive power compared to non-linear training methods. The authors introduce AFA-LoRA, a novel training strategy that incorporates an annealed activation function, allowing for a transition from non-linear to linear transformations during training. Experimental results demonstrate that AFA-LoRA effectively reduces the performance gap between LoRA and full-parameter training across various applications, including supervised fine-tuning, reinforcement learning, and speculative decoding, thus enhancing the practical utility of parameter-efficient adaptation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决低秩适应（LoRA）在线性适应过程中存在的局限性，这限制了其相较于非线性训练方法的表现力。作者提出了AFA-LoRA，这是一种新颖的训练策略，通过引入退火激活函数，在训练过程中实现从非线性到线性变换的过渡，从而增强LoRA的表现能力。实验结果表明，AFA-LoRA在监督微调、强化学习和推测解码等多种应用中有效缩小了LoRA与全参数训练之间的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization</div>
<div class="meta-line">Authors: Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang</div>
<div class="meta-line">First: 2024-01-18T04:23:21+00:00 · Latest: 2026-01-05T02:33:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.09716v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.09716v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HCVP：利用层次对比视觉提示进行领域泛化</div>
<div class="mono" style="margin-top:8px">领域泛化（DG）旨在通过学习不变特征来创建在未见场景中表现出色的机器学习模型。在DG中，限制模型为固定结构或统一参数化以封装不变特征的普遍做法可能会无意中混合特定方面。这种方法在细微区分跨领域变异方面存在困难，并可能对某些领域表现出偏见，从而妨碍对领域不变特征的精确学习。鉴于此，我们提出了一种新方法，旨在为模型补充领域级和任务特定特征。该方法旨在指导模型更有效地将不变特征与特定特征分离，从而提升泛化能力。基于DG范式中视觉提示的新兴趋势，我们的工作引入了新颖的层次对比视觉提示（HCVP）方法。这在该领域代表了重要的进展，以独特的生成方法、明确的模型结构和专门的损失函数区分开来。与通常在整个数据集中共享的传统视觉提示不同，HCVP利用增强对比学习的层次提示生成网络。这些生成提示是实例依赖的，满足不同领域和任务固有的独特特征。此外，我们设计了一个提示调制网络，作为桥梁，有效地将生成的视觉提示融入视觉变换器主干。对五个DG数据集进行的实验表明，HCVP的有效性，超越了既有的DG算法和适应协议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve domain generalization (DG) in machine learning by addressing the limitations of fixed model structures that can obscure the learning of invariant features. The authors propose a novel method called Hierarchical Contrastive Visual Prompt (HCVP), which incorporates domain-level and task-specific characteristics to enhance the separation of invariant features from domain-specific traits. Experimental results on five DG datasets show that HCVP significantly outperforms existing DG algorithms and adaptation protocols, demonstrating its effectiveness in promoting better generalization across unseen scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法的局限性来增强机器学习中的领域泛化（DG），现有方法往往混合特定方面并对某些领域表现出偏见。作者提出了一种名为层次对比视觉提示（HCVP）的新方法，该方法引入了层次提示生成网络和提示对比学习，以更好地区分不变特征和领域特定特征。对五个DG数据集的实验结果表明，HCVP显著优于传统DG算法和适应协议，证明了其在改善模型在未见场景中的泛化能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">VisualActBench: Can VLMs See and Act like a Human?</div>
<div class="meta-line">Authors: Daoan Zhang, Pai Liu, Xiaofei Zhou, Yuan Ge, Guangchen Lan, Jing Bi, Christopher Brinton, Ehsan Hoque, Jiebo Luo</div>
<div class="meta-line">First: 2025-12-10T18:36:18+00:00 · Latest: 2026-01-04T23:12:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09907v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models&#x27; human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs&#x27; ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisualActBench：视觉语言模型能像人类一样看和行动吗？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在感知和描述视觉环境方面取得了显著进展。然而，它们仅基于视觉输入而不依赖明确文本提示的主动推理和行动能力仍然未被充分探索。我们引入了一项新任务——视觉行动推理，并提出了VisualActBench，这是一个大规模基准，包含1,074个视频和3,733个跨四个真实场景的人类标注动作。每个动作都标记有行动优先级水平（APL）和主动-反应类型，以评估模型的人类对齐推理和价值敏感性。我们在VisualActBench上评估了29个VLMs，发现尽管像GPT4o这样的前沿模型表现相对强劲，但与人类水平的推理相比仍存在显著差距，特别是在生成主动、高优先级动作方面。我们的结果突显了当前VLMs在解释复杂背景、预测结果和与人类决策框架对齐方面的局限性。VisualActBench为评估和改善主动、以视觉为中心的人工智能代理的现实世界准备性奠定了全面基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate the proactive reasoning and action capabilities of Vision-Language Models (VLMs), which have made strides in visual perception but lack the ability to act based solely on visual inputs. The authors introduce VisualActBench, a benchmark consisting of 1,074 videos and 3,733 human-annotated actions across various scenarios, with actions categorized by Action Prioritization Level and proactive-reactive types. Evaluation of 29 VLMs reveals that while advanced models like GPT4o perform relatively well, there remains a significant gap in their ability to generate proactive, high-priority actions compared to human reasoning, indicating limitations in interpreting complex contexts and aligning with human decision-making processes.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨视觉语言模型（VLMs）在仅基于视觉输入主动推理和行动方面的局限性，因为它们目前的能力主要集中在感知和描述上。作者提出了一项新任务，称为视觉行动推理，并创建了VisualActBench，这是一个基准，包含1074个视频和3733个人工标注的动作，涵盖多个现实场景，每个动作都标记了优先级和类型。对29个VLMs在该基准上的评估显示，尽管像GPT4o这样的先进模型表现相对较好，但与人类推理相比，它们在生成主动、高优先级动作方面仍存在显著差距，表明在解释复杂上下文和与人类决策过程对齐方面存在挑战。</div>
</details>
</div>
<div class="card">
<div class="title">CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment</div>
<div class="meta-line">Authors: Kazi Ramisa Rifa, Jie Zhang, Abdullah Imran</div>
<div class="meta-line">First: 2026-01-04T17:30:45+00:00 · Latest: 2026-01-04T17:30:45+00:00</div>
<div class="meta-line">Comments: 18 pages, 9 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01613v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01613v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAP-IQA：上下文感知的提示引导CT图像质量评估</div>
<div class="mono" style="margin-top:8px">基于提示的方法通过描述性文本编码医学先验知识，但在CT图像质量评估（IQA）中仅被最小限度地探索。虽然这些提示可以嵌入关于诊断质量的先验知识，但它们往往通过反映理想化的定义而引入偏差，这些定义在现实世界的降级（如噪声、运动伪影或扫描仪变异性）下可能不成立。为此，我们提出了上下文感知提示引导图像质量评估（CAP-IQA）框架，该框架将文本级先验与实例级上下文提示相结合，并应用因果去偏差以将理想化知识与事实的、图像特定的降级分离。我们的框架结合了基于CNN的视觉编码器和特定领域的文本编码器，以评估腹部CT图像中的诊断可见性、解剖清晰度和噪声感知。在2023年LDCTIQA挑战基准上，CAP-IQA的整体相关性得分为2.8590（PLCC、SROCC和KROCC之和），超过了排名第一的团队（2.7427）4.24%。此外，我们的综合消融实验确认，提示引导融合和简化的仅编码器设计共同增强了特征对齐和可解释性。此外，在91,514张儿科CT图像的内部数据集上的评估证明了CAP-IQA在不同患者群体中评估感知保真度的真正泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve CT Image Quality Assessment (IQA) by addressing the limitations of prompt-based methods that can introduce bias from idealized definitions of diagnostic quality. The authors propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which combines a CNN-based visual encoder with a domain-specific text encoder and employs causal debiasing to differentiate between idealized knowledge and actual image-specific degradations. The experimental results show that CAP-IQA achieves a correlation score of 2.8590 on the 2023 LDCTIQA challenge benchmark, outperforming the leading team by 4.24%, and further validation on a dataset of 91,514 pediatric CT images confirms its generalizability in assessing perceptual fidelity across different patient populations.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决传统基于提示的方法所引入的偏差，来改善CT图像质量评估(IQA)。作者提出了上下文感知提示引导的图像质量评估(CAP-IQA)框架，该框架结合了基于CNN的视觉编码器和特定领域的文本编码器，整合了文本级先验与实例级上下文提示，并采用因果去偏见方法。实验结果表明，CAP-IQA在2023年LDCTIQA挑战基准上获得了2.8590的相关性得分，超越了领先团队4.24%，并且在91,514个儿科CT图像的数据集上的进一步验证确认了其在不同患者群体中评估感知保真度的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Guiding Token-Sparse Diffusion Models</div>
<div class="meta-line">Authors: Felix Krause, Stefan Andreas Baumann, Johannes Schusterbauer, Olga Grebenkova, Ming Gui, Vincent Tao Hu, Björn Ommer</div>
<div class="meta-line">First: 2026-01-04T17:18:27+00:00 · Latest: 2026-01-04T17:18:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01608v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>引导性稀疏扩散模型</div>
<div class="mono" style="margin-top:8px">扩散模型在图像合成中提供高质量，但在训练和推理过程中仍然昂贵。最近的研究利用视觉内容中的固有冗余，通过仅在视觉信息的子集上进行训练，使训练变得更加经济。尽管这些方法在提供更便宜和更有效的训练方面取得了成功，但稀疏训练的扩散模型在推理中表现不佳。这是因为它们对无分类器引导（CFG）的反应不足，导致推理时表现平平。为了解决这个问题，我们提出了稀疏引导（SG）。SG不使用条件丢弃作为引导扩散模型的信号，而是使用令牌级稀疏性。因此，SG更好地保留了条件预测的高方差，获得了良好的质量和高方差输出。在推理中利用令牌级稀疏性，SG在较低计算下提高了保真度，在常用的ImageNet-256基准上实现了1.58 FID，且减少了25%的FLOPs，并在匹配基线质量的情况下节省了高达58%的FLOP。为了证明稀疏引导的有效性，我们训练了一个25亿参数的文本到图像扩散模型，利用训练时间稀疏性，并在推理过程中利用SG。SG在组成和人类偏好评分上取得了改善，同时提高了吞吐量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of diffusion models in image synthesis, which are typically resource-intensive during training and inference. The authors introduce a novel method called Sparse Guidance (SG), which utilizes token-level sparsity instead of conditional dropout to guide the diffusion process. Experimental results demonstrate that SG significantly improves the fidelity of image outputs while reducing computational costs, achieving a 1.58 FID score on the ImageNet-256 benchmark with 25% fewer FLOPs and up to 58% FLOP savings at comparable quality levels, alongside improvements in composition and human preference scores in a 2.5B text-to-image model.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像合成中扩散模型的效率，这些模型在训练和推理过程中通常资源密集。作者提出了一种名为稀疏引导（SG）的方法，该方法利用令牌级稀疏性而不是条件丢弃来指导扩散过程。实验结果表明，SG显著提高了保真度和输出质量，在ImageNet-256基准上实现了1.58 FID，计算量减少25%，同时在保持基线质量的情况下节省了高达58%的计算量，并在一个25亿参数的文本到图像扩散模型中提高了构图和人类偏好评分。</div>
</details>
</div>
<div class="card">
<div class="title">Sim2Real SAR Image Restoration: Metadata-Driven Models for Joint Despeckling and Sidelobes Reduction</div>
<div class="meta-line">Authors: Antoine De Paepe, Pascal Nguyen, Michael Mabelle, Cédric Saleun, Antoine Jouadé, Jean-Christophe Louvigne</div>
<div class="meta-line">Venue: Proceedings of the Conference on Artificial Intelligence for Defense (CAID), 2025</div>
<div class="meta-line">First: 2026-01-04T14:32:04+00:00 · Latest: 2026-01-04T14:32:04+00:00</div>
<div class="meta-line">Comments: Accepted at the Conference on Artificial Intelligence for Defense (CAID), 2025, Rennes, France</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01541v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synthetic aperture radar (SAR) provides valuable information about the Earth&#x27;s surface under all weather and illumination conditions. However, the inherent phenomenon of speckle and the presence of sidelobes around bright targets pose challenges for accurate interpretation of SAR imagery. Most existing SAR image restoration methods address despeckling and sidelobes reduction as separate tasks. In this paper, we propose a unified framework that jointly performs both tasks using neural networks (NNs) trained on a realistic SAR simulated dataset generated with MOCEM. Inference can then be performed on real SAR images, demonstrating effective simulation to real (Sim2Real) transferability. Additionally, we incorporate acquisition metadata as auxiliary input to the NNs, demonstrating improved restoration performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sim2Real SAR图像恢复：基于元数据驱动的联合去斑和旁瓣抑制模型</div>
<div class="mono" style="margin-top:8px">合成孔径雷达（SAR）在各种天气和光照条件下提供有关地球表面的宝贵信息。然而，斑点现象和明亮目标周围的旁瓣存在给SAR图像的准确解释带来了挑战。现有的大多数SAR图像恢复方法将去斑和旁瓣抑制视为独立任务。本文提出了一个统一框架，使用在MOCEM生成的真实SAR模拟数据集上训练的神经网络（NNs）联合执行这两个任务。然后可以在真实SAR图像上进行推理，展示有效的模拟到真实（Sim2Real）可转移性。此外，我们将获取的元数据作为NNs的辅助输入，展示了改进的恢复性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the interpretation of synthetic aperture radar (SAR) imagery, which is often hindered by speckle noise and sidelobes around bright targets. The authors propose a unified framework that utilizes neural networks trained on a realistic SAR simulated dataset to jointly address despeckling and sidelobes reduction. The experimental results indicate that incorporating acquisition metadata as auxiliary input significantly enhances the performance of the restoration process, demonstrating effective transferability from simulation to real SAR images.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善合成孔径雷达（SAR）图像的解读，因其常常受到斑点噪声和亮目标周围的旁瓣影响。作者提出了一个统一框架，利用在MOCEM生成的真实SAR模拟数据集上训练的神经网络，联合处理去斑和旁瓣减少。实验结果表明，将获取元数据作为辅助输入显著提高了在真实SAR图像上应用模型时的恢复性能，展示了有效的模拟到真实（Sim2Real）转移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Video Editing via Optical Flow-Enhanced Score Distillation</div>
<div class="meta-line">Authors: Lianghan Zhu, Yanqi Bao, Jing Huo, Jing Wu, Yu-Kun Lai, Wenbin Li, Yang Gao</div>
<div class="meta-line">First: 2024-06-07T12:33:59+00:00 · Latest: 2026-01-04T11:18:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.04888v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.04888v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement in visual generation, particularly the emergence of pre-trained text-to-image and text-to-video models, has catalyzed growing interest in training-free video editing research. Mirroring training-free image editing techniques, current approaches preserve original video information through video input inversion and manipulating intermediate features and attention during the inference process to achieve content editing. Although they have demonstrated promising results, the lossy nature of the inversion process poses significant challenges in maintaining unedited regions of the video. Furthermore, feature and attention manipulation during inference can lead to unintended over-editing and face challenges in both local temporal continuity and global content consistency. To address these challenges, this study proposes a score distillation paradigm based on pre-trained text-to-video models, where the original video is iteratively optimized through multiple steps guided by editing gradients provided by score distillation to ultimately obtain the target video. The iterative optimization starting from the original video, combined with content preservation loss, ensures the maintenance of unedited regions in the original video and suppresses over-editing. To further guarantee video content consistency and temporal continuity, we additionally introduce a global consistency auxiliary loss and optical flow prediction-based local editing gradient smoothing. Experiments demonstrate that these strategies effectively address the aforementioned challenges, achieving comparable or superior performance across multiple dimensions including preservation of unedited regions, local temporal continuity, and global content consistency of editing results, compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无训练视频编辑通过光流增强的评分蒸馏</div>
<div class="mono" style="margin-top:8px">视觉生成的快速进展，特别是预训练文本到图像和文本到视频模型的出现，催生了对无训练视频编辑研究的日益关注。与无训练图像编辑技术相似，当前的方法通过视频输入反演和在推理过程中操控中间特征和注意力来保持原始视频信息，以实现内容编辑。尽管它们展示了有希望的结果，但反演过程的有损特性在保持视频未编辑区域方面带来了重大挑战。此外，推理过程中的特征和注意力操控可能导致意外的过度编辑，并在局部时间连续性和全局内容一致性方面面临挑战。为了解决这些挑战，本研究提出了一种基于预训练文本到视频模型的评分蒸馏范式，其中原始视频通过多个步骤迭代优化，受评分蒸馏提供的编辑梯度指导，最终获得目标视频。从原始视频开始的迭代优化，结合内容保持损失，确保保持原始视频中的未编辑区域并抑制过度编辑。为了进一步保证视频内容一致性和时间连续性，我们还引入了全局一致性辅助损失和基于光流预测的局部编辑梯度平滑。实验表明，这些策略有效解决了上述挑战，在未编辑区域保持、局部时间连续性和编辑结果的全局内容一致性等多个维度上实现了与最先进方法相当或更优的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study is motivated by the challenges in training-free video editing, particularly the issues of maintaining unedited regions and ensuring temporal continuity and content consistency during the editing process. The authors propose a score distillation paradigm that iteratively optimizes the original video using editing gradients from pre-trained text-to-video models, incorporating a content preservation loss and a global consistency auxiliary loss, along with optical flow-based local editing gradient smoothing. Experimental results indicate that this approach effectively preserves unedited regions and enhances both local temporal continuity and global content consistency, achieving performance that is comparable or superior to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了无训练视频编辑中的挑战，特别是在保持未编辑区域和确保时间连续性及内容一致性方面的问题。作者提出了一种基于预训练文本到视频模型的评分蒸馏范式，通过编辑梯度对原始视频进行迭代优化，结合内容保持损失和全局一致性辅助损失，以及基于光流的局部编辑梯度平滑。实验结果表明，该方法有效地保持了未编辑区域，并改善了局部时间连续性和全局内容一致性，其性能与现有最先进的方法相比具有可比性或优越性。</div>
</details>
</div>
<div class="card">
<div class="title">EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction</div>
<div class="meta-line">Authors: Hsi-Che Lin, Yu-Chu Yu, Kai-Po Chang, Yu-Chiang Frank Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-13T17:59:58+00:00 · Latest: 2026-01-04T08:08:32+00:00</div>
<div class="meta-line">Comments: Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Project page: https://hsi-che-lin.github.io/EMLoC/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12015v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12015v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hsi-che-lin.github.io/EMLoC/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model, which originally required 95GB of memory, on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMLoC：基于仿真的内存高效微调与LoRA修正</div>
<div class="mono" style="margin-top:8px">开源基础模型的快速采用和发展，使其在各个领域具备强大的通用能力。然而，由于微调大型基础模型以适应特定领域或个性化任务的内存开销远超推理，绝大多数用户仍然面临高昂的成本。我们提出了EMLoC，一种基于仿真的内存高效微调框架，结合LoRA修正，使得模型微调在与推理相同的内存预算内进行。EMLoC通过对小型下游校准集进行激活感知的奇异值分解（SVD），构建任务特定的轻量级仿真器。然后，通过LoRA在这个轻量级仿真器上进行微调。为了解决原始模型与压缩仿真器之间的错位问题，我们提出了一种新颖的补偿算法来修正微调后的LoRA模块，从而可以将其合并到原始模型中进行推理。EMLoC支持灵活的压缩比和标准训练流程，使其适应广泛的应用。大量实验表明，EMLoC在多个数据集和模态上优于其他基线。此外，在不进行量化的情况下，EMLoC能够在单个24GB消费级GPU上微调一个原本需要95GB内存的38B模型，为个体用户带来了高效且实用的模型适应能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high memory costs associated with fine-tuning large foundation models for specific tasks, which limits accessibility for many users. The authors introduce EMLoC, a framework that utilizes an emulator-based approach combined with LoRA correction to enable fine-tuning within the memory constraints of inference. Experimental results show that EMLoC significantly outperforms existing methods across various datasets and modalities, allowing for the fine-tuning of a 38B model on a single 24GB consumer GPU, which previously required 95GB of memory, thus making model adaptation more efficient and practical for individual users.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决针对特定任务微调大型基础模型所需的高内存要求，这限制了许多用户的可访问性。作者提出了EMLoC框架，该框架利用基于仿真器的方法结合LoRA校正，使微调能够在与推理相同的内存限制内进行。实验结果表明，EMLoC在多个数据集和模态上显著优于现有方法，使得在单个24GB消费级GPU上无量化地微调38B模型成为可能，从而使模型适应更加高效和实用。</div>
</details>
</div>
<div class="card">
<div class="title">DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer</div>
<div class="meta-line">Authors: Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, Qian He</div>
<div class="meta-line">First: 2026-01-04T08:07:11+00:00 · Latest: 2026-01-04T08:07:11+00:00</div>
<div class="meta-line">Comments: Project: https://guoxu1233.github.io/DreamID-V/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01425v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.01425v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://guoxu1233.github.io/DreamID-V/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamID-V：通过扩散变换器弥合高保真面部交换的图像到视频的差距</div>
<div class="mono" style="margin-top:8px">视频面部交换（VFS）需要无缝地将源身份注入目标视频，同时仔细保留原始姿势、表情、光照、背景和动态信息。现有方法在保持身份相似性和属性保留的同时，难以维持时间一致性。为了解决这一挑战，我们提出了一个全面的框架，将图像面部交换（IFS）的优势无缝转移到视频领域。我们首先引入了一种新颖的数据管道SyncID-Pipe，预训练一个身份锚定的视频合成器，并将其与IFS模型结合，构建双向ID四元组以进行显式监督。在配对数据的基础上，我们提出了第一个基于扩散变换器的框架DreamID-V，采用核心的模态感知条件模块以区分性地注入多模态条件。同时，我们提出了一种从合成到真实的课程机制和身份一致性强化学习策略，以增强在挑战场景下的视觉真实感和身份一致性。为了解决基准测试有限的问题，我们引入了IDBench-V，这是一个涵盖多样场景的综合基准。大量实验表明，DreamID-V超越了最先进的方法，并进一步展现出卓越的多功能性，可以无缝适应各种与交换相关的任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve Video Face Swapping (VFS) by ensuring identity similarity and attribute preservation while maintaining temporal consistency, which existing methods struggle to achieve. The authors propose a novel framework called DreamID-V, which utilizes a Diffusion Transformer and a new data pipeline, SyncID-Pipe, to create bidirectional ID quadruplets for explicit supervision. Experimental results show that DreamID-V significantly outperforms current state-of-the-art methods in terms of visual realism and identity consistency, demonstrating its versatility across various face-swapping tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视频换脸（VFS），确保在保持姿势、表情、光照和时间一致性的同时，将源身份无缝整合到目标视频中，而现有方法在这方面存在困难。作者提出了一种名为DreamID-V的新框架，该框架利用扩散变换器和新的数据管道SyncID-Pipe，创建双向身份四元组以实现更好的监督。实验结果表明，DreamID-V在视觉真实感和身份一致性方面显著优于当前最先进的方法，并且在各种换脸任务中表现出多样性，同时引入了一个全面的基准IDBench-V以供评估。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
