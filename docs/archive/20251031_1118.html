<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-31 11:18</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251031_1118</div>
    <div class="row"><div class="card">
<div class="title">Improving Temporal Consistency and Fidelity at Inference-time in   Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</div>
<div class="meta-line">Authors: Nasrin Rahimi, A. Murat Tekalp</div>
<div class="meta-line">First: 2025-10-29T11:40:06+00:00 · Latest: 2025-10-29T11:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25420v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as powerful priors for single-image
restoration, but their application to zero-shot video restoration suffers from
temporal inconsistencies due to the stochastic nature of sampling and
complexity of incorporating explicit temporal modeling. In this work, we
address the challenge of improving temporal coherence in video restoration
using zero-shot image-based diffusion models without retraining or modifying
their architecture. We propose two complementary inference-time strategies: (1)
Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired
perceptual straightening hypothesis, which steers the diffusion denoising
process towards smoother temporal evolution by incorporating a curvature
penalty in a perceptual space to improve temporal perceptual scores, such as
Fr\&#x27;echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path
Ensemble Sampling (MPES), which aims at reducing stochastic variation by
ensembling multiple diffusion trajectories to improve fidelity (distortion)
scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these
training-free techniques provide a practical path toward temporally stable
high-fidelity perceptual video restoration using large pretrained diffusion
models. We performed extensive experiments over multiple datasets and
degradation types, systematically evaluating each strategy to understand their
strengths and limitations. Our results show that while PSG enhances temporal
naturalness, particularly in case of temporal blur, MPES consistently improves
fidelity and spatio-temporal perception--distortion trade-off across all tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过零-shot基于图像的扩散模型在推理时改善感知视频恢复的时间一致性和保真度</div>
<div class="mono" style="margin-top:8px">扩散模型作为单图像恢复的强大先验已逐渐崭露头角，但其在零-shot视频恢复中的应用由于采样的随机性和显式时间建模的复杂性而面临时间不一致性的问题。在本研究中，我们解决了在不重新训练或修改架构的情况下，使用零-shot基于图像的扩散模型改善视频恢复中的时间一致性这一挑战。我们提出了两种互补的推理时策略：(1) 基于神经科学启发的感知拉直假设的感知拉直引导（PSG），通过在感知空间中引入曲率惩罚，引导扩散去噪过程朝向更平滑的时间演变，以提高时间感知评分，如Fréchet视频距离（FVD）和感知直线性；(2) 多路径集成采样（MPES），旨在通过集成多个扩散轨迹来减少随机变化，以提高保真度（失真）评分，如PSNR和SSIM，而不牺牲清晰度。这些无训练的技术共同为使用大型预训练扩散模型实现时间稳定的高保真感知视频恢复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估每种策略以理解其优缺点。我们的结果表明，尽管PSG在时间自然性方面有所增强，特别是在时间模糊的情况下，MPES在所有任务中始终改善了保真度和时空感知-失真权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the temporal inconsistencies encountered in zero-shot video restoration using diffusion models, which arise from their stochastic sampling nature and the difficulty of incorporating explicit temporal modeling. The authors propose two inference-time strategies: Perceptual Straightening Guidance (PSG), which improves temporal coherence by applying a curvature penalty in perceptual space, and Multi-Path Ensemble Sampling (MPES), which reduces stochastic variation by combining multiple diffusion trajectories. Experimental results demonstrate that PSG enhances temporal naturalness, especially in cases of temporal blur, while MPES consistently improves fidelity and the trade-off between perception and distortion across various tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过扩散模型提高零-shot视频恢复中的时间一致性，而扩散模型通常由于其随机采样特性面临挑战。作者提出了两种推理时策略：感知直线引导（PSG），利用曲率惩罚来提高时间感知分数，以及多路径集成采样（MPES），通过结合多个扩散轨迹来减少随机变化，从而提高保真度分数。实验结果表明，PSG在时间模糊的情况下有效改善了时间自然性，而MPES在各种任务中始终增强了保真度和感知与失真之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired   Monocentric Design</div>
<div class="meta-line">Authors: Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang</div>
<div class="meta-line">First: 2025-10-29T09:27:38+00:00 · Latest: 2025-10-29T09:27:38+00:00</div>
<div class="meta-line">Comments: The source code will be publicly available at
  https://github.com/ZongxiYu-ZJU/BMI</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25314v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25314v1">PDF</a> · <a href="https://github.com/ZongxiYu-ZJU/BMI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清晰而深刻的观察：一种基于生物启发的单心设计的RGBD成像方法</div>
<div class="mono" style="margin-top:8px">实现高保真、紧凑的RGBD成像面临双重挑战：传统紧凑光学在整个景深范围内难以保持RGB清晰度，而仅依赖软件的单目深度估计（MDE）是一个不适定问题，依赖于不可靠的语义先验。虽然具有衍射光学元件（DOE）的深度光学可以编码深度，但它们在制造复杂性和色差方面引入了权衡，妨碍了简单性。为了解决这个问题，我们首先介绍了一种新颖的生物启发的全球面单心透镜，围绕它构建了仿生单心成像（BMI）框架，这是一个整体协同设计。该光学设计自然地将深度编码到其深度变化的点扩散函数（PSF）中，而无需复杂的衍射或自由形状元件。我们建立了一个严格的基于物理的前向模型，通过精确模拟光学退化过程生成合成数据集。该模拟管道与一个双头多尺度重建网络共同设计，该网络采用共享编码器共同恢复高保真的全聚焦（AiF）图像和精确的深度图。大量实验验证了所提框架的最先进性能。在深度估计中，该方法达到了0.026的绝对相对误差（Abs Rel）和0.130的均方根误差（RMSE），显著优于领先的软件方法和其他深度光学系统。在图像恢复方面，该系统达到了0.960的结构相似性指数（SSIM）和0.082的感知LPIPS分数，从而确认了图像保真度与深度准确性之间的优越平衡。本研究表明，生物启发的全球面光学与联合重建算法的结合构成了解决高性能紧凑RGBD成像内在挑战的有效策略。源代码将公开发布在https://github.com/ZongxiYu-ZJU/BMI。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of achieving high-fidelity, compact RGBD imaging, where traditional optics struggle with RGB sharpness and software-based depth estimation relies on unreliable priors. The authors introduce a bio-inspired all-spherical monocentric lens and develop the Bionic Monocentric Imaging (BMI) framework, which co-designs optical elements and a dual-head reconstruction network to recover high-fidelity images and precise depth maps from single captures. Experimental results demonstrate that the method significantly outperforms existing software-only approaches and deep optics systems, achieving an Abs Rel of 0.026 and RMSE of 0.130 for depth estimation, and an SSIM of 0.960 and LPIPS score of 0.082 for image restoration, indicating a successful balance between image fidelity and depth accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于克服传统紧凑光学和仅依赖软件的单目深度估计（MDE）在实现高保真RGBD成像方面的局限性。作者提出了一种新颖的生物启发的全球面单光心透镜，并开发了生物单光心成像（BMI）框架，该框架将光学设计与双头多尺度重建网络相结合。实验结果表明，所提出的方法显著优于现有方法，在深度估计中达到0.026的绝对相对误差和0.130的均方根误差，在图像恢复中实现0.960的结构相似性指数和0.082的感知LPIPS得分，表明在图像保真度和深度准确性之间成功实现了平衡。</div>
</details>
</div>
<div class="card">
<div class="title">DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt   State Space Model</div>
<div class="meta-line">Authors: Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao</div>
<div class="meta-line">First: 2025-04-24T16:46:32+00:00 · Latest: 2025-10-29T07:04:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.17732v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.17732v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">All-in-One image restoration aims to address multiple image degradation
problems using a single model, offering a more practical and versatile solution
compared to designing dedicated models for each degradation type. Existing
approaches typically rely on Degradation-specific models or coarse-grained
degradation prompts to guide image restoration. However, they lack fine-grained
modeling of degradation information and face limitations in balancing
multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a
novel All-in-One image restoration framework that introduces a fine-grained
degradation extractor and a Degradation-Aware Prompt State Space Model
(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured
by the extractor as dynamic prompts, which are then incorporated into the state
space modeling process. This enhances the model&#x27;s adaptability to diverse
degradation types, while a complementary High-Frequency Enhancement Block (HEB)
recovers local high-frequency details. Extensive experiments on a mixed dataset
containing seven degradation types show that DPMambaIR achieves the best
performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These
results highlight the potential and superiority of DPMambaIR as a unified
solution for All-in-One image restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DPMambaIR：通过降解感知提示的全能图像修复状态空间模型</div>
<div class="mono" style="margin-top:8px">全能图像修复旨在通过单一模型解决多种图像降解问题，提供比为每种降解类型设计专用模型更实用和多功能的解决方案。现有方法通常依赖于特定降解模型或粗粒度降解提示来指导图像修复。然而，它们缺乏对降解信息的细粒度建模，并在平衡多任务冲突方面面临限制。为克服这些限制，我们提出了DPMambaIR，这是一种新颖的全能图像修复框架，引入了细粒度降解提取器和降解感知提示状态空间模型（DP-SSM）。DP-SSM利用提取器捕获的细粒度降解特征作为动态提示，然后将其纳入状态空间建模过程中。这增强了模型对多种降解类型的适应性，同时补充的高频增强模块（HEB）恢复局部高频细节。在包含七种降解类型的混合数据集上进行的广泛实验表明，DPMambaIR在PSNR和SSIM中分别达到了27.69dB和0.893的最佳性能。这些结果突显了DPMambaIR作为全能图像修复统一解决方案的潜力和优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a versatile solution for All-in-One image restoration that can effectively handle multiple types of image degradation without the need for separate models. The authors propose DPMambaIR, which utilizes a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM) to dynamically incorporate detailed degradation features into the restoration process. Experimental results demonstrate that DPMambaIR outperforms existing methods, achieving a PSNR of 27.69dB and an SSIM of 0.893 across a mixed dataset with seven degradation types, indicating its effectiveness as a unified restoration framework.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一种多功能的全能图像修复解决方案，能够有效处理多种类型的图像退化，而无需单独的模型。作者提出了DPMambaIR，利用细粒度退化提取器和退化感知提示状态空间模型（DP-SSM），将详细的退化特征动态融入修复过程中。对包含七种退化类型的混合数据集的实验结果表明，DPMambaIR的表现优于现有方法，PSNR达到27.69dB，SSIM为0.893，表明其作为统一修复框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Depth-Aware Super-Resolution via Distance-Adaptive Variational   Formulation</div>
<div class="meta-line">Authors: Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu</div>
<div class="meta-line">First: 2025-09-06T15:35:37+00:00 · Latest: 2025-10-29T04:32:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05746v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.05746v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于距离自适应变分公式的深度感知超分辨率</div>
<div class="mono" style="margin-top:8px">单幅图像超分辨率传统上假设空间不变的退化模型，但现实世界的成像系统表现出复杂的距离依赖效应，包括大气散射、景深变化和透视失真。这一基本限制需要空间自适应重建策略，明确结合几何场景理解以实现最佳性能。我们提出了一个严格的变分框架，将超分辨率表征为空间变化的逆问题，将退化算子表述为具有距离依赖谱特性的伪微分算子，从而能够对不同深度范围的重建极限进行理论分析。我们的神经架构通过级联残差块和深度条件卷积核实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时结合学习的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的谱约束防止了远场区域的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。对五个基准数据集的全面评估表明，性能达到最先进水平，在KITTI户外场景中，在2倍和4倍缩放下分别实现36.89/0.9516和30.54/0.8721的PSNR/SSIM，分别比现有方法提高0.44dB和0.36dB。该工作建立了第一个理论基础的距离自适应超分辨率框架，并在深度变化场景中展示了显著改进，同时在传统基准上保持竞争性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of traditional single image super-resolution methods that rely on spatially-invariant degradation models, which do not account for real-world complexities such as atmospheric scattering and depth variations. The authors propose a variational framework that treats super-resolution as a spatially-varying inverse problem, utilizing a pseudodifferential operator to analyze reconstruction limits across different depths. Experimental results on five benchmark datasets show that their method achieves state-of-the-art performance, with PSNR/SSIM scores of 36.89/0.9516 and 30.54/0.8721 at scales of 2 and 4 on KITTI outdoor scenes, surpassing existing techniques by notable margins.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统单幅图像超分辨率方法的局限性，这些方法依赖于空间不变的降解模型，而未考虑现实世界成像系统中存在的复杂距离依赖效应。作者提出了一种变分框架，将超分辨率视为一个空间变化的逆问题，利用具有距离依赖特性的伪微分算子来分析重建极限。实验结果表明，他们的方法在五个基准数据集上实现了最先进的性能，在KITTI户外场景中，PSNR和SSIM指标显著提高，分别比现有技术高出0.44dB和0.36dB。</div>
</details>
</div>
<div class="card">
<div class="title">WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and   Mamba-based Channel Modeling with Texture Enhancement</div>
<div class="meta-line">Authors: Shengyu Zhu, Congyi Fan, Fuxuan Zhang</div>
<div class="meta-line">First: 2025-10-19T09:11:58+00:00 · Latest: 2025-10-29T02:07:16+00:00</div>
<div class="meta-line">Comments: Chinese Conference on Pattern Recognition and Computer Vision (PRCV),
  Oral</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16765v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.16765v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaMaIR：通过多尺度小波卷积和基于Mamba的通道建模进行图像恢复与纹理增强</div>
<div class="mono" style="margin-top:8px">图像恢复是计算机视觉中的一项基础而具有挑战性的任务，基于CNN的框架展示了显著的计算效率。然而，之前的基于CNN的方法在充分恢复细腻纹理细节方面常常面临挑战，这受到CNN结构小感受野和缺乏通道特征建模的限制。本文提出了WaMaIR，这是一个具有大感受野的新框架，用于图像感知，并改善恢复图像中的纹理细节重建。具体而言，我们引入了全球多尺度小波变换卷积（GMWTConvs），以扩展感受野以提取图像特征，保留和丰富模型输入中的纹理特征。同时，我们提出了基于Mamba的通道感知模块（MCAM），专门设计用于捕捉特征通道中的长程依赖性，从而增强模型对颜色、边缘和纹理信息的敏感性。此外，我们提出了多尺度纹理增强损失（MTELoss）用于图像恢复，以有效指导模型保留详细的纹理结构。大量实验确认WaMaIR优于最先进的方法，实现了更好的图像恢复和模型的高效计算性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by CNN-based frameworks in image restoration, particularly in restoring fine texture details due to their limited receptive fields and inadequate channel feature modeling. The authors propose WaMaIR, a novel framework that utilizes Global Multiscale Wavelet Transform Convolutions to expand the receptive field and enhance texture feature extraction, alongside a Mamba-Based Channel-Aware Module designed to capture long-range dependencies in feature channels. Experimental results demonstrate that WaMaIR significantly outperforms state-of-the-art methods, achieving superior image restoration and computational efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决基于CNN的图像恢复框架在恢复细微纹理细节方面的局限性，特别是由于小感受野和不足的通道特征建模。作者提出了WaMaIR，这是一种新颖的框架，利用全局多尺度小波变换卷积来扩展感受野并增强纹理特征提取，同时采用基于Mamba的通道感知模块来捕捉特征通道中的长程依赖。实验结果表明，WaMaIR在图像恢复方面显著优于现有的最先进方法，同时保持高效的计算性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
