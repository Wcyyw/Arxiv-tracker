<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0330</div>
    <div class="row"><div class="card">
<div class="title">HiCache: A Plug-in Scaled-Hermite Upgrade for Taylor-Style Cache-then-Forecast Diffusion Acceleration</div>
<div class="meta-line">Authors: Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang</div>
<div class="meta-line">First: 2025-08-23T10:35:16+00:00 · Latest: 2026-01-26T18:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16984v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.16984v2">PDF</a> · <a href="https://github.com/fenglang918/HiCache">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have achieved remarkable success in content generation but often incur prohibitive computational costs due to iterative sampling. Recent feature caching methods accelerate inference via temporal extrapolation, yet can suffer quality degradation from inaccurate modeling of the complex dynamics of feature evolution. We propose HiCache (Hermite Polynomial-based Feature Cache), a training-free acceleration framework that improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature-derivative approximations in diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials as a potentially optimal basis for Gaussian-correlated processes. We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy, and is also effective when applied standalone or integrated with TaylorSeer. Extensive experiments demonstrate HiCache&#x27;s superiority, achieving 5.55x speedup on FLUX.1-dev while matching or exceeding baseline quality, and maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Moreover, HiCache can be naturally added to previous caching methods to enhance their performance, e.g., improving ClusCa from 0.9480 to 0.9840 in terms of image rewards. Code: https://github.com/fenglang918/HiCache</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiCache：一种用于泰勒风格缓存-然后预测扩散加速的插件式缩放-埃尔米特升级</div>
<div class="mono" style="margin-top:8px">扩散模型在内容生成方面取得了显著成功，但由于迭代采样，通常会产生高昂的计算成本。最近的特征缓存方法通过时间外推加速推理，但可能因特征演变复杂动态建模不准确而导致质量下降。我们提出了HiCache（基于埃尔米特多项式的特征缓存），这是一种无训练加速框架，通过将数学工具与经验特性对齐来改善特征预测。我们的关键见解是，扩散变换器中的特征导数近似表现出多元高斯特征，这促使我们使用埃尔米特多项式作为高斯相关过程的潜在最佳基。我们进一步引入了一种双缩放机制，确保数值稳定性，同时保持预测准确性，并且在单独应用或与TaylorSeer集成时也有效。大量实验表明，HiCache的优越性，在FLUX.1-dev上实现了5.55倍的加速，同时匹配或超过基线质量，并在文本到图像、视频生成和超分辨率任务中保持强劲表现。此外，HiCache可以自然地添加到以前的缓存方法中以增强其性能，例如，在图像奖励方面将ClusCa从0.9480提高到0.9840。代码：https://github.com/fenglang918/HiCache</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational costs associated with iterative sampling in diffusion models, which are widely used for content generation. The authors propose HiCache, a training-free acceleration framework that leverages Hermite polynomials to improve feature prediction by aligning mathematical tools with empirical properties. Experimental results show that HiCache achieves a 5.55x speedup on the FLUX.1-dev dataset while maintaining or exceeding the quality of baseline models, and it demonstrates strong performance across various tasks such as text-to-image, video generation, and super-resolution, also enhancing existing caching methods like ClusCa significantly.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决扩散模型在内容生成中由于迭代采样而导致的高计算成本。作者提出了HiCache，这是一种无训练的加速框架，通过使用Hermite多项式来改善特征预测，将数学工具与经验特性相结合。实验结果表明，HiCache在FLUX.1-dev上实现了5.55倍的加速，同时在各种任务（包括文本到图像、视频生成和超分辨率）中保持或超过基线质量，并且还可以增强现有缓存方法如ClusCa的性能。</div>
</details>
</div>
<div class="card">
<div class="title">TensLoRA: Tensor Alternatives for Low-Rank Adaptation</div>
<div class="meta-line">Authors: Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, François Leduc-Primeau</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-22T17:15:23+00:00 · Latest: 2026-01-26T17:51:38+00:00</div>
<div class="meta-line">Comments: Published at ICASSP 2026. 5 pages, 1 figure, 2 tables. Code can be found at https://github.com/ax-le/TensLoRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19391v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19391v2">PDF</a> · <a href="https://github.com/ax-le/TensLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TensLoRA：低秩适应的张量替代方案</div>
<div class="mono" style="margin-top:8px">低秩适应（LoRA）广泛用于通过向注意力投影添加可训练的低秩矩阵来高效适应变换器。尽管有效，这些矩阵被认为是每个注意力投影（查询、键和值）和每层独立的。最近的扩展考虑了联合的基于张量的适应，但仅限于有限的形式且没有系统框架。我们引入了TensLoRA，一个统一框架，将LoRA更新聚合为高阶张量，并建模广泛的基于张量的低秩适应家族。我们的公式推广了现有的基于张量的方法，并实现了特定模式的压缩率，使参数预算能够根据模态和任务进行调整。在视觉和语言基准上的实验表明，张量构造直接影响性能，有时在相似参数数量下优于标准LoRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the efficiency of Low-Rank Adaptation (LoRA) in Transformers by addressing the limitations of independent low-rank matrices used in attention projections. The authors propose TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors, allowing for a broader range of tensor-based low-rank adaptations with mode-specific compression rates. Experimental results on vision and language benchmarks demonstrate that the proposed tensor construction can improve performance compared to standard LoRA while maintaining similar parameter counts.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决在注意力投影中使用的独立低秩矩阵的局限性，来提高变换器中低秩适应（LoRA）的效率。作者提出了TensLoRA，一个将LoRA更新聚合为高阶张量的统一框架，从而实现对基于张量的适应的更系统化的方法。在视觉和语言基准上的实验结果表明，所提出的张量构造可以显著影响性能，通常在保持相似参数数量的情况下优于标准LoRA。</div>
</details>
</div>
<div class="card">
<div class="title">No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</div>
<div class="meta-line">Authors: Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-05T17:58:05+00:00 · Latest: 2026-01-26T17:30:19+00:00</div>
<div class="meta-line">Comments: ICLR 2026. Self-Representation Alignment for Diffusion Transformers. Code: https://github.com/vvvvvjdy/SRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02831v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.02831v5">PDF</a> · <a href="https://github.com/vvvvvjdy/SRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have demonstrated that learning a meaningful internal representation can accelerate generative training. However, existing approaches necessitate to either introduce an off-the-shelf external representation task or rely on a large-scale, pre-trained external representation encoder to provide representation guidance during the training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We propose SelfRepresentation Alignment (SRA), a simple yet effective method that obtains representation guidance using the internal representations of learned diffusion transformer. SRA aligns the latent representation of the diffusion transformer in the earlier layer conditioned on higher noise to that in the later layer conditioned on lower noise to progressively enhance the overall representation learning during only the training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements, and largely outperforms approaches relying on auxiliary representation task. Our approach achieves performance comparable to methods that are dependent on an external pre-trained representation encoder, which demonstrates the feasibility of acceleration with representation alignment in diffusion transformers themselves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需其他表示组件：扩散变换器可以自我提供表示指导</div>
<div class="mono" style="margin-top:8px">最近的研究表明，学习有意义的内部表示可以加速生成训练。然而，现有的方法需要引入现成的外部表示任务或依赖于大规模的预训练外部表示编码器在训练过程中提供表示指导。在本研究中，我们认为扩散变换器固有的独特判别过程使其能够在不需要外部表示组件的情况下提供这种指导。我们提出了自表示对齐（SRA），这是一种简单而有效的方法，通过学习的扩散变换器的内部表示获得表示指导。SRA将扩散变换器在较高噪声条件下的早期层的潜在表示与在较低噪声条件下的后期层的潜在表示对齐，以逐步增强整体表示学习，仅在训练过程中进行。实验结果表明，将SRA应用于DiTs和SiTs可以持续提高性能，并且大大优于依赖辅助表示任务的方法。我们的方法在性能上可与依赖外部预训练表示编码器的方法相媲美，这证明了在扩散变换器内部通过表示对齐加速的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to improve generative training by utilizing internal representations in diffusion transformers, eliminating the need for external representation tasks or pre-trained encoders. The authors introduce a method called Self-Representation Alignment (SRA), which aligns latent representations of diffusion transformers across different noise levels to enhance representation learning during training. Experimental results show that SRA applied to diffusion transformers leads to consistent performance improvements and outperforms methods that rely on auxiliary representation tasks, achieving results comparable to those using external pre-trained encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用内部表示来增强生成训练，而无需依赖外部组件。作者提出了一种名为自表示对齐（SRA）的方法，该方法利用扩散变换器独特的区分能力，在训练过程中提供表示指导。实验结果表明，与依赖外部表示任务的传统方法相比，SRA显著提高了扩散变换器的性能，且其结果与使用预训练外部编码器的方法相当。</div>
</details>
</div>
<div class="card">
<div class="title">A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models</div>
<div class="meta-line">Authors: Shihab Aaqil Ahamed, Udaya S. K. P. Miriya Thanthrige, Ranga Rodrigo, Muhammad Haris Khan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-30T12:45:24+00:00 · Latest: 2026-01-26T17:12:54+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26441v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs&#x27; reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A-TPT：用于视觉语言模型测试时提示调优的角度多样性校准属性</div>
<div class="mono" style="margin-top:8px">测试时提示调优（TPT）已成为一种有前景的技术，用于在不依赖标记数据的情况下将大型视觉语言模型（VLM）适应于未见任务。然而，文本特征之间缺乏分散性可能会影响校准性能，这引发了对VLM可靠性、可信度和安全性的担忧。目前的TPT方法主要集中在通过最大化平均文本特征分散性或强制正交约束来改善提示校准，以鼓励角度分离。然而，这些方法可能并不总是实现类间文本特征之间的最佳角度分离，这意味着忽视了角度多样性的重要作用。为了解决这个问题，我们提出了A-TPT，这是一种新颖的TPT框架，引入角度多样性以鼓励由相应可学习提示引起的归一化文本特征分布的均匀性。这种均匀性是通过最大化单位超球面上特征之间的最小成对角度距离来实现的。我们通过在不同数据集上对各种骨干网络进行广泛实验，表明我们的方法在减少聚合平均校准误差方面始终超过最先进的TPT方法，同时保持可比的准确性。值得注意的是，我们的方法在自然分布变化上的零样本校准性能优越，并且在医学数据集上具有良好的泛化能力。我们提供了广泛的分析，包括理论方面，以建立A-TPT的基础。这些结果突显了促进角度多样性以实现良好分散的文本特征的有效性，显著改善了VLM在测试时适应过程中的校准。我们的代码将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reliability and calibration performance of vision-language models (VLMs) during test-time prompt tuning (TPT), particularly in scenarios lacking labeled data. The authors propose a novel framework called A-TPT, which introduces angular diversity to improve the uniformity of normalized textual feature distributions by maximizing the minimum pairwise angular distance between features on the unit hypersphere. Experimental results demonstrate that A-TPT consistently outperforms existing TPT methods in reducing average calibration error while maintaining accuracy, showing particularly strong zero-shot calibration performance on natural distribution shifts and good generalization to medical datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高视觉语言模型（VLMs）在测试时提示调优（TPT）过程中的可靠性和安全性，特别是解决文本特征分散不足的问题，这可能对校准性能产生负面影响。作者提出了一种名为A-TPT的新框架，该框架引入了角度多样性，通过最大化单位超球面上特征之间的最小成对角距离，促进归一化文本特征的更均匀分布。实验结果表明，A-TPT在减少平均校准误差方面始终优于现有的TPT方法，同时保持相似的准确性，特别是在自然分布变化下表现出强大的零-shot 校准性能，并有效地推广到医学数据集。</div>
</details>
</div>
<div class="card">
<div class="title">Is In-Context Learning Learning?</div>
<div class="meta-line">Authors: Adrian de Wynter</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-12T17:12:04+00:00 · Latest: 2026-01-26T16:34:06+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10414v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.10414v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model&#x27;s ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input&#x27;s linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression&#x27;s ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文学习算不算学习？</div>
<div class="mono" style="margin-top:8px">上下文学习（ICL）允许一些自回归模型通过下一个标记预测来解决任务，而无需进一步训练。这导致了关于这些模型能够仅通过提示中的少量示例解决（学习）未见任务的说法。然而，推理并不总意味着学习，因为ICL并未明确编码给定的观察。相反，模型依赖于其先前知识和给定的示例（如果有的话）。我们认为，从数学上讲，ICL确实构成学习，但其完整特征需要实证工作。然后，我们对ICL进行了大规模分析，剔除或考虑了记忆、预训练、分布变化以及提示风格和措辞。我们发现ICL是一种有效的学习范式，但在学习和推广未见任务的能力上有限。我们注意到，在示例数量增多的极限情况下，准确性对示例分布、模型、提示风格和输入的语言特征不敏感。相反，它从提示中的规律中推导出模式，这导致了分布敏感性，尤其是在链式思维等提示风格中。鉴于在形式上相似的任务上准确性差异，我们得出结论，自回归的临时编码不是一种稳健的机制，并且暗示了有限的通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to investigate the claims regarding in-context learning (ICL) in autoregressive models, particularly whether these models genuinely learn to solve unseen tasks through next-token prediction. The authors conducted a large-scale analysis of ICL, examining factors such as memorization, pretraining, distributional shifts, and prompting styles. The findings indicate that while ICL is an effective learning paradigm, it has limitations in learning and generalizing to new tasks, with accuracy being largely insensitive to various factors when exemplars are abundant, suggesting that the mechanism of autoregression may not provide robust generalizability across tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于探讨自回归模型中上下文学习（ICL）的相关主张，特别是其在仅用少量示例的情况下学习未见任务的能力。作者进行了大规模的ICL分析，考察了记忆、预训练、分布转变和提示风格等因素。研究结果表明，虽然ICL是一种有效的学习范式，但在学习和推广新任务方面存在局限性，当示例数量丰富时，准确性在很大程度上不受示例分布和模型变化的影响，这表明自回归机制可能无法提供跨任务的稳健泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">DVD-Quant: Data-free Video Diffusion Transformers Quantization</div>
<div class="meta-line">Authors: Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Haotong Qin, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang</div>
<div class="meta-line">First: 2025-05-24T11:56:02+00:00 · Latest: 2026-01-26T16:04:47+00:00</div>
<div class="meta-line">Comments: Code and models will be available at https://github.com/lhxcs/DVD-Quant</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18663v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18663v3">PDF</a> · <a href="https://github.com/lhxcs/DVD-Quant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have emerged as the state-of-the-art architecture for video generation, yet their computational and memory demands hinder practical deployment. While post-training quantization (PTQ) presents a promising approach to accelerate Video DiT models, existing methods suffer from two critical limitations: (1) dependence on computation-heavy and inflexible calibration procedures, and (2) considerable performance deterioration after quantization. To address these challenges, we propose DVD-Quant, a novel Data-free quantization framework for Video DiTs. Our approach integrates three key innovations: (1) Bounded-init Grid Refinement (BGR) and (2) Auto-scaling Rotated Quantization (ARQ) for calibration data-free quantization error reduction, as well as (3) $δ$-Guided Bit Switching ($δ$-GBS) for adaptive bit-width allocation. Extensive experiments across multiple video generation benchmarks demonstrate that DVD-Quant achieves an approximately 2$\times$ speedup over full-precision baselines on advanced DiT models while maintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ for Video DiTs without compromising video quality. Code and models will be available at https://github.com/lhxcs/DVD-Quant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DVD-Quant: 无数据视频扩散变换器量化</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）已成为视频生成的最先进架构，但其计算和内存需求阻碍了实际部署。尽管后训练量化（PTQ）为加速视频DiT模型提供了有前景的方法，但现有方法存在两个关键限制：（1）依赖于计算密集且不灵活的校准程序，以及（2）量化后性能显著下降。为了解决这些挑战，我们提出了DVD-Quant，一种新颖的无数据量化框架用于视频DiTs。我们的方法整合了三个关键创新：（1）有界初始化网格细化（BGR）和（2）自适应缩放旋转量化（ARQ）用于无校准数据的量化误差减少，以及（3）$δ$-引导位切换（$δ$-GBS）用于自适应位宽分配。在多个视频生成基准上的广泛实验表明，DVD-Quant在先进的DiT模型上实现了约2$\times$的速度提升，同时保持视觉保真度。值得注意的是，DVD-Quant首次实现了W4A4 PTQ用于视频DiTs而不影响视频质量。代码和模型将可在https://github.com/lhxcs/DVD-Quant获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational and memory requirements of Diffusion Transformers (DiTs) for video generation, which limit their practical use. The authors propose a novel data-free quantization framework called DVD-Quant that incorporates innovations such as Bounded-init Grid Refinement, Auto-scaling Rotated Quantization, and δ-Guided Bit Switching to reduce quantization error without the need for calibration data. Experimental results show that DVD-Quant achieves approximately a 2x speedup over full-precision baselines on advanced DiT models while preserving visual fidelity, marking a significant advancement in enabling W4A4 post-training quantization for Video DiTs without degrading video quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决扩散变换器（DiTs）在视频生成中的高计算和内存需求，这限制了其实际应用。作者提出了一种名为DVD-Quant的新型无数据量化框架，该框架结合了边界初始化网格细化、自适应旋转量化和δ引导位宽切换等创新，以在不需要校准数据的情况下减少量化误差。实验结果表明，DVD-Quant在先进的DiT模型上实现了约2倍于全精度基线的加速，同时保持了视觉保真度，标志着首次成功实现了视频DiTs的W4A4后训练量化而不影响视频质量。</div>
</details>
</div>
<div class="card">
<div class="title">Autiverse: Eliciting Autistic Adolescents&#x27; Daily Narratives through AI-guided Multimodal Journaling</div>
<div class="meta-line">Authors: Migyeong Yang, Kyungah Lee, Jinyoung Han, SoHyun Park, Young-Ho Kim</div>
<div class="meta-line">Venue: In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI &#x27;26), April 13-17, 2026, Barcelona, Spain. ACM, New York, NY, USA, 26 pages</div>
<div class="meta-line">First: 2025-09-22T08:02:09+00:00 · Latest: 2026-01-26T15:44:08+00:00</div>
<div class="meta-line">Comments: 19 pages excluding reference. Conditionally accepted to ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17466v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.17466v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Journaling can potentially serve as an effective method for autistic adolescents to improve narrative skills. However, its text-centric nature and high executive functioning demands present barriers to practice. We present Autiverse, an AI-guided multimodal journaling app for tablets that scaffolds daily narratives through conversational prompts and visual supports. Autiverse elicits key details of an adolescent-selected event through a stepwise dialogue with peer-like, customizable AI and composes them into an editable four-panel comic strip. Through a two-week deployment study with 10 autistic adolescent-parent dyads, we examine how Autiverse supports autistic adolescents to organize their daily experience and emotion. Our findings show Autiverse scaffolded adolescents&#x27; coherent narratives, while enabling parents to learn additional details of their child&#x27;s events and emotions. Moreover, the customized AI peer created a comfortable space for sharing, fostering enjoyment and a strong sense of agency. Drawing on these results, we discuss implications for adaptive scaffolding across autism profiles, socio-emotionally appropriate AI peer design, and balancing autonomy with parental involvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Autiverse：通过AI引导的多模态日记引发自闭症青少年的日常叙事</div>
<div class="mono" style="margin-top:8px">日记写作可能成为自闭症青少年提高叙事技能的有效方法。然而，其以文本为中心的特性和高执行功能需求给实践带来了障碍。我们提出了Autiverse，一款为平板电脑设计的AI引导多模态日记应用，通过对话提示和视觉支持来支撑日常叙事。Autiverse通过与类似同龄人的可定制AI进行逐步对话，引发青少年选择事件的关键细节，并将其编排成可编辑的四格漫画。通过与10对自闭症青少年-家长的两周部署研究，我们考察了Autiverse如何支持自闭症青少年组织他们的日常经历和情感。我们的研究结果表明，Autiverse支撑了青少年连贯的叙事，同时使家长能够了解孩子事件和情感的更多细节。此外，定制的AI同伴创造了一个舒适的分享空间，促进了乐趣和强烈的自主感。基于这些结果，我们讨论了在自闭症谱系中适应性支撑的影响、社会情感适宜的AI同伴设计，以及在自主性与家长参与之间的平衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance narrative skills in autistic adolescents, who face challenges with traditional journaling due to its text-centric nature and high executive functioning demands. The study introduces Autiverse, an AI-guided multimodal journaling app that uses conversational prompts and visual supports to help users articulate their daily experiences. In a two-week deployment study involving 10 autistic adolescent-parent pairs, findings indicate that Autiverse effectively facilitated coherent narrative construction for adolescents, while also allowing parents to gain deeper insights into their child&#x27;s emotions and experiences, with the AI peer fostering a supportive environment for sharing and promoting a sense of agency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高自闭症青少年的叙事能力，因为他们在传统的基于文本的日记写作中面临执行功能的挑战。研究介绍了Autiverse，这是一款AI引导的多模态日记应用程序，利用对话提示和视觉辅助，帮助青少年以结构化的方式表达他们的日常经历。来自与10对自闭症青少年-家长二人组的为期两周的部署研究的结果表明，Autiverse有效地支持青少年组织他们的叙事，使父母能够深入了解孩子的情感和经历，并通过可定制的AI同伴营造了一个舒适的分享环境，最终促进了乐趣和自主感。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP&#x27;s Visual Embedding Projector is a Few-shot Cornucopia</div>
<div class="meta-line">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-10-07T17:59:59+00:00 · Latest: 2026-01-26T14:50:34+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05270v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.05270v4">PDF</a> · <a href="https://github.com/astra-vision/ProLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation&#x27;&#x27; and ``validation-free&#x27;&#x27; settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP&#x27;s lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP的视觉嵌入投影器是一个少样本的丰饶之地</div>
<div class="mono" style="margin-top:8px">我们介绍了ProLIP，这是一种简单且与架构无关的方法，用于将对比预训练的视觉-语言模型（如CLIP）适应于少样本分类。ProLIP通过对预训练权重的偏差进行Frobenius范数正则化，微调视觉编码器的投影矩阵。它在11个少样本分类基准测试中，在“少样本验证”和“无验证”设置下均实现了最先进的性能。此外，通过ProLIP的视角重新思考非线性CLIP-Adapter，我们设计了一个正则化线性适配器（RLA），其性能更佳，无需超参数调优，对学习率值的敏感性较低，并在模型权重不可访问的黑箱场景中提供了ProLIP的替代方案。除了少样本分类，ProLIP在跨数据集迁移、领域泛化、基础到新类泛化和测试时适应方面表现出色——在训练速度上比提示调优快一个数量级。代码可在https://github.com/astra-vision/ProLIP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective adaptation of vision-language models like CLIP for few-shot classification tasks. The authors propose ProLIP, a method that fine-tunes the projection matrix of the vision encoder using Frobenius norm regularization to align it with pretrained weights. Experimental results demonstrate that ProLIP achieves state-of-the-art performance across 11 few-shot classification benchmarks and excels in various tasks such as cross-dataset transfer and domain generalization, outperforming existing methods while being significantly faster to train.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高使用对比预训练视觉-语言模型（如CLIP）的少样本分类性能。作者提出了ProLIP，这是一种通过Frobenius范数正则化微调视觉编码器投影矩阵的方法，以使其与预训练权重对齐。实验结果表明，ProLIP在11个少样本分类基准上实现了最先进的性能，并在跨数据集迁移和领域泛化等多种任务中表现优异，超越了现有的方法，如提示调优，同时训练速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning</div>
<div class="meta-line">Authors: Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu, Chen-Wei Xie, Zhaoyu Chen, Yun Zheng, Wenqiang Zhang</div>
<div class="meta-line">First: 2026-01-26T14:49:04+00:00 · Latest: 2026-01-26T14:49:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18543v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18543v1">PDF</a> · <a href="https://github.com/deep-kaixun/GenAgent}{this">Code1</a> · <a href="https://github.com/deep-kaixun/GenAgent">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenAgent：通过代理多模态推理扩展文本到图像生成</div>
<div class="mono" style="margin-top:8px">我们介绍了GenAgent，通过代理多模态模型统一视觉理解和生成。与面临高昂训练成本和理解-生成权衡的统一模型不同，GenAgent通过代理框架解耦这些能力：理解由多模态模型本身处理，而生成则通过将图像生成模型视为可调用工具来实现。至关重要的是，与受限于静态管道的现有模块化系统不同，这种设计使得自主多轮交互成为可能，代理生成包含推理、工具调用、判断和反思的多模态思维链，以迭代地优化输出。我们采用两阶段训练策略：首先，在高质量工具调用和反思数据上进行监督微调以启动代理行为；其次，结合点奖励（最终图像质量）和对偶奖励（反思准确性）的端到端代理强化学习，并进行轨迹重采样以增强多轮探索。GenAgent显著提升了基础生成器（FLUX.1-dev）在GenEval++（+23.6%）和WISE（+14%）上的表现。除了性能提升外，我们的框架还展示了三个关键特性：1）对具有不同能力的生成器的跨工具泛化，2）在交互轮次中一致改进的测试时扩展，3）自动调整到不同任务的任务自适应推理。我们的代码将可在\href{https://github.com/deep-kaixun/GenAgent}{此网址}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to enhance text-to-image generation by integrating visual understanding and generation through an agentic multimodal model. The authors developed GenAgent, which separates understanding and generation processes, allowing for autonomous multi-turn interactions and iterative output refinement. Experimental results show that GenAgent improves the performance of the base generator FLUX.1-dev on GenEval++ by 23.6% and on WISE by 14%, while also demonstrating cross-tool generalization, consistent test-time scaling, and task-adaptive reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过代理多模态模型增强文本到图像生成，解决现有统一模型的局限性。作者提出了GenAgent，该模型将理解和生成过程分开，允许自主的多轮交互和迭代输出优化。实验结果表明，GenAgent在GenEval++上提升了基础生成器FLUX.1-dev的性能23.6%，在WISE上提升了14%，同时展示了跨工具泛化、一致的测试时扩展和任务自适应推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Distillation-Enabled Knowledge Alignment for Generative Semantic Communications of AIGC Images</div>
<div class="meta-line">Authors: Jingzhi Hu, Geoffrey Ye Li</div>
<div class="meta-line">First: 2025-06-24T10:50:14+00:00 · Latest: 2026-01-26T14:18:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19893v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.19893v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to the surging amount of AI-generated images, its provisioning to edges and mobile users from the cloud incurs substantial traffic on networks. Generative semantic communication (GSC) offers a promising solution by transmitting highly compact information, i.e., prompt text and latent representations, instead of high-dimensional image data. However, GSC relies on the alignment between the knowledge in the cloud generative AI (GAI) and that possessed by the edges and users, and between the knowledge for wireless transmission and that of actual channels, which remains challenging. In this paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems. The core idea is to distill the image generation knowledge from the cloud-GAI into low-rank matrices, which can be incorporated by the edge and used to adapt the transmission knowledge to diverse wireless channel conditions. DeKA-g comprises two novel methods: metaword-aided knowledge distillation (MAKD) and condition-aware low-rank adaptation (CALA). For MAKD, an optimized metaword is employed to enhance the efficiency of knowledge distillation, while CALA enables efficient adaptation to diverse rate requirements and channel conditions. From simulation results, DeKA-g improves the consistency between the edge-generated images and the cloud-generated ones by 44% and enahnces the average transmission quality in terms of PSNR by 6.5 dB over the baselines without knowledge alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于蒸馏的知识对齐用于AIGC图像的生成语义通信</div>
<div class="mono" style="margin-top:8px">由于AI生成图像数量激增，从云端向边缘和移动用户提供这些图像会对网络造成大量流量。生成语义通信（GSC）通过传输高度紧凑的信息，即提示文本和潜在表示，而不是高维图像数据，提供了一个有前景的解决方案。然而，GSC依赖于云端生成AI（GAI）中的知识与边缘和用户所拥有的知识之间的对齐，以及无线传输知识与实际信道知识之间的对齐，这仍然具有挑战性。本文提出了DeKA-g，一种用于GSC系统的基于蒸馏的知识对齐算法。其核心思想是将云端GAI中的图像生成知识蒸馏为低秩矩阵，边缘可以将其纳入并用于适应多样的无线信道条件。DeKA-g包含两种新方法：元词辅助知识蒸馏（MAKD）和条件感知低秩适应（CALA）。对于MAKD，采用优化的元词来提高知识蒸馏的效率，而CALA则能够有效适应多样的速率要求和信道条件。模拟结果表明，DeKA-g提高了边缘生成图像与云生成图像之间的一致性，提升了44%，并在PSNR方面比没有知识对齐的基线提高了6.5 dB的平均传输质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing volume of AI-generated images poses significant network traffic challenges when delivering content to edge and mobile users from the cloud. To address this issue, the paper introduces DeKA-g, a distillation-enabled knowledge alignment algorithm designed for generative semantic communication (GSC) systems. The method involves distilling image generation knowledge from cloud generative AI into low-rank matrices, which are then used by edge devices to adapt transmission knowledge to varying wireless conditions. Experimental results demonstrate that DeKA-g enhances the consistency of images generated at the edge compared to those from the cloud by 44% and improves average transmission quality, measured by PSNR, by 6.5 dB compared to baseline methods lacking knowledge alignment.</div>
<div class="mono" style="margin-top:8px">随着AI生成图像数量的激增，迫切需要高效的传输方法以减少网络流量，这激发了对生成语义通信（GSC）作为解决方案的探索。本文提出了DeKA-g，一种基于蒸馏的知识对齐算法，旨在通过将云端生成AI的图像生成知识蒸馏为低秩矩阵供边缘使用，从而促进GSC。实验结果表明，DeKA-g提高了边缘生成图像与云端生成图像之间的一致性，提升幅度达到44%，并且在平均传输质量（以PSNR衡量）上比缺乏知识对齐的基线方法提高了6.5 dB。</div>
</details>
</div>
<div class="card">
<div class="title">DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training &amp; Inference</div>
<div class="meta-line">Authors: Zihan wang, Hao Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yiqun Zhang, Jinghao Lin, Haihua Yang, Xiaozhong Ji</div>
<div class="meta-line">First: 2026-01-26T13:57:48+00:00 · Latest: 2026-01-26T13:57:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18496v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus &quot;find it but fail to use it,&quot; leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DEEPMED：通过多跳医学搜索数据和可控的代理训练与推理构建医学深度研究代理</div>
<div class="mono" style="margin-top:8px">医学推理模型受到参数知识的限制，因此容易遗忘和产生幻觉。深度研究（DR）模型将输出基于可验证的工具证据，并在一般领域表现强劲，但直接转移到医学领域的收益相对有限。我们将其归因于两个差距：任务特征和工具使用扩展。医学问题需要在知识密集的临床背景中进行证据解释；而一般的DR模型可以检索信息，但往往缺乏临床背景推理，因此“找到但未能使用”，使得医学能力限制了性能。此外，在医学场景中，盲目扩展工具调用可能会引入噪声背景，干扰敏感的医学推理，并促使沿错误路径重复寻求证据。因此，我们提出了DeepMed。对于数据，我们部署了一种多跳医学搜索问答合成方法，支持模型在医学背景中应用DR范式。对于训练，我们引入了一种难度感知的回合惩罚，以抑制过度的工具调用增长。对于推理，我们引入了一个监控器，以帮助在受控步骤内验证假设，避免背景腐烂。总体而言，在七个医学基准上，DeepMed平均提高了其基础模型9.79\%，并超越了更大的医学推理和DR模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of medical reasoning models, which often suffer from issues like forgetting and hallucinations due to their reliance on parametric knowledge. The authors propose DeepMed, a model that utilizes a multi-hop med-search QA synthesis method to enhance the application of deep research paradigms in medical contexts. Key experimental findings indicate that DeepMed improves its base model&#x27;s performance by an average of 9.79% across seven medical benchmarks, surpassing larger medical reasoning and deep research models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决医学推理模型的局限性，这些模型由于参数知识的限制，常常面临遗忘和幻觉等问题。作者提出了DeepMed模型，利用多跳医学搜索问答合成方法来增强DeepResearch范式在临床环境中的应用。主要实验结果表明，DeepMed在七个医学基准测试中平均提高了9.79%的性能，超越了更大规模的医学推理和DeepResearch模型。</div>
</details>
</div>
<div class="card">
<div class="title">DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment</div>
<div class="meta-line">Authors: Sara Tehrani, Yonghao Xu, Leif Haglund, Amanda Berg, Michael Felsberg</div>
<div class="meta-line">First: 2026-01-26T13:48:11+00:00 · Latest: 2026-01-26T13:48:11+00:00</div>
<div class="meta-line">Comments: Under review at ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18493v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灾害洞察：面向功能的多模态灾害评估基准</div>
<div class="mono" style="margin-top:8px">及时解读卫星图像对灾害响应至关重要，但现有的遥感视觉语言基准主要关注粗略标签和图像级识别，忽视了真实人道工作流程中所需的功能理解和指令鲁棒性。我们推出了灾害洞察，这是一个旨在评估视觉语言模型（VLM）在现实灾害分析任务上的多模态基准。灾害洞察将xBD数据集重构为约112K个以建筑为中心的实例，并支持跨多个任务的指令多样性评估，包括建筑功能分类、损坏级别和灾害类型分类、计数以及与人道评估指南对齐的结构化报告生成。
为了建立领域适应的基线，我们提出了DI-Chat，通过在灾害特定指令数据上微调现有的VLM骨干网络，采用参数高效的低秩适应（LoRA）。对最先进的通用和遥感VLM进行的广泛实验揭示了任务间的显著性能差距，特别是在损坏理解和结构化报告生成方面。DI-Chat在损坏级别和灾害类型分类以及报告生成质量上取得了显著改善，而建筑功能分类对所有评估模型仍然具有挑战性。灾害洞察为研究灾害图像中的基础多模态推理提供了统一的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for timely and accurate interpretation of satellite imagery for disaster response, as existing benchmarks do not adequately address the functional understanding required in humanitarian workflows. The authors introduce DisasterInsight, a multimodal benchmark that restructures the xBD dataset into around 112K building-centered instances and facilitates diverse evaluation across various disaster analysis tasks. Experimental results show that the proposed DI-Chat model, fine-tuned on disaster-specific instruction data, significantly improves performance in damage-level and disaster-type classification and report generation, although building-function classification remains a challenge for all models tested.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于及时有效地解读卫星图像以应对灾害的需求，因为现有的基准未能充分解决人道主义工作流程中的功能理解和指令稳健性。作者提出了DisasterInsight，这是一个多模态基准，将xBD数据集重构为约112,000个以建筑为中心的实例，并促进在建筑功能分类、损坏级别分类、灾害类型分类、计数和结构化报告生成等多种任务上的多样化评估。实验结果表明，提出的DI-Chat模型通过灾害特定指令数据的微调，在损坏级别和灾害类型分类以及报告生成方面显著提高了性能，尽管所有测试模型在建筑功能分类方面仍面临挑战。</div>
</details>
</div>
<div class="card">
<div class="title">From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance</div>
<div class="meta-line">Authors: Ardalan Aryashad, Parsa Razmara, Amin Mahjoub, Seyedarmin Azizi, Mahdi Salmani, Arad Firouzkouhi</div>
<div class="meta-line">Venue: WACV 2026 Oral</div>
<div class="meta-line">First: 2025-10-04T19:05:04+00:00 · Latest: 2026-01-26T12:26:48+00:00</div>
<div class="meta-line">Comments: Accepted at WACV 2026 Proceedings (Oral), 5th Workshop on Image, Video, and Audio Quality Assessment in Computer Vision, with a focus on VLM and Diffusion Models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03906v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03906v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aradfir.github.io/filters-to-vlms-defogging-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving perception systems are particularly vulnerable in foggy conditions, where light scattering reduces contrast and obscures fine details critical for safe operation. While numerous defogging methods exist, from handcrafted filters to learned restoration models, improvements in image fidelity do not consistently translate into better downstream detection and segmentation. Moreover, prior evaluations often rely on synthetic data, raising concerns about real-world transferability.
  We present a structured empirical study that benchmarks a comprehensive set of defogging pipelines, including classical dehazing filters, modern defogging networks, chained variants combining filters and models, and prompt-driven visual language image editing models applied directly to foggy images. To bridge the gap between simulated and physical environments, we evaluate these pipelines on both the synthetic Foggy Cityscapes dataset and the real-world Adverse Conditions Dataset with Correspondences (ACDC).
  We examine generalization by evaluating performance on synthetic fog and real-world conditions, assessing both image quality and downstream perception in terms of object detection mean average precision and segmentation panoptic quality. Our analysis identifies when defogging is effective, the impact of combining models, and how visual language models compare to traditional approaches. We additionally report qualitative rubric-based evaluations from both human and visual language model judges and analyze their alignment with downstream task metrics.
  Together, these results establish a transparent, task-oriented benchmark for defogging methods and identify the conditions under which pre-processing meaningfully improves autonomous perception in adverse weather.
  Project page: https://aradfir.github.io/filters-to-vlms-defogging-page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从滤波器到视觉语言模型：通过目标检测和分割性能基准测试去雾方法</div>
<div class="mono" style="margin-top:8px">自动驾驶感知系统在雾天条件下特别脆弱，光散射降低了对比度，模糊了安全操作所需的细节。虽然存在许多去雾方法，从手工滤波器到学习恢复模型，但图像保真度的提高并不总是能转化为更好的下游检测和分割。此外，先前的评估通常依赖于合成数据，这引发了对现实世界可转移性的担忧。我们提出了一项结构化的实证研究，基准测试一套全面的去雾管道，包括经典去雾滤波器、现代去雾网络、结合滤波器和模型的链式变体，以及直接应用于雾图像的提示驱动视觉语言图像编辑模型。为了弥合模拟和物理环境之间的差距，我们在合成的Foggy Cityscapes数据集和真实的具有对应关系的不良条件数据集（ACDC）上评估这些管道。我们通过评估合成雾和现实世界条件下的性能来检查泛化，评估图像质量和下游感知在目标检测平均精度和分割全景质量方面的表现。我们的分析确定了去雾何时有效、模型组合的影响，以及视觉语言模型与传统方法的比较。我们还报告了来自人类和视觉语言模型评审的定性评分评估，并分析它们与下游任务指标的一致性。这些结果共同建立了一个透明的、面向任务的去雾方法基准，并确定了预处理在不良天气条件下如何有意义地改善自动感知的条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to address the challenges faced by autonomous driving perception systems in foggy conditions, where traditional defogging methods do not consistently enhance downstream object detection and segmentation performance. The authors conducted a structured empirical study benchmarking various defogging pipelines, including classical filters, modern networks, and visual language models, using both synthetic and real-world datasets to evaluate their effectiveness. The findings reveal insights into the conditions under which defogging improves perception, the benefits of combining different models, and the comparative performance of visual language models against traditional methods, ultimately providing a task-oriented benchmark for future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决自主驾驶感知系统在雾天条件下面临的挑战，传统的去雾方法可能无法有效提升下游物体检测和分割性能。作者进行了结构化的实证研究，基准测试了多种去雾技术，包括经典滤波器、现代网络和视觉语言模型，使用了合成和真实世界的数据集。研究结果表明，尽管某些去雾方法改善了图像质量，但它们对检测和分割的影响各不相同，强调了在现实场景中评估这些技术的重要性，以确定它们在恶劣天气条件下增强自主感知的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码的共享空间记忆</div>
<div class="mono" style="margin-top:8px">构建一致的共享空间记忆是多智能体系统中的一个关键挑战，其中部分可观测性和带宽限制常常导致协调中的灾难性失败。我们引入了一种多智能体预测编码框架，将协调形式化为智能体之间互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体学习不仅是与谁和什么进行沟通，还包括何时沟通。该框架的基础是类似网格细胞的度量，作为自我定位的内部空间编码，自我监督的运动预测自发产生。在此内部空间编码的基础上，智能体逐渐发展出一种带宽高效的通信机制和专门的神经群体，编码伙伴的位置——一种海马体社交位置细胞（SPC）的人工类比。这些社交表征进一步被分层强化学习策略利用，该策略积极探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法在带宽限制下表现出卓越的韧性：成功率从73.5%优雅地降至64.4%，当带宽从128降至4位/步时，而全广播基线则从67.6%崩溃至28.6%。我们的发现为复杂社交表征如何从统一的预测驱动中出现提供了理论上合理和生物上可信的基础，从而导致集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of constructing a consistent shared spatial memory in multi-agent systems, which often face issues due to partial observability and limited bandwidth. The authors propose a multi-agent predictive coding framework that minimizes mutual uncertainty among agents, utilizing an information bottleneck objective to guide communication strategies. Experimental results on the Memory-Maze benchmark demonstrate that their approach maintains a success rate of 73.5% at 128 bits/step, which gracefully degrades to 64.4% at 4 bits/step, in contrast to a full-broadcast baseline that drops significantly from 67.6% to 28.6%, highlighting the framework&#x27;s resilience to bandwidth constraints and its potential for fostering collective intelligence through social representations.</div>
<div class="mono" style="margin-top:8px">本研究解决了在多智能体系统中构建一致共享空间记忆的挑战，部分可观察性和带宽限制可能会妨碍协调。作者提出了一种多智能体预测编码框架，通过最小化智能体之间的互不确定性来指导通信策略，利用信息瓶颈目标。Memory-Maze基准测试的关键实验结果表明，随着带宽从128降至4比特/步，其方法的成功率保持在73.5%到64.4%之间，显著优于全广播基线的67.6%降至28.6%，从而为复杂社会表征和集体智能的出现提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment</div>
<div class="meta-line">Authors: Fu-An Chao, Bi-Cheng Yan, Berlin Chen</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-18T08:10:24+00:00 · Latest: 2026-01-26T08:58:34+00:00</div>
<div class="meta-line">Comments: Accepted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16387v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.16387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the untapped potential of Whisper, a well-established automatic speech recognition (ASR) foundation model, in the context of L2 spoken language assessment (SLA). Unlike prior studies that extrinsically analyze transcriptions produced by Whisper, our approach goes a step further to probe its latent capabilities by extracting acoustic and linguistic features from hidden representations. With only a lightweight classifier being trained on top of Whisper&#x27;s intermediate and final outputs, our method achieves strong performance on the GEPT picture-description dataset, outperforming existing cutting-edge baselines, including a multimodal approach. Furthermore, by incorporating image and text-prompt information as auxiliary relevance cues, we demonstrate additional performance gains. Finally, we conduct an in-depth analysis of Whisper&#x27;s embeddings, which reveals that, even without task-specific fine-tuning, the model intrinsically encodes both ordinal proficiency patterns and semantic aspects of speech, highlighting its potential as a powerful foundation for SLA and other spoken language understanding tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究ASR基础模型在L2英语口语评估中的隐藏潜力</div>
<div class="mono" style="margin-top:8px">本文探讨了Whisper这一成熟的自动语音识别（ASR）基础模型在L2口语语言评估（SLA）中的未开发潜力。与之前对Whisper生成的转录进行外部分析的研究不同，我们的方法进一步探测其潜在能力，通过从隐藏表示中提取声学和语言特征。仅在Whisper的中间和最终输出上训练一个轻量级分类器，我们的方法在GEPT图片描述数据集上取得了强劲的表现，超越了现有的尖端基线，包括多模态方法。此外，通过将图像和文本提示信息作为辅助相关线索，我们展示了额外的性能提升。最后，我们对Whisper的嵌入进行了深入分析，揭示了即使没有特定任务的微调，该模型也内在地编码了顺序熟练度模式和语音的语义方面，突显了其作为SLA和其他口语理解任务强大基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential of the Whisper automatic speech recognition (ASR) model for assessing L2 spoken language, motivated by the need for improved evaluation methods in language learning. The researchers employed a lightweight classifier trained on Whisper&#x27;s intermediate and final outputs, extracting acoustic and linguistic features from its hidden representations. The results demonstrated that this approach achieved superior performance on the GEPT picture-description dataset compared to existing state-of-the-art methods, including a multimodal approach, and further enhancements were observed by integrating image and text-prompt information as auxiliary cues. Additionally, an analysis of Whisper&#x27;s embeddings indicated that the model inherently captures proficiency patterns and semantic elements of speech, underscoring its potential for spoken language assessment and understanding tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Whisper自动语音识别（ASR）模型在L2口语评估中的潜力，动机是需要有效的语言学习评估工具。研究人员采用了一种方法，从Whisper的隐藏表示中提取声学和语言特征，并在其输出上训练轻量级分类器。结果表明，该方法在GEPT图片描述数据集上的表现优于现有的最先进方法，并且通过整合图像和文本提示作为辅助线索，进一步提高了性能，揭示了Whisper能够在没有特定微调的情况下编码熟练度模式和语音的语义元素。</div>
</details>
</div>
<div class="card">
<div class="title">Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing</div>
<div class="meta-line">Authors: Chao Wang, Xuanying Li, Cheng Dai, Jinglei Feng, Yuxiang Luo, Yuqi Ouyang, Hao Qin</div>
<div class="meta-line">First: 2026-01-26T08:16:02+00:00 · Latest: 2026-01-26T08:16:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18252v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18252v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Co-PLNet：一种用于提示引导的线框解析的协作点线网络</div>
<div class="mono" style="margin-top:8px">线框解析旨在恢复线段及其交点，以形成结构化的几何表示，便于下游任务，如同时定位与地图构建（SLAM）。现有方法分别预测线条和交点，并在事后进行调和，导致不匹配和鲁棒性降低。我们提出了Co-PLNet，一种点线协作框架，在两个任务之间交换空间线索，其中早期检测通过点线提示编码器（PLP-Encoder）转换为空间提示，该编码器将几何属性编码为紧凑且空间对齐的地图。然后，交叉引导线解码器（CGL-Decoder）在互补提示的条件下，通过稀疏注意力精炼预测，强制执行点线一致性和效率。在Wireframe和YorkUrban上的实验显示了准确性和鲁棒性的一致提升，以及良好的实时效率，证明了我们在结构化几何感知方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve wireframe parsing, which is essential for creating structured geometric representations for applications like SLAM, by addressing the limitations of existing methods that treat line and junction predictions separately. The authors propose Co-PLNet, a collaborative framework that utilizes a Point-Line Prompt Encoder to convert early detections into spatial prompts, and a Cross-Guidance Line Decoder to refine predictions through sparse attention, ensuring point-line consistency. Experimental results on the Wireframe and YorkUrban datasets show significant enhancements in accuracy and robustness, along with favorable real-time performance, validating the proposed method&#x27;s effectiveness in structured geometry perception.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善线框解析，这对于创建结构化几何表示在诸如同时定位与地图构建（SLAM）等应用中至关重要。作者提出了Co-PLNet，这是一种协作框架，通过点线提示编码器（PLP-Encoder）和交叉引导线解码器（CGL-Decoder）整合线段和交点的预测，促进任务之间空间线索的交换。对Wireframe和YorkUrban数据集的实验结果表明，Co-PLNet在准确性和鲁棒性方面取得了显著改善，同时保持实时效率，突显了其在结构化几何感知中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PaperTok: Exploring the Use of Generative AI for Creating Short-form Videos for Research Communication</div>
<div class="meta-line">Authors: Meziah Ruby Cristobal, Hyeonjeong Byeon, Tze-Yu Chen, Ruoxi Shang, Donghoon Shin, Ruican Zhong, Tony Zhou, Gary Hsieh</div>
<div class="meta-line">Venue: In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI &#x27;26), Apr 13-17, 2026, Barcelona, Spain. ACM, New York, NY, USA</div>
<div class="meta-line">First: 2026-01-26T07:08:57+00:00 · Latest: 2026-01-26T07:08:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18218v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18218v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The dissemination of scholarly research is critical, yet researchers often lack the time and skills to create engaging content for popular media such as short-form videos. To address this gap, we explore the use of generative AI to help researchers transform their academic papers into accessible video content. Informed by a formative study with science communicators and content creators (N=8), we designed PaperTok, an end-to-end system that automates the initial creative labor by generating script options and corresponding audiovisual content from a source paper. Researchers can then refine based on their preferences with further prompting. A mixed-methods user study (N=18) and crowdsourced evaluation (N=100) demonstrate that PaperTok&#x27;s workflow can help researchers create engaging and informative short-form videos. We also identified the need for more fine-grained controls in the creation process. To this end, we offer implications for future generative tools that support science outreach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PaperTok：探索生成性人工智能在研究传播中创建短视频的应用</div>
<div class="mono" style="margin-top:8px">学术研究的传播至关重要，但研究人员往往缺乏时间和技能来为短视频等流行媒体创作引人入胜的内容。为了解决这一问题，我们探索了生成性人工智能的使用，帮助研究人员将学术论文转化为易于理解的视频内容。通过对科学传播者和内容创作者的形成性研究（N=8），我们设计了PaperTok，一个端到端的系统，通过从源论文生成脚本选项和相应的视听内容来自动化初始创作劳动。研究人员可以根据自己的偏好进行进一步的调整。混合方法用户研究（N=18）和众包评估（N=100）表明，PaperTok的工作流程可以帮助研究人员创建引人入胜且信息丰富的短视频。我们还发现了在创作过程中需要更细致控制的需求。为此，我们为未来支持科学传播的生成工具提供了启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the dissemination of scholarly research through engaging short-form videos, addressing the challenges researchers face in creating such content. The authors developed PaperTok, an end-to-end system that utilizes generative AI to automate the creation of scripts and audiovisual content from academic papers, based on insights from a formative study with science communicators. The mixed-methods user study and crowdsourced evaluation revealed that PaperTok effectively aids researchers in producing engaging videos, while also highlighting the necessity for more precise controls in the video creation process for future generative tools.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过引人入胜的短视频增强学术研究的传播，解决研究人员在创建此类内容时面临的挑战。作者开发了PaperTok，这是一个端到端的生成性人工智能系统，能够根据学术论文自动生成视频脚本和视听材料，该系统的设计基于与科学传播者的形成性研究。混合方法用户研究和众包评估表明，PaperTok有效地帮助研究人员制作引人入胜的视频，同时也强调了未来生成工具在视频创作过程中需要更详细控制的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">VIBEVOICE-ASR Technical Report</div>
<div class="meta-line">Authors: Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi, Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei</div>
<div class="meta-line">First: 2026-01-26T06:11:51+00:00 · Latest: 2026-01-26T06:11:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18184v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18184v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIBEVOICE-ASR技术报告</div>
<div class="mono" style="margin-top:8px">本报告介绍了VibeVoice-ASR，这是一个通用的语音理解框架，基于VibeVoice构建，旨在解决尽管短语音识别技术最近取得进展，但在长音频（如会议、播客）中仍然存在的上下文碎片化和多说话者复杂性等持续挑战。与依赖音频分块的传统管道方法不同，VibeVoice-ASR支持对长达60分钟音频的单次处理。它将自动语音识别、说话者分离和时间戳统一为一个端到端生成任务。此外，VibeVoice-ASR支持50多种语言，无需显式语言设置，并且能够原生处理话语内外的代码切换。此外，我们引入了一种基于提示的上下文注入机制，允许用户提供自定义上下文，显著提高了领域特定术语和多音字符消歧的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the VibeVoice-ASR framework is to tackle the ongoing issues of context fragmentation and multi-speaker complexity in long-form audio, which persist despite improvements in short-form speech recognition. The method involves a single-pass processing approach that integrates Automatic Speech Recognition, Speaker Diarization, and Timestamping into one end-to-end task, allowing for the analysis of up to 60 minutes of audio without the need for audio chunking. Key experimental findings indicate that the framework supports over 50 languages, operates without explicit language settings, effectively manages code-switching, and enhances accuracy for domain-specific terminology through a prompt-based context injection mechanism.</div>
<div class="mono" style="margin-top:8px">VibeVoice-ASR的动机在于解决长音频中的上下文碎片化和多说话者复杂性的问题，这些问题在短音频语音识别的进展下依然存在。该方法涉及一个通用的语音理解框架，将自动语音识别、说话者区分和时间戳整合为一个单一的端到端生成任务，能够在不需要音频分块的情况下对长达60分钟的音频进行单次处理。主要实验结果表明，VibeVoice-ASR支持超过50种语言，有效管理代码切换，并通过基于提示的上下文注入机制显著提高了领域特定术语和多音字符消歧的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph</div>
<div class="meta-line">Authors: Ziyu Zheng, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyan Huang, Weigang Lu</div>
<div class="meta-line">First: 2025-10-10T13:48:34+00:00 · Latest: 2026-01-26T05:44:55+00:00</div>
<div class="meta-line">Comments: Accepted by WWW2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09394v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.09394v4">PDF</a> · <a href="https://github.com/zhengziyu77/MSGCOT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ``pre-train, prompt&quot; paradigm, designed to bridge the gap between pre-training tasks and downstream objectives, has been extended from the NLP domain to the graph domain and has achieved remarkable progress. Current mainstream graph prompt-tuning methods modify input or output features using learnable prompt vectors. However, existing approaches are confined to single-granularity (e.g., node-level or subgraph-level) during prompt generation, overlooking the inherently multi-scale structural information in graph data, which limits the diversity of prompt semantics. To address this issue, we pioneer the integration of multi-scale information into graph prompt and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework. Specifically, we design a lightweight, low-rank coarsening network to efficiently capture multi-scale structural features as hierarchical basis vectors for prompt generation. Subsequently, mimicking human cognition from coarse-to-fine granularity, we dynamically integrate multi-scale information at each reasoning step, forming a progressive coarse-to-fine prompt chain. Extensive experiments on eight benchmark datasets demonstrate that MSGCOT outperforms the state-of-the-art single-granularity graph prompt-tuning method, particularly in few-shot scenarios, showcasing superior performance. The code is available at: https://github.com/zhengziyu77/MSGCOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单粒度提示：一种用于图的多尺度思维链提示学习</div>
<div class="mono" style="margin-top:8px">“预训练，提示”范式旨在弥合预训练任务与下游目标之间的差距，已从NLP领域扩展到图领域，并取得了显著进展。目前主流的图提示调优方法通过可学习的提示向量修改输入或输出特征。然而，现有方法在提示生成过程中局限于单粒度（例如，节点级或子图级），忽视了图数据中固有的多尺度结构信息，限制了提示语义的多样性。为了解决这个问题，我们首创将多尺度信息整合到图提示中，并提出了一种多尺度图思维链（MSGCOT）提示框架。具体而言，我们设计了一种轻量级、低秩的粗化网络，以高效捕捉多尺度结构特征作为提示生成的层次基础向量。随后，模仿人类从粗到细的认知过程，我们在每个推理步骤动态整合多尺度信息，形成渐进的粗到细提示链。在八个基准数据集上的大量实验表明，MSGCOT在少样本场景中优于最先进的单粒度图提示调优方法，展示了卓越的性能。代码可在以下链接获取：https://github.com/zhengziyu77/MSGCOT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance graph prompt-tuning methods by addressing their limitation of single-granularity approaches, which fail to utilize the multi-scale structural information inherent in graph data. The authors propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework that incorporates multi-scale information through a lightweight, low-rank coarsening network to generate hierarchical basis vectors for prompts. Experimental results on eight benchmark datasets indicate that MSGCOT significantly outperforms existing single-granularity methods, especially in few-shot learning scenarios, demonstrating its effectiveness in improving prompt diversity and performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于现有的图提示调优方法局限于单一粒度，限制了图数据中提示语义的多样性。为此，作者提出了一种多尺度图思维链（MSGCOT）提示框架，将多尺度结构信息整合到提示生成中。该方法采用轻量级低秩粗化网络来捕捉层次特征，并在每个推理步骤动态结合这些特征，形成渐进的提示链。在八个基准数据集上的实验结果表明，MSGCOT显著优于传统的单粒度方法，尤其在少样本学习场景中，展示了其在提升图提示调优性能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Localizing Knowledge in Diffusion Transformers</div>
<div class="meta-line">Authors: Arman Zarei, Samyadeep Basu, Keivan Rezaei, Zihao Lin, Sayan Nag, Soheil Feizi</div>
<div class="meta-line">First: 2025-05-24T19:02:20+00:00 · Latest: 2026-01-26T04:03:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18832v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18832v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding how knowledge is distributed across the layers of generative models is crucial for improving interpretability, controllability, and adaptation. While prior work has explored knowledge localization in UNet-based architectures, Diffusion Transformer (DiT)-based models remain underexplored in this context. In this paper, we propose a model- and knowledge-agnostic method to localize where specific types of knowledge are encoded within the DiT blocks. We evaluate our method on state-of-the-art DiT-based models, including PixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show that the identified blocks are both interpretable and causally linked to the expression of knowledge in generated outputs. Building on these insights, we apply our localization framework to two key applications: model personalization and knowledge unlearning. In both settings, our localized fine-tuning approach enables efficient and targeted updates, reducing computational cost, improving task-specific performance, and better preserving general model behavior with minimal interference to unrelated or surrounding content. Overall, our findings offer new insights into the internal structure of DiTs and introduce a practical pathway for more interpretable, efficient, and controllable model editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器中的知识本地化</div>
<div class="mono" style="margin-top:8px">理解知识在生成模型各层中的分布对于提高可解释性、可控性和适应性至关重要。尽管之前的研究探讨了基于UNet架构的知识本地化，但基于扩散变换器（DiT）的模型在这一背景下仍然未被充分研究。本文提出了一种与模型和知识无关的方法，以定位特定类型知识在DiT块中的编码位置。我们在包括PixArt-alpha、FLUX和SANA在内的最先进的DiT模型上评估了我们的方法，涵盖六个不同的知识类别。我们展示了识别出的块既可解释又与生成输出中知识的表达有因果联系。基于这些见解，我们将本地化框架应用于两个关键应用：模型个性化和知识遗忘。在这两种情况下，我们的本地化微调方法实现了高效和有针对性的更新，降低了计算成本，提高了任务特定性能，并在对无关或周围内容的干扰最小的情况下更好地保留了模型的整体行为。总体而言，我们的发现为DiT的内部结构提供了新的见解，并引入了一条更可解释、高效和可控的模型编辑的实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the interpretability, controllability, and adaptability of generative models by understanding how knowledge is distributed across their layers, particularly in Diffusion Transformers (DiTs), which have been less studied compared to UNet-based architectures. The authors propose a model- and knowledge-agnostic method to identify where specific types of knowledge are encoded within DiT blocks and evaluate this method on several state-of-the-art DiT models across six knowledge categories. The results demonstrate that the identified blocks are interpretable and causally linked to knowledge expression in generated outputs, leading to effective applications in model personalization and knowledge unlearning, which improve task-specific performance while minimizing computational costs and preserving overall model behavior.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于理解知识在生成模型中的分布，特别是在尚未充分研究的扩散变换器（DiTs）中。作者提出了一种与模型和知识无关的方法，以识别特定类型知识在DiT块中的编码位置，并在多个最先进的DiT模型上对该方法进行了评估，涵盖六个知识类别。研究结果表明，识别出的块是可解释的，并与输出中的知识表达存在因果关系，该定位框架使得模型个性化和知识遗忘的过程更加高效，提升了性能并降低了计算成本，同时保持了模型的整体行为。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</div>
<div class="meta-line">Authors: Shanshan Zhao, Xinjie Zhang, Jintao Guo, Jiakui Hu, Lunhao Duan, Minghao Fu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</div>
<div class="meta-line">First: 2025-05-05T11:18:03+00:00 · Latest: 2026-01-26T03:08:38+00:00</div>
<div class="meta-line">Comments: In this version, we incorporate new papers (after Aug. 2025), datasets, and benchmarks. This work is still in progress; Github project: https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02567v6">Abs</a> · <a href="https://arxiv.org/pdf/2505.02567v6">PDF</a> · <a href="https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o&#x27;s new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一的多模态理解与生成模型：进展、挑战与机遇</div>
<div class="mono" style="margin-top:8px">近年来，多模态理解模型和图像生成模型都取得了显著进展。尽管各自取得了成功，这两个领域却独立发展，导致了不同的架构范式：自回归架构主导了多模态理解，而扩散模型成为图像生成的基石。最近，开发整合这些任务的统一框架的兴趣日益增长。GPT-4o新能力的出现体现了这一趋势，突显了统一的潜力。然而，这两个领域之间的架构差异带来了重大挑战。为了清晰概述当前的统一努力，我们提供了一项全面的调查，旨在指导未来的研究。首先，我们介绍多模态理解和文本到图像生成模型的基础概念和最新进展。接下来，我们回顾现有的统一模型，将其分为三种主要架构范式：基于扩散的、自回归的和融合自回归与扩散机制的混合方法。对于每个类别，我们分析相关工作的结构设计和创新。此外，我们编制了针对统一模型的数据集和基准，提供未来探索的资源。最后，我们讨论了这一新兴领域面临的关键挑战，包括标记策略、跨模态注意力和数据。由于该领域仍处于早期阶段，我们预计将迅速发展，并将定期更新此调查。我们的目标是激励进一步研究，并为社区提供有价值的参考。与此调查相关的参考文献可在GitHub上找到（https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the independent evolution of multimodal understanding and image generation models, which have developed distinct architectural paradigms. The authors conducted a comprehensive survey that categorizes existing unified models into diffusion-based, autoregressive-based, and hybrid approaches, analyzing their structural designs and innovations. Key findings indicate significant challenges in unifying these domains, particularly regarding tokenization strategies, cross-modal attention, and data, while also highlighting the potential for rapid advancements in this emerging field.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态理解和图像生成模型的独立发展，这两者形成了不同的架构范式。作者进行了一项全面的调查，将现有的统一模型分为基于扩散的、基于自回归的和混合方法，分析它们的结构设计和创新。主要发现强调了统一过程中的挑战，如标记化策略和跨模态注意力，同时提供了数据集和基准等资源，以促进这一新兴领域的未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Crash Test Dummies&quot; for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners</div>
<div class="meta-line">Authors: Brian Gin, Ahreum Lim, Flávia Silva e Oliveira, Kuan Xing, Xiaomei Song, Gayana Amiyangoda, Thilanka Seneviratne, Alison F. Doubleday, Ananya Gangopadhyaya, Bob Kiser, Lukas Shum-Tim, Dhruva Patel, Kosala Marambe, Lauren Maggio, Ara Tekian, Yoon Soo Park</div>
<div class="meta-line">First: 2026-01-26T02:47:28+00:00 · Latest: 2026-01-26T02:47:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18085v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI &quot;simulated learners&quot; to stress-test and psychometrically characterize assessment pipelines before human use.
  Objective: Develop an open-source AI virtual patient platform and measurement model for robust competency evaluation across cases and rating conditions.
  Methods: We built a platform with virtual patients, virtual learners with tunable ACGME-aligned competency profiles, and multiple independent AI raters scoring encounters with structured Key-Features items. Transcripts were analyzed with a Bayesian HRM-SDT model that treats ratings as decisions under uncertainty and separates learner ability, case performance, and rater behavior; parameters were estimated with MCMC.
  Results: The model recovered simulated learners&#x27; competencies, with significant correlations to the generating competencies across all ACGME domains despite a non-deterministic pipeline. It estimated case difficulty by competency and showed stable rater detection (sensitivity) and criteria (severity/leniency thresholds) across AI raters using identical models/prompts but different seeds. We also propose a staged &quot;safety blueprint&quot; for deploying AI tools with learners, tied to entrustment-based validation milestones.
  Conclusions: Combining a purpose-built virtual patient platform with a principled psychometric model enables robust, interpretable, generalizable competency estimates and supports validation of AI-assisted assessment prior to use with human learners.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于人工智能临床评估的“碰撞测试假人”：验证虚拟患者场景与虚拟学习者</div>
<div class="mono" style="margin-top:8px">背景：在医学和健康专业教育（HPE）中，人工智能越来越多地用于评估临床能力，包括通过虚拟标准化患者。然而，大多数评估依赖于人工智能与人类评分者之间的评分一致性，缺乏一个测量框架来说明案例、学习者和评分者如何共同影响评分。这使得评估的稳健性不确定，并可能使学习者受到未验证系统的误导。我们通过使用人工智能“模拟学习者”来进行压力测试，并在人工使用之前对评估流程进行心理测量特征分析。
  目标：开发一个开源的人工智能虚拟患者平台和测量模型，以实现跨案例和评分条件的稳健能力评估。
  方法：我们构建了一个平台，包含虚拟患者、具有可调节的与ACGME对齐的能力特征的虚拟学习者，以及多个独立的人工智能评分者对带有结构化关键特征项目的接触进行评分。使用贝叶斯HRM-SDT模型分析成绩，将评分视为不确定性下的决策，并分离学习者能力、案例表现和评分者行为；参数通过MCMC估计。
  结果：该模型恢复了模拟学习者的能力，与所有ACGME领域的生成能力之间存在显著相关性，尽管流程是非确定性的。它通过能力估计案例难度，并显示出在使用相同模型/提示但不同种子的人工智能评分者之间的稳定评分者检测（敏感性）和标准（严重性/宽松阈值）。我们还提出了一个分阶段的“安全蓝图”，用于在学习者中部署人工智能工具，关联到基于信任的验证里程碑。
  结论：将专门构建的虚拟患者平台与原则性心理测量模型相结合，可以实现稳健、可解释、可推广的能力估计，并支持在与人类学习者使用之前对人工智能辅助评估的验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the reliability of AI-enabled clinical assessments in medical education, addressing concerns about the validity of current evaluation methods that depend on AI-human interrater reliability. The authors developed an open-source platform featuring virtual patients and simulated learners with adjustable competency profiles, utilizing a Bayesian HRM-SDT model to analyze ratings as decisions under uncertainty. Key findings indicate that the model effectively recovered the competencies of simulated learners with significant correlations to the original competencies across ACGME domains, estimated case difficulty, and demonstrated stable detection of rater behavior, thereby providing a framework for the safe deployment of AI tools in educational settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高医学教育中AI辅助临床评估的可靠性，解决当前评估方法依赖AI与人类评分者之间一致性的问题。作者开发了一个开源平台，包含虚拟患者和模拟学习者，利用贝叶斯HRM-SDT模型分析评估数据，区分学习者能力、案例表现和评分者行为。主要发现表明，该模型有效恢复了模拟学习者的能力，并与原始能力显著相关，准确估计了案例难度，并在不同AI评分者之间展示了评分者行为的稳定检测，最终提出了在教育环境中部署AI工具的安全蓝图。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring LGBTQ+ Bias in Generative AI Answers across Different Country and Religious Contexts</div>
<div class="meta-line">Authors: Lilla Vicsek, Anna Vancsó, Mike Zajko, Judit Takacs</div>
<div class="meta-line">First: 2024-07-03T19:38:19+00:00 · Latest: 2026-01-25T23:40:48+00:00</div>
<div class="meta-line">Comments: Replacement version -- includes link to BD&amp;S journal publication (significantly revised) in abstract, but the manuscript here remains unchanged from the original arXiv version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.03473v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.03473v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Previous discussions have highlighted the need for generative AI tools to become more culturally sensitive, yet often neglect the complexities of handling content about minorities, who are perceived differently across cultures and religions. Our study examined how two generative AI systems respond to homophobic statements with varying cultural and religious context information. Findings showed ChatGPT 3.5&#x27;s replies exhibited cultural relativism, in contrast to Bard&#x27;s, which stressed human rights and provided more support for LGBTQ+ issues. Both demonstrated significant change in responses based on contextual information provided in the prompts, suggesting that AI systems may adjust in their responses the degree and forms of support for LGBTQ+ people according to information they receive about the user&#x27;s background. The study contributes to understanding the social and ethical implications of AI responses and argues that any work to make generative AI outputs more culturally diverse requires a grounding in fundamental human rights. A revised edition of this preprint is available open access at Big Data &amp; Society at https://doi.org/10.1177/20539517251396069</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索不同国家和宗教背景下生成性人工智能回答中的LGBTQ+偏见</div>
<div class="mono" style="margin-top:8px">以往的讨论强调了生成性人工智能工具需要变得更加文化敏感，但常常忽视了处理关于少数群体内容的复杂性，这些群体在不同文化和宗教中被视为不同。我们的研究考察了两个生成性人工智能系统如何回应具有不同文化和宗教背景信息的恐同言论。研究结果显示，ChatGPT 3.5的回复表现出文化相对主义，而Bard则强调人权，并对LGBTQ+问题提供了更多支持。两者的回应在很大程度上受到提示中提供的背景信息的影响，表明人工智能系统可能会根据用户背景信息调整对LGBTQ+人群的支持程度和形式。该研究有助于理解人工智能回应的社会和伦理影响，并认为任何使生成性人工智能输出更加文化多样化的工作都需要以基本人权为基础。该预印本的修订版可在Big Data &amp; Society上开放获取，链接为https://doi.org/10.1177/20539517251396069。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Previous discussions have highlighted the need for generative AI tools to become more culturally sensitive, yet often neglect the complexities of handling content about minorities, who are perceived differently across cultures and religions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成性人工智能系统在不同文化和宗教背景下对仇恨言论的反应，旨在提高人工智能工具对少数群体的文化敏感性。研究人员比较了ChatGPT 3.5和Bard的反应，发现ChatGPT表现出文化相对主义，而Bard则强调人权并对LGBTQ+问题提供更多支持。两种人工智能系统的反应在很大程度上受到提供的背景信息的影响，表明它们对LGBTQ+个体的支持程度可能会受到用户背景信息的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation</div>
<div class="meta-line">Authors: Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-02-02T04:30:51+00:00 · Latest: 2026-01-25T22:18:42+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00662v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.00662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缓解模态差距：基于多模态原型和图像偏差估计的少样本分布外检测</div>
<div class="mono" style="margin-top:8px">现有的基于视觉语言模型（VLM）的分布外（OOD）检测方法通常依赖于输入图像与分布内（ID）文本原型之间的相似性评分。然而，图像与文本之间的模态差距常常导致高假阳性率，因为OOD样本可能与ID文本原型表现出高相似性。为了减轻这种模态差距的影响，我们提出将ID图像原型与ID文本原型结合。我们提供理论分析和实证证据表明，这种方法在不需要额外训练的情况下增强了基于VLM的OOD检测性能。为了进一步缩小图像与文本之间的差距，我们引入了一种新颖的少样本调优框架SUPREME，包括偏置提示生成（BPG）和图像-文本一致性（ITC）模块。BPG通过基于高斯的估计图像域偏差对ID文本原型进行条件化，增强了图像-文本融合并改善了泛化能力；ITC通过最小化模态内和模态间距离来减少模态差距。此外，基于我们的理论和实证发现，我们引入了一种新颖的OOD评分$S_{\textit{GMP}}$，利用单模态和跨模态相似性。最后，我们进行了广泛的实验，证明SUPREME始终优于现有的基于VLM的OOD检测方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high false positive rates in out-of-distribution (OOD) detection caused by the modality gap between images and text in vision-language models (VLMs). The authors propose a method that incorporates both in-distribution (ID) image and text prototypes, along with a few-shot tuning framework called SUPREME, which includes biased prompts generation and image-text consistency modules. Experimental results show that this approach significantly enhances the performance of VLM-based OOD detection without requiring additional training, outperforming existing methods in the field.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决视觉语言模型（VLM）中，由于图像和文本之间的模态差距导致的高假阳性率问题。作者提出了一种方法，结合了分布内（ID）图像和文本原型，以在不增加额外训练的情况下提高检测性能。实验结果表明，新的少样本调优框架SUPREME，包括偏置提示生成和图像-文本一致性模块，显著增强了OOD检测，通过减少模态差距并持续超越现有的VLM方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Specialist to Generalist: Unlocking SAM&#x27;s Learning Potential on Unlabeled Medical Images</div>
<div class="meta-line">Authors: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-25T18:13:48+00:00 · Latest: 2026-01-25T18:13:48+00:00</div>
<div class="meta-line">Comments: Accepted to ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17934v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17934v1">PDF</a> · <a href="https://github.com/vnlvi2k3/SC-SAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM&#x27;s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从专家到通才：释放SAM在无标签医学图像上的学习潜力</div>
<div class="mono" style="margin-top:8px">基础模型如Segment Anything Model (SAM)展现出强大的泛化能力，但由于领域转移、标签稀缺以及参数高效微调（PEFT）无法利用无标签数据，适应医学图像仍然困难。虽然传统模型如U-Net在半监督医学学习中表现出色，但它们在辅助PEFT SAM方面的潜力被大大忽视。我们提出了SC-SAM，一个专家-通才框架，其中U-Net提供基于点的提示和伪标签来指导SAM的适应，而SAM则作为强大的通才监督者来规范U-Net。这种相互指导形成了一个双向共同训练循环，使两个模型能够有效利用无标签数据。在前列腺MRI和息肉分割基准测试中，我们的方法取得了最先进的结果，超越了其他现有的半监督SAM变体，甚至医学基础模型如MedSAM，突显了专家-通才合作在标签高效医学图像分割中的价值。我们的代码可在https://github.com/vnlvi2k3/SC-SAM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting foundation models like the Segment Anything Model (SAM) to medical images, which is hindered by domain shifts and the scarcity of labeled data. The authors propose a specialist-generalist framework called SC-SAM, where a U-Net model provides point-based prompts and pseudo-labels to assist SAM&#x27;s adaptation, while SAM acts as a generalist supervisor to regularize U-Net. Experimental results demonstrate that SC-SAM achieves state-of-the-art performance on prostate MRI and polyp segmentation tasks, surpassing existing semi-supervised SAM variants and other medical foundation models, thus emphasizing the effectiveness of specialist-generalist collaboration in label-efficient medical image segmentation.</div>
<div class="mono" style="margin-top:8px">本研究解决了将基础模型如Segment Anything Model（SAM）适应于医学图像的挑战，这一过程受到领域转移和标注数据稀缺的限制。作者提出了一种名为SC-SAM的专家-通用框架，其中U-Net模型提供基于点的提示和伪标签以协助SAM的适应，而SAM则作为通用监督者来规范U-Net。实验结果表明，SC-SAM在前列腺MRI和息肉分割任务上实现了最先进的性能，超越了现有的半监督SAM变体和医学基础模型，从而展示了结合专家和通用方法在标签高效医学图像分割中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RemEdit: Efficient Diffusion Editing with Riemannian Geometry</div>
<div class="meta-line">Authors: Eashan Adhikarla, Brian D. Davison</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-25T17:58:57+00:00 · Latest: 2026-01-25T17:58:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17927v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17927v1">PDF</a> · <a href="https://www.github.com/eashanadhikarla/RemEdit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold&#x27;s structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RemEdit：基于黎曼几何的高效扩散编辑</div>
<div class="mono" style="margin-top:8px">可控图像生成是现代生成性人工智能成功的基础，但它面临语义保真度与推理速度之间的关键权衡。RemEdit基于扩散的框架通过两项协同创新解决了这一权衡。首先，为了提高编辑保真度，我们将潜在空间视为黎曼流形。一个基于眼镜蛇的模块有效地学习流形的结构，使得能够直接和准确地计算测地线，以实现平滑的语义编辑。这种控制通过双SLERP混合技术和来自视觉-语言模型的目标感知提示增强过程进一步优化。其次，为了额外加速，我们引入了一种新颖的任务特定注意力剪枝机制。一个轻量级剪枝头学习保留对编辑至关重要的标记，从而实现有效优化，而不会出现内容无关方法中常见的语义退化。RemEdit超越了之前的最先进编辑框架，同时在50%剪枝下保持实时性能。因此，RemEdit为实用且强大的图像编辑建立了新的基准。源代码：https://www.github.com/eashanadhikarla/RemEdit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve controllable image generation by addressing the trade-off between semantic fidelity and inference speed in generative AI. The authors propose the RemEdit framework, which utilizes Riemannian geometry to navigate the latent space for accurate geodesic path computation, combined with a dual-SLERP blending technique and prompt enrichment from a Vision-Language Model to enhance editing fidelity. Additionally, a novel task-specific attention pruning mechanism is introduced to accelerate the process while preserving essential tokens, resulting in effective optimization without semantic degradation. The experimental results demonstrate that RemEdit outperforms existing state-of-the-art editing frameworks while achieving real-time performance with over 50% pruning.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决生成AI中语义保真度与推理速度之间的权衡来增强可控图像生成。作者提出了RemEdit框架，该框架利用黎曼几何在潜在空间中进行精确编辑，并结合了双SLERP混合技术以及来自视觉-语言模型的目标感知提示增强。关键实验结果表明，RemEdit在实现超过50%剪枝的情况下，超越了现有的最先进编辑框架，同时实现实时性能，从而为高效图像编辑设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Photography Perspective Composition: Towards Aesthetic Perspective Recommendation</div>
<div class="meta-line">Authors: Lujian Yao, Siming Zheng, Xinbin Yuan, Zhuoxuan Cai, Pu Wu, Jinwei Chen, Bo Li, Peng-Tao Jiang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-27T03:04:48+00:00 · Latest: 2026-01-25T15:54:06+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20655v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.20655v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional photography composition approaches are dominated by 2D cropping-based methods. However, these methods fall short when scenes contain poorly arranged subjects. Professional photographers often employ perspective adjustment as a form of 3D recomposition, modifying the projected 2D relationships between subjects while maintaining their actual spatial positions to achieve better compositional balance. Inspired by this artistic practice, we propose photography perspective composition (PPC), extending beyond traditional cropping-based methods. However, implementing the PPC faces significant challenges: the scarcity of perspective transformation datasets and undefined assessment criteria for perspective quality. To address these challenges, we present three key contributions: (1) An automated framework for building PPC datasets through expert photographs. (2) A video generation approach that demonstrates the transformation process from less favorable to aesthetically enhanced perspectives. (3) A perspective quality assessment (PQA) model constructed based on human performance. Our approach is concise and requires no additional prompt instructions or camera trajectories, helping and guiding ordinary users to enhance their composition skills.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>摄影透视构图：朝向美学透视推荐</div>
<div class="mono" style="margin-top:8px">传统摄影构图方法主要依赖于基于2D裁剪的方法。然而，当场景中包含排列不当的主体时，这些方法显得不足。专业摄影师常常采用透视调整作为一种3D重构形式，修改主体之间投影的2D关系，同时保持其实际空间位置，以实现更好的构图平衡。受到这种艺术实践的启发，我们提出了摄影透视构图（PPC），超越传统的基于裁剪的方法。然而，实施PPC面临重大挑战：透视变换数据集的稀缺和透视质量评估标准的不明确。为了解决这些挑战，我们提出了三个关键贡献：（1）通过专家照片构建PPC数据集的自动化框架。（2）展示从不理想到美学增强透视的转变过程的视频生成方法。（3）基于人类表现构建的透视质量评估（PQA）模型。我们的方法简洁，无需额外的提示指令或相机轨迹，帮助和指导普通用户提升其构图技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve traditional 2D cropping-based photography composition methods, which often struggle with poorly arranged subjects in scenes. The authors propose a novel approach called photography perspective composition (PPC), which allows for 3D recomposition by adjusting perspective while keeping the spatial positions of subjects intact. Key experimental findings include the development of an automated framework for creating PPC datasets from expert photographs, a video generation technique that illustrates the transformation to aesthetically improved perspectives, and a perspective quality assessment model based on human performance, all aimed at enhancing the compositional skills of ordinary users without requiring additional instructions or camera trajectories.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于传统的二维裁剪方法在处理排列不佳的主题时的局限性。作者提出了一种新的方法，称为摄影透视构图（PPC），通过透视调整来改善构图平衡，同时保持主题的空间关系。为了实现PPC，研究引入了一种自动化框架，通过专家照片创建PPC数据集，一种视频生成方法来展示向更好透视的转变，以及基于人类表现的透视质量评估模型，证明了该方法在提升普通用户构图技能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</div>
<div class="meta-line">Authors: Yoonwoo Jeong, Cheng Sun, Yu-Chiang Frank Wang, Minsu Cho, Jaesung Choe</div>
<div class="meta-line">First: 2026-01-25T15:00:37+00:00 · Latest: 2026-01-25T15:00:37+00:00</div>
<div class="meta-line">Comments: Project page, https://jaesung-choe.github.io/mv_sam/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17866v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17866v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jaesung-choe.github.io/mv_sam/index.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MV-SAM：使用点图引导的多视角可提示分割</div>
<div class="mono" style="margin-top:8px">可提示分割已成为计算机视觉中的一种强大范式，使用户能够通过点击、框或文本提示等方式引导模型解析复杂场景。最近的进展，以Segment Anything Model（SAM）为例，已将这一范式扩展到视频和多视角图像。然而，缺乏3D意识常常导致结果不一致，迫使每个场景进行昂贵的优化以强制执行3D一致性。在本研究中，我们介绍了MV-SAM，一个实现3D一致性的多视角分割框架，使用点图——由最近的视觉几何模型从未摆姿态图像重建的3D点。利用点图的像素-点一对一对应关系，MV-SAM将图像和提示提升到3D空间，消除了对显式3D网络或注释3D数据的需求。具体而言，MV-SAM通过将图像嵌入从其预训练编码器提升到3D点嵌入，扩展了SAM，这些嵌入通过使用与3D提示嵌入的交叉注意力的变换器进行解码。该设计将2D交互与3D几何对齐，使模型能够通过3D位置嵌入隐式学习跨视角的一致掩码。在SA-1B数据集上训练后，我们的方法在各个领域表现良好，超越了SAM2-Video，并在NVOS、SPIn-NeRF、ScanNet++、uCo3D和DL3DV基准测试中与每个场景优化基线达到了可比的性能。代码将会发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the consistency of multi-view segmentation in computer vision, addressing the limitations of existing models that lack 3D awareness and often require costly optimizations for each scene. The authors propose MV-SAM, a framework that utilizes pointmaps—3D points reconstructed from unposed images—to achieve 3D consistency without the need for explicit 3D networks or annotated data. Experimental results demonstrate that MV-SAM, trained on the SA-1B dataset, outperforms the SAM2-Video model and achieves performance comparable to per-scene optimization baselines across various benchmarks, including NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善计算机视觉中多视角分割的一致性，解决现有方法缺乏三维意识并且通常需要昂贵优化的问题。作者提出了MV-SAM框架，利用点图（从未定位图像重建的三维点）实现三维一致性，而无需显式的三维网络或注释数据。实验结果表明，MV-SAM在SA-1B数据集上训练后，优于SAM2-Video模型，并在多个基准（包括NVOS、SPIn-NeRF、ScanNet++、uCo3D和DL3DV）上达到了与逐场优化基线相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">RAICL: Retrieval-Augmented In-Context Learning for Vision-Language-Model Based EEG Seizure Detection</div>
<div class="meta-line">Authors: Siyang Li, Zhuoya Wang, Xiyan Gui, Xiaoqing Chen, Ziwei Wang, Yaozhi Wen, Dongrui Wu</div>
<div class="meta-line">First: 2026-01-25T13:58:31+00:00 · Latest: 2026-01-25T13:58:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17844v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalogram (EEG) decoding is a critical component of medical diagnostics, rehabilitation engineering, and brain-computer interfaces. However, contemporary decoding methodologies remain heavily dependent on task-specific datasets to train specialized neural network architectures. Consequently, limited data availability impedes the development of generalizable large brain decoding models. In this work, we propose a paradigm shift from conventional signal-based decoding by leveraging large-scale vision-language models (VLMs) to analyze EEG waveform plots. By converting multivariate EEG signals into stacked waveform images and integrating neuroscience domain expertise into textual prompts, we demonstrate that foundational VLMs can effectively differentiate between different patterns in the human brain. To address the inherent non-stationarity of EEG signals, we introduce a Retrieval-Augmented In-Context Learning (RAICL) approach, which dynamically selects the most representative and relevant few-shot examples to condition the autoregressive outputs of the VLM. Experiments on EEG-based seizure detection indicate that state-of-the-art VLMs under RAICL achieved better or comparable performance with traditional time series based approaches. These findings suggest a new direction in physiological signal processing that effectively bridges the modalities of vision, language, and neural activities. Furthermore, the utilization of off-the-shelf VLMs, without the need for retraining or downstream architecture construction, offers a readily deployable solution for clinical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAICL：基于视觉语言模型的增强检索上下文学习用于脑电图癫痫发作检测</div>
<div class="mono" style="margin-top:8px">脑电图（EEG）解码是医学诊断、康复工程和脑机接口的关键组成部分。然而，当前的解码方法仍然严重依赖于特定任务的数据集来训练专门的神经网络架构。因此，数据的有限可用性阻碍了可推广的大脑解码模型的发展。在本研究中，我们提出了一种从传统信号解码向利用大规模视觉语言模型（VLM）分析EEG波形图的范式转变。通过将多变量EEG信号转换为堆叠波形图像，并将神经科学领域的专业知识整合到文本提示中，我们证明了基础VLM能够有效区分人脑中的不同模式。为了解决EEG信号固有的非平稳性，我们引入了一种增强检索上下文学习（RAICL）方法，该方法动态选择最具代表性和相关性的少量示例，以调节VLM的自回归输出。基于EEG的癫痫发作检测实验表明，RAICL下的最先进VLM在性能上优于或可与传统时间序列方法相媲美。这些发现表明了一种新的生理信号处理方向，有效地桥接了视觉、语言和神经活动的模态。此外，利用现成的VLM，无需重新训练或下游架构构建，为临床应用提供了一种可立即部署的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve EEG decoding methods, which are crucial for medical diagnostics and brain-computer interfaces, by addressing the limitations of task-specific datasets. The authors propose a novel approach called Retrieval-Augmented In-Context Learning (RAICL), which utilizes large-scale vision-language models (VLMs) to analyze EEG waveform plots by converting multivariate EEG signals into images and incorporating neuroscience expertise into textual prompts. Experimental results demonstrate that the RAICL method allows state-of-the-art VLMs to achieve performance that is better or comparable to traditional time series approaches in EEG-based seizure detection, indicating a promising new direction for integrating vision, language, and neural activity analysis in physiological signal processing.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善用于医学诊断和脑机接口的EEG解码，目前这一领域受到对特定任务数据集依赖的限制。作者提出了一种新方法，称为检索增强上下文学习（RAICL），利用大规模视觉-语言模型（VLM）分析EEG波形图，通过将多变量EEG信号转换为图像并将神经科学专业知识融入文本提示中。实验结果表明，在RAICL框架下，VLM在基于EEG的癫痫检测中取得了优于或可与传统时间序列方法相媲美的性能，表明在生理信号处理领域整合视觉、语言和神经活动分析的前景广阔。</div>
</details>
</div>
<div class="card">
<div class="title">VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</div>
<div class="meta-line">Authors: Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen, Xiangjie Kong, Yong Liu, Guang Dai, Jingdong Wang</div>
<div class="meta-line">First: 2026-01-25T13:22:38+00:00 · Latest: 2026-01-25T13:22:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17830v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAE-REPA：用于高效扩散训练的变分自编码器表示对齐</div>
<div class="mono" style="margin-top:8px">基于去噪的扩散变换器尽管具有强大的生成性能，但在训练收敛方面效率低下。现有方法如REPA（依赖外部表示编码器）或SRA（需要双模型设置）在训练过程中不可避免地会因外部依赖而产生大量计算开销。为了解决这些挑战，本文提出了\textbf{\namex}，一种轻量级内在引导框架，用于高效的扩散训练。\name利用现成的预训练变分自编码器（VAE）特征：其重构特性确保了对视觉先验的内在编码，如丰富的纹理细节、结构模式和基本语义信息。具体而言，\name通过轻量级投影层将扩散变换器的中间潜在特征与VAE特征对齐，并通过特征对齐损失进行监督。该设计加速了训练，无需额外的表示编码器或双模型维护，从而形成一个简单而有效的流程。大量实验表明，\name在生成质量和训练收敛速度上均优于普通扩散变换器，匹配或超越了最先进的加速方法，并且仅增加了4\%的GFLOPs，外部引导模型没有额外成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the training convergence efficiency of denoising-based diffusion transformers, which typically face challenges due to heavy computational overhead from existing methods like REPA and SRA. The authors propose VAE-REPA, a lightweight intrinsic guidance framework that utilizes pre-trained Variational Autoencoder (VAE) features to align the intermediate latent features of diffusion transformers with VAE features through a projection layer, guided by a feature alignment loss. Experimental results indicate that VAE-REPA enhances both the quality of generated outputs and the speed of training convergence, while only adding 4% extra GFLOPs and avoiding the need for external representation encoders or dual-model setups.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决基于去噪的扩散变换器在训练收敛方面的低效问题，这些方法通常依赖于外部表示编码器或双模型设置，导致计算开销增加。作者提出了VAE-REPA，这是一种轻量级的内在指导框架，利用预训练的变分自编码器（VAE）特征，通过投影层对扩散变换器的中间潜在特征进行对齐，并通过特征对齐损失进行指导。实验结果表明，VAE-REPA提高了生成输出的质量和训练收敛的速度，同时仅增加了4%的GFLOPs，并消除了对外部指导模型的需求，从而提供了一种比现有方法更高效的训练管道。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0326.html">20260127_0326</a>
<a href="archive/20260126_0317.html">20260126_0317</a>
<a href="archive/20260125_0317.html">20260125_0317</a>
<a href="archive/20260124_0326.html">20260124_0326</a>
<a href="archive/20260123_0327.html">20260123_0327</a>
<a href="archive/20260122_0328.html">20260122_0328</a>
<a href="archive/20260121_0414.html">20260121_0414</a>
<a href="archive/20260120_0321.html">20260120_0321</a>
<a href="archive/20260119_0316.html">20260119_0316</a>
<a href="archive/20260118_0316.html">20260118_0316</a>
<a href="archive/20260117_0322.html">20260117_0322</a>
<a href="archive/20260116_0326.html">20260116_0326</a>
<a href="archive/20260115_0321.html">20260115_0321</a>
<a href="archive/20260114_0322.html">20260114_0322</a>
<a href="archive/20260113_0322.html">20260113_0322</a>
<a href="archive/20260112_0315.html">20260112_0315</a>
<a href="archive/20260111_0316.html">20260111_0316</a>
<a href="archive/20260110_0321.html">20260110_0321</a>
<a href="archive/20260109_0321.html">20260109_0321</a>
<a href="archive/20260108_0323.html">20260108_0323</a>
<a href="archive/20260107_0319.html">20260107_0319</a>
<a href="archive/20260106_0322.html">20260106_0322</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
